course_name,chapter_name,chapter_url,content
12 Factor App,Disposability,https://notes.kodekloud.com/docs/12-Factor-App/Twelve-Factor-App-methodology/Disposability,"12 Factor App Twelve Factor App methodology Disposability In this lesson, we explore the essential principle of disposability as outlined in the 12 Factor App methodology. Disposability means that application processes should be designed to be disposable—they can be started or stopped at a moment's notice. This ability is critical for dynamic scaling, where additional instances can be quickly provisioned as application load increases, and unnecessary processes can be terminated just as rapidly when the load decreases. Key Insight Disposability is not just about fast startups; it also emphasizes graceful shutdowns to handle ongoing work seamlessly. A core aspect of disposability is the process of graceful shutdown. When a process receives a termination signal (SIGTERM) from the process manager, it must not immediately exit but rather conclude its active operations to prevent abrupt disruption. This approach ensures users experience minimal interruption, even when scaling down resources. Graceful Shutdown in Docker Containers A practical example of disposability can be observed in Docker’s handling of container shutdowns. When executing the docker stop command, Docker sends a SIGTERM signal to the container to initiate a graceful shutdown. If the container does not respond within a certain grace period, Docker then sends a SIGKILL signal to forcefully terminate the process. This two-step mechanism ensures that the container has ample time to complete processing any active requests before termination. $ docker stop <container_id> The graceful shutdown process is especially crucial for applications handling multiple requests concurrently. By allowing the application to finish processing existing requests while stopping further intake, the risk of data loss or resource leaks is minimized. Practical Application For instance, consider a Flask application. It should be engineered to intercept the SIGTERM signal and begin a controlled shutdown process. This design ensures that any ongoing tasks are completed before the application terminates, thereby maintaining service continuity and protecting data integrity. Implementation Tip Ensure your application handles SIGTERM signals properly to prevent abrupt terminations that might lead to data loss or resource leaks. Design your termination routines to complete current transactions before shutting down completely. Watch Video Watch video content"
12 Factor App,Codebase,https://notes.kodekloud.com/docs/12-Factor-App/Twelve-Factor-App-methodology/Codebase,"12 Factor App Twelve Factor App methodology Codebase In this article, we will explore the 12-Factor App methodology while demonstrating a simple example built with Python and the Flask web framework. These principles are designed to guide the development of scalable, resilient, and maintainable applications. The 12 factors are as follows: Have one codebase. Explicitly declare and isolate dependencies. Store configuration in the environment. Treat backing services as attached resources. Strictly separate build and run stages. Execute the app as one or more stateless processes. Export services via port binding. Scale out via the process model. Maximize robustness with fast startup and graceful shutdown. Keep development, staging, and production as similar as possible. Treat logs as event streams. Run admin or management tasks as one-off processes. A Simple Flask Example We begin with a basic Flask application that displays a simple message when accessed via a browser. The entry point for our Flask app is the app.py file, which is responsible for handling all incoming and outgoing requests. Below is the code for our simple Flask app: from flask import Flask

app = Flask(__name__)

@app.route('/')
def welcomeToKodeKloud():
    return ""Welcome to KODEKLOUD!""

if __name__ == ""__main__"":
    app.run(host=""0.0.0.0"", debug=True) When you run this application and navigate to its URL in your browser, the homepage will display the message ""Welcome to KODEKLOUD!"". Note At this stage, the code is stored only on your local machine. The First Factor: One Codebase The first principle of the 12-Factor App methodology insists on maintaining a single codebase per application. This approach is essential as your user base expands and new features are added, especially when multiple developers are involved. Each developer works on their own local environment while committing changes to a central repository. Without a proper version control system, concurrent modifications by different developers could lead to conflicts. This is where Git becomes invaluable. Git enables concurrent contributions to the same codebase by facilitating operations like pulling the latest changes with the git pull command, making local updates, and pushing new commits using git push . The central repository is typically hosted on a cloud platform. For example, GitHub is a popular platform for hosting Git repositories, while GitLab and Bitbucket offer similar solutions. Managing Multiple Applications Consider starting with an initial web application. Over time, you might expand your system by adding services such as order processing or delivery functionalities. In the past, it was common to maintain a single codebase for all related applications. However, once multiple services are deployed, the architecture becomes distributed, and sharing one codebase across multiple applications violates the 12-Factor App principles. Each application should reside in its own codebase. Within a single codebase, you can still deploy multiple instances of your application across various environments (such as development, staging, and production). This strategy ensures that while each application maintains its isolated codebase, you can manage multiple deployments seamlessly. This structure not only keeps your development process organized but also promotes consistency across different environments, allowing for smoother transitions between development, testing, and production stages. Summary By following the principles outlined in the 12-Factor App methodology, you ensure that every aspect of your application—from a single codebase to distinct deployments across environments—is optimized for scalability and maintainability. Leveraging tools like Git further enhances collaborative development, helping you manage changes effectively as your application grows. For more insights on scalable application design and best practices, explore our additional resources and documentation linked below. Flask Documentation GitHub Guides 12 Factor App Watch Video Watch video content"
12 Factor App,Why 12 Factor app,https://notes.kodekloud.com/docs/12-Factor-App/Introduction/Why-12-Factor-app,"12 Factor App Introduction Why 12 Factor app Imagine having a brilliant idea and building an application to share that vision with the world. In the past, launching an application involved overcoming numerous obstacles such as long waiting periods for a dedicated server. Once acquired, that server was permanently tied to your application, and scaling meant adding more resources to that single machine. Often, session data was stored locally, meaning that if the server failed, user progress was lost, and users had to restart from scratch. Fast forward to today. High-growth SaaS startups can see user numbers rise from zero to millions in mere months. The speed of innovation now hinges on how quickly you can write, test, and deploy your code. Thanks to modern cloud platforms, provisioning and hosting resources can take minutes—or even seconds. With Platform-as-a-Service (PaaS) and serverless technologies, you simply write your code, push it, and see it live. These platforms boast uptimes of 99.999%, making downtime for maintenance, patching, or scaling nearly unacceptable. Note For optimal performance and reliability, your application must be architected to decouple from the underlying infrastructure. This means designing for portability and seamless operation across various environments—be it on-premises, Google Cloud Platform (GCP), Amazon Web Services (AWS), or Microsoft Azure—without necessitating changes to your source code. In earlier architectures, scaling was achieved by vertically enhancing a server, an approach that often required taking the application offline for upgrades. Modern applications, however, scale horizontally by adding more servers and spinning up additional instances. To succeed in today’s fast-paced cloud environments, your application needs to be consistent across development, testing, and production while remaining easily scalable. A decade ago, engineers at Heroku distilled a set of guiding principles for building modern applications, known today as the 12-Factor App. These twelve principles provide a blueprint for creating scalable, resilient, and maintainable applications. For additional details, refer to the 12factor.net website. Watch Video Watch video content"
12 Factor App,Build Release and Run,https://notes.kodekloud.com/docs/12-Factor-App/Twelve-Factor-App-methodology/Build-Release-and-Run,"12 Factor App Twelve Factor App methodology Build Release and Run In this article, we explore the key phases in our deployment process: Build, Release, and Run. A recent typo in the browser message highlighted the importance of separating these stages. While minor typos can be fixed with a simple commit, using a strict separation between build and run phases minimizes downtime in more complex environments. For example, here is the corrected version of our Flask application that fixes the typo: import os
from flask import Flask
from redis import Redis

app = Flask(__name__)
redisDb = Redis(host=os.getenv('HOST'), port=os.getenv('PORT'))

@app.route('/')
def welcomeToKodeKloud():
    redisDb.incr('visitorCount')
    visitCount = str(redisDb.get('visitorCount'), 'utf-8')
    return ""Welcome to KODEKLOUD! Visitor Count: "" + visitCount

if __name__ == ""__main__"":
    app.run(host=""0.0.0.0"", debug=True) After pushing this commit, visiting the application displays: Welcome to KODEKLOUD! Visitor Count: 9 Note For minor fixes, a direct commit may suffice. However, in complex environments where rapid deployment is critical, separating the build, release, and run phases is essential to maintain uptime. Adhering to the principles of the 12 Factor App methodology, our deployment process strictly separates these stages. The diagram below illustrates the separation between build, release, and run phases: Phases of the Deployment Cycle Build Phase In the build phase, developers write code using their favorite editors (such as VS Code or PyCharm). The source code is then transformed into an executable artifact—like a binary or a Docker image. Our process uses a Dockerfile along with the Docker build command to create the image for our application: $ docker build -t myapp:latest . Release Phase Once the build is ready, the executable artifact is bundled with environment-specific configuration files to form the release object. Each release is uniquely identifiable (using versions like v1, v2, v3, or even timestamps), ensuring that even a minor change, such as a typo fix, generates a new release. Consider this sample configuration: Configuration Variable Value Description HOST ""redis_db_dev"" Hostname of the Redis server PORT ""6379"" Port for connecting to Redis HOST = ""redis_db_dev""
PORT = ""6379"" Run Phase In the run phase, the same build artifact is deployed across various environments (development, testing, production) to ensure consistency. This uniformity makes it easier to roll back to previous releases or redeploy specific versions when necessary. The final running version of our application remains identical to the artifact built earlier: import os
from flask import Flask
from redis import Redis

app = Flask(__name__)
redisDb = Redis(host=os.getenv('HOST'), port=os.getenv('PORT'))

@app.route('/')
def welcomeToKodeKloud():
    redisDb.incr('visitorCount')
    visitCount = str(redisDb.get('visitorCount'), 'utf-8')
    return ""Welcome to KODEKLOUD! Visitor Count: "" + visitCount

if __name__ == ""__main__"":
    app.run(host=""0.0.0.0"", debug=True) Deploy the application using the same Docker build process: $ docker build -t myapp:latest . With the corresponding configuration: HOST = ""redis_db_dev""
PORT = ""6379"" Warning Ensure that environment configurations are correctly managed during the release phase to avoid deployment issues. Misconfiguration at this stage can lead to unexpected application behavior. By delineating the build, release, and run phases, we can store build artifacts in a dedicated repository, allowing seamless rollback or redeployment of specific versions. This methodology not only streamlines software management but also significantly enhances deployment consistency and reliability. For further reading on this methodology, refer to the 12 Factor App documentation . Watch Video Watch video content"
12 Factor App,Admin Processes,https://notes.kodekloud.com/docs/12-Factor-App/Twelve-Factor-App-methodology/Admin-Processes,"12 Factor App Twelve Factor App methodology Admin Processes In this lesson, we delve into the final principle of the 12-Factor App methodology—admin processes. This principle emphasizes the importance of isolating one-off or periodic administrative tasks from the main application processes to ensure that these tasks run on an identical setup as the production environment. Currently, our application leverages a Redis database to store the count of total visitors. However, there may be instances where the counter becomes inaccurate or requires a reset. In such cases, it is crucial to execute a one-time administrative task without disrupting the running application. Key Principle Administrative tasks—such as resetting visitor counts, executing database migrations, or correcting specific user records—must be performed as isolated, one-off processes. This approach enables automation, scalability, and reproducibility while maintaining a production-like environment. For example, to reset the visitor count stored in Redis, you can execute an admin script. In our setup, this might involve launching an additional Docker container that connects to the same Redis database and runs the reset script: import os
from redis import Redis

# Establish a connection to the Redis database using environment variables for host and port.
redis_db = Redis(host=os.getenv('HOST'), port=os.getenv('PORT'))
redis_db.set('visitorCount', 0) The code above demonstrates how to connect to the Redis database and reset the visitorCount to 0. Running tasks like this as isolated, one-off processes ensures that they are automated, scalable, and reproducible, in alignment with the 12-Factor App principle of keeping admin tasks separate from long-running application processes. Summary The admin processes principle advocates for executing any administrative task—whether it is a one-time operation or a periodic task—in isolation. This guarantees that these operations remain automated, scalable, and reproducible, while mirroring the configuration of the primary application environment. Watch Video Watch video content"
12 Factor App,Logs,https://notes.kodekloud.com/docs/12-Factor-App/Twelve-Factor-App-methodology/Logs,"12 Factor App Twelve Factor App methodology Logs In this article, we explore the logging mechanism used by our application and how it handles various output events like server startup, port listening, HTTP requests, and error reporting. Logs are crucial for monitoring system activities and troubleshooting issues. When the application starts, it produces logs detailing the server startup sequence, including server addresses and port numbers. Every HTTP request served is recorded, as illustrated in the example below. * Serving Flask app 'main'
* Debug mode: on
* Running on all addresses (0.0.0.0)
* Running on http://127.0.0.1:8080
Press CTRL+C to quit
* Restarting with stat
* Debugger is active!
* Debugger PIN: 547-019-069
127.0.0.1 - - [25/Feb/2023 16:19:24] ""GET / HTTP/1.1"" 200 -
127.0.0.1 - - [25/Feb/2023 16:19:24] ""GET /favicon.ico HTTP/1.1"" 404 -
127.0.0.1 - - [25/Feb/2023 16:19:26] ""GET / HTTP/1.1"" 200 -
127.0.0.1 - - [25/Feb/2023 16:19:27] ""GET / HTTP/1.1"" 200 -
127.0.0.1 - - [25/Feb/2023 16:19:27] ""GET / HTTP/1.1"" 200 -
127.0.0.1 - - [25/Feb/2023 16:19:27] ""GET / HTTP/1.1"" 200 - These logs not only capture standard operations of the server but also record errors and other significant events, making them indispensable for diagnosing issues when failures occur. Logging Storage Approaches Traditionally, applications write logs to local files. However, in containerized environments, this method presents challenges: Volatility: A container may terminate at any time, causing the loss of local log files. Inflexibility: Tying your logging system to a specific file system location restricts scalability and portability. An alternative is to send logs to a centralized logging server using systems such as Fluentd, the ELK Stack, or Splunk. While centralized logging enhances management and analysis, directly integrating your application with a specific logging provider is not recommended. Best Practice Always design your application so that it remains agnostic to any logging backend, which improves flexibility, scalability, and ease of maintenance. Example: Sending Logs via Fluentd The following Python code demonstrates how logs can be sent to a Fluentd logging server. Note, however, that this pattern directly couples your application to Fluentd, which is discouraged: from fluent import sender

# Configure logger for remote logging via Fluentd
logger = sender.FluentSender('app', host='host', port=24224)

# Emit a log event with details
logger.emit('follow', {'from': 'userA', 'to': 'userB'}) According to the 11th principle of the 12 Factor App methodology, applications should not be responsible for log storage or routing. Instead, all logs should be directed to standard output or written as structured JSON to a local file. This practice allows an external agent to collect and forward logs to a centralized repository, where they can be queried and analyzed efficiently. Centralized logging solutions like the ELK Stack and Splunk are designed to ingest and process structured log data, making log analysis faster and more effective. By decoupling the logging mechanism from your application, you ensure that your system remains agile and well-suited for cloud-native and containerized environments. Watch Video Watch video content"
12 Factor App,Backing Services,https://notes.kodekloud.com/docs/12-Factor-App/Twelve-Factor-App-methodology/Backing-Services,"12 Factor App Twelve Factor App methodology Backing Services Backing services are external resources that your application depends on. These services can range from caching solutions like Redis to email providers, object storage services, and more. For instance, we integrated Redis as a caching service in our application to store the visitor count. Other typical backing services include: SMTP services for sending emails S3 integrations for storing images Managed databases and search engines Your application should be designed to interact with these backing services as attached resources. This means that regardless of whether the service is hosted locally, on a managed cloud platform, or as a cloud-native service, the integration remains consistent. The application code should remain unchanged when switching between different deployment environments. Key Point The concept of treating backing services as attached resources enables seamless scaling and flexibility. Simply update configuration details to point your application to a new instance without modifying any code logic. Redis as a Backing Service Example Consider Redis, which we use as a caching layer: Local Instance: You might run Redis on your local machine during development. Cloud Deployment: In a production environment, Redis might be hosted on a cloud provider like AWS or Azure. Managed Service: Alternatively, you could use a managed Redis service offered by various vendors. Despite these variations, your application logic remains the same. You only need to update the connection settings to point to the chosen Redis instance. Implementation Hint Ensure that all your backing services are configurable via environment variables or external configurations. This decouples service specifics from your application code, enhancing portability and maintainability. Why This Architecture Matters By decoupling applications from the specific implementations of backing services, you gain flexibility and resilience in your deployment. Whether scaling up in cloud environments or switching service providers, your application's core functionality remains intact. For more detailed insights and integration guidelines, explore comprehensive examples and further documentation on working with backing services. Additional Resources Kubernetes Basics Kubernetes Documentation Docker Hub Terraform Registry This approach not only simplifies deployments but also enhances the maintainability of your system by following best practices for service-oriented design. Watch Video Watch video content"
12 Factor App,Dependencies,https://notes.kodekloud.com/docs/12-Factor-App/Twelve-Factor-App-methodology/Dependencies,"12 Factor App Twelve Factor App methodology Dependencies In this article, we explore the second rule of the 12-Factor App methodology: explicitly declaring and isolating dependencies. To illustrate this concept, we use the popular Python web framework, Flask. Before writing any application code, you must install Flask in your local development environment. For example: $ pip install flask Below is a simple Flask application (app.py): from flask import Flask

app = Flask(__name__)

@app.route('/')
def welcomeToKodeKloud():
    return ""Welcome to KODEKLOUD!""

if __name__ == ""__main__"":
    app.run(host=""0.0.0.0"", debug=True) As your application grows, you may need to integrate additional third-party libraries. The 12-Factor App approach dictates that you must not rely on the implicit presence of system-wide packages. In other words, you cannot assume that dependencies like Flask will already be installed on the system where your application executes. They must be explicitly declared and isolated. A common Python practice is to list dependencies in a file named requirements.txt . This file should include both package names and specific version numbers. For example: flask==2.0.0 Specifying version numbers is crucial to ensure consistency across various environments such as development, staging, and production. Installing the dependencies is as simple as running: $ pip install -r requirements.txt This command ensures that every dependency listed in requirements.txt —including the specified version of Flask—is installed uniformly. Tip When developing multiple applications on a single machine, use isolated environments to avoid dependency conflicts. Virtual environments ensure each application manages its own version of every dependency. Python's virtual environments solve version conflicts by isolating dependencies for each application. This isolation is imperative when one app requires one version of Flask while another requires a different version. Combining the use of requirements.txt with virtual environments guarantees that your explicit dependency packages are consistent across development, staging, and production. In some cases, your application might depend on system-level tools (such as the curl command) that fall outside Python’s dependency management. In these instances, leveraging a platform like Docker can be highly effective. Docker containers offer a self-contained environment that ensures all necessary tools and configurations are present. Below is an example Dockerfile that packages the Flask application along with its dependencies into a Docker container: FROM python:3.10-alpine

WORKDIR /kodekloud-twelve-factor-app

COPY requirements.txt /kodekloud-twelve-factor-app

RUN pip install -r requirements.txt --no-cache-dir

COPY . /kodekloud-twelve-factor-app

CMD python app.py This Dockerfile accomplishes the following steps: Creates a Docker image based on the Python 3.10 Alpine base image. Sets the working directory to /kodekloud-twelve-factor-app . Copies the requirements.txt file into the working directory. Installs the dependencies specified in requirements.txt , without caching. Copies the remaining application code into the image. Defines the command to run the application. After building the Docker image with the appropriate Docker build command, you can run your application using the Docker run command. If you're new to Docker, be sure to check out our free Docker Training Course for the Absolute Beginner on KodeKloud. This interactive course offers hands-on labs and an immersive learning environment where you can practice with real systems and servers. Happy coding! Watch Video Watch video content"
12 Factor App,Port Binding,https://notes.kodekloud.com/docs/12-Factor-App/Twelve-Factor-App-methodology/Port-Binding,"12 Factor App Twelve Factor App methodology Port Binding Accessing our Flask web application is as straightforward as entering the URL along with the port number into your web browser. In our example, the application runs on port 5000. When accessed successfully, a welcome message along with a visitor count is displayed. By default, the Python Flask framework listens on port 5000. However, when running multiple instances of the application on the same server, each instance can bind to a unique port (such as 5001, 5002, etc.). In multi-service environments, different services are assigned distinct ports—for instance, Redis typically operates on port 6379. Binding an application to a specific port allows it to export HTTP as a service and listen directly for incoming requests. In contrast to traditional web applications that depend on an external web server, the 12-Factor App methodology encourages creating self-contained applications with built-in web servers. This design approach not only simplifies deployment but also enhances scalability. Note For environments that host multiple services simultaneously, ensuring each service is bound to a unique port is crucial for preventing conflicts and maintaining smooth communication between services. Watch Video Watch video content"
12 Factor App,Concurrency,https://notes.kodekloud.com/docs/12-Factor-App/Twelve-Factor-App-methodology/Concurrency,"12 Factor App Twelve Factor App methodology Concurrency In this lesson, we explore the critical concept of concurrency, which is the eighth factor in the 12 Factor App methodology. Up to now, we have containerized our application and executed it as a Docker container, running a single process capable of serving multiple users concurrently. While this setup works well under moderate demand, it has limitations under increased load. Scalability Considerations Scaling strategies come in two flavors: vertical and horizontal. Vertical scaling adds more resources to a single server, but this approach can lead to downtime and has inherent resource limits. Modern deployment strategies favor horizontal scaling, where additional servers are provisioned rapidly to run multiple instances of the application simultaneously. A load balancer then distributes incoming user requests across these instances, ensuring smooth performance even during peak demand. For horizontal scaling to be effective, your application must be designed as an independent, stateless service. In line with the 12 Factor App principles, processes are treated as first-class citizens. Instead of scaling up a single instance with more resources, the application should scale out by running multiple instances concurrently. Key Benefits Avoids the single point of failure inherent in vertical scaling. Improves fault tolerance and overall system reliability. Facilitates easier deployment and maintenance of application instances. In the next lesson, we will delve deeper into how these principles impact application processes and discuss practical strategies for designing applications that can scale horizontally. For more information, explore: Kubernetes Basics Kubernetes Documentation Docker Hub Terraform Registry Watch Video Watch video content"
12 Factor App,Dev Prod Parity,https://notes.kodekloud.com/docs/12-Factor-App/Twelve-Factor-App-methodology/Dev-Prod-Parity,"12 Factor App Twelve Factor App methodology Dev Prod Parity In this article, we delve into the importance of maintaining parity between development and production environments. By reducing discrepancies across these stages, teams can streamline their deployment process and ensure consistency in how applications perform across environments. Traditional Environment Setup Most development workflows historically involve three distinct environments: Development (Dev): Developers build and test new features in this phase. Often, lightweight tools or databases (e.g., SQLite) are used to accelerate iterative development. Staging: This environment closely mirrors production and is used for final testing and validation, ensuring that new changes behave as expected under realistic conditions. Production (Prod): The live environment accessed by end users, which typically relies on more robust tools and services (e.g., PostgreSQL) to support real-world usage. Challenges in Traditional Workflows Historically, transitioning code from development to production could span weeks or even months. This separation often led to several issues: Time Gap: Delays between development and production deployment can introduce discrepancies. Features may evolve after initial development, potentially affecting performance in production. Personnel Gap: When separate teams handle development and deployment, operations teams might not be fully aware of the latest changes, complicating troubleshooting. Tools Gap: Using different tools and environments in each stage can result in unexpected issues once changes are deployed. Key Insight The 10th principle of the 12 Factor App framework emphasizes minimizing differences between development, staging, and production. This approach streamlines continuous integration and delivery pipelines, reducing the inherent gaps in traditional setups. Modern Practices for Dev-Prod Parity Advances in continuous integration (CI) and continuous delivery/deployment (CD) now enable teams to roll out changes in hours—or even minutes. This rapid feedback loop ensures that new changes function correctly and issues are identified early. Moreover, the adoption of modern containerization platforms like Docker has enhanced the ability to maintain similar environments across all stages. By using the same set of tools from development to production, teams can effectively minimize surprises during deployment. Maintaining parity across development, staging, and production is crucial for achieving reliable, continuous deployments. By bridging the gaps in time, personnel, and tooling, teams can ensure that new features and updates are deployed smoothly and perform consistently in production. Watch Video Watch video content"
12 Factor App,Processes,https://notes.kodekloud.com/docs/12-Factor-App/Twelve-Factor-App-methodology/Processes,"12 Factor App Twelve Factor App methodology Processes In this article, we explore the design of stateless processes as defined in the 12 Factor App methodology. A stateless process shares nothing, a principle that is critical for scaling applications and maintaining consistency when multiple processes are running concurrently. Suppose we want to implement a new feature that displays the visitor count on our website. Each time a visitor accesses the page, the application should update and display the total visit count. Initially, we modify our code to include a global variable that tracks the number of visits, incrementing it with every incoming request: from flask import Flask
app = Flask(__name__)
visitCount = 0

@app.route('/')
def welcomeToKodeKloud():
    global visitCount
    visitCount += 1
    return ""Welcome to KODEKLOUD! Visitor Count: "" + str(visitCount)

if __name__ == ""__main__"":
    app.run(host=""0.0.0.0"", debug=True) This solution works perfectly when running a single process because the visit count is stored within that process's memory. However, when multiple instances are deployed, each process maintains its own independent version of the variable. Consequently, visitors might see different counts depending on which process handles their request. A similar challenge arises with session-specific data. For instance, when a user logs in, session details (like user location and session expiration) are stored in the server's memory. If subsequent requests are routed to a different process, the session information may be missing and the user might be inadvertently logged out. Sticky Sessions Limitation Even though load balancers can use session-aware mechanisms (sticky sessions) to direct a user to the same process, this approach is unreliable. In case of a process failure, any locally stored data is lost. This scenario emphasizes a core principle of the 12-Factor methodology: processes must be stateless and share nothing. Relying on sticky sessions contradicts this principle. Instead, all state information should be stored in external backing services, allowing all processes to access uniform data regardless of which instance handles a given request. A typical solution is to use an external service, such as a database or caching system like Redis , for persistent state or session data. To implement this approach, we modify our application so that the visit count is maintained in a Redis database rather than in the process's memory: from flask import Flask
from redis import Redis

app = Flask(__name__)
redisDb = Redis(host='redis-db', port=6380)

@app.route('/')
def welcomeToKodeKloud():
    redisDb.incr('visitorCount')
    visitCount = str(redisDb.get('visitorCount'), 'utf-8')
    return ""Welcome to KODEKLOUD! Visitor Count: "" + visitCount

if __name__ == ""__main__"":
    app.run(host=""0.0.0.0"", debug=True) With this updated implementation, the application scales seamlessly. Multiple instances can operate concurrently, all accessing the same consistent state through the centralized Redis database. This design adheres to the 12-Factor principle of statelessness and ensures your application remains robust and scalable in dynamic environments. Watch Video Watch video content"
12 Factor App,Config,https://notes.kodekloud.com/docs/12-Factor-App/Twelve-Factor-App-methodology/Config,"12 Factor App Twelve Factor App methodology Config In this article, we explore how to externalize your application's configuration to support different environments such as production, staging, and development. In our initial Python example, configuration values like the Redis host and port are hard-coded. This approach leads to issues, as each environment may require distinct settings, making it error-prone and inflexible. Tip By moving configuration values out of the application code, you can manage settings dynamically and securely across different environments. Hard-Coded Configuration: The Problem Consider the following Python snippet, which uses fixed values for connecting to a Redis instance: from flask import Flask
from redis import Redis

app = Flask(__name__)
redisDb = Redis(host='redis-db', port=6380)

@app.route('/')
def welcomeToKodeKLOUD():
    redisDb.incr('visitorCount')
    visitCount = str(redisDb.get('visitorCount'), 'utf-8')
    return ""Welcome to KODEKLOUD! Visitor Count: "" + visitCount

if __name__ == ""__main__"":
    app.run(host=""0.0.0.0"", debug=True) Hard-coding configuration values like the Redis host and port can introduce discrepancies when deploying your application to multiple environments. Each environment might use a different Redis instance, and updating these values in the code increases the risk of running into inconsistencies and deployment errors. Using Environment Variables for Configuration To separate configuration from core application logic and secure sensitive details, store your environment-specific settings in a dedicated file, typically named .env . With the "".env"" file, Python libraries can automatically load environment variables that can be accessed directly in your code. This approach aligns with the 12 Factor App methodology, which emphasizes storing configuration in the environment. As a result, you can seamlessly transition between testing, staging, and production setups without making changes to the core code. Example .env File Below is an example of how your .env file might look: HOST=redis_db
PORT=6379 Depending on your deployment environment, the configuration might vary as follows: Development Environment: HOST=redis_db_dev
PORT=6379 Staging Environment: HOST=redis_db_staging
PORT=6379 Production Environment: HOST=redis_db_prod
PORT=6379 Best Practice Using environment variables to manage configuration ensures that you can safely open source your project without exposing sensitive details. This setup also facilitates smoother deployments across various environments. By adopting this method, you reduce the risk of configuration-related errors and enhance the security and scalability of your application. For more insights into environment configuration best practices, visit our Kubernetes Documentation and Docker Hub . Watch Video Watch video content"
Advanced Bash Scripting,Interactive vs non interactive shell,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Introduction/Interactive-vs-non-interactive-shell,"Advanced Bash Scripting Introduction Interactive vs non interactive shell Understanding the distinction between interactive and non-interactive shells is fundamental when writing reliable Bash scripts. Interactive shells are designed for on-the-fly command entry, while non-interactive shells execute prewritten scripts without user intervention. Two Modes of Shell Operation Interactive Mode You type commands at a prompt and see immediate feedback. Non-Interactive Mode You place commands in a file (a script) and run them all at once. Note Interactive shells load initialization files like ~/.bashrc , enabling features such as command history and prompt customization. Non-interactive shells skip most of these initializations. Invoking Interactive vs. Non-Interactive Shells Interactive Example $ ls
file1.txt  file2.txt
$ cd /tmp
$ pwd
/tmp Non-Interactive Example Create a script script.sh : #!/bin/bash
ls
cd /tmp
pwd Make it executable and run: chmod +x script.sh
./script.sh
# Output:
# file1.txt  file2.txt
# /tmp Comparing Shell Modes Feature Interactive Shell Non-Interactive Shell Invocation Direct terminal prompt ./script.sh or bash script.sh Input Keyboard Script file or piped commands Initialization files ~/.bashrc , PROMPT_COMMAND /etc/profile , ~/.bash_profile Common Use Cases Ad hoc tasks, debugging Automation, cron jobs, CI/CD Prompt Variable ( PS1 ) Typically set ( \u@\h:\w\$ ) Usually unset Why It Matters Some environment variables and shell behaviors differ between modes: Inspecting PS1 Interactive mode : $ echo ""$PS1""
\u@\h:\w\$ Non-interactive script ( show_ps1.sh ) : #!/bin/bash
echo ""$PS1"" Running it produces no output because PS1 is unset in non-interactive shells. Warning Don’t rely on interactive-only variables (like PS1 ) in scripts. Always initialize or provide defaults for any environment variable your script requires. Best Practices for Shell Scripts Start with a shebang #!/usr/bin/env bash for portability. Use set -euo pipefail to catch errors early. Double-quote variable expansions to prevent word splitting: ""$VAR"" . Explicitly source files when needed: source ~/.bashrc Test scripts in both modes if they must run interactively and non-interactively. Further Reading Bash Reference Manual Bash Startup Files Learn Shell Watch Video Watch video content"
Advanced Bash Scripting,Shell scripting vs Bash scripting,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Introduction/Shell-scripting-vs-Bash-scripting,"Advanced Bash Scripting Introduction Shell scripting vs Bash scripting Shell scripting is the practice of writing executable text files that interpret and execute commands in a Unix-like shell. While Bash (Bourne Again SHell) is the most common shell today, there are many others: Shell Year Introduced Key Feature Bourne sh 1979 Original shell scripting standard Bash 1989 Rich scripting constructs Z shell zsh 1990 Typo correction & plugin system Korn ksh 1983 Advanced variables & functions Dash 1997 Focus on speed & minimalism Use shell scripting when referring to any shell. If you depend on Bash-specific features (arrays, [[ … ]] , process substitution), say shell scripting with Bash . Note Always include a shebang ( #! ) at the top of your scripts to declare which shell to use: #!/usr/bin/env bash When to Use “Shell” vs. “Bash” Scripting Shell scripting – generic term for writing scripts in any shell Shell scripting with Bash – when your code relies on Bash-only features Shell scripting with zsh , ksh , etc. – when targeting those environments Illustrative Differences Between Bash and Other Shells Below are two examples that show how Bash and other shells (like sh and zsh ) can behave differently. 1. zsh Autocorrects Typos Zsh offers a user‐friendly autocorrect feature: $ LS -l FILE.TXT
file.txt Here, zsh automatically corrects LS → ls and FILE.TXT → file.txt . Bash does not do this by default. 2. echo -n Behavior The -n flag suppresses the trailing newline in Bash, but not in all sh implementations: # In Bash
$ echo -n ""Hello""; echo ""END""
HelloEND

# In sh
$ sh -c 'echo -n ""Hello""; echo ""END""'
-n Hello
END Under sh , -n is treated as text and printed. Under bash , -n removes the newline as expected. Warning For portable scripts, avoid relying on echo -n . Use printf instead: printf ""Hello""; echo ""END"" Conclusion Refer to any interpreter as shell scripting when you don’t need to specify. Use shell scripting with Bash (or another shell) if you rely on that shell’s extensions. For maximum portability and advanced features, Bash is the de facto standard. Links and References Bash Reference Manual Zsh Manual POSIX Shell Specification Watch Video Watch video content"
Advanced Bash Scripting,Why learn advanced bash scripting,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Introduction/Why-learn-advanced-bash-scripting,"Advanced Bash Scripting Introduction Why learn advanced bash scripting Learning advanced Bash scripting unlocks powerful automation and rapid troubleshooting capabilities—often faster to deploy than higher-level languages like Python. In this guide, you’ll discover how mastering Bash can streamline tasks in production environments, boost your productivity, and strengthen your career as a DevOps or systems engineer. Why Bash Still Matters 90% of corporate servers run Linux. 70% of academic servers use Unix-like OSes. Microsoft embraces Linux via WSL2 and Azure. Although Python is bundled with most distributions, adding new runtimes often requires approval in enterprise settings. In contrast, Bash is POSIX-compliant and available out of the box on Linux and macOS. Note Bash is pre-installed on virtually every server and requires no additional packages to get started. Real-World Example: On-Call Incident Automation When CI pipelines break in production, you often need a quick fix. There’s no time to write Ansible playbooks or install Python modules. In one high-pressure incident, I needed to monitor total open connections on multiple load balancers over 24 hours. Instead of a heavyweight solution, I used this concise Bash script: #!/usr/bin/env bash

log() {
    echo ""$(date -u +""%Y-%m-%dT%H:%M:%SZ"") $@""
}

while true; do
    log ""Open connections:"" ""$(netstat -tnlp | wc -l)""
    sleep 30
done Script Features at a Glance Feature Description Example Shebang portability Uses /usr/bin/env to locate bash #!/usr/bin/env bash Logging with timestamps ISO-8601 formatted UTC timestamps date -u +""%Y-%m-%dT%H:%M:%SZ"" Functions Encapsulate reusable logic log() { … } Infinite loops Polling or watchdog tasks while true; do …; done Command substitution Capture command output into variables $(netstat -tnlp | wc -l) Networking & text utils Leverage built-in tools for quick data parsing netstat , wc -l The Development Cycle Bash scripting offers an immediate feedback loop: write code, run it, and adjust on the fly. This three-stage cycle accelerates learning and troubleshooting. Core Benefits of Advanced Shell Scripting By working directly with files, directories, and OS primitives—without extra frameworks—you gain practical skills that translate to real-world tasks: Direct file and directory manipulation Minimal dependencies—no external libraries Instant script execution and iteration Reusable functions and modular patterns Note Investing time in Bash improves your problem-solving agility, from daily automation to critical production fixes. Further Reading and References GNU Bash Reference Manual POSIX Shell and Utilities Kubernetes Basics Terraform Registry Watch Video Watch video content"
Advanced Bash Scripting,Overview,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Conventions/Overview,"Advanced Bash Scripting Conventions Overview Welcome to this comprehensive guide on shell scripting conventions. Consistent coding style makes scripts more readable, maintainable, and approachable—whether you’re working solo or as part of a team. Section Description What Are Coding Conventions? Definitions and real-world analogies Benefits of a Unified Style How consistency boosts readability and reduces bugs Recommended Shell Script Conventions Naming patterns, formatting rules, and directory organization What Are Coding Conventions? Coding conventions are agreed-upon practices that determine how code is structured and named. Much like social norms guide behavior in gatherings, technical conventions help developers understand each other’s work at a glance. Why Consistent Style Matters In any engineering discipline—from aircraft cockpit layouts to network topologies—standardized designs enhance safety and efficiency. In software, conventions: Reduce cognitive load when switching between projects Simplify onboarding for new contributors Prevent subtle bugs caused by inconsistent formatting Note Conventions focus on style, not interpreter rules. Violating them won’t usually break your code, but following them makes future maintenance far easier. Shell Script Conventions: Naming & Style Below are high-level recommendations for shell scripts. We’ll dive into each in detail later. Use lowercase, snake_case for variable names (e.g., user_home ) Prefix boolean flags with is_ or has_ (e.g., is_enabled ) Name functions with verbs and snake_case (e.g., backup_database ) Indent with two spaces for readability Organize scripts into bin/ , lib/ , and conf/ directories Community Standards in Other Languages Languages like Java and Python have de facto style guides that virtually every developer follows—think PEP 8 or Google’s Java Style Guide. In those ecosystems, naming functions, classes, and methods requires little debate. Shell scripting hasn’t settled on one universal standard, so teams often craft their own. Whether you’re a novice or a seasoned scripter, adopting these guidelines will sharpen your skills and improve collaboration. Scope & Getting Started This guide targets style—naming conventions, file layouts, and formatting—rather than performance optimizations or interpreter quirks. For example, the following assignments behave differently at runtime: file=""somefile.txt""  # ✅ Valid: no spaces around =
file= ""somefile.txt"" # ❌ Runtime error: unexpected token Warning Shell interpreters enforce certain rules (like no spaces around = in assignments). Always validate your scripts with ShellCheck before deployment. If you already follow a style guide, feel free to refactor the examples here. If you’re new to conventions, adopt these recommendations to jump-start your productivity. Further Reading ShellCheck – A static analysis tool for shell scripts Bash Guide for Beginners POSIX Shell and Utilities Watch Video Watch video content"
Advanced Bash Scripting,Variables,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Conventions/Variables,"Advanced Bash Scripting Conventions Variables Proper variable management in Bash scripts improves readability, prevents accidental overwrites, and makes your code easier to maintain. In this guide, you’ll learn best practices for naming variables, defining constants, and exporting values for child processes. Table of Contents 1. Naming Conventions 2. Defining Constants 3. Exporting Variables 4. References 1. Naming Conventions Follow these guidelines to create clear, self-documenting variable names: Convention Description Example Lowercase All variable names in lowercase to avoid conflicts username=""alice"" Snake case Use underscores for multi-word names first_name_last_name=""Alice"" Descriptive names Reflect the variable’s purpose config_file=""/etc/app.conf"" Single-letter (optional) Only for simple counters or arithmetic operations x=10 Example: first_name_last_name=""Alice""
source_file=""/etc/config.yaml""
count=42
x=10 Note Avoid uppercase names for regular variables—uppercase is conventionally reserved for constants (see Defining Constants ). 2. Defining Constants When you have values that should never change, define them in uppercase snake_case and mark them as read-only: readonly FILE_NAME=""file.txt""
readonly MAX_RETRIES=5
readonly PI=3.14159 Using readonly serves two purposes: Clarity: Signals to readers that these values are fixed. Safety: Prevents accidental reassignment later in the script. Warning Attempting to modify a readonly variable will produce an error: PI=3.14   # bash: PI: readonly variable 3. Exporting Variables Export variables when you need them in child processes, subshells, or external scripts. Export an existing variable source_file=""/etc/config.yaml""
export source_file Define and export in one step export HOME_DIR=""/home/user"" Exported variables become part of the environment for all subsequent commands and processes started from the current shell. 4. References Bash Reference Manual – Variables Advanced Bash-Scripting Guide Watch Video Watch video content"
Advanced Bash Scripting,Terminology,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Introduction/Terminology,"Advanced Bash Scripting Introduction Terminology In this lesson, we’ll clarify the often-interchanged terms— shell , CLI , terminal , console , TTY , and POSIX —and explain what POSIX compliance means for your Unix-like environment. Having a consistent vocabulary will help you follow advanced Bash scripting techniques with confidence. Shell A shell is a command interpreter: it reads your commands, executes them, and displays results. Different shells offer unique features, scripting syntax, and built-in utilities. Here are some of the most common: Shell Platform Description Bash Linux, macOS Bourne Again Shell; default on many Zsh macOS, Linux Z Shell with powerful customization KornShell Unix ksh ; emphasizes scripting consistency PowerShell Windows, Linux Object-oriented automation framework CMD Windows Legacy command-line interpreter CLI A Command Line Interface (CLI) is the text-based interface that lets you type commands directly to a shell. While most shells share fundamental capabilities, each CLI might include: Unique prompts and themes Built-in command history and completion Custom scripting hooks CLI tools often integrate with system components like SSH, Docker, or package managers. Terminal A terminal , or terminal emulator, is the application window where your CLI session runs. It renders text, handles keyboard input, and manages multiple tabs or panes. Terminal Emulator Platform Key Features GNOME Terminal Linux Profiles, tabs, custom theming xterm Cross-OS Lightweight, highly configurable Windows Terminal Windows Tabbed interface, PowerShell Note You can run the same shell (e.g., Bash) in different terminal emulators just as you might browse the same website in different web browsers. Console Originally, a console was the physical keyboard-and-display unit directly wired into a machine via a dedicated port. Think of a video game console—hardware designed for one system. Modern usage sometimes treats “console” as synonymous with “terminal,” but the historical distinction remains. TTY TTY stands for teletypewriter. Early terminals were electromechanical devices that functioned like remote printers. Today, Unix-like systems assign each terminal session (for example, a tab or SSH session) a pseudo-TTY number. You can inspect your current TTY with: tty
# e.g., /dev/pts/0 POSIX and POSIX-compliance POSIX (Portable Operating System Interface) is an IEEE standard that defines a common API and shell behavior for Unix-like systems. POSIX guarantees that scripts using standard utilities and syntax will run across compliant environments. Warning Not all shells are fully POSIX-compliant. If you need maximum portability, use /bin/sh or check your shell’s compliance level. Inline features like arrays or extended globbing may break on strict POSIX systems. By distinguishing between shell, CLI, terminal, console, TTY, and POSIX, you’ll reduce confusion and build a strong foundation for advanced Bash scripting. Links and References POSIX Standard on Wikipedia Bash Reference Manual GNU Coreutils GNU Terminal Emulators Watch Video Watch video content"
Advanced Bash Scripting,Functions,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Conventions/Functions,"Advanced Bash Scripting Conventions Functions Overview Functions are essential for writing clean, modular shell scripts. They help you: Reuse code and avoid duplication Improve readability and maintainability Handle complex tasks in a structured way This guide covers best practices for naming, defining, and styling functions in Bash. Naming Conventions Adopt consistent naming to make your functions self-documenting. Follow these rules: Guideline Recommendation Example Lowercase names Enhances uniformity calculate_area Descriptive identifiers Conveys purpose instantly backup_database Underscores between words Improves readability get_user_info No single-letter or CamelCase Prevents ambiguity process_files Note Descriptive, lowercase names with underscores help developers and automation tools understand your code at a glance. Defining Functions Use this standard syntax for declaring functions: function_name() {
    # function body
} Key style guidelines: Include parentheses () after the name. Place the opening brace { on the same line, preceded by one space. Align the closing brace } with the function declaration (no extra indentation). Example Definitions calculate_area() {
    local radius=$1
    echo ""Area: $(( 3 * radius * radius ))""
}

get_name() {
    local user_id=$1
    # Retrieve the user’s name from a data source
    echo ""User Name for ID $user_id""
}

clone_repo() {
    local repo_url=$1
    git clone ""$repo_url""
} Warning Misplacing braces or omitting parentheses leads to syntax errors and unexpected behavior. Advanced Function Patterns In upcoming sections, you’ll learn how to: Define functions without the function keyword Use local variables to prevent global namespace pollution Return status codes and handle errors gracefully Parse command-line options using getopts Further Reading Advanced Bash-Scripting Guide Bash Reference Manual getopts Bash Builtin Watch Video Watch video content"
Advanced Bash Scripting,Expanding,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Conventions/Expanding,"Advanced Bash Scripting Conventions Expanding In shell scripting, variable expansion uses the dollar sign ( $ ) to tell the shell to replace the variable name with its stored value. #!/bin/bash
var=""value of var""
echo ${var} Running this script: $ ./var-sample.sh
value of var Braces vs No Braces You can reference variables with or without braces. Braces become essential when you append characters immediately after the variable name. With Braces #!/bin/bash
var=""value of var""
echo ${var} Without Braces #!/bin/bash
var=""value of var""
echo $var Both scripts output: $ ./var-sample.sh
value of var Delimiting Variable Names Without braces, the shell cannot determine where the variable name ends: #!/bin/bash
height=170

# Incorrect: $heightcm is undefined
echo ""Your height is = $heightcm""

# Correct: ${height}cm expands properly
echo ""Your height is = ${height}cm"" $ ./height.sh
Your height is = 
Your height is = 170cm Quoting and Word Splitting By default, unquoted expansions are split on whitespace defined by IFS (space, tab, newline). Use quotes to preserve the exact value. Unquoted Expansion #!/bin/bash
string=""One Two Three""

# Splits into words
for element in ${string}; do
  echo ""${element}""
done Quoted Expansion #!/bin/bash
string=""One Two Three""

# Preserves the entire string as one element
for element in ""${string}""; do
  echo ""${element}""
done $ ./string.sh
One
Two
Three

$ ./string2.sh
One Two Three Note Always quote expansions when dealing with filenames, paths, or URLs to prevent unintended splitting. Intentional Splitting Sometimes you want to iterate over each word in a list: #!/bin/bash
readonly SERVERS=""server1 server2 server3""

for server in ${SERVERS}; do
  echo ""${server}.example.com""
done $ ./expanding.sh
server1.example.com
server2.example.com
server3.example.com Quoting the variable in this case treats the entire list as one element: #!/bin/bash
readonly SERVERS=""server1 server2 server3""

for server in ""${SERVERS}""; do
  echo ""${server}.example.com""
done $ ./expanding2.sh
server1 server2 server3.example.com Best Practices Use this quick reference to decide when to quote or brace variables: Scenario Quoting Braces Example Simple expansion Optional Optional echo $var Appending text to a variable Optional Required echo ""${var}suffix"" File paths and filenames Recommended Optional ls ""${directory}/file.txt"" URLs and complex strings Recommended Optional curl ""${URL}?id=123&name=abc"" Iterating over words intentionally Optional Optional for x in ${list}; do ...; done Preventing word splitting Required Optional read -r line <<< ""${input}"" Warning Never rely on unquoted variable expansions for user input or file names—they can introduce security risks or unexpected behavior. Further Reading Bash Parameter Expansion Advanced Bash-Scripting Guide POSIX Shell Command Language Watch Video Watch video content"
Advanced Bash Scripting,Scriptflow,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Refresher/Scriptflow,"Advanced Bash Scripting Refresher Scriptflow Understanding and controlling your shell script’s execution path is crucial for writing reliable Bash scripts. By default, a script runs sequentially, but you can alter this sequence using control constructs such as conditionals, loops, sourcing files, and functions. These tools allow you to: Run commands only if specific conditions are met Repeat commands multiple times with varying inputs A Real-World Analogy: Buying a Movie Ticket Imagine you walk up to a theater ticket booth. If you hand over a valid ticket, you enter; if not, you’re turned away. This decision-making process mirrors how an if statement in Bash evaluates conditions and branches accordingly. A Factory Analogy for Complex Workflows Consider a widget factory where each item travels along a conveyor. At an inspection station, defective widgets are removed while good ones proceed to the next stage. This inspection step functions like a control construct in your script, deciding whether data moves forward or is handled differently. Key Constructs That Alter Scriptflow Shell scripts use these four core constructs to modify the default linear execution: Construct Purpose Example Syntax Conditional ( if , case ) Branch logic based on conditions if [[ $x -gt 5 ]]; then … fi Loop ( for , while , until ) Repeat code blocks until a condition changes for i in {1..3}; do … done Sourcing External Files Include and execute another script at runtime source config.sh Function Encapsulate and reuse code segments my_func() { echo ""Hi""; } Conditional Statements if Statement Bash’s [[ … ]] test command provides richer conditional checks than [ … ] : #!/usr/bin/env bash
# This block never runs because 3 is not greater than 4
if [[ 3 -gt 4 ]]; then
    echo ""This will never be printed""
fi Note We recommend [[ … ]] over [ … ] for its support of pattern matching and logical operators. case Statement Use case for clear branching when matching a variable against multiple patterns: #!/usr/bin/env bash
action=""$1""

case ""$action"" in
  start)
    echo ""Starting service"";;
  stop)
    echo ""Stopping service"";;
  restart)
    echo ""Restarting service"";;
  *)
    echo ""Usage: $0 {start|stop|restart}"";;
esac Loops Loops are ideal for executing commands multiple times, either a fixed count or until a condition changes. Here are five versatile loop patterns: while loop with a counter #!/usr/bin/env bash
i=1
while [[ $i -le 3 ]]; do
    echo ""Iteration $i""
    i=$(( i + 1 ))
done for loop with brace expansion #!/usr/bin/env bash
for i in {1..3}; do
    echo ""Iteration $i""
done until loop counting down #!/usr/bin/env bash
i=3
until [[ $i -eq 0 ]]; do
    echo ""Iteration $i""
    i=$(( i - 1 ))
done Piping seq into while #!/usr/bin/env bash
seq 1 3 | while read -r i; do
    echo ""Iteration $i""
done Reading lines from a file #!/usr/bin/env bash
while read -r line; do
    echo ""Line: $line""
done < fds.txt Note Use for loops when the number of iterations is predetermined. Note Choose while or until when waiting for a dynamic condition or external event. Sourcing External Files You can import another script or configuration file mid-execution using source or the shorthand . . This merges the external content into your current shell environment. Example v1: Basic Sourcing # .conf content:
#!/usr/bin/env bash
source .conf
echo ""${name}"" $ ./conf_read-v1.sh
Bob Doe Example v2: Safe Sourcing with Fallback #!/usr/bin/env bash
readonly CONF_FILE="".conf""

if [[ -f ""${CONF_FILE}"" ]]; then
    source ""${CONF_FILE}""
else
    name=""Bob""
fi

echo ""${name}""
exit 0 $ ./conf_read-v2.sh
Bob When .conf exists: $ echo 'name=""Juan Carlos""' > .conf
$ ./conf_read-v2.sh
Juan Carlos Warning Always verify or sanitize sourced files to avoid executing untrusted code. Next Steps In the next section, we explore how functions can modularize your scriptflow, making your Bash scripts more maintainable and reusable. Links and References Bash Reference Manual Advanced Bash-Scripting Guide ShellCheck: Bash Linter Watch Video Watch video content"
Advanced Bash Scripting,Overview,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Refresher/Overview,"Advanced Bash Scripting Refresher Overview This guide revisits essential shell scripting fundamentals—commands, functions, script flow, shebangs—and demonstrates how to weave them into reliable, maintainable scripts. We assume you’re already comfortable with looping ( for , while ) and branching ( if , case ) constructs; hands-on labs will reinforce these concepts. Prerequisite Knowledge You should know basic shell constructs such as loops and conditionals. If you need a refresher, check out the Bash Reference Manual . Shell Script Execution Lifecycle Every shell script follows a predictable lifecycle. Understanding these phases will help you write clearer, more robust Bash scripts. Lifecycle Phase Description Invocation Interpreter launched via the #! shebang (e.g., #!/bin/bash ) Parsing The shell reads, tokenizes, and checks syntax Execution Commands run in sequence or via function calls Termination Script exits with a status code (0 for success, nonzero for error) Top-to-Bottom Imperative Execution By default, Bash scripts execute commands in order from top to bottom. This imperative style is simple but can become hard to manage as scripts grow: #!/bin/bash

echo ""Hello World""
echo ""Hello World one more time""
echo ""Hello World one last time"" Interactive commands work the same way: $ ls
documents  download  music  pict We combine external binaries, built-ins, conditional logic, and special syntax to automate workflows and repetitive tasks. Organizing Code with Functions Functions let you group logic into reusable blocks. Declaring a function doesn’t execute it—you must explicitly call it: #!/bin/bash

echo_function() {
    echo ""This function runs only when called, even if declared above.""
}

echo ""This is the first line of the script.""

# Invoke the function twice
echo_function
echo_function Using functions improves readability, maintainability, and testability of your scripts. Error Handling and Exit Codes Reliable scripts must report success or failure at each step. Imagine dropping off two kids at school: if one is absent due to illness, you must report that accurately to avoid confusion. The same principle applies in scripting—if a directory creation fails, your script should exit immediately rather than allowing downstream errors. Here are two common techniques: Use set -e at the top of your script to exit on any error. Check the exit status of critical commands explicitly: mkdir /important/dir
if [[ $? -ne 0 ]]; then
    echo ""Failed to create /important/dir"" >&2
    exit 1
fi Error Handling Best Practices Always validate results of filesystem operations and external commands. Unhandled failures can cascade into bigger incidents. With these principles—shebang usage, script lifecycle, imperative and functional structures, and robust error handling—you’re ready to write and maintain high-quality Bash scripts. See you in the next lesson! Watch Video Watch video content"
Advanced Bash Scripting,Keywords builtins,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Refresher/Keywords-builtins,"Advanced Bash Scripting Refresher Keywords builtins In this lesson, we explore the fundamental distinctions between Bash built-in commands and shell keywords. Built-ins execute inside the shell without spawning extra processes, whereas keywords are parsed tokens that implement control structures and logic flow. Shell Built-ins vs Keywords Aspect Built-in Commands Keywords Execution Runs inside the shell Parsed by the shell Process Forking No new process No new process Documentation help or man available No separate manual page Role Utility operations Control structures & tokens Shell built-ins (like echo , cd , or [ ] ) are implemented directly within Bash, complete with flags and documentation via help or man . Keywords (like if , for , or [[ ]] ) are special words the shell interpreter parses to direct execution order and logic. Note Built-in commands minimize overhead by avoiding additional process creation. Keywords define the script’s flow without calling external binaries. Single vs Double Square Brackets Single brackets ( [ ] ) are a built-in alias for the test command. Double brackets ( [[ ]] ) are keywords with enhanced features and direct parsing by Bash. builtin-sample.sh #!/bin/bash
if [ 2 -eq 2 ]; then
    echo ""two equals two""
fi $ ./builtin-sample.sh
two equals two keyword-sample.sh #!/bin/bash
if [[ 2 -eq 2 ]]; then
    echo ""two equals two""
fi $ ./keyword-sample.sh
two equals two [ vs test Under the hood, [ ] is just the test built-in. You can invoke either name interchangeably: $ test 2 -eq 2 && echo ""two equals two""
two equals two View its documentation with: $ man test Since [[ ]] has no external binary, Bash handles it entirely as a keyword. Built-in vs Keyword Evaluations Single brackets interpret < as a redirection operator: $ [ 1 < 2 ] && echo ""1 is less than 2""
-bash: 2: No such file or directory Double brackets support < as a comparison operator: $ [[ 5 < 7 ]] && echo ""5 is less than 7""
5 is less than 7 Advanced Double Bracket Features Double brackets unlock logical grouping, glob patterns, and regular expressions: $ [[ 3 -eq 3 && (2 -eq 2 && 1 -eq 1) ]] && echo ""Parentheses can be used""
Parentheses can be used

$ name=""Bob Doe""
$ [[ $name = *o* ]] && echo ""Patterns can be used""
Patterns can be used

$ name=""Bob Doe""
$ [[ $name =~ B.*Doe ]] && echo ""Regular expressions can be used""
Regular expressions can be used Warning The [[ ]] syntax is not POSIX compliant and may not be available in all shells. Use it only when Bash-specific features are acceptable. Advantages and Disadvantages Single Square Brackets ( [ ] ) Advantages: Portable across POSIX-compliant shells Standard conditional syntax Disadvantages: Limited to basic comparisons No pattern matching or grouping Double Square Brackets ( [[ ]] ) Advantages: Extended conditionals (regex, globs, grouping) Safer string comparisons Disadvantages: Not POSIX compliant Bash-specific feature Summary: Built-ins vs Keywords Shell built-ins run inside Bash without forking. Keywords are parsed tokens controlling flow or behavior. Neither built-ins nor keywords spawn external processes. Certain keywords (e.g., time ) act like directives rather than forming explicit control structures. Armed with these distinctions, you can choose the most efficient and appropriate syntax for your Bash scripts. Links and References Bash Reference Manual POSIX Shell Syntax Bash Conditional Expression Documentation Watch Video Watch video content Practice Lab Practice lab"
Advanced Bash Scripting,Input,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Streams/Input,"Advanced Bash Scripting Streams Input Input in shell scripting refers to data fed to a script from the terminal, a file, or another program. In Unix-like systems, every process has three standard streams: File Descriptor Stream Default Device 0 stdin Keyboard 1 stdout Screen 2 stderr Screen What Is Standard Input (stdin)? Standard input (stdin) uses file descriptor 0. Originally, Unix designers designated stdin as the “inlet” for data streams—primarily from the keyboard. Types of Input Sources Broadly, shell scripts receive input in two ways: Directly from the terminal (stdin). From pipes or files redirected into stdin. Redirection Operators Unix provides special operators to reroute these streams: > Redirects standard output to a file. < Feeds a file’s content into stdin. | Pipes stdout of one command into stdin of another. Interactive Input with wc By default, many commands read from stdin if you don’t supply a file. For example, running wc (word count) without arguments enters interactive mode: $ wc
Juan Carlos
[Ctrl-D]
1 2 11 Here, wc reports: Lines: 1 Words: 2 Characters: 11 Piping Input to wc More commonly, you chain commands with pipelines: $ echo ""Juan Carlos"" | wc
      1       2      12 This pipeline connects echo ’s stdout (fd 1) to wc ’s stdin (fd 0). Detecting Input Source in a Script You can check whether stdin is a terminal or a pipe/file with test -t 0 : #!/usr/bin/env bash
if [[ -t 0 ]]; then
  echo ""Interactive input (keyboard)""
  read
else
  echo ""Input via file or pipeline""
  read
fi Note The -t flag returns true if the given file descriptor (e.g., 0 for stdin) is open and refers to a terminal. Example: $ ./input_fd.sh
Interactive input (keyboard)
Hello

$ echo ""Pipeline"" | ./input_fd.sh
Input via file or pipeline Redirecting from a file also works: $ wc < input.txt
      1       4      24 Testing Other File Descriptors You can similarly test fd 1 (stdout) or any descriptor: $ echo ""Output"" | test -t 1 && echo ""Stdout is terminal""
Stdout is terminal

$ echo ""Output"" | test -t 0 && echo ""Stdin is terminal""
# No output—stdin is not a terminal in this case. Tracing System Calls with strace Inspect how the shell reads keystrokes in low-level detail: sudo strace -Tfp $$ 2>&1 | grep -E 'read' & Typing “hello” produces lines like: read(0, ""h"", 1) = 1 <0.00012>
read(0, ""e"", 1) = 1 <0.00009>
... Each read call fetches one byte from stdin. Echoing occurs via stderr (fd 2) on each keystroke; stdout (fd 1) handles command output after Enter. File Descriptors in the Terminal In a live terminal session: FD 0 reads keystrokes. FD 2 echoes keystrokes back. FD 1 (stdout) or FD 2 (stderr) displays command results. Capstone Exercise: Using a Custom File Descriptor Practice redirection with an extra descriptor (fd 3). Fix an email typo in email_file.txt by replacing a space with a dot: # Create file with typo
echo ""Juan [email protected] "" > email_file.txt

# Open fd 3 for read/write on the file
exec 3<> email_file.txt

# Read first 4 characters (""Juan"")
read -n 4 <&3

# Write a dot at current offset
echo -n ""."" >&3

# Close fd 3
exec 3<&-
exec 3>&-

# Verify
cat email_file.txt
# Output: [email protected] This demonstrates how custom file descriptors can modify files in place. We’ve covered interactive stdin, pipes, redirection, descriptor testing, and system-call tracing. Next, explore Here Documents (Heredocs) for multiline input blocks. References Bash Manual: Redirecting Input and Output strace Documentation Unix File Descriptors Advanced Bash-Scripting Guide Watch Video Watch video content Practice Lab Practice lab"
Advanced Bash Scripting,pid,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Refresher/pid,"Advanced Bash Scripting Refresher pid If you think about it, a shell script is essentially a list of commands organized in a file, using programming constructs to automate complex tasks and workflows. When you run a command or script, the kernel assigns a unique Process ID (PID) to each new process. Tracking these PIDs helps you monitor and control processes in real time. But how many PIDs get created when you execute a shell script? Parent and Child Processes: The Chef Analogy Imagine your interactive shell as a head chef. Every command you type is like an instruction to a kitchen worker (a child process). Parent shell : The chef leading the kitchen. When you open a terminal, the shell process you see is the parent. Child process : A sous-chef executing a single task. Each command you issue spawns its own PID, but it remains tied to the parent shell. Each instruction the chef gives is a separate process you can inspect, pause, or terminate—just like sending signals to worker processes. TTYs and Session Leaders Every terminal window or tab runs on its own TTY (teletypewriter). The shell process that controls that TTY is called the session leader . Child processes inherit the same TTY but get new PIDs. To discover your current TTY and Bash session PID, run: $ tty
/dev/ttys007

$ ps -ef | grep bash
501  29998  27796  0 11:00PM ttys007  0:00.01 bash
501  29928  29998  0 11:00PM ttys007  0:00.00 grep bash Spawning Scripts and Their Children When you execute a shell script ( ./script.sh ), the shell launches it as a child process. That script can then spawn its own children, all under the same TTY. Demonstration Across Two Tabs Let’s walk through an example using two terminal tabs: First tab (TTY /dev/ttys000 ): $ tty
/dev/ttys000

$ ps -ef | grep bash
501  87852  87851  0  8:32PM ttys000  0:00.76 -bash Second tab (TTY /dev/ttys001 ): $ tty
/dev/ttys001

$ ps -ef | grep bash
501  8561  8560  0 12:36AM ttys001  0:00.04 -bash Back in the first tab , create and run a simple script: $ vi script.sh       # add: sleep 180
$ chmod +x script.sh
$ ./script.sh The script occupies the shell. In tab one, you’ll now see: $ ps -ef | grep bash
501  87852  87851  0  8:32PM ttys000  0:00.76 -bash
501  8689   87852  0 12:36AM ttys000  0:00.00 /bin/bash ./script.sh Inspect the sleep process : $ ps -ef | grep sleep
501  8690  8689  0 12:36AM ttys000  0:00.00 sleep 180 This hierarchy clearly shows: The parent shell ( -bash ) The script.sh child The sleep grandchild Note Commands built into Bash (built-ins) run in the shell itself and don’t generate new PIDs. External commands and scripts always spawn child processes. Managing Process Lifecycles Press Ctrl+C in the first tab to send SIGINT and terminate the script. Run a job in the background: $ ./script.sh &
[1] 8862 Warning Background jobs started with & still depend on the terminal. Closing the tab kills them. Detach completely using nohup : $ nohup ./script.sh &
[1] 9028
appending output to nohup.out This frees the process from the TTY—closing the terminal won’t stop it. Quick Process Management Reference Action Command Description Run program by name $ cat file.txt Uses $PATH Run by absolute path $ /usr/bin/cat file.txt Full path invocation Run from current directory $ ./script.sh Prefixed with ./ List processes $ ps -ef | grep bash Filters bash processes Terminate (SIGTERM) $ kill PID Graceful shutdown Terminate (SIGKILL) $ kill -9 PID Forceful kill Trace system calls $ strace -Tfp PID Monitor system calls Command Type Creates New PID? Example Commands Shell Built-in No cd , echo External Command Yes ls , sleep Useful Links and References GNU Bash Reference Manual ps(1) Manual Page nohup(1) Manual Page kill(1) Manual Page strace.io Watch Video Watch video content"
Advanced Bash Scripting,devnull,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Streams/devnull,"Advanced Bash Scripting Streams devnull Redirecting standard error (stderr) into standard output (stdout) is a powerful shell idiom that helps you capture or suppress all command output in one place. In this guide, we’ll start with basic redirection operators and build up to the full pattern: 2>&1 By the end, you’ll know how to merge streams, write them to files, and even discard them using /dev/null . 1. Basic redirection with > A single greater‐than symbol sends a file descriptor to a file (or another stream): $ ls > stdout.txt Left of > is the source FD (default is 1, i.e. stdout). Right of > is the destination (often a filename). You can explicitly specify the FD: $ ls 1>stdout.txt    # same as `ls > stdout.txt`
$ ls 2>stderr.txt    # redirect stderr (fd 2) into stderr.txt Note When you omit the FD before > , Bash assumes 1 (standard output). 2. Merging stdout and stderr with &> Bash provides a shorthand to capture both streams at once: $ ls -z &> all-logs.txt
$ cat all-logs.txt
ls: cannot access '-z': No such file or directory Here, &> is equivalent to >file 2>&1 in Bash. 3. Duplicating file descriptors using >&n When & appears on the right side of > , you’re duplicating an FD instead of writing to a filename: $ echo ""warning"" >&2    # send stdout (fd 1) into stderr (fd 2) This is different from &> file.txt , which writes both stdout and stderr into a file. 4. Swapping streams with n>&m You can redirect one FD into another: Send stdout into stderr: $ echo ""warning"" >&2 Send stderr into stdout: $ ls -z 2>&1
ls: cannot access '-z': No such file or directory Warning Order matters when combining redirections. Always redirect stdout first, then redirect stderr into it. 5. Redirecting both streams to a file To capture both stdout and stderr in a single file, use: $ ls -z > file.txt 2>&1 > file.txt sends stdout (fd 1) into file.txt . 2>&1 redirects stderr (fd 2) into wherever stdout is now pointing. 6. Discarding all output with /dev/null If you want a command to produce no output , redirect both streams to /dev/null , the special “black hole” in Unix-like systems: > /dev/null 2>&1 Use this pattern when you need a silent command—no stdout, no stderr. Key takeaways Operator Purpose Example >file Redirect stdout (fd 1) to a file ls > out.txt 2>file Redirect stderr (fd 2) to a file ls -z 2>err.txt &>file Redirect both stdout and stderr to a file cmd &>all.txt n>&m Duplicate FD n into FD m echo ""err"" >&2 2>&1 Merge stderr into stdout cmd >out.txt 2>&1 > /dev/null 2>&1 Discard both stdout and stderr some_command > /dev/null 2>&1 Links and References Bash Redirections (GNU Manual) The Linux /dev/null Explained (StackOverflow) Advanced Bash-Scripting Guide Watch Video Watch video content"
Advanced Bash Scripting,Built in Commands,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Refresher/Built-in-Commands,"Advanced Bash Scripting Refresher Built in Commands Every time you invoke an external binary in a shell script, the shell forks a new process—adding latency and consuming CPU/memory. Bash and other modern shells mitigate this overhead by providing built-in commands that execute inside the shell process. Leveraging built-ins can dramatically speed up your scripts and reduce resource usage. In this guide, we’ll cover: Command categories and types How to identify built-in commands Performance benefits and benchmarks Verifying process creation with strace Listing all built-in commands and keywords The Chef Analogy Think of your shell as a chef in a kitchen. If the chef delegated every simple task—chopping vegetables, stirring soup, plating food—to sous-chefs, the overhead would be enormous. Instead, the chef handles routine tasks directly and only calls for help on specialized jobs. Built-in commands work the same way: the shell “chef” handles them instantly, while external binaries require spawning a separate “assistant” process. Command Categories and Types Shell commands fall into two main categories: Category Description Examples Built-in Command Implemented inside the shell; runs without forking a new process. cd , echo , true External Binary Stored on disk; invoking them forks a new process then uses execve . /bin/ls , /usr/bin/cat Example: $ ls
file1.txt  file2.txt

$ echo ""Hello, world!""
Hello, world! In the first case, ls is an external binary loaded from disk; echo is handled directly by the shell. Identifying Built-ins vs. External Binaries Use the type built-in to check how a command is implemented: $ type echo
echo is a shell builtin

$ type cat
cat is /usr/bin/cat Note You can also use which or command -v , but type gives the most accurate distinction between built-in, keyword, and function. Performance Benefits of Built-ins Benchmarks: true vs /usr/bin/true The true command simply returns a zero exit status. Compare its built-in version to the external binary: $ time true
real    0m0.000s
user    0m0.000s
sys     0m0.000s

$ time /usr/bin/true
real    0m0.009s
user    0m0.001s
sys     0m0.005s Command Execution Times at a Glance Command Built-in (real) External /usr/bin (real) true 0.000s 0.009s echo 0.000s 0.324s Built-ins not only start faster but also complete quicker, especially when called repeatedly in loops. Verifying Process Creation with strace To confirm built-ins don’t fork, trace execve system calls in your current shell: Find your shell’s PID $ pgrep -o bash
56120 Attach strace and filter for execve sudo strace -Tfp $(pgrep -o bash) 2>&1 | grep execve & In another terminal, run a built-in vs an external binary $ echo ""Hello""
Hello
# No execve call appears for echo

$ cat file.txt
[pid 56147] execve(""/usr/bin/cat"", [""cat"",""file.txt""], 0x... ) = 0 <0.000186>
Hello, world! You’ll see execve only for cat , confirming echo runs inside the shell. External vs. Built-in Counterparts Many built-ins have binary counterparts on disk: $ which echo
/bin/echo

$ type /bin/echo
/bin/echo is /bin/echo

$ type echo
echo is a shell builtin Performance comparison: $ time /usr/bin/echo ""Test""
$ time echo ""Test""
# real 0m0.000s Built-in echo completes instantly compared to the external /usr/bin/echo . Listing Built-ins and Keywords List built-in commands: $ compgen -b List shell keywords: $ compgen -k Check if a word is a keyword: $ type time
time is a shell keyword Warning Keywords (like time , if , for ) are parsed by the shell and do not spawn new processes. Confusing them with external binaries can lead to unexpected behavior. For a comprehensive list of shell built-ins and keywords, see the Bash manual: Bash BUILTIN Commands Shell Keywords References GNU Bash Reference Manual Linux strace Documentation Bash Performance Tuning Tips Watch Video Watch video content"
Advanced Bash Scripting,Pipefail,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Streams/Pipefail,"Advanced Bash Scripting Streams Pipefail In this guide, we’ll dive into how Unix-like shells handle pipelines, why errors can be hidden, and how to enforce early exits using set -o pipefail . You’ll learn best practices for robust Bash scripting and see practical examples. How Pipelines Work When you connect commands with a pipe ( | ), each command’s standard output ( stdout ) feeds into the next command’s standard input ( stdin ). However, if a middle command writes to standard error ( stderr ), that error goes straight to your terminal—even though the rest of the pipeline keeps running. Behavior Without pipefail Consider this simple pipeline: $ sort somefile.txt | uniq | cat file.txt
sort: cannot read somefile.txt: No such file or directory
hello What happens here: sort fails (exit code ≠ 0) and emits an error. uniq still runs (no input) and exits successfully. cat file.txt prints its content. Even though sort failed, the pipeline’s final exit status is 0 , which masks the error. Checking Exit Status Inspect the pipeline’s return code with echo $? : $ sort somefile.txt | uniq
sort: cannot read somefile.txt: No such file or directory
$ echo $?
0 Despite the failure, you get 0 . Likewise, boolean operators behave unexpectedly: $ sort somefile.txt | uniq && echo ""Won't stop on error""
sort: cannot read somefile.txt: No such file or directory
Won't stop on error Here, echo still runs because the pipeline exit code is 0 . Enabling pipefail To force a pipeline to return a non-zero status if any command fails, enable pipefail : #!/usr/bin/env bash
set -o pipefail

sort somefile.txt | uniq && echo ""This won't print""
echo ""Exit status: $?"" Save as set-pipefail.sh and execute: $ ./set-pipefail.sh
sort: cannot read somefile.txt: No such file or directory
Exit status: 2 With pipefail : The pipeline returns the exit status of the rightmost failing command. Subsequent commands and && branches are skipped on error. Common Shell Options Option Description Default pipefail Pipeline fails if any command errors off errexit Exit script on any non-zero command ( set -e ) off noclobber Prevent overwriting files via redirection ( set -C ) off Tip Stack each set -o on its own line for clarity: set -o errexit
set -o pipefail
set -o noclobber Adding a Guard Clause Combine pipefail with an exit-on-failure guard: #!/usr/bin/env bash
set -o pipefail

sort somefile.txt | uniq || exit 80 If any pipeline stage fails, the script exits immediately with status 80. Warning Always choose a non-zero exit code that makes sense for your script. Avoid overlapping with common system codes. Combining pipefail with Other Options Here’s a script that prevents file overwrites and enforces pipeline errors: #!/usr/bin/env bash
set -o noclobber
set -o pipefail

echo ""First line"" > file.txt
echo ""Second line"" > file.txt      # Fails due to noclobber
sort somefile.txt | uniq || exit 100

echo ""This line never runs""
exit 0 Run it: $ ./set-pipefail3.sh
$ cat file.txt
First line The second redirect fails, and because of pipefail plus the guard clause, the script exits with code 100. Links and References Bash Reference Manual – Bourne Shell Builtins Advanced Bash-Scripting Guide Stack Overflow: What does set -o pipefail do? GitHub Gist: Bash Exit Codes Watch Video Watch video content"
Advanced Bash Scripting,Overview,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Expansions-Part-One/Overview,"Advanced Bash Scripting Expansions Part One Overview In this guide, we’ll dive into the core shell expansions available in POSIX-compliant shells (like Bash and Zsh). You’ll learn how to use: Brace expansions Parameter expansions Command substitutions Filename generation (globs) These mechanisms let you generate sequences, extract substrings, capture command output, and match multiple filenames—streamlining your shell scripts and command-line workflows. Table of Expansion Types Expansion Type Description Example Brace Generate comma- or range-separated strings echo {A,B,C} → A B C Parameter Manipulate variable values (substrings, defaults) ${var##*/} → strips longest */ prefix Command Substitution Insert command output into another command echo ""Date: $(date +%F)"" Filename Generation Match files with wildcards (globs) ls *.txt → lists all .txt files For an in-depth reference, see the Bash manual on Shell Expansions . 1. Brace Expansion Brace expansion quickly generates arbitrary strings. It’s purely a string generation mechanism—no variables or globs involved. # Generates A, B, C
echo {A,B,C}

# Generates numbers 1 through 5
echo {1..5} Note Brace expansions must not be quoted for the shell to recognize them. For example, echo ""{A,B}"" will literally output {A,B} . 2. Parameter Expansion Parameter expansion lets you inspect or transform variable values without invoking external commands. Basic Variable Expansion USER_HOME=$HOME
echo ""Your home directory is: $USER_HOME"" Removing Directory Components To strip the longest matching prefix (e.g., remove everything up to the last slash), use ##*/ : #!/usr/bin/env bash

some_script=""/usr/bin/my_script.sh""
# Remove the longest match of '*/' from the front
echo ""${some_script##*/}"" Output: $ ./expansions.sh
my_script.sh Warning Always quote your expansions (e.g., ""${var}"" ) to prevent word splitting and globbing in unexpected ways. 3. Command Substitution Command substitution captures the stdout of a command and embeds it in another command’s arguments: current_date=$(date +%Y-%m-%d)
echo ""Today is $current_date"" 4. Filename Generation (Globbing) Globbing uses wildcard patterns to match filenames: # List all .log files
ls *.log

# Recursive match in subdirectories (with Bash extglob)
shopt -s globstar
echo **/*.md Next Steps Now that you’ve seen the major shell expansions, try combining them to simplify your scripts—generate file lists, parse log entries, or batch-rename files with a single command. For more examples and edge cases, consult the GNU Bash Reference Manual . [object Object],"
Advanced Bash Scripting,Variables,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Expansions-Part-One/Variables,"Advanced Bash Scripting Expansions Part One Variables Variable expansion allows you to retrieve and manipulate the value stored in a Bash variable using the $ syntax. This powerful feature is essential for automation, text processing, and script flexibility. Introduction In shell scripting, variables can store filenames, paths, numbers, or strings. By applying parameter expansion, you can: Access variable contents Perform string operations Provide fallback values Calculate lengths and more filename=""file.txt""
number=100
string=""Hello World""
command_output=$(ls)

dir_path=""/path/to/directory""
greeting=""Hello""
name=""User""
echo ""$greeting, $name! Welcome to $dir_path"" Basic Variable Expansion Whenever the shell sees $variable_name , it replaces it with the variable’s value: name=""John Doe""
echo $name
# Output:
# John Doe Using braces helps delimit variable names clearly: name=""John Doe""
echo ""Hello, ${name}""
# Output:
# Hello, John Doe Variable names are case-sensitive and may contain letters, digits, and underscores: # Invalid:
# Valid:
name_user=""John Doe"" Using Curly Braces Curly braces {} not only disambiguate names but also unlock advanced parameter expansion features. Avoiding Ambiguity Without braces, attached text can be misinterpreted: #!/usr/bin/env bash
height=170

# Wrong: shell looks for 'heightcm'
echo ""Your height is: $heightcm""

# Correct: braces isolate 'height'
echo ""Your height is: ${height}cm""
# Output:
# Your height is: 170cm Default Values Provide a fallback when a variable is unset or empty: Note Use ${var:-default} to safely reference a variable that may not exist. #!/usr/bin/env bash
echo ""Hello, ${name:-Unknown}""
# If 'name' is unset:
# Output:
# Hello, Unknown Parameter Expansion Operators Expansion Type Syntax Description Default Value ${var:-default} Use default if var is unset or null Substring Extraction ${var:offset:length} Extract a substring starting at offset String Replacement ${var/pattern/repl} Replace the first match of pattern with repl Length ${#var} Return the length of var ’s value String Manipulation Substring Extraction Pull out part of a string using an offset and length: #!/usr/bin/env bash
name=""John Doe""
echo ""Hello, ${name:0:4}""
# Output:
# Hello, John Substring Replacement Replace the first occurrence of a pattern: #!/usr/bin/env bash
path=""/home/user/file.txt""
echo ""${path/file/data}""
# Output:
# /home/user/data.txt Length of a Variable Determine the number of characters in a variable’s value: #!/usr/bin/env bash
name=""John Doe""
echo ""${#name}""
# Output:
# 8 Quoting and Word Splitting By default, unquoted expansions undergo word splitting. To preserve spaces, quote your variables. If you want to iterate over words, leave them unquoted: #!/usr/bin/env bash
readonly SERVERS=""server1 server2 server3""

# Iterate over each hostname:
for server in $SERVERS; do
  echo ""${server}.kodekloud.com""
done
# Output:
# server1.kodekloud.com
# server2.kodekloud.com
# As a single string when quoted:
for server in ""$SERVERS""; do
  echo ""${server}.kodekloud.com""
done
# Output:
# server1 server2 server3.kodekloud.com Warning Always quote expansions containing spaces unless you explicitly need word splitting. Additional Resources GNU Bash Manual – Shell Parameter Expansion Bash Reference Manual Understanding Quoting in Bash Watch Video Watch video content"
Advanced Bash Scripting,Functions,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Refresher/Functions,"Advanced Bash Scripting Refresher Functions In this lesson, you’ll learn how Bash functions help you structure, reuse, and maintain your scripts. While Bash offers conditionals, loops, and script sourcing, functions are key to modular, readable code. Why Define Functions in Bash? Functions encapsulate a sequence of commands into a single callable unit. This reduces repetition, minimizes errors, and makes your scripts easier to update and test. Imagine a chef perfecting a recipe once and reusing it whenever needed—functions work the same way in scripting. Backup Script: Before vs. After Without a function: #!/bin/bash

mkdir backup
cd backup
cp -r ""${1}"" .
tar -czvf backup.tar.gz *
echo ""Backup complete!"" Refactored with a function: #!/bin/bash

perform_backup() {
    mkdir -p backup
    cd backup || exit 1
    cp -r ""${1}"" .
    tar -czvf backup.tar.gz *
    echo ""Backup complete!""
}

perform_backup ""${1}""
exit 0 Warning Always use mkdir -p to avoid errors if the directory already exists, and add || exit 1 after cd to stop the script on failure. Refactoring a Git Clone Example Grouping related tasks into functions clarifies your script’s main flow. Script Version Content Ad-hoc bash<br># Clone and count files<br>git clone ""${1}""<br>find . -type f | wc -l Refactored bash<br>git_url=""${1}""<br><br>clone_git() {<br>  git clone ""${1}""<br>}<br><br>count_files() {<br>  find . -type f | wc -l<br>}<br><br>clone_git ""${git_url}""<br>count_files By naming clone_git and count_files , you isolate logic and make testing easier. Function Declaration Syntax Bash supports two portable styles. Use the first for maximum compatibility: Style Syntax Example Preferred (POSIX-compatible) bash<br>my_function() {<br>  echo ""Hello from my_function""<br>}<br> Using function keyword bash<br>function my_function {<br>  echo ""Hello from my_function""<br>}<br> You can even define and call a function inline: $ hello() { echo ""Hi there""; }
$ hello
Hi there Local Variables in Functions Limit variable scope inside functions with local to avoid unintended side effects. Example where var1 is not visible outside: #!/bin/bash

my_function() {
    local var1=""Hello""
}

my_function
echo ""${var1}""  # No output Example printing the local variable: #!/bin/bash

my_function() {
    local var1=""Hello""
    echo ""${var1}""
}

my_function  # Outputs: Hello Note Using local helps prevent variable collisions in larger scripts. For more details, see Bash Scripting Guide . Benefits of Using Functions Organization : Break large scripts into logical units. Reusability : Call the same code multiple times without duplication. Readability : Name complex logic for better clarity. Maintainability : Update one function rather than many code blocks. Links and References GNU Bash Reference Manual LinuxCommand.org: Bash Functions Stack Overflow: Bash Function Best Practices Watch Video Watch video content Practice Lab Practice lab"
Advanced Bash Scripting,File descriptors,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Streams/File-descriptors,"Advanced Bash Scripting Streams File descriptors On the command line, the numbers 1 and 2 refer to standard output and standard error—these are examples of file descriptors . In this guide, you will learn: Additional file descriptor numbers beyond 0, 1, and 2 The precise definition of a file descriptor Why the term “file” descriptor applies to streams like pipes and sockets A file descriptor is a nonnegative integer that indexes an open I/O resource in the operating system, such as a disk file, network socket, or pipe. Although the name originates from early Unix file-handling, modern OSes treat sockets, pipes, and devices the same way. Reserved Standard Streams By convention on Linux and most Unix-like systems, three file descriptors are preallocated for the main I/O streams: Descriptor Number Stream Description 0 stdin Standard input 1 stdout Standard output 2 stderr Standard error Any descriptor ≥ 3 can reference additional resources: open files, sockets, pipes, or terminal devices. Note File descriptors are assigned per process. When a program opens a new file, the OS returns the smallest unused descriptor number (starting at 3). Real-World Analogy: Librarian and Call Numbers Imagine a library where: The librarian is your operating system Books are files, sockets, or pipes Call numbers on the books are file descriptors When you hand a call number (descriptor) to the librarian, they fetch the corresponding book (I/O resource). This models how your shell uses descriptors to route input/output. Standard Streams Mapped to Descriptors Applying the library analogy to standard streams: stdin (0): Asking the librarian which book you want stdout (1): Receiving the book from the librarian stderr (2): Getting an error message if the book is unavailable Just as a library holds many books, a running process can have dozens or even thousands of open descriptors (3, 4, 5, …). Further Reading and References Advanced Bash-Scripting Guide: Redirection Unix File Descriptor Tutorial Bash Manual: Redirections Watch Video Watch video content"
Advanced Bash Scripting,Overview,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Streams/Overview,"Advanced Bash Scripting Streams Overview In this lesson, we’ll dive into Linux streams and learn how to manipulate them in shell scripting. Linux streams are the flow of data between processes—much like water flowing through a pipe. Just as you can redirect a river with gates, you can reroute streams in the shell using redirection operators and pipes. Understanding streams is essential for robust shell scripts. You’ll see how to capture command output, handle errors, and chain commands with pipes to build powerful one-liners. Standard Streams in Linux Every Linux process is born with three standard file descriptors: File Descriptor Stream Name Description Default Source / Destination 0 stdin Reads input (keyboard or another stream) Keyboard or pipe 1 stdout Writes normal output Terminal or redirected file 2 stderr Writes error messages and diagnostics Terminal or redirected file Standard input (fd 0) typically reads from your keyboard Standard output (fd 1) displays command results Standard error (fd 2) sends error messages Note stdout and stderr both default to your terminal. Redirecting one does not affect the other unless you explicitly combine them. Redirecting Streams You can reroute streams using < , > , and pipes ( | ). This allows you to save output to files, read input from files, or chain commands together. # Redirect stdout to a file
grep ""error"" logfile.txt > results.txt

# Redirect stderr to a file
gcc program.c 2> compile_errors.log

# Send both stdout and stderr to the same file
./run_tests.sh > all_output.log 2>&1

# Read stdin from a file
sort < unsorted_list.txt

# Pipe stdout of one command into stdin of another
ps aux | grep sshd Warning When combining redirections, order matters. Always place 2>&1 after > file to capture both streams. Examples Capture command output and errors separately: ls -l /some/path > listing.txt 2> errors.txt Chain commands using pipes to filter data: cat server.log | grep ""WARN"" | sort | uniq -c Use input redirection to feed a script: bash < setup_script.sh Links and References GNU Bash Redirections Linux Input/Output Linux Streams and Pipes Further Reading Topic Description Link Bash Scripting Comprehensive guide to Bash syntax https://www.gnu.org/software/bash/manual/ Advanced Pipelines Building complex command pipelines https://www.linuxjournal.com/content/beauty-pipelines File Descriptors Low-level I/O in Unix/Linux https://opensource.com/article/18/4/introduction-file-descriptors Mastering streams and redirection will elevate your shell scripting from basic commands to automation powerhouses. Practice these techniques to handle data flows confidently in your scripts. Watch Video Watch video content"
Advanced Bash Scripting,Command Line Arguments,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Refresher/Command-Line-Arguments,"Advanced Bash Scripting Refresher Command Line Arguments Command-line arguments are essential in shell scripting for creating flexible, reusable scripts. Instead of hard-coding values, you can accept inputs at runtime—much like using a remote control to switch channels on a TV. This guide covers everything from basic positional parameters to advanced iteration techniques. Table of Contents Understanding Positional Parameters Practical Example: Cloning and Counting Files Parameterizing Your Script Common Special Variables Handling Maximum Argument Size ( ARG_MAX ) Iterating with shift References Understanding Positional Parameters When you invoke a script with arguments: $ ./myscript.sh foo bar baz Inside myscript.sh , the inputs map to: #!/bin/bash
echo ""First argument: $1""
echo ""Second argument: $2""
echo ""Third argument: $3"" Output: First argument: foo
Second argument: bar
Third argument: baz Best Practice Always quote your positional parameters to handle spaces and special characters safely: echo ""User provided: $1"" Practical Example: Cloning and Counting Files Suppose you need to clone a Git repository and count its files. A hard-coded approach looks like this: #!/bin/bash

git clone [email protected] :kodedkloud/kodekloud-advanced-shell-scripting.git
find . -type f | wc -l This works but requires editing the script for each repository URL. Parameterizing Your Script By using $1 , you can pass the repository URL when running the script: #!/bin/bash

# Clone the repository passed as the first argument
git clone ""$1""

# Count files in the cloned repo
find . -type f | wc -l Run it as: $ ./clone-and-count.sh [email protected] :kodedkloud/kodekloud-advanced-shell-scripting.git Now the script clones any repository you specify. Common Special Variables Variable Description Example Output $0 Script name ./myscript.sh $1 , $2 , … First, second, … arguments apple banana $# Total number of arguments 3 $@ All arguments as separate words apple banana cherry $* All arguments as a single word ( ""$*"" joins) apple banana cherry Example: #!/bin/bash
echo ""Script name: $0""
echo ""Number of arguments: $#""
echo ""All arguments (\$@): $@"" $ ./myscript.sh apple banana cherry
Script name: ./myscript.sh
Number of arguments: 3
All arguments ($@): apple banana cherry Handling Maximum Argument Size ( ARG_MAX ) Unix-like systems impose a limit on the total size of command-line arguments. Check it with: $ getconf ARG_MAX
1048576 On most Linux distributions, ARG_MAX is around 1 MiB, which is sufficient for tens of thousands of small arguments. Warning Exceeding ARG_MAX will cause a “Argument list too long” error. For bulk operations, consider using xargs or reading from a file. Iterating with shift The shift command discards $1 and shifts all other parameters down by one. This is useful when you don’t know the number of arguments in advance: #!/bin/bash

# Loop through all arguments
while [ $# -gt 0 ]; do
  echo ""Current argument: $1""
  shift
done $ ./shift-example.sh arg1 arg2 arg3
Current argument: arg1
Current argument: arg2
Current argument: arg3 Command-line arguments empower you to build dynamic, user-driven shell scripts. Next up: advanced option parsing with getopts and long-form flags. References Bash Reference Manual getconf find(1) — Find Files wc(1) — Word, Line, Character, and Byte Count Watch Video Watch video content Practice Lab Practice lab"
Advanced Bash Scripting,Guard clause,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Refresher/Guard-clause,"Advanced Bash Scripting Refresher Guard clause Guard clauses are a proven pattern in Shell scripting to handle error cases upfront—exiting when preconditions aren’t met—so that the main logic remains flat, readable, and maintainable. By bailing out early on failures, you avoid deeply nested conditionals and reduce the risk of bugs. Note Guard clauses help you validate inputs, permissions, or environment states at the top of your script. This keeps the primary workflow free from repetitive checks. 1. Simple if-else Example A straightforward file existence check using an if-else block: #!/bin/bash
# Check if a file exists
if [[ -e myfile.txt ]]; then
    echo ""File exists""
else
    echo ""File does not exist""
fi 2. The Pitfall of Nested Conditionals When you chain several checks, your script can become difficult to read and maintain: #!/bin/bash
# Bad example: many nested checks
if [[ ""${USER_NAME}"" == ""admin"" ]]; then
    if [[ -e ""${FILE_PATH}"" ]]; then
        if [[ -s ""${FILE_PATH}"" ]]; then
            run_process
        else
            echo ""File exists but is empty""
        fi
    else
        echo ""File does not exist""
    fi
else
    echo ""User is not admin""
fi
exit 0 3. Refactoring with Guard Clauses First, declare constants and helper functions: #!/bin/bash
readonly FILE_PATH=""/home/ubuntu/guard_clause/file.txt""
readonly USER_NAME=""admin""

run_process() {
    echo ""running process...""
} Next, validate each precondition at the top. Exit immediately on failure: #!/bin/bash
readonly FILE_PATH=""/home/ubuntu/guard_clause/file.txt""
readonly USER_NAME=""admin""

run_process() {
    echo ""running process...""
}

# Guard clauses
if [[ ""${USER_NAME}"" != ""admin"" ]]; then
    echo ""User is not admin""
    exit 1
fi

if [[ ! -e ""${FILE_PATH}"" ]]; then
    echo ""File does not exist""
    exit 1
fi

if [[ ! -s ""${FILE_PATH}"" ]]; then
    echo ""File exists but is empty""
    exit 1
fi

# Main logic
run_process
exit 0 Notice how each check uses a negative test ( != , ! -e , ! -s ) to bail out early. Only when all conditions pass does the script reach run_process . 4. Guard Clause for Command-Line Arguments You can apply the same pattern to verify script arguments. Here’s a basic example that ensures an argument is provided before cloning a Git repository: #!/bin/bash
# Bail out if no argument is supplied
if [[ -z ${1} ]]; then
    echo ""Usage: $0 <git-repo-url>""
    exit 1
fi

git_url=""${1}""

clone_git() {
    git clone ""${git_url}""
}

clone_git
exit 0 Warning Always validate user input to prevent unwanted behavior or security issues. Exiting early helps you avoid partial or corrupted operations. 5. One-Line Guard Clauses with Logical Operators Bash’s && and || allow you to write compact guard clauses: OR ( || ) : Execute the right side if the left side fails [[ -f ""file.txt"" ]] || echo ""file does not exist"" AND ( && ) : Execute the right side if the left side succeeds [[ -z ${1} ]] && echo ""argument empty"" To include an exit in a one-liner, group commands with { …; } : [[ -f ""file.txt"" ]] || { echo ""file does not exist""; exit 1; } Common Guard Clause Patterns Operator Behavior Example ` ` && Run right side only if left side succeeds [[ ${USER} == root ]] && echo ""Running as root"" ! Negates the test, useful for early exits [[ ! -r file ]] && { echo ""Cannot read file""; exit 1; } Links and References Bash Reference Manual ShellCheck Linter Advanced Bash-Scripting Guide Watch Video Watch video content"
Advanced Bash Scripting,Pipes,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Streams/Pipes,"Advanced Bash Scripting Streams Pipes In Bash shell scripting, pipes allow you to direct the output of one command into another, enabling powerful workflows without intermediate files. This guide covers two primary pipe types: Pipe Type Symbol/Command Description Named Pipe mkfifo / FIFO Creates a persistent file endpoint for inter-process communication. Anonymous Pipe ` ` Named Pipes (FIFOs) Note Named pipes, also known as FIFOs, are special files you create with mkfifo . They let processes communicate through a named file path. Here, < and > redirect standard input and output to or from file resources, sometimes referred to as file-based piping. Below is an example that uses both input and output redirection: Suppose abc.txt contains five unsorted letters. You can sort them and save the results: $ sort < abc.txt > abc_sorted.txt
$ cat abc_sorted.txt
a
b
c
d
e < abc.txt feeds the contents of abc.txt into sort . > abc_sorted.txt writes the sorted output to abc_sorted.txt . cat abc_sorted.txt displays the sorted list. Anonymous Pipes Anonymous pipes use the | symbol to pass commands’ output directly as input to the next command, without creating files. Think of a pipeline like an assembly line: The first command produces data. Each subsequent command processes the incoming data and passes it along. The final output appears in your terminal. Example: Filtering and Sorting a List Given animals.txt containing unsorted names (with duplicates): $ cat animals.txt
Frog
Cheetah
Elephant
Giraffe
Antelope
Bear
Deer
Iguana
Jaguar
Hippopotamus
Ostrich
Frog
Cheetah
Elephant
Giraffe
Antelope
Bear
Deer
Iguana
Jaguar
Hippopotamus Step 1: Filter names containing “a” (case-insensitive): $ cat animals.txt | grep -i ""a""
Cheetah
Elephant
Giraffe
Antelope
Bear
Iguana
Jaguar
Hippopotamus
Cheetah
Elephant
Giraffe
Antelope
Bear
Iguana
Jaguar
Hippopotamus Step 2: Further filter for “o” and sort alphabetically: $ cat animals.txt | grep -i ""o"" | sort
Antelope
Antelope
Frog
Frog
Hippopotamus
Hippopotamus
Ostrich cat animals.txt reads the list. grep -i ""o"" selects lines with “o” (case-insensitive). sort orders the filtered names. You can extend pipelines by appending more commands after each | , letting each stage transform the data in sequence. Handling Errors in Pipelines By default, pipelines only pass standard output (stdout) to the next command. Errors (stderr) are displayed directly on the terminal, and the pipeline continues. $ ls -z | echo ""Hello world""
Hello world
ls: unknown option -- z
Try 'ls --help' for more information Warning Errors are not piped between commands. Use redirection like 2>&1 if you need to capture stderr in your pipeline. Links and References Bash Reference Manual: Pipelines and I/O Redirection Linux mkfifo Command GNU grep Documentation Sorting Text Files with sort Watch Video Watch video content"
Advanced Bash Scripting,Shebang,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Refresher/Shebang,"Advanced Bash Scripting Refresher Shebang In this lesson, we’ll explore how the shebang ( #! ) directive affects script execution and portability. You will learn to: Remove the shebang from a script and analyze its behavior. Trace system calls with strace to observe kernel execution. Demonstrate shebangs across different shells (e.g., Bash, C shell). Adopt a modern, portable shebang and avoid common pitfalls. The Shebang Analogy Think of yourself as a polyglot translator facing an ancient manuscript. Each dialect (shell) has subtle differences. A shebang acts like a translator’s guide, ensuring your script is read by the intended interpreter. Without it, your login shell takes over, which may lead to unexpected behavior and portability issues. Example of a classic shebang: #!/bin/bash The term shebang blends “hash” ( # , 0x23) with “bang” ( ! , 0x21), sometimes pronounced “shbang.” What Happens Without a Shebang? Create noshebang.sh without a shebang: # noshebang.sh
WORDS=""I don't have a shebang and I still run""
for word in ${WORDS}; do
  if [[ 2 < 3 ]]; then
    echo ""${word}""
  fi
done Make it executable and run: chmod +x noshebang.sh
./noshebang.sh
# Each word prints because Bash (your login shell) interprets it
echo $SHELL
# /bin/bash Relying on the parent shell works locally but isn't portable—other environments may default to /bin/sh , which lacks Bash-specific features. Tracing Kernel Execution with strace Compare a script without and with a shebang: # shebang.sh
#!/bin/bash
WORDS=""I don't have a shebang and I still run""
for word in ${WORDS}; do
  if [[ 2 < 3 ]]; then
    echo ""${word}""
  fi
done Trace your shell’s PID ( $$ ) in the background: sudo strace -Tfp $$ 2>&1 | grep -E 'execve' & Run both scripts: ./noshebang.sh
./shebang.sh
# [pid …] execve(""./shebang.sh"", [""./shebang.sh""], …) = 0
# Script runs successfully under /bin/bash When the kernel detects #! followed by a valid interpreter, it invokes that program directly. Using a Different Shell: C Shell Example C shell ( csh ) syntax is distinct. Running a C shell script under Bash without a shebang will fail: # is_csh.sh
set x = 'a'
if ($x == 'a') then
    echo ""running on a c shell csh""
endif chmod +x is_csh.sh
./is_csh.sh
# -bash: ./is_csh.sh: /bin/csh: syntax error: unexpected end of file Add the correct shebang: #!/bin/csh
set x = 'a'
if ($x == 'a') then
    echo ""running on a c shell csh""
endif ./is_csh.sh
# running on a c shell csh Modern, Portable Shebang Hardcoding interpreter paths can break across systems. Instead, use: #!/usr/bin/env bash This invokes env to locate bash via your PATH , enhancing cross-platform compatibility. Note Using #!/usr/bin/env bash avoids assumptions about interpreter locations, but it relies on env being in /usr/bin . Shebang Line Description Pros & Cons #!/bin/bash Direct path to Bash Fast invocation, but not portable if Bash is installed elsewhere. #!/usr/bin/env bash Finds Bash in PATH via env Portable across environments, depends on a correct PATH . Demo: Bash Version Features Create bash_versions.sh : #!/usr/bin/env bash
echo ""Current Unix timestamp (integer): ${EPOCHSECONDS}""
echo ""Current Unix timestamp (floating-point): ${EPOCHREALTIME}"" On macOS default Bash (v3): bash --version
./bash_versions.sh
# Current Unix timestamp (integer):
# Current Unix timestamp (floating-point): After upgrading to Bash 5.2 (e.g., via Homebrew) and updating your PATH : bash --version
./bash_versions.sh
# Current Unix timestamp (integer): 1679282700
# Current Unix timestamp (floating-point): 1679282700.176761 By leveraging #!/usr/bin/env bash , you automatically use the most appropriate Bash installed on your system. Caveats & Recommendations Minimal environments (e.g., BusyBox) may not include bash . Verify available interpreters in /etc/shells : cat /etc/shells
# /bin/sh
# /bin/bash
# /sbin/nologin
… Warning If /usr/bin/env or your chosen shell isn’t available, scripts will fail. Always confirm interpreter paths before deployment. Select a shebang that aligns with your target environment and installed shells. For all remaining course examples, we’ll use: #!/usr/bin/env bash Adjust this line if your system requires a different interpreter path. Links and References GNU Bash Manual Shebang (Wikipedia) strace Documentation env Command Help Watch Video Watch video content Practice Lab Practice lab"
Advanced Bash Scripting,Heredocs,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Streams/Heredocs,"Advanced Bash Scripting Streams Heredocs Heredocs let you include multi-line text or commands directly within a Bash script, avoiding repetitive echo statements and manual editing. They’re invaluable for generating configuration files, embedding SQL queries, or executing batches of commands over SSH—especially in non-interactive environments where editors like Vim or Nano aren’t available. Note A Heredoc sends an entire block of text as input to a command. This keeps your scripts clean and reduces the risk of accidental overwrites when using > vs. >> . Why Choose Heredocs Over Multiple echo Commands Using individual echo lines quickly becomes error-prone as your script grows: echo ""line1"" > file.txt   # creates or overwrites file.txt
echo ""line2"" >> file.txt  # appends to file.txt
echo ""line3"" >> file.txt  # appends to file.txt If you mistakenly use > instead of >> , you can overwrite your file. Heredocs treat the block as a single unit of input, eliminating this risk. Basic Heredoc Syntax Feed a block of text into any command—here, cat —using the <<DELIM operator: cat <<EOF
line1
line2
line3
EOF Output: line1
line2
line3 To write directly to a file, redirect the command’s output: cat > file.txt <<EOF
line1
line2
line3
EOF Tips for smooth usage: The closing delimiter must match exactly (case-sensitive) and have no leading spaces. Common delimiters include EOF , EOM , or a custom token of your choice. cat <<CUSTOM
alpha
beta
gamma
CUSTOM Warning If you mistype the closing token, the shell will continue to prompt for input until you either provide the correct delimiter or cancel with Ctrl+C . Common Use Cases for Heredocs Use Case Example Command Generate configuration files cat > app.conf <<EOF ... EOF Embed SQL scripts psql <<SQL ... SQL Batch SSH commands ssh user@host <<EOF ... EOF Create Dockerfiles on the fly docker build - <<EOF ... EOF Quick Checklist: Heredoc Benefits One-liner for simple operations Embed large blocks of text without quotes Support complex scripting scenarios Remote Command Execution via SSH You can run multiple commands on a remote host in one go. First, check your remote directory: ssh [email protected] ls
# Example output:
# guard_clause  pid  shebang Next, supply commands through a Heredoc: ssh [email protected] <<EOF
mkdir -p ~/heredocs
echo ""Sample content for Heredocs file"" > ~/heredocs/heredocsfile.txt
EOF Verify the result: ssh [email protected] <<EOF
ls ~/heredocs
cat ~/heredocs/heredocsfile.txt
EOF Expected output: heredocsfile.txt
Sample content for Heredocs file Here Strings for Single-Line Input When you need to pass a single line of text into a command, a Here String ( <<< ) is more concise: export MESSAGE=""Hello from Here String""
cat <<<""$MESSAGE""

# Or directly:
cat <<<""Hello World"" Here Strings avoid quoting headaches and extra redirection symbols for small data payloads. Links and References Bash Scripting Guide SSH Official Site Docker Documentation Kubernetes Official Docs Feel free to explore these resources to deepen your understanding of shell scripting techniques and automation workflows. Watch Video Watch video content Practice Lab Practice lab"
Advanced Bash Scripting,Xargs,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Streams/Xargs,"Advanced Bash Scripting Streams Xargs xargs is a powerful GNU utility that transforms piped data into arguments for another command. Instead of reading from stdin and writing to stdout like typical pipelines, xargs gathers items and appends them as parameters—enabling more flexible shell scripting and one-liners. Table of Contents Piping Basics How xargs Works Common Use Cases Handling Special Characters Additional Resources Piping Basics Most shell utilities can read from stdin or files. For example, to count words: echo ""How many words are in this text?"" | wc -w
# 7 You can redirect or pipe interchangeably: sort file.txt
sort < file.txt

cat file.txt
cat < file.txt By contrast, xargs acts like a “bucket”—it collects output from a previous command and then invokes another command, passing those collected items as arguments. How xargs Works Assume file.txt contains: file
content
to demonstrate
xargs functionality Piping to echo without xargs preserves newlines in the input stream but not in output: cat file.txt | xargs echo
# file content to demonstrate xargs functionality Under the hood, xargs : Reads whitespace (spaces, tabs, newlines) by default. Constructs a single command line by concatenating all items. Executes that command: echo file content to demonstrate xargs functionality Common Use Cases Supplying Arguments to Commands Commands like rm , ls , mkdir or even custom scripts require positional arguments. Instead of writing loops, xargs can automate this: # Prepend a custom message to file.txt contents
cat file.txt \
  | xargs echo ""The contents of file.txt passed by xargs are:""
# The contents of file.txt passed by xargs are: file content to demonstrate xargs functionality Creating Multiple Directories Generate directories from a whitespace-separated list: echo ""dir1 dir2 dir3"" \
  | xargs mkdir
ls
# dir1  dir2  dir3 Table of Handy xargs Examples Use Case Command Example Remove log files find . -name '*.log' | xargs rm -f Create directories echo ""a b c"" | xargs mkdir Parallel SSH sessions cat hosts.txt | xargs -P4 -I{} ssh {} hostname Handling Special Characters Warning By default, xargs splits on any whitespace. Filenames containing spaces or special characters may break. Use -0 with NUL-separated data (e.g., find . -print0 \| xargs -0 ) to handle arbitrary names safely. Additional Resources GNU xargs Manual Bash Guide: Process Substitution & xargs Linux find + xargs Examples Watch Video Watch video content"
Advanced Bash Scripting,Exit code,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Streams/Exit-code,"Advanced Bash Scripting Streams Exit code Applications and commands in Unix return an exit status—a numerical code indicating success or failure. By convention, a zero exit code means success, while any non-zero code signals an error. Understanding these codes is crucial for writing robust shell scripts and ensuring predictable automation. Analogy: The Food Delivery Driver Consider a food delivery driver verifying an address before dispatch. If the address is correct, the driver proceeds. If it’s wrong, they return with an error report. This mirrors how scripts and commands operate: they check conditions, perform tasks, and then report their status. How Exit Codes Work in Unix Every Unix-based command finishes with an exit code: 0 : Success Non-zero : An error occurred Common Exit Codes Table Exit Code Description 0 Success 1 General error 2 Misuse of shell builtins 126 Command invoked cannot execute 127 Command not found 128+ Fatal error (invalid argument to exit) Warning Exit codes are constrained to the range 0–255. Any value above 255 wraps around modulo 256. Certain exit codes are reserved by the shell or operating system. Defining your own codes (above 2) helps callers distinguish between different failure modes. Custom Exit Codes in Your Script Below is a template that checks for a configuration file and terminates with meaningful exit statuses. #!/usr/bin/env bash

export CONF_FILE=""/var/tmp/file.conf""

terminate() {
    local message=""$1""
    local code=""${2:-1}""
    echo ""${message}"" >&2
    exit ""${code}""
}

echo ""Starting script execution""
echo ""Sourcing configuration file""
if [[ ! -f ""${CONF_FILE}"" ]]; then
    terminate ""Configuration file not found: ${CONF_FILE}"" 2
fi

exit 0 Note Use clear, documented exit codes in your scripts. This makes it easier for users and other scripts to handle errors automatically. References GNU Bash Manual: Exit Status Linux Shell Exit Codes Watch Video Watch video content Practice Lab Practice lab"
Advanced Bash Scripting,Parameter Part One,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Expansions-Part-One/Parameter-Part-One,"Advanced Bash Scripting Expansions Part One Parameter Part One In Bash scripting, parameter expansion lets you transform variable values using patterns inside ${} . Previously, we replaced “file.txt” with “data” in a path: #!/usr/bin/env bash
path=""/home/user/file.txt""
echo ""${path/file.txt/data}""
# Output: /home/user/data In this lesson, we’ll focus on pattern matching and how to remove prefixes and suffixes from strings using Bash parameter expansion. Patterns in Bash A pattern is a sequence of characters recognized by the shell for matching or substitution. You’ll find patterns in file globbing (e.g., *.txt ), text processing, and data validation. In Bash, patterns power parameter expansion, letting you transform variable content without external tools. Variable Expansion vs. Parameter Expansion By default, ${var} simply expands to its value: name=""John Doe""
echo ""Hello, ${name}""
# Hello, John Doe When you include operators like # or % inside the braces, Bash invokes parameter expansion, applying pattern-based modifications to the variable’s value. Note Always enclose the expression in double quotes ( ""${...}"" ) to preserve spaces and prevent word splitting. Removing Prefixes and Suffixes Bash supports removing the shortest matching prefix or suffix from a string with ${var#pattern} and ${var%pattern} . Understanding Prefix and Suffix Consider these job titles in an IT company: Prefixes (associate, senior, junior, mid) indicate tenure or level. The suffix ( Engineer ) indicates the job field. Prefix Removal with The # operator deletes the shortest matching pattern from the start of the string: Example 1: Remove the leading H : greetings=""Hello World""
echo ""${greetings#H}""
# Output: ello World If the pattern isn't found at the beginning, the value remains unchanged: echo ""${greetings#e}""
# Output: Hello World Example 2: Remove the word “Hello ” (including the space): echo ""${greetings#Hello }""
# Output: World Patterns are case-sensitive: ""h"" won’t match ""H"" . echo ""${greetings#h}""
# Output: Hello World Suffix Removal with % The % operator removes the shortest matching pattern from the end of the string: Example 1: Drop the trailing d : echo ""${greetings%d}""
# Output: Hello Worl Example 2: Remove “rld” at the end: echo ""${greetings%rld}""
# Output: Hello Wo If the suffix doesn't match exactly, the string stays the same: echo ""${greetings%world}""
# Output: Hello World Warning Pattern matching in Bash is case-sensitive. Ensure your pattern matches the exact case of the prefix or suffix. Quick Reference Table Operator Action Example ${var#pattern} Remove shortest prefix match ${greetings#Hello } → World ${var%pattern} Remove shortest suffix match ${greetings%rld} → Hello Wo Next Steps In the next lesson, we'll explore longest -match removals using ## and %% to strip more complex patterns. References Bash Reference Manual: Shell Parameter Expansion Advanced Bash-Scripting Guide Watch Video Watch video content"
Advanced Bash Scripting,Parameter Part Two,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Expansions-Part-One/Parameter-Part-Two,"Advanced Bash Scripting Expansions Part One Parameter Part Two In Part One, we covered basic parameter‐expansion techniques for stripping fixed prefixes or suffixes. Here, we’ll explore more flexible patterns using wildcards to handle arbitrary extensions, path segments, or words in a string. Removing Fixed Prefixes and Suffixes Often in Linux you work with file paths and extensions: Component Example 1 Example 2 Example 3 Prefix /home/my_username/ /home/my_username/ /usr/bin/ Name text_file text_file2 app.py Suffix .txt .txt .py A fixed‐suffix removal like ${var%.txt} only matches when the filename actually ends in .txt : $ var=""/home/my_username/text_file.txt""
$ echo ""${var%.txt}""
/home/my_username/text_file

$ var=""/home/my_username/text_file2.txt""
$ echo ""${var%.txt}""
/home/my_username/text_file2

$ var=""/usr/bin/app.py""
$ echo ""${var%.txt}""
/usr/bin/app.py Note Using ${var%.txt} leaves .py files untouched. For arbitrary extensions or dynamic patterns, you’ll need wildcards. Using Wildcards for General Cases By introducing * in the pattern, you can remove everything up to or after a delimiter (space, slash, dot, etc.). Strip the First Word For a space-separated string, ${var#* } removes the shortest match from the front (everything up to the first space): export position1=""Senior Cloud Architect""
export position2=""Senior DevOps Engineer""
export position3=""Associate Frontend Engineer""
export position4=""Junior Software Developer""

echo ""${position1#* }""   # Cloud Architect
echo ""${position2#* }""   # DevOps Engineer
echo ""${position3#* }""   # Frontend Engineer
echo ""${position4#* }""   # Software Developer Strip the Last Word Using ${var% *} removes the shortest match from the end (from the last space onward): echo ""${position1% *}""   # Senior Cloud
echo ""${position2% *}""   # Senior DevOps
echo ""${position3% *}""   # Associate Frontend
echo ""${position4% *}""   # Junior Software Wildcard Literal Pairing Always pair * with a literal character (e.g., space or slash). A pattern like ${var%*} matches the entire string, returning an empty result. Handling Unix‐Style Filenames Consider two variables: my_text_file=""/home/my_username/text_file.txt""
my_python_file=""/usr/bin/app.py"" Prefix : directory path Name : file name Suffix : extension Remove All Directory Components ${var#*/} strips up to the first slash ${var##*/} strips up to the last slash (longest‐prefix removal) echo ""${my_text_file#*/}""    # home/my_username/text_file.txt
echo ""${my_python_file#*/}""  # usr/bin/app.py

echo ""${my_text_file##*/}""   # text_file.txt
echo ""${my_python_file##*/}"" # app.py Strip File Extension Use shortest‐suffix ( % ) or longest‐suffix ( %% ). With a single dot, both behave identically: echo ""${my_text_file%.*}""   # /home/my_username/text_file
echo ""${my_text_file%%.*}""  # /home/my_username/text_file

echo ""${my_python_file%.*}""   # /usr/bin/app
echo ""${my_python_file%%.*}""  # /usr/bin/app Choosing the Right Operator Operator Description Example ${var#…} Remove shortest prefix match Strip up to first delimiter ${var##…} Remove longest prefix match Strip up to last delimiter ${var%…} Remove shortest suffix match Remove first occurrence from end ${var%%…} Remove longest suffix match Remove all occurrences from end Use shortest‐suffix ( % ) for extensions. Use longest‐prefix ( ## ) for directory paths. With these four operators, you can tailor string manipulations to filenames, paths, or any delimited data. Links and References Bash Parameter Expansion Kubernetes Basics Docker Hub Watch Video Watch video content Practice Lab Practice lab"
Advanced Bash Scripting,Stdouterr,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Streams/Stdouterr,"Advanced Bash Scripting Streams Stdouterr In this lesson, we dive into advanced mechanics of standard output (stdout) and standard error (stderr) in Bash. Building on basic shell scripting, mastering these streams—especially when paired with redirection and pipelines—is essential for robust, complex scripts. What Are Stdout and Stderr? When you execute a command, it sends data to: Standard Output (stdout) : the default channel for normal output. Standard Error (stderr) : the default channel for error and diagnostic messages. Both streams appear in your terminal by default, but they serve different purposes. Think of them as two separate pipes under your sink: clean water (stdout) versus wastewater (stderr). Keeping them distinct helps you handle success and failure conditions independently. Examples of Stdout vs. Stderr A simple ls shows stdout: $ ls
Documents  noshebang.sh  test.sh With a long listing format ( -l ): $ ls -l
total 8
-rw-rw-r-- 1 ubuntu ubuntu    0 Mar 12 17:14 Documents
-rwxrwxr-x 1 ubuntu ubuntu   46 Mar  4 08:54 noshebang.sh
-rw-rw-r-- 1 ubuntu ubuntu    1 Mar 12 17:14 test.sh If you pass an invalid flag ( -j ), it emits stderr: $ ls -j
ls: invalid option -- 'j'
Try 'ls --help' for more information. Not all commands print to stdout by default. For example, mv works silently unless you use -v : $ mv file.txt renamedfile.txt
$ mv -v file.txt renamedfile.txt
file.txt -> renamedfile.txt If mv can’t find a file, it writes to stderr: $ mv missing.txt dest.txt
mv: cannot stat 'missing.txt': No such file or directory Why It Matters Separating stdout and stderr allows you to log normal output separately from errors, making debugging and automation much cleaner. Redirecting Stdout and Stderr By default, > captures only stdout. To see how this works, consider redirecting the output of an echo : # Overwrite or create file.txt with stdout
echo ""hello"" > file.txt

# Append to file.txt instead of overwriting
echo ""hello again"" >> file.txt
cat file.txt
# Output:
# hello
# hello again Redirecting stderr To catch error messages, prefix the redirection operator with 2 (stderr’s file descriptor): # Redirect stderr to errors.txt
ls -j 2> errors.txt Now, the invalid-option error goes into errors.txt while stdout (if any) stays on the terminal. File Descriptors and Redirection Table Linux assigns numeric file descriptors to each stream: Stream File Descriptor Redirect Syntax Standard Output 1 1> file.txt Standard Error 2 2> file.txt Combine stderr into stdout N/A 2>&1 Combine stdout into stderr N/A 1>&2 Separating Both Streams To send stdout and stderr to different files: # stdout → stdout.txt; stderr → stderr.txt
ls -z > stdout.txt 2> stderr.txt

cat stderr.txt
# ls: invalid option -- 'z'
cat stdout.txt
# (empty) Or, when a command succeeds: # List directory normally; redirect streams anyway
ls > stdout.txt 2> stderr.txt
# stdout.txt now has the listing; stderr.txt is empty Combining Streams Merge stderr into stdout, writing both to the same file or pipe: # Send both stdout and stderr to combined.log
my-command > combined.log 2>&1

# Or merge them into a pipeline
my-command 2>&1 | grep ""pattern"" Overwriting Caution Using > will overwrite existing files. To avoid data loss, double-check your redirections or use >> to append. See Also Bash Shell Redirection Linux I/O References Kubernetes Documentation Docker Hub Terraform Registry Watch Video Watch video content"
Advanced Bash Scripting,printf,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Streams/printf,"Advanced Bash Scripting Streams printf In Unix-like systems, printf is a POSIX-standard command for producing formatted output in the terminal. Unlike echo , which is usually a shell builtin, printf is an external executable that you invoke each time – trading a slight performance cost for precise control over formatting, field widths, alignment, and escape sequences. echo vs printf When you run echo , the shell handles it internally: $ echo ""Hello, world!""
Hello, world! By contrast, printf is an external program: $ printf ""Hello, printf!\n""
Hello, printf! Note Because printf is external, the shell forks a new process to execute it. Despite this overhead, printf offers richer formatting capabilities and consistent behavior across different shells. Format Specifiers printf uses format specifiers similar to C's printf() . Here are the most common ones: Specifier Description Example %s String printf ""%s\n"" ""text"" %d Signed integer printf ""%d\n"" 42 %f Floating-point number printf ""%.2f\n"" 3.14159 %o Octal representation printf ""%o\n"" 10 %x /% %X Hexadecimal (lowercase/uppercase) printf ""%x\n"" 255 %% Literal percent sign printf ""%%\n"" Example: Strings and Numbers #!/usr/bin/env bash
course=""Advanced Shell Scripting""
printf ""Welcome to the %s\n"" ""$course""

name=""Alice""
age=28
weight=65.2
printf ""Name: %s | Age: %d | Weight: %.1f kg\n"" \
    ""$name"" ""$age"" ""$weight"" This prints: Welcome to the Advanced Shell Scripting
Name: Alice | Age: 28 | Weight: 65.2 kg Escape Sequences and Portability Different shells handle echo escape sequences inconsistently. For example: $ echo ""Line1\nLine2""
Line1\nLine2
$ echo -e ""Line1\nLine2""
Line1
Line2 To ensure portability, use printf which interprets \n , \t , and other escapes by default: $ printf ""Line 1\nLine 2\n""
Line 1
Line 2 Warning Avoid relying on echo -e for escape handling across scripts. Use printf for consistent results in Bash, Dash, Zsh, and other POSIX-compliant shells. Return Status and Character Count printf returns an exit status ( 0 on success) and you can pipe its output to wc -c to count characters: #!/usr/bin/env bash
message=""Hello, world!""
if printf ""%s"" ""$message""; then
    count=$(printf ""%s"" ""$message"" | wc -c)
    echo -e ""\nPrinted $count characters successfully.""
else
    echo ""Error: printf failed with code $?""
fi References POSIX printf documentation GNU Coreutils printf manual Bash Shell Builtins Watch Video Watch video content Practice Lab Practice lab"
Advanced Bash Scripting,Globs Question Mark,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Globs/Globs-Question-Mark,"Advanced Bash Scripting Globs Globs Question Mark In this lesson, we dive into how the ? wildcard works in glob patterns. In Bash and most shells, ? matches exactly one character, so the filenames must align in length with your pattern. This enables precise control over file selection by combining fixed text and single-character placeholders. ? Wildcard Basics ? replaces exactly one character. Does not match zero characters. Treats the dot ( . ) as an actual character (unless dotglob is enabled). Note The ? wildcard differs from * , which matches zero or more characters. Use ? when you need strict length matching. Example 1: Single Character Prefix Before “ail” Assume the directory contains: rail, Document.doc, fail, hail, bar, sail, foo, mail, 4ail, foobar. Use ?ail to match any single-character prefix followed by ail : $ ls
rail  Document.doc  fail  hail  bar  sail  foo  mail  4ail  foobar

$ ls ?ail
rail  fail  hail  sail  mail  4ail ? matches one character ( r , f , h , s , m , 4 ). ail anchors the suffix. Example 2: “tes” + Two Characters + “.txt” Directory listing: test.sh, file.txt, tes1t.txt, test2.txt, file1.txt. With the pattern tes??.txt : $ ls
test.sh  file.txt  tes1t.txt  test2.txt  file1.txt

$ ls tes??.txt
tes1t.txt  test2.txt tes fixes the first three letters. ?? matches exactly two characters ( 1t , 2. ). .txt matches the file extension. Example 3: “test” + One Character + “.txt” Consider these files: test1-2.txt, tes1t.txt, test2.txt, test3.sh, test1.txt, file1.txt. Use test?.txt to find files with a single character after test and before .txt : $ ls
test1-2.txt  tes1t.txt  test2.txt  test3.sh  test1.txt  file1.txt

$ ls test?.txt
test1.txt  test2.txt test anchors the prefix. ? matches one character ( 1 , 2 ). .txt ensures the extension. Example 4: One Character + “est.” + Three Characters Matching pattern ?est.??? to include any one-character prefix, est. , and exactly three characters as the extension: $ ls
test.txt  vest.txt  test.jpg  file.sh  rest.txt  west.doc

$ ls ?est.???
test.txt  vest.txt  test.jpg  rest.txt  west.doc ?est allows any single first character. The . after est is literal. ??? requires exactly three-character extension. Example 5: Exactly Four-Character Filenames Directory content: 1234, abcd, kei5, some_file.txt, x0f4p, 90c1, dir, keio5, touch, x0fp. Pattern ???? finds names exactly four characters long: $ ls
1234  abcd  kei5  some_file.txt  x0f4p  90c1  dir  keio5  touch  x0fp

$ ls ????
1234  abcd  kei5  90c1  x0fp Four ? must each match one character. Filters out any name not exactly four characters. Conclusion Using the ? wildcard in your glob patterns helps you precisely filter filenames by length and content. Combine fixed text with ? placeholders to tailor your matches exactly. Links and References Bash Pattern Matching Advanced Bash-Scripting Guide Shell Globbing on Wikipedia Ensure you enable dotglob if you want to include hidden files in matches: shopt -s dotglob Watch Video Watch video content"
Advanced Bash Scripting,Overview,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Globs/Overview,"Advanced Bash Scripting Globs Overview In earlier lessons, we explored Bash parameter expansion —using operators like # and % (and their double variants ## / %% ) to strip prefixes and suffixes from variable values. We even combined these with the wildcard * to broaden matches. Note Globs (also called wildcards or pathname expansion patterns) differ from parameter expansion. They operate directly on filenames and strings in the shell, not on variable values. What Are Globs? Globs let you match file and directory names—or any arbitrary strings—using a concise pattern syntax. They’re simpler than regular expressions (no lookahead, named groups, etc.), but cover most everyday use cases: Glob Pattern Matches Example * Zero or more characters *.txt ? Exactly one character file?.sh [abc] Exactly one of the set (a, b, c) report[12].pdf When to Use Globs vs. Regex Feature Globs Regular Expressions Simplicity Very easy to write More complex (advanced syntax) Common Use Case File matching Text parsing, validation Advanced Constructs Not supported Lookahead, named groups, etc. Learning Approach To master globs, follow these steps: Gather Sample Strings Assemble a broad list of filenames or test strings you need to match. Identify Common Patterns Group similar strings to pinpoint shared prefixes, suffixes, or character sets. Select the Appropriate Glob Choose from * , ? , or bracket expressions ( [ ] ) to cover your pattern. Test with Shell Commands Run a command like ls or echo to verify that your glob matches the intended files: # Matches all .log files starting with ""app""
ls app*.log If the output aligns with your expectations, your glob is correct! Next Steps In the following sections, we’ll apply these principles to real-world examples—filtering logs, batch-renaming files, and more. Let’s start by examining a directory of mixed files and crafting precise globs for each case. Links and References Bash Reference Manual – Filename Expansion Regular Expressions on Wikipedia ShellCheck – Automated Shell Script Analysis Watch Video Watch video content"
Advanced Bash Scripting,Mixed,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Globs/Mixed,"Advanced Bash Scripting Globs Mixed In this lesson, we’ll take your globbing skills to the next level by combining ? , * , and escape sequences to match complex filename patterns. Glob patterns allow you to filter filenames efficiently without resorting to regular expressions. Note For an in-depth reference on Bash filename expansion, see Bash Pattern Matching . Below is a quick summary of common glob operators: Wildcard Description Example * Matches zero or more characters *.txt ? Matches exactly one character file.? \ Escapes the next character literally file\*.* Workflow for Building Complex Globs Sample Selection Collect filenames and mark matches (✅) vs. non-matches (❌). Identify blue segments (literal text) and yellow segments (wildcards). Category Order Note the sequence of blue and yellow segments in the target filenames. Construct the Glob Concatenate literals and wildcards in the order you determined. Example 1: Match “file.” plus at Least One Character Samples file.        ❌  
file.c       ✅  
file.conf    ✅  
file.txt     ✅  
data.sh      ❌ Blue: file. Yellow: ? (one char) then * (rest) Glob: file.?* $ ls file.?*
file.c  file.conf  file.txt Example 2: Match “file_” Prefix and a Three-Character Extension Samples file_1.txt    ✅  
file_2.doc    ✅  
file_1_a.sh   ❌  
file_2_b.doc  ✅ Blue: file_ Yellow: * (any chars before dot) Blue: . Yellow: ??? (exactly three) Glob: file_*.___ $ ls file_*.___
file_1.txt  file_2.doc  file_2_b.doc Example 3: Match Any Name with a Four-Character Extension Samples document1.txt  ❌  
image1.py      ❌  
report2.pdf    ❌  
memo1.docx     ✅  
invoice3.xlsx  ✅ Yellow: * (any prefix) Blue: . Yellow: ???? (exactly four) Glob: *.???? $ ls *.????
memo1.docx  invoice3.xlsx Example 4: Match Names Containing “1” Then One More Character, Then an Extension Samples file1.sh           ❌  
document1M.docx    ✅  
file3.txt          ❌  
image1T.png        ✅ Yellow: * (any prefix) Blue: 1 Yellow: ? (one char) Blue: . Yellow: * (any extension) Glob: *1?.* $ ls *1?.*
document1M.docx  image1T.png Example 5: Escaping a Literal “*” in Filenames Samples R*al.py        ✅  
f*a*il.txt     ✅  
hail.doc       ❌  
S*ail.txt      ✅  
m*a?i*l.sh     ✅  
4ai*l.txt      ❌ Yellow: ? (any single) Blue: \* (escaped * ) Blue: a Yellow: * Blue: . Yellow: * Glob: ?\*a*.* Warning Be sure to quote or escape the pattern in your shell to prevent expansion before ls sees it. $ ls '?\*a*.*'
R*al.py  f*a*il.txt  S*ail.txt  m*a?i*l.sh Example 6: Escaping a Backslash and Question Mark Samples file5\?xt             ✅  
file_1234567\?890?xt  ✅  
docA\?.docx           ✅  
docZ.docx             ❌  
rep1.txt              ❌ Yellow: * (any prefix) Blue: \\? (escaped \? ) Yellow: * (any remainder) Glob: *\\?* $ ls '*\\?*'
file5\?xt  file_1234567\?890?xt  docA\?.docx Conclusion By splitting filenames into literal (blue) and wildcard (yellow) segments, you can craft precise glob patterns for matching even the trickiest file names. Links and References Bash Pattern Matching GNU Bash Manual Watch Video Watch video content"
Advanced Bash Scripting,Star,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Globs/Star,"Advanced Bash Scripting Globs Star The asterisk ( * ) is a powerful wildcard used in Unix-like shells and many programming languages to match any sequence of characters (including none). In file path patterns (globs), it lets you flexibly select files based on prefixes, extensions, or arbitrary substrings. By mastering * you can streamline file operations, automate tasks in scripts, and improve your command-line efficiency. How * Works * matches zero or more characters in a filename or path segment. It does not match the path separator ( / ) unless the shell supports recursive globs (e.g., ** in Bash). Always quote globs when you need to pass the literal pattern to a command rather than having the shell expand it first. Quoting Patterns Prevent premature expansion by the shell. For example: grep ""TODO"" ""*.txt"" Here, grep receives the pattern *.txt instead of the shell expanding it. 1. Matching Files with a Common Prefix Imagine a directory containing: $ ls
report.txt  report.docx  report.pdf  image1.jpg  backup2.tar.gz  1.log2.log To list all files beginning with report. and any extension: $ ls report.*
report.txt  report.docx  report.pdf report. is matched literally. * covers any extension (including an empty extension if it existed). 2. Matching a Specific Extension Given files: $ ls
report.py   report.pdf   notes.docx   notes.pdf   image1.jpg   config.pdf   log2.log List only PDF documents: $ ls *.pdf
report.pdf  notes.pdf  config.pdf * matches any filename prefix. .pdf filters for that exact extension. 3. Matching a Prefix with Varying Suffixes In a directory like: $ ls
notes.pdf   image1.jpg   image2.jpeg   image3.png   script.sh   backup1.tar.gz Select all files whose names start with image regardless of extension: $ ls image*
image1.jpg  image2.jpeg  image3.png image is the fixed prefix. * grabs everything that follows, covering .jpg , .jpeg , .png , etc. Common * Patterns at a Glance Pattern Description Example Result report.* Files starting with report. report.txt , report.pdf *.pdf Files ending in .pdf notes.pdf , config.pdf image* Files starting with image image1.jpg , image2.jpeg * All files in the current directory Every visible file Links and References Bash Manual: Filename Expansion Advanced Bash-Scripting Guide: Globbing Shell Pattern Matching (TLDP) Watch Video Watch video content"
Advanced Bash Scripting,Args,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Special-Shell-Variables/Args,"Advanced Bash Scripting Special Shell Variables Args When writing Bash scripts, handling command-line arguments efficiently is crucial. You can access each argument by its position ( $1 , $2 , …), but when the number of parameters varies, special variables like $@ and $* simplify your logic. This guide covers: Positional parameters Grouping arguments Iterating with ""$@"" vs ""$*"" The impact of quoting Customizing the internal field separator (IFS) Compatibility considerations 1. Positional Parameters: $1 , $2 , … By default, Bash assigns each argument to a numbered variable: #!/usr/bin/env bash
firstarg=$1
secondarg=$2

echo ""First argument: ${firstarg}""
echo ""Second argument: ${secondarg}"" $ ./simple-args.sh ""arg1""
First argument: arg1
Second argument:

$ ./simple-args.sh ""arg1"" ""arg2""
First argument: arg1
Second argument: arg2 When you need to accept an unpredictable number of parameters, indexing each one becomes cumbersome. That’s where $@ and $* come in. 2. Grouping All Arguments: $@ vs $* Both $@ and $* collect all positional arguments: #!/usr/bin/env bash
packaged_args1=""$@""
packaged_args2=""$*""

echo ""Using \$@: ${packaged_args1}""
echo ""Using \$*: ${packaged_args2}"" $ ./simple-args2.sh arg1 arg2 arg3
Using $@: arg1 arg2 arg3
Using $*: arg1 arg2 arg3 Both variables contain the full list of parameters, but they differ when you iterate over them. 3. Iterating with a For Loop Compare two scripts that loop over their arguments: #!/usr/bin/env bash
echo ""Number of arguments: $#""
echo ""All arguments: $@""
for arg in ""$@""; do
  echo ""Argument: $arg""
done #!/usr/bin/env bash
echo ""Number of arguments: $#""
echo ""All arguments: $*""
for arg in ""$*""; do
  echo ""Argument: $arg""
done $ ./atsign-example1.sh one two three
Number of arguments: 3
All arguments: one two three
Argument: one
Argument: two
Argument: three

$ ./star-example1.sh one two three
Number of arguments: 3
All arguments: one two three
Argument: one two three In the soda‐can analogy: $@ places each can in its own compartment. $* pours all the soda into one big bottle—individual cans are no longer separate. 4. The Importance of Double Quotes Note Always quote $@ and $* : ""$@"" expands each argument separately. ""$*"" joins all arguments into a single string, separated by the first character of IFS (default: space). Unquoted, both behave identically: #!/usr/bin/env bash
print_section_header() {
  local title=""$1""
  echo ""===============================""
  echo ""= Section ${title} =""
  echo ""===============================""
}

print_section_header ""1: \$@""
echo ""--> Output of \$@: $@""
echo ""Looping \$@ without quotes:""
for arg in $@; do
  echo ""$arg""
done

print_section_header ""2: \$*""
echo ""--> Output of \$*: $*""
echo ""Looping \$* without quotes:""
for arg in $*; do
  echo ""$arg""
done $ ./special-shell-noquotes.sh one two three
===============================
= Section 1: $@ =
===============================
--> Output of $@: one two three
Looping $@ without quotes:
one
two
three
===============================
= Section 2: $* =
===============================
--> Output of $*: one two three
Looping $* without quotes:
one
two
three 5. Modifying the Internal Field Separator (IFS) You can change IFS to alter how ""$*"" joins arguments: #!/usr/bin/env bash
IFS=','

echo ""Output of \$@: $@""
echo ""Output of \$*: $*"" $ ./modified-ifs-v1.sh one two three
Output of $@: one two three
Output of $*: one,two,three Splitting later requires unquoted iteration: #!/usr/bin/env bash
IFS='_'
args_at=""$@""
args_star=""$*""

print_section_header() { ... }

print_section_header ""1: \$@""
echo ""--> \$@: $@""
echo ""Looping over args_at:""
for arg in $args_at; do
  echo ""$arg""
done

print_section_header ""2: \$*""
echo ""--> \$*: $*""
echo ""Looping over args_star:""
for arg in $args_star; do
  echo ""$arg""
done 6. Compatibility with Older Bash Versions Some pre-4.0 Bash releases split unquoted assignments differently. For example, under Bash 3: #!/usr/bin/env bash
IFS=','
args_at=$@
echo ""--> \$@: ${args_at}""
for arg in $args_at; do
  echo ""$arg""
done Still, quoting on assignment ensures consistent splitting: #!/usr/bin/env bash
IFS=','
args_at=""$@""
echo ""--> \$@: $@""
for arg in $args_at; do
  echo ""$arg""
done 7. Summary: $@ vs $* Variable Quoted Expansion Unquoted Expansion Use Case $@ Each arg is separate Each word separate Looping over arguments $* All args as one Splits on IFS Passing all args to another command or function 8. Conclusion Use ""$@"" when you need to preserve each argument. Use ""$*"" to aggregate them into a single string with a custom delimiter. Always quote both to maintain consistent behavior across Bash versions and avoid word-splitting pitfalls. Links and References Bash Reference Manual – Special Parameters Shell Parameter Expansion Advanced Bash-Scripting Guide Watch Video Watch video content"
Advanced Bash Scripting,Underscore,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Special-Shell-Variables/Underscore,"Advanced Bash Scripting Special Shell Variables Underscore The special shell variable $_ holds the last argument of the previous command. It’s especially handy in interactive Bash sessions and scripts when you want to avoid retyping long or dynamic arguments. Why Use $_ ? Boosts productivity by reducing repetitive typing Seamlessly reuses file names, directory paths, or any last argument Works in interactive shells and within scripts Note $_ refers strictly to the last argument of the previous command. If that command had no arguments, $_ will be empty. Interactive Shell Examples Listing and Copying a File $ ls -l file.conf
total 16
-rw-r--r-- 1 root root 896 Jun 18 2020 file.conf
$ cp $_ /tmp Here, $_ expands to file.conf , so you don’t have to type it twice. Chaining Commands $ ls -l file.conf; echo ""Done""
total 16
-rw-r--r-- 1 root root 896 Jan 18 2020 file.conf
Done
$ echo $_
Done Since the last command was echo ""Done"" , $_ now contains Done . Using $_ in Scripts #!/usr/bin/env bash

# Create a new directory
mkdir project_build

# Change into the new directory using $_
cd $_

# Show current path
pwd Here, $_ saves you from typing project_build again. Common Use Cases Scenario Last Command $_ Value Copying a file cp large_archive.tar.gz /backup /backup Editing a file vim /etc/nginx/nginx.conf /etc/nginx/nginx.conf Moving a directory mv logs_old logs_archive logs_archive Pipelining commands grep ERROR logfile.log logfile.log Advanced Examples # Remove a file, then verify its removal
rm temp_data.csv
echo ""Removed"" $_

# Using in a pipeline
find . -name '*.log' | xargs gzip
echo ""Compressed"" $_ Further Reading Bash Reference Manual Advanced Bash-Scripting Guide Warning If you chain multiple commands with ; or && , $_ always reflects the very last argument of the last executed command. Watch Video Watch video content"
Advanced Bash Scripting,Overview,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Special-Shell-Variables/Overview,"Advanced Bash Scripting Special Shell Variables Overview Shell scripting thrives on feedback—knowing whether each step succeeded, failed, or requires special handling. Special shell variables provide this introspection by exposing the shell environment, command-line arguments, process identifiers, and more. Leveraging these built-in indicators leads to more robust, maintainable scripts. Understanding Exit Status in Bash Every command you run returns an exit status : 0 means success Any nonzero value indicates failure The special variable $? always holds the exit status of the last executed command: $ ls /nonexistent
ls: cannot access '/nonexistent': No such file or directory
$ echo $?
2
$ echo ""Listing current directory:""
$ ls
KodeKloud_introduction
$ echo $?
0 Note Use echo $? immediately after a command to capture its exit status—subsequent commands will overwrite this value. Exploring Common Special Shell Variables Here’s a quick reference for the most widely used POSIX-compliant special parameters: Variable Description Example $* All positional parameters as a single word echo ""Args: $*"" $# Number of positional parameters echo ""Count: $#"" $0 Name of the script or shell echo ""Script: $0"" $! PID of the last background command sleep 30 & then echo $! $- Current shell options (flags) as a string echo ""Options: $-"" Each of these variables updates dynamically based on the context in which your script runs. Warning Expanding $* without quotes can lead to word splitting and unintended globbing. Use quotes or consider $@ when preserving argument boundaries is critical. Practical Use Cases Argument Validation if [ $# -lt 2 ]; then
  echo ""Usage: $0 <source> <destination>""
  exit 1
fi Logging Script Name echo ""[$(date)] Starting backup script: $0"" Background Job Monitoring long_running_task &
bg_pid=$!
wait $bg_pid
echo ""Job $bg_pid finished with status $?"" Links and References GNU Bash Manual: Special Parameters POSIX Shell Command Language Shell Scripting Best Practices Watch Video Watch video content"
Advanced Bash Scripting,Square,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Globs/Square,"Advanced Bash Scripting Globs Square Shell globbing lets you match filenames using patterns. While * matches any string and ? matches a single character, square brackets ( [ ] ) define character classes , enabling you to match one character from a specific set or range. Matching a Range of Characters Given a directory: $ ls
fileA fileB fileC fileD fileE You can match only fileA , fileB , and fileC by specifying a range inside brackets: $ ls file[A-C]
fileA fileB fileC Here, [A-C] matches any uppercase letter from A through C. Note that the dash ( - ) defines a range and must go from lower to higher: $ ls file[C-A]
ls: cannot access 'file[C-A]': No such file or directory Common Examples Pattern Matches Description file[A-C] fileA, fileB, fileC Uppercase A–C file[a-c] filea, fileb, filec Lowercase a–c file[1-3] file1, file2, file3 Numeric 1–3 file[ACE] fileA, fileC, fileE Specific letters A, C, E Negated Character Classes Prefix ! or ^ inside brackets to exclude characters or ranges: $ ls
fileA fileB fileC fileD fileE

$ ls file[^A-C]
fileD fileE

$ ls file[!A-C]
fileD fileE Note In Bash both ! and ^ work for negation. POSIX shells require ! at the start of the class. Listing Specific Characters To match non-consecutive filenames such as fileA , fileC , and fileE , simply list them: $ ls file[ACE]
fileA fileC fileE Case Sensitivity Globbing in Bash is case sensitive. If you have both uppercase and lowercase files: $ touch filea fileb filec filed filee
$ ls
fileA fileB fileC fileD fileE filea fileb filec filed filee To match only lowercase: $ ls file[a-c]
filea fileb filec To remove both lowercase and uppercase a–e , combine ranges: $ rm file[a-eA-E]
$ ls
fileD fileE Note When mixing ranges, list them in the order you want matched: here a-e before A-E . Numeric Ranges and Negation Numeric ranges behave the same way: $ touch file1 file2 file3 file4 file5
$ ls file[1-3]
file1 file2 file3

$ ls file[4-5]
file4 file5

$ ls file[^1-3]
file4 file5

$ ls file[!1-3]
file4 file5 Multiple Character Classes You can chain classes to match multiple positions. For example, to match filea1 , filea2 , fileb1 , fileb2 : $ touch filea1 filea2 filea3 fileb1 fileb2 fileb3

$ ls file[a-b][1-2]
filea1 filea2 fileb1 fileb2 Each [a-b] matches one letter, and [1-2] matches one digit. If you try only [1-2] , it won’t match because the letter is missing: $ ls file[1-2]
ls: cannot access 'file[1-2]': No such file or directory Literal Characters Inside Brackets Inside character classes, special glob characters lose their meaning: $ ls file[a][*]
ls: cannot access 'file[a][*]': No such file or directory To use * as a wildcard, place it outside the brackets: $ ls filea*
filea1 filea2 filea3 Or constrain both positions: $ ls file[a][1-3]
filea1 filea2 filea3 Globbing vs. File Creation Globs match existing filenames; they do not generate names. If you use a glob in a command like touch when no files match, the pattern is taken literally: $ touch ?ail
$ ls
'?ail' To produce a series of filenames based on a pattern, consider using brace expansion instead of globs. Warning Globbing won’t create files—only match them. If you expect new files, use brace expansion or a loop. Links and References Bash Pattern Matching KornShell Globbing Advanced Bash-Scripting Guide Watch Video Watch video content Practice Lab Practice lab"
Advanced Bash Scripting,Escape,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Globs/Escape,"Advanced Bash Scripting Globs Escape When working in the shell, special characters like ? , * , and [ ] are interpreted by the globbing engine for filename matching. Preceding these characters with a backslash ( \ ) or quoting them forces the shell to treat them as literal characters. Note Using an escape ( \ ) or quotes disables globbing for the next character or entire string. This gives you precise control over filename creation and listing. 1. Creating Sample Files First, generate files whose names differ only by the first letter: $ touch sail hail mail fail tail
$ ls
fail hail mail sail tail 2. Wildcard vs. Literal ? 2.1 Using the ? Wildcard The ? matches exactly one character. Attempting to create ?ail without escaping: $ touch ?ail
$ ls
fail hail mail sail tail No new file appears because ?ail expanded to all existing matches ( fail , hail , etc.). 2.2 Escaping ? for Literal Filenames To create a file literally named ?ail : $ touch \?ail
$ ls
?ail fail hail mail sail tail 3. Listing Files with a Leading ? You can retrieve the ?ail file by escaping or quoting the pattern: $ ls \?ail
?ail

$ ls ""?ail""
?ail Both methods disable globbing and match the literal filename. 4. Mixing Literals and Wildcards Assume these files exist: $ touch hail fail mail \?ail
$ touch hailTwo failTwo mailTwo \?ailTwo
$ ls
?ail ?ailTwo fail failTwo hail hailTwo mail mailTwo To list all files starting with a literal ?ail : $ ls \?ail*
?ail  ?ailTwo Here, \?ail* treats \? as literal ? and * as the wildcard for any suffix. Warning Be cautious: unescaped wildcards can match unintended files. Always check your patterns with echo or ls before running destructive commands. 5. Merging Globs into a Single Pattern For files named Pail , Pail* , and PailTwo : $ touch Pail Pail* PailTwo
$ ls
Pail Pail* PailTwo You can match them all using: $ ls Pail*
Pail Pail* PailTwo The * wildcard expands to zero or more characters following Pail . 6. Quick Reference Table Special Character Behavior Escaped Form Example ? Single-character match \? touch \?file * Zero or more characters \* ls Pail\* [ ] Character set \[ \] ls \[abc\]* Summary A backslash ( \ ) or quotes disables globbing for the next character or entire string. Use wildcards ( ? , * , [ ] ) without escaping to match patterns. Combine escaped literals and wildcards for precise filename operations. Links and References GNU Bash Manual – Pattern Matching Bash Guide: Filename Expansion Watch Video Watch video content"
Advanced Bash Scripting,Hashtag,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Special-Shell-Variables/Hashtag,"Advanced Bash Scripting Special Shell Variables Hashtag In shell scripting, robust error handling and input validation are crucial. Earlier, we saw how $? captures the exit status of the last command. In this guide, we’ll dive into $# , which returns the number of positional parameters passed to your script or function. By checking $# early, you can prevent unexpected behavior and make your scripts more reliable. Table of Contents Why $# Matters Basic Usage of $# Positional Parameters and Empty Defaults Guard Clauses with $# Exact Number of Arguments Minimum Number of Arguments Range of Arguments Real-World Example: Walking Calorie Expenditure Script Flawed Version Improved Version with set -e and terminate() Summary of Key Variables Links and References Why $# Matters Shell scripts often rely on user-supplied arguments. If your script assumes a certain number of inputs but none (or too many) are provided, it can silently fail or produce erroneous results. Checking $# allows you to: Validate inputs before executing critical logic Provide clear usage messages Exit early on misuse Note Always validate positional parameters to avoid silent errors and improve script maintainability. Basic Usage of $# Create a script called count-args.sh : #!/usr/bin/env bash
echo ""Number of arguments: $#"" Run it with different argument counts: $ ./count-args.sh
Number of arguments: 0

$ ./count-args.sh alpha
Number of arguments: 1

$ ./count-args.sh alpha beta
Number of arguments: 2

$ ./count-args.sh """"
Number of arguments: 1 Positional Parameters and Empty Defaults By default, referencing an unset positional parameter expands to an empty string without an error: #!/usr/bin/env bash
echo ""First: '${1}'""
echo ""Second: '${2}'"" $ ./show-params.sh hello
First: 'hello'
Second: ''
$ echo $?
0 Without validation, scripts can continue with missing data, leading to downstream failures. Guard Clauses with $# Validate $# early in your script to enforce expected usage. Below are common patterns: Pattern Description Example [[ $# -ne N ]] Exact number of arguments if [[ $# -ne 1 ]]; then echo ""Usage: $0 <arg>"" >&2; exit 1; fi [[ $# -lt N ]] At least N arguments if [[ $# -lt 1 ]]; then echo ""Need at least one arg"" >&2; exit 1; fi `[[ $# -lt MIN $# -gt MAX ]]` Exact Number of Arguments #!/usr/bin/env bash

if [[ $# -ne 1 ]]; then
  echo ""Usage: $0 <arg>"" >&2
  exit 1
fi

echo ""Argument received: $1""
exit 0 Minimum Number of Arguments #!/usr/bin/env bash

if [[ $# -lt 1 ]]; then
  echo ""Error: At least one argument is required."" >&2
  exit 1
fi

echo ""Received $# argument(s)."" Range of Arguments #!/usr/bin/env bash
readonly MIN=1
readonly MAX=2

if [[ $# -lt $MIN || $# -gt $MAX ]]; then
  echo ""Usage: $0 <arg1> [arg2]"" >&2
  echo ""Provide between $MIN and $MAX arguments."" >&2
  exit 1
fi

echo ""Arguments are within the expected range."" Real-World Example: Walking Calorie Expenditure Script Calculating calories burned from step counts is a practical script. Let’s see a flawed version and then improve it with input validation. Flawed Version #!/usr/bin/env bash
steps=${1}
cal_per_step=0.04
calories=$(echo ""${steps} * ${cal_per_step}"" | bc)
echo ""Calories burned for ${steps} steps: ${calories}"" Running without arguments shows a syntax error but exits with status 0 : $ ./calorie.sh
(standard_in) 1: syntax error
$ echo $?
0 Warning Scripts that continue after errors can hide critical failures. Always combine guard clauses with strict error handling. Improved Version with set -e and terminate() #!/usr/bin/env bash
set -e

CL_ARGS_ERROR=155
CAL_PER_STEP=0.04

terminate() {
  local msg=""$1""
  local code=""${2:-160}""
  echo ""Error: $msg"" >&2
  exit ""$code""
}

# Guard clause: expect exactly one argument
if [[ $# -ne 1 ]]; then
  terminate ""Please provide exactly one argument (number of steps)"" ""$CL_ARGS_ERROR""
fi

steps=$1
calories=$(echo ""$steps * $CAL_PER_STEP"" | bc)
echo ""Calories burned for $steps steps: $calories""
exit 0 Usage $ ./calorie.sh
Error: Please provide exactly one argument (number of steps)
$ echo $?
155

$ ./calorie.sh 10000
Calories burned for 10000 steps: 400.00
$ echo $?
0 Summary of Key Variables Variable Description Example $? Exit status of the last command echo $? $# Number of positional parameters passed to a script if [[ $# -ne 1 ]]; then ... fi $1, $2 Positional parameters (first, second, etc.) echo ""First: $1"" Links and References Bash Reference Manual Advanced Bash-Scripting Guide Shell Parameter Expansion Watch Video Watch video content"
Advanced Bash Scripting,Brace,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Expansions-Part-Two/Brace,"Advanced Bash Scripting Expansions Part Two Brace Brace expansion is a powerful shell feature that generates arbitrary strings or arguments by evaluating expressions within curly braces {} . Unlike globs, which match existing filenames, brace expansions create new text before any other expansion (such as pathname or parameter expansion) takes place. Note Brace expansions are always processed before globs and parameter expansions. This lets you quickly generate lists of filenames, parameters, or other strings in a single command. Table of Content Basic Range Expansion Department Usernames Example Comma-Separated List Expansion Numeric Range Expansion Nested Brace Expansions Step-Based Range Expansion (Bash 4.0+) Prefix and Suffix with Brace Expansion Integrating Brace Expansion in Scripts Summary & Best Practices Links and References 1. Basic Range Expansion Alphabetic and numeric ranges let you generate sequences with minimal syntax. # Alphabetic sequence: a, b, c
touch sample{a..c}
ls
# Numeric sequence: 1, 2, 3
touch sample{1..3}
ls
# Descending numeric sequence
echo {3..1}
# → 3 2 1 Expansion Type Syntax Result Alphabetic Range {a..z} a b c ... z Numeric Range {1..5} 1 2 3 4 5 2. Department Usernames Example Suppose you need to create usernames for Marketing (MKT), Sales (SL), and Development (DEV), each with a three-digit suffix from 001 to 004 . Brace expansion handles this in one command: touch {MKT,SL,DEV}{001..004}
ls
# → DEV001 DEV002 DEV003 DEV004 MKT001 MKT002 MKT003 MKT004 SL001 SL002 SL003 SL004 {MKT,SL,DEV} expands to each department code. {001..004} produces four numeric suffixes. Combined, you get all 12 unique usernames in one step. 3. Comma-Separated List Expansion Use commas for arbitrary lists of items. Each element is substituted in place of the braces. echo file{alpha,beta,gamma}.txt
# → filealpha.txt filebeta.txt filegamma.txt 4. Numeric Range Expansion Generate a series of numeric filenames quickly: echo file{1..5}.txt
# → file1.txt file2.txt file3.txt file4.txt file5.txt 5. Nested Brace Expansions Nest multiple sets of braces to produce Cartesian products: echo {a,b}{1,2,3}
# → a1 a2 a3 b1 b2 b3 The shell pairs each element of the first set ( a , b ) with each element of the second set ( 1 , 2 , 3 ). 6. Step-Based Range Expansion (Bash 4.0+) Include a step value to skip elements in a numeric range. Requires Bash 4.0 or newer. Warning Step-based expansions ( {start..end..step} ) are supported only in Bash 4.0+ and some other modern shells. Verify your shell version with bash --version . echo file{1..10..2}.txt
# → file1.txt file3.txt file5.txt file7.txt file9.txt 7. Prefix and Suffix with Brace Expansion Wrap fixed text around your expansions: echo pre-{A..C}-post
# → pre-A-post pre-B-post pre-C-post 8. Integrating Brace Expansion in Scripts Brace expansions work seamlessly within loops, conditionals, and functions: #!/usr/bin/env bash
for i in {1..3}; do
  touch ""report_${i}.log""
done
ls
# → report_1.log report_2.log report_3.log Combine expansions with other shell features to automate repetitive tasks. 9. Summary & Best Practices Brace expansions run before all other shell expansions, making them ideal for generating arguments. Use numeric ranges for file series or versioned assets. Leverage nested braces for multi-dimensional data generation. Remember that stepped ranges require Bash 4.0+. By mastering brace expansion, you can cut down on repetitive typing, automate bulk operations, and keep your scripts concise and readable. 10. Links and References Bash Reference Manual: Shell Expansions Advanced Bash-Scripting Guide Stack Overflow: Brace Expansion Examples Watch Video Watch video content"
Advanced Bash Scripting,Command Substitution,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Expansions-Part-Two/Command-Substitution,"Advanced Bash Scripting Expansions Part Two Command Substitution Command substitution is a fundamental feature of Bash scripting that lets you capture the output of one command and embed it into another. This technique improves script readability and enables dynamic data handling in your shell scripts. Recap: Variable Expansion Use $ followed by a variable name to expand its value. Enclose the name in curly braces {} to avoid ambiguity, and wrap expansions in quotes to prevent word splitting and globbing. name=""John""
echo ""${name}""
# Output: John Tip Always quote your variable expansions ( ""${var}"" ) to preserve whitespace and avoid unexpected globbing. What Is Command Substitution? Command substitution runs a command in a subshell and replaces the command with its standard output. There are two syntaxes: Syntax Description $( ... ) Preferred form; allows nesting easily. `...` Deprecated; harder to read and nest. Example: Counting Files on the Command Line $ ls
DEV001  DEV002  DEV003  DEV004  MKT001  MKT002  MKT003  MKT004  command-substitution.sh
$ find . -type f | wc -l
9 Using $( ... ) Create command-substitution.sh : #!/usr/bin/env bash
file_count=$(find . -type f | wc -l)
echo ""Total files: ${file_count}"" Run it: $ chmod +x command-substitution.sh
$ ./command-substitution.sh
Total files: 9 Using Backticks (Deprecated) #!/usr/bin/env bash
file_count=`find . -type f | wc -l`
echo ""Total files: ${file_count}"" Warning Backticks are deprecated. They complicate nesting and reduce readability. Always prefer $( ... ) in modern Bash scripts. Accepting a Directory Argument Require the user to specify a target directory: #!/usr/bin/env bash
if [[ -z ""${1}"" ]]; then
  echo ""Usage: $0 <directory>""
  exit 1
fi

file_count=$(find ""${1}"" -type f | wc -l)
echo ""Files in ${1}: ${file_count}"" $ ./command-substitution-v2.sh .
Files in .: 9 Timing Considerations Command substitution runs once when assigned. If you modify files later, the stored value stays unchanged until you reassign it. #!/usr/bin/env bash
if [[ -z ""${1}"" ]]; then
  echo ""Usage: $0 <directory>""
  exit 1
fi

file_count=$(find ""${1}"" -type f | wc -l)
echo ""Initial count: ${file_count}""

touch samplefile
echo ""After touch: ${file_count}"" $ ./command-substitution-v3.sh .
Initial count: 9
After touch: 9 Re-running the script updates the count: $ ./command-substitution-v3.sh .
Initial count: 10
After touch: 10 Subshell Scope Command substitution executes in a subshell. Variables modified inside do not affect the parent shell: #!/usr/bin/env bash
if [[ -z ""${1}"" ]]; then
  echo ""Usage: $0 <directory>""
  exit 1
fi

dir=${1}
file_count=$(find ""${dir}"" -type f | wc -l)

sub_output=$(
  dir=""/some/other/dir""
  echo ""Dir in subshell: ${dir}""
)

echo ""Dir in parent shell: ${dir}""
echo ""${sub_output}"" $ ./command-substitution-v4.sh /usr/bin
Dir in parent shell: /usr/bin
Dir in subshell: /some/other/dir A subshell inherits your environment but keeps its changes local. Performance Considerations Each command substitution spawns a new process. In most scripts this overhead is negligible, but in performance-critical loops, minimize unnecessary substitutions. Performance Warning Avoid placing heavy command substitutions inside tight loops. Consider alternatives like readarray or built-in string operations when processing large datasets. Honorable Mentions Use Case Example Capture stderr files=$(ls -j 2>&1) Capture timestamp current_date=$(date) echo ""Errors: ${files}""
echo ""Today's date: ${current_date}"" Links and References Bash Reference Manual Advanced Bash-Scripting Guide ShellCheck: Shell Script Analysis Watch Video Watch video content"
Advanced Bash Scripting,Dash,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Special-Shell-Variables/Dash,"Advanced Bash Scripting Special Shell Variables Dash The $- variable in Bash shows the current shell’s option flags. These flags differ when you run commands interactively versus inside a script. In this guide, we’ll: Inspect $- interactively Compare interactive vs. non-interactive outputs Demonstrate how set -e affects $- Review common $- flags in a summary table Checking $- Interactively Run the following command in your terminal to see which flags are active in your interactive shell: echo $- Example output: himBHs Each letter (such as h , i , m , B , H , s ) represents a specific shell option. These may vary between systems and user configurations. Note The exact set of flags depends on your Bash version and how you’ve configured your environment (e.g., via .bashrc or .bash_profile ). Comparing with a Non‐Interactive Script Create a file named special-dash.sh : #!/usr/bin/env bash
echo $- Make it executable and run it: chmod +x special-dash.sh
./special-dash.sh Typical output: hB You’ll notice the i flag (interactive mode) is missing because scripts run non-interactively by default. Enabling “Exit on Error” with set -e The set -e option causes Bash to exit immediately if any command returns a non-zero status. Let’s enable it in our script: #!/usr/bin/env bash
set -e
echo $- Run the updated script: ./special-dash.sh Now you should see: ehB The leading e indicates that “exit on error” is active. Warning Using set -e can make debugging harder if you’re not careful. Always test scripts thoroughly or combine with set -u and set -o pipefail for stricter error checking. Common $- Flags Flag Description i Shell is running in interactive mode e Exit immediately if a command exits with failure u Treat unset variables as an error x Print commands and their arguments as they execute Further Reading Bash Reference Manual – Invocations Bash Scripting Guide By understanding and inspecting the $- variable, you can better control your shell’s behavior in both interactive sessions and automated scripts. Watch Video Watch video content"
Advanced Bash Scripting,pid,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Special-Shell-Variables/pid,"Advanced Bash Scripting Special Shell Variables pid In Bash and other POSIX-compatible shells, special variables like $$ and $! help you manage process IDs (PIDs) for debugging, automation, and scripting tasks. While they might not pop up every day, knowing how to use them can streamline background jobs, service management, and process monitoring. Before diving into these variables, let’s clarify how TTYs, shells, and PIDs relate: When you open a terminal, it’s assigned a TTY (teletype) name. A shell (e.g., Bash) runs on that TTY and acts as the parent process. Any command or script executed in that shell becomes a child process. Every process—from parent shells to background jobs—has a PID and a lifecycle (start → run → exit). Table of Special PID Variables Variable Description Typical Use Case $! PID of the most recently launched background job Tracking & controlling background tasks $$ PID of the current shell or shell-script process Self-awareness in scripts Using $! to Capture Background Job PIDs The $! variable returns the PID of the last job you sent to the background. $ sleep 5 &
[1] 93506
$ echo $!
93506 Even if you run foreground commands afterward, $! holds the PID until you start another background job: $ echo ""First background PID is $!""
First background PID is 93506

$ echo ""Hello"" &
[1] 93761
Hello
[1] + done echo ""Hello""

$ echo $!
93761 Storing $! in a Bash Script A common pattern is launching a service in the background, capturing its PID, and later terminating it. For example, starting an Apache JMeter server: #!/usr/bin/env bash

jmeter_pid=""""

start_jmeter() {
  echo ""Starting JMeter server...""
  jmeter-server &
  jmeter_pid=$!
}

start_jmeter

# Other workflow steps...
echo ""Now running additional setup tasks...""
,[object Object],

echo ""Stopping JMeter server (PID: $jmeter_pid)""
kill -SIGTERM ""$jmeter_pid"" Using $$ to Identify Your Shell or Script The $$ variable prints the PID of the current shell or script process: $ echo $$
79315
$ ps -p $$ -o pid,cmd
  PID CMD
79315 -bash Opening a new terminal tab or window yields a different $$ value: $ echo $$
91933 Inspecting $$ Inside a Script Create print_pid.sh : #!/usr/bin/env bash
echo ""This script’s PID is $$""
sleep 60 Run it in the background: $ ./print_pid.sh &
[1] 94479
94479
$ ps --pid 94479,94481 -o pid,ppid,cmd
  PID  PPID CMD
94479 79315 /bin/bash ./print_pid.sh
94481 94479 sleep 60 PID 94479 corresponds to the script itself ( $$ ). PID 94481 is the child sleep process. $$ and Subshell Behavior Subshells inherit the parent shell’s PID, so $$ stays constant: #!/usr/bin/env bash

echo ""Parent shell PID: $$""
(
  echo ""Inside subshell PID: $$""
) Output: $ ./subshell_pid.sh
Parent shell PID: 54344
Inside subshell PID: 54344 All references to $$ show the same PID because subshells share the parent’s process ID. Key Takeaways $! returns the PID of the last command run in the background . $$ returns the PID of your current shell or the running script. Understanding these variables will help you write more robust scripts, automate process control, and debug complex workflows with confidence. Links and References Bash Reference Manual Linux Process Management Apache JMeter Watch Video Watch video content"
Advanced Bash Scripting,Subshells,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Expansions-Part-Two/Subshells,"Advanced Bash Scripting Expansions Part Two Subshells Command substitution with $(…) captures the output of commands into a variable, but it does so by spawning a subshell —a child process separate from your main shell: #!/usr/bin/env bash
file_count=$(find . -type f | wc -l) Note Assignments made inside the subshell do not affect variables in the parent shell. Because a new process is created, there’s a small performance cost compared to running commands directly in the parent shell. Warning Overusing complex command substitutions inside tight loops can degrade script performance. Measure and optimize if needed. #!/usr/bin/env bash
var=""a""
subshell=$( var=""b"" )
echo ""$var""        # still ""a""
echo ""$subshell""   # empty, because var=""b"" was in the subshell 1. Subshell Syntax Wrap commands in parentheses to run them in a subshell: #!/usr/bin/env bash
current_env=""shell""
(
  echo ""This is running in a ${current_env}""
) $ ./subshell-v0.sh
This is running in a shell To prove isolation: #!/usr/bin/env bash
current_env=""a""
(
  current_env=""b""
  echo ""Inside subshell: $current_env""
)
echo ""Outside subshell: $current_env"" $ ./subshell-v1.sh
Inside subshell: b
Outside subshell: a 2. Command Substitution with a Subshell Combine a subshell with $(…) to capture its output separately from the parent shell: #!/usr/bin/env bash
current_env=""a""

subsh_var=$(
  current_env=""b""
  echo ""$current_env""
)

echo ""Parent environment: $current_env""
echo ""Subshell output: $subsh_var"" $ ./subshell-v2.sh
Parent environment: a
Subshell output: b Errors inside the subshell still appear on the terminal: #!/usr/bin/env bash
subsh_var=$(
  echo ""Output""
  ls -fakeoption
) $ ./subshell-v2-stderr.sh
ls: unrecognized option '--fakeoption' 3. Command Layout in Subshells Within parentheses, you can: Put commands on separate lines Separate with semicolons ( ; ) Pipe between them ( | ) Chain with && or || # Separate lines
(
  command1
  command2
  command3
)

# Semicolons
(command1; command2; command3)

# Pipes
(command1 | command2 | command3)

# AND operator
(command1 && command2 && command3)

# OR operator
(command1 || command2) 4. Common Subshell Scenarios 4.1 One-Liners to Change Directory Temporarily Run commands in a different folder without affecting your current directory: #!/usr/bin/env bash
# Update and build inside /home/user/project
(cd /home/user/project && git pull && make) Interactive example: $ pwd
/home/user/workspace
$ (cd /tmp && ls)
file_in_tmp
$ pwd
/home/user/workspace 4.2 Jenkins Pipeline Steps In Jenkins Pipelines, each sh step executes in its own subshell. This isolation explains differences when scripts run in CI/CD environments. 5. Verifying Process IDs Use $$ and $BASHPID to compare parent and subshell PIDs: Variable Description $$ PID of the parent shell $BASHPID PID of the current Bash process (even in a subshell) #!/usr/bin/env bash
parent_pid=$$

(
  echo ""Inside subshell: PID=$BASHPID""
)

echo ""Outside subshell: PID=$parent_pid"" $ ./subshell-v5.sh
Inside subshell: PID=12345
Outside subshell: PID=12344 6. Propagating Values Back to the Parent Shell Since subshells can’t modify parent variables directly, use a temporary file or another IPC mechanism: #!/usr/bin/env bash
tmpfile=""/tmp/$$.tmp""
counter=1

# Initialize counter
echo ""$counter"" > ""$tmpfile""

# Increment inside subshell
(
  new_count=$(( $(<""$tmpfile"") + 1 ))
  echo ""$new_count"" > ""$tmpfile""
)

# Read updated value
counter=$(<""$tmpfile"")
echo ""Counter after subshell: $counter""

# Clean up
rm ""$tmpfile"" $ ./subshell-v6.sh
Counter after subshell: 2 This pattern is useful in scripts, loops, and CI/CD jobs when you need to retrieve data from a subshell. Watch Video Watch video content"
Advanced Bash Scripting,Special shell Question mark,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Special-Shell-Variables/Special-shell-Question-mark,"Advanced Bash Scripting Special Shell Variables Special shell Question mark In Bash and other POSIX-compliant shells, the special variable $? holds the exit status of the last executed command, script, or function. Checking this value is essential for robust error handling in shell scripts. Table of Contents Part 1: Using $? What Is an Exit Status? Inspecting $? in Practice Common Exit Codes Back-to-Back Commands Masking Errors Part 2: Writing Scripts That Leverage $? Technique 1: if After Each Command Technique 2: OR Operator ( || ) Technique 3: set -e Custom Exit Codes and a terminate Function References Part 1: Using $? What Is an Exit Status? Every command returns an integer exit status. 0 means success. Non-zero indicates failure or a specific error condition. Note If you redirect both stdout and stderr (e.g., > /dev/null 2>&1 ), you won’t see any output, but $? still reflects success or failure. Inspecting $? in Practice Script with a typo: #!/usr/bin/env bash
ehco ""Hello!"" $ ./wrong_echo.sh > /dev/null 2>&1
$ echo $?
127 Exit code 127 means “command not found.” Successful command: $ ls
music videos photos documents
$ echo $?
0 File-not-found error: $ ls some_file.txt
ls: cannot access 'some_file.txt': No such file or directory
$ echo $?
2 Common Exit Codes Exit Code Meaning 0 Success 1 General error 2 Misuse of shell built-ins 126 Command found but not executable 127 Command not found 130 Script terminated by Ctrl-C Note You can define custom exit codes (128 and above) to represent specific failure modes in your scripts. Back-to-Back Commands When you execute multiple commands, $? always reflects the last exit status: $ ./script.sh
permission denied: ./script.sh
$ echo $?
126

$ ls script.sh
script.sh
$ echo $?
0 Masking Errors A trailing exit 0 can hide earlier failures: #!/usr/bin/env bash
ehco ""Hello!""
exit 0 $ ./wrong_echo.sh
./wrong_echo.sh: line 2: ehco: command not found
$ echo $?
0 Part 2: Writing Scripts That Leverage $? To ensure your script stops on errors and reports accurate statuses, apply one of these techniques. Technique 1: if After Each Command #!/usr/bin/env bash

ehco ""Hello!""
if [[ $? -ne 0 ]]; then
    echo ""Error: Failed to run command.""
    exit 1
fi

echo ""Command ran successfully!""
exit 0 $ ./wrong_echo-v2.sh
./wrong_echo-v2.sh: line 2: ehco: command not found
Error: Failed to run command.
$ echo $?
1 Technique 2: OR Operator ( || ) #!/usr/bin/env bash
ehco ""Hello!"" || { echo ""Error: Failed to run command.""; exit 1; }
exit 0 $ ./wrong_echo-v3.sh
./wrong_echo-v3.sh: line 1: ehco: command not found
Error: Failed to run command.
$ echo $?
1 Technique 3: set -e #!/usr/bin/env bash
set -e
ehco ""Hello!""
exit 0 $ ./wrong_echo-sete.sh
./wrong_echo-sete.sh: line 2: ehco: command not found
$ echo $?
127 Warning Using set -e in an interactive shell will terminate your session on the first error. $ set -e
$ ehco ""Hello""
-bash: ehco: command not found Custom Exit Codes and a terminate Function Initial Script: server_appender.sh #!/usr/bin/env bash

readonly CONF_FILE=""./fqdn.properties""
readonly SERVER_NAMES=""server1 server2 server3""
readonly DEFAULT_USER=""mummshad""

fqdn=$(cat ""${CONF_FILE}"")

for server in ${SERVER_NAMES}; do
    echo ""${DEFAULT_USER}@${server}.${fqdn}""
done

exit 0 If fqdn.properties is empty, this outputs malformed hostnames. Adding an Empty-File Check #!/usr/bin/env bash

readonly CONF_FILE=""./fqdn.properties""
readonly SERVER_NAMES=""server1 server2 server3""
readonly DEFAULT_USER=""mummshad""

if [[ ! -s ""${CONF_FILE}"" ]]; then
    echo ""Error: ${CONF_FILE} is empty""
    exit 1
fi

fqdn=$(cat ""${CONF_FILE}"")

for server in ${SERVER_NAMES}; do
    echo ""${DEFAULT_USER}@${server}.${fqdn}""
done

exit 0 $ ./server_appender.sh
Error: ./fqdn.properties is empty
$ echo $?
1 Defining a terminate Function #!/usr/bin/env bash
set -e

readonly CONF_FILE=""./fqdn.properties""
readonly SERVER_NAMES=""server1 server2 server3""
readonly DEFAULT_USER=""mummshad""
readonly ERROR_FILE=150

terminate() {
    local msg=""$1""
    local code=""${2:-160}""
    echo ""Error: ${msg}"" >&2
    exit ""${code}""
}

if [[ ! -s ""${CONF_FILE}"" ]]; then
    terminate ""FQDN file is empty"" ""${ERROR_FILE}""
fi

fqdn=$(cat ""${CONF_FILE}"")

for server in ${SERVER_NAMES}; do
    echo ""${DEFAULT_USER}@${server}.${fqdn}""
done

exit 0 $ ./custom_exit_code_final.sh
Error: FQDN file is empty
$ echo $?
150 By combining set -e , custom exit codes, and a reusable terminate function, your Bash scripts will halt on failures and report meaningful statuses. References Bash Reference Manual Advanced Bash-Scripting Guide Shell Exit Status Conventions Watch Video Watch video content"
Advanced Bash Scripting,Zero,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Special-Shell-Variables/Zero,"Advanced Bash Scripting Special Shell Variables Zero In Bash scripting, the special parameter $0 holds the name (and path) used to invoke the script. Understanding and manipulating $0 lets you: Retrieve the script’s invoked name or full path Derive the absolute directory where the script resides Below, we explore each technique with practical examples and patterns for robust, user-friendly scripts. Table of Contents Getting the Invoked Script Name Extracting Only the Basename Dynamic Usage Messages with SCRIPT_NAME Graceful Exits via a terminate Helper Resolving the Script’s Directory ( WORK_DIR ) Quick Reference Table Links and References 1. Getting the Invoked Script Name By default, $0 prints exactly how the script was called: #!/usr/bin/env bash
echo ""$0"" Save this as show-zero.sh and run: $ ./show-zero.sh
./show-zero.sh

$ # If on your PATH:
$ show-zero.sh
/usr/local/bin/show-zero.sh 2. Extracting Only the Basename To obtain just the filename (dropping any leading directories), use shell parameter expansion: #!/usr/bin/env bash
readonly SCRIPT_NAME=${0##*/}
echo ""${SCRIPT_NAME}"" Running: $ ./show-zero.sh
show-zero.sh Here ${0##*/} strips everything up to the last slash. 3. Dynamic Usage Messages with SCRIPT_NAME Embedding the script’s basename in help text ensures accuracy, even if the file is renamed: #!/usr/bin/env bash
readonly SCRIPT_NAME=${0##*/}

usage() {
  cat <<USAGE
Usage: ${SCRIPT_NAME} <name>

Greet a user by name.

Arguments:
  name       The name to greet.

Options:
  -h, --help Show this help message and exit.
USAGE
}

# Show usage
usage Example output: $ ./show-zero.sh
Usage: show-zero.sh <name>

Greet a user by name.

Arguments:
  name       The name to greet.

Options:
  -h, --help Show this help message and exit. 4. Graceful Exits via a terminate Helper Centralize error reporting and custom exit codes: #!/usr/bin/env bash
readonly SCRIPT_NAME=${0##*/}
readonly ERR_MISSING_ARG=155

usage() {
  cat <<USAGE
Usage: ${SCRIPT_NAME} <name>

Greet a user by name.

Options:
  -h, --help Show this help message and exit.
USAGE
}

terminate() {
  echo ""Error: ${1}"" >&2
  exit ""${2:-1}""
}

# Argument count check
if [[ $# -ne 1 ]]; then
  usage
  terminate ""Missing argument"" ""$ERR_MISSING_ARG""
fi

# Help flag
if [[ ""$1"" == ""-h"" || ""$1"" == ""--help"" ]]; then
  usage
  exit 0
fi

name=""$1""
echo ""Hello, ${name}! Welcome!""
exit 0 Example runs: $ ./greet.sh
Usage: greet.sh <name>
Error: Missing argument
$ echo $?
155

$ ./greet.sh --help
Usage: greet.sh <name>
$ echo $?
0 Note For more advanced flag parsing, consider using getopts to handle short and long options. 5. Resolving the Script’s Directory ( WORK_DIR ) Hard-coding relative paths can break when you run scripts from different locations. Instead, compute the script’s own directory: #!/usr/bin/env bash
readonly WORK_DIR=$(dirname ""$(readlink -f ""$0"")"") readlink -f ""$0"" returns the script’s canonical absolute path (following symlinks). dirname extracts the parent directory. Now you can reliably reference files relative to the script’s location: cd ""${WORK_DIR}/../assets""
cd ""${WORK_DIR}/subdir""
# ...other tasks Warning On macOS, readlink -f may not be available. Use brew install coreutils or alternative methods ( realpath ). 6. Quick Reference Table Feature Purpose Example $0 How the script was invoked echo ""$0"" ${0##*/} Basename of the script SCRIPT_NAME=${0##*/} Dynamic heredoc usage messages Embed SCRIPT_NAME in help text cat <<USAGE… terminate() Standardize error output and exit codes terminate ""message"" 42 readlink -f + dirname Compute absolute script directory ( WORK_DIR ) WORK_DIR=$(dirname ""$(readlink -f ""$0"")"")"" 7. Links and References Bash Parameter Expansion readlink (Linux) Manual getopts Bash Built-in These patterns make your Bash scripts more predictable, portable, and user-friendly—leveraging $0 effectively is a key skill for any shell scripter. Watch Video Watch video content Practice Lab Practice lab"
Advanced Bash Scripting,ifs,https://notes.kodekloud.com/docs/Advanced-Bash-Scripting/Special-Shell-Variables/ifs,"Advanced Bash Scripting Special Shell Variables ifs The Internal Field Separator ( IFS ) is a special shell variable that defines how Bash—and other POSIX-compatible shells—split strings into fields. While environment variables provide static, system-wide settings, shell variables like IFS are dynamic and scoped to your shell session. Properly managing IFS enhances portability by ensuring your scripts behave consistently across different systems. By default, IFS includes space, tab, and newline. This default behavior impacts many common tasks, such as iterating over lists of filenames or parsing command output. Table of Default IFS Separators Separator Escape Sequence Description Space <space> Splits on spaces Tab \t Splits on tabs Newline \n Splits on newlines Default IFS in Action Consider a simple Bash script that iterates over a space-separated list: #!/usr/bin/env bash
elements=""alpha beta gamma""
for element in ${elements}; do
  echo ""${element} is now separated from the elements list""
done Running this script yields: alpha is now separated from the elements list
beta is now separated from the elements list
gamma is now separated from the elements list Because elements splits on spaces by default, each word becomes its own iteration. Customizing IFS with ANSI-C Quoting To explicitly redefine IFS to include space, tab, and newline, use ANSI-C quoting: IFS=$' \t\n' Note The $'…' syntax tells Bash to process backslash escapes within single quotes, ensuring you include the exact characters you need. Changing the Field Separator If your data uses a different delimiter—such as a colon ( : )—override IFS before expanding variables. Without overriding, the string remains intact: #!/usr/bin/env bash
elements=""one:two:three""
for element in ${elements}; do
  echo ""${element} is now separated from the elements list""
done Output: one:two:three is now separated from the elements list Now set IFS to a colon: #!/usr/bin/env bash
IFS="":""
elements=""one:two:three""
for element in ${elements}; do
  echo ""${element} is now separated from the elements list""
done Output: one is now separated from the elements list
two is now separated from the elements list
three is now separated from the elements list Warning Changing IFS globally can affect subsequent commands and scripts. Always restore it to its default value after making temporary adjustments. IFS and Literal Strings IFS only applies when expanding variables. Looping over a quoted literal string bypasses field splitting: #!/usr/bin/env bash
IFS="":""
for element in ""one:two:three""; do
  echo ""${element} is a value""
done Output: one:two:three is a value Because there’s no variable expansion, IFS has no effect. Interactive IFS Changes You can also reassign IFS directly in your interactive shell session: $ IFS="",""
$ val=""apple,banana,cherry""
$ set -- ${val}
$ echo $1
apple
$ echo $2
banana
$ echo $3
cherry Here, set -- ${val} replaces the shell’s positional parameters with the split values of val . Restoring IFS and Handling Empty Strings Restore IFS to default IFS=$' \t\n' This command works in Bash, Zsh, and KornShell. Older shells might require specialized syntax. Set IFS to an empty string IFS='' When IFS is empty, no field splitting occurs—every expansion remains as a single, unsplit string. References Bash Manual: Special Parameters POSIX Shell Syntax Advanced Bash-Scripting Guide Watch Video Watch video content"
Advanced Jenkins,Create a Shared Library for Slack Notification,https://notes.kodekloud.com/docs/Advanced-Jenkins/Shared-Libraries-in-Jenkins/Create-a-Shared-Library-for-Slack-Notification,"Advanced Jenkins Shared Libraries in Jenkins Create a Shared Library for Slack Notification In this guide, you’ll learn how to extract Slack notification logic from a Jenkinsfile into a reusable Shared Library. By the end, any pipeline in your organization can send Slack updates with a single method call. Why Refactor Slack Notifications? Embedding Slack logic directly in each Jenkinsfile leads to: Duplication across repositories Harder maintenance when updating message formats Inconsistent notification behavior By centralizing the code in a Shared Library, you maintain one source of truth and simplify pipeline scripts. Current Jenkinsfile Implementation Here’s a typical approach you might find in your project (e.g., the Solar System repository): def slackNotificationMethod(String buildStatus = 'STARTED') {
    buildStatus = buildStatus ?: 'SUCCESS'

    def color
    if (buildStatus == 'SUCCESS') {
        color = '#47ec05'
    } else if (buildStatus == 'UNSTABLE') {
        color = '#d5ee0d'
    } else {
        color = '#ec2805'
    }

    def msg = ""${buildStatus}: *${env.JOB_NAME}* #${env.BUILD_NUMBER}:\n${env.BUILD_URL}""
    slackSend(color: color, message: msg)
}

pipeline {
    agent any
    // ...
} While functional, this code is locked to a single repository and must be copied everywhere you need it. Benefits of a Shared Library Benefit Description Single Source of Truth Update message format or colors in one place, apply everywhere Simplified Jenkinsfiles Pipelines call SlackNotification(...) instead of inlining Groovy logic Better Collaboration Teams can contribute improvements to notifications without touching individual repos 1. Create the Git Repository On your Git hosting platform (e.g., Gitea), create a new repository for your Shared Libraries. For example, under the dasher-org organization name it shared-libraries . 2. Initialize the Repository Clone and prepare the repo locally: git init
git checkout -b main
touch README.md
git add README.md
git commit -m ""Initial commit for Shared Libraries""
git remote add origin http://64.227.187.25:5555/dasher-org/shared-libraries.git
git push -u origin main For full details on directory layout and configuration, see the Jenkins Shared Libraries documentation . 3. Define the Directory Structure A standard Shared Library uses this layout: shared-libraries/
├── vars/
│   └── SlackNotification.groovy    # global step for Slack notifications
├── src/
│   └── org/
│       └── foo/
│           ├── Bar.groovy          # org.foo.Bar class
│           ├── foo.groovy          # global 'foo' variable
│           └── foo.txt             # help text for 'foo'
└── resources/
    └── org/
        └── foo/
            └── Bar.json            # static helper data 4. Add the Slack Notification Script Create vars/SlackNotification.groovy via your Git hosting UI or locally: Paste the extracted logic, rename the entry method to call so Jenkins treats it like a native step: // vars/SlackNotification.groovy
def call(String buildStatus = 'STARTED') {
    buildStatus = buildStatus ?: 'SUCCESS'

    def color
    if (buildStatus == 'SUCCESS') {
        color = '#47ec05'
    } else if (buildStatus == 'UNSTABLE') {
        color = '#d5ee0d'
    } else {
        color = '#ec2805'
    }

    def msg = ""${buildStatus}: ${env.JOB_NAME} #${env.BUILD_NUMBER}:\n${env.BUILD_URL}""
    slackSend(color: color, message: msg)
} Note Defining call in the vars directory lets you invoke SlackNotification(...) directly in any pipeline stage. 5. How the call Method Works Jenkins treats each Groovy script in vars/ with a call method as a pipeline step. For instance, vars/sayHello.groovy : // vars/sayHello.groovy
def call(String name = 'human') {
    echo ""Hello, ${name}.""
} You’d use it in a pipeline like: pipeline {
    agent any
    stages {
        stage('Greet') {
            steps {
                sayHello('Jenkins')
            }
        }
    }
} Likewise, after configuring your new Shared Library in Manage Jenkins → Configure System , invoke Slack notifications: pipeline {
    agent any
    stages {
        stage('Notify') {
            steps {
                SlackNotification('SUCCESS')
            }
        }
    }
} Warning Ensure your Jenkins instance has the Slack Plugin installed and configured with valid credentials. Next Steps Commit and push all changes to shared-libraries . Configure the Shared Library in Jenkins global settings: Library name: shared-libraries Default version: main Update your pipelines to replace inline Slack logic with SlackNotification(...) . Once configured, you’ll enjoy centralized, consistent Slack alerts across every Jenkins pipeline. References Jenkins Pipeline Shared Libraries Slack Plugin for Jenkins Watch Video Watch video content"
Advanced Jenkins,WhatWhyCreate LibraryResource,https://notes.kodekloud.com/docs/Advanced-Jenkins/Shared-Libraries-in-Jenkins/WhatWhyCreate-LibraryResource,"Advanced Jenkins Shared Libraries in Jenkins WhatWhyCreate LibraryResource Library Resources let you bundle non-Groovy static assets—such as shell scripts, YAML files, or configuration templates—inside your shared library’s resources directory. By using the libraryResource step in Jenkins pipelines, you can load these files at runtime, write them to the workspace, and execute or parse them as needed. In this guide, we’ll replace multiple hard-coded Trivy scan invocations with a single parameterized shell script ( trivy.sh ) stored under resources/scripts . You’ll learn how to load it dynamically from your shared library, keeping your pipeline code DRY and maintainable. 1. The Hardcoded Approach Here’s a typical vars/TrivyScan.groovy with duplicated logic for different severities: // vars/TrivyScan.groovy
def vulnerability(String imageName) {
    sh """"""
    trivy image ${imageName} \
        --severity LOW,MEDIUM,HIGH \
        --exit-code 0 \
        --quiet \
        --format json -o trivy-image-MEDIUM-results.json

    trivy image ${imageName} \
        --severity CRITICAL \
        --exit-code 1 \
        --quiet \
        --format json -o trivy-image-CRITICAL-results.json
    """"""
}

def reportsConverter() {
    sh '''
    trivy convert \
        --format template --template ""@/usr/local/share/trivy/templates/html.tpl""
    '''
} Each variation (severity, exit code, output) requires its own method or string interpolation—leading to code duplication and maintenance headaches. 2. Extracting a Parameterized Shell Script Instead of embedding multiple commands in Groovy, create a flexible Bash script that accepts arguments: File Structure Path Description resources/scripts/trivy.sh Parameterized Trivy scan script vars/loadScript.groovy Generic loader for any script in resources vars/TrivyScanScript.groovy Entry point for vulnerability scans resources/scripts/trivy.sh #!/bin/bash
#
# Usage:
#   trivy.sh <imageName> <severity> <exitCode>
#

echo ""imageName  = $1""
echo ""severity   = $2""
echo ""exitCode   = $3""

trivy image ""$1"" \
  --severity ""$2"" \
  --exit-code ""$3"" \
  --quiet \
  --format json \
  -o ""trivy-image-$2-results.json"" Warning Ensure you commit trivy.sh with executable permissions ( chmod +x trivy.sh ). Otherwise, Jenkins won’t be able to run it. 3. Creating a Generic Loader: loadScript.groovy Place the following in vars/loadScript.groovy . This step reads any file from resources/scripts and writes it to the workspace: // vars/loadScript.groovy
def call(Map config = [:]) {
    // config.name: filename under resources/scripts (e.g. ""trivy.sh"")
    def scriptData = libraryResource ""scripts/${config.name}""
    writeFile file: config.name, text: scriptData
    sh ""chmod +x ./${config.name}""
} libraryResource : Reads the script content as a string. writeFile : Persists it to the workspace. chmod +x : Makes it executable. For more details, see Jenkins libraryResource documentation . 4. Wiring It Together: TrivyScanScript.groovy Use your generic loader and invoke the script with parameters: // vars/TrivyScanScript.groovy
def vulnerability(Map config = [:]) {
    // Load trivy.sh from resources/scripts
    loadScript(name: 'trivy.sh')

    // Execute the script with user-supplied arguments
    sh ""./trivy.sh ${config.imageName} ${config.severity} ${config.exitCode}""
} config.imageName , config.severity , config.exitCode are passed from the Jenkinsfile. Omitting any required key triggers a clear Groovy map-handling error. Note You can extend this pattern to other scripts or configuration files without changing your library code. 5. Invoking from a Jenkinsfile Here’s how to call your new step in a declarative pipeline: @Library('shared-libraries') _
pipeline {
    agent any

    stages {
        stage('Vulnerability Scan') {
            steps {
                TrivyScanScript.vulnerability(
                    imageName: 'nginx:latest',
                    severity: 'HIGH',
                    exitCode: 1
                )
            }
        }
    }
} 6. Summary Extract repeated shell logic into resources/scripts/trivy.sh . Load it via a generic loadScript step using libraryResource . Invoke it parametrically in your Groovy entrypoint ( TrivyScanScript.groovy ). Customize severity levels, exit codes, and image names without editing library code. By following this pattern, your shared library remains DRY, maintainable, and flexible for future scan configurations. Links and References Jenkins libraryResource Step Trivy Official Documentation Jenkins Shared Libraries Watch Video Watch video content"
Advanced Jenkins,Load TrivyScan Library in Jenkins Pipeline,https://notes.kodekloud.com/docs/Advanced-Jenkins/Shared-Libraries-in-Jenkins/Load-TrivyScan-Library-in-Jenkins-Pipeline,"Advanced Jenkins Shared Libraries in Jenkins Load TrivyScan Library in Jenkins Pipeline Enhance your Jenkins CI/CD workflow by integrating a custom TrivyScan shared library. This guide walks you through creating the library, configuring Jenkins, referencing a feature branch, invoking scan methods, handling common errors, and reviewing pipeline artifacts. In this tutorial we will cover: Creating the TrivyScan Groovy script Configuring a global trusted library in Jenkins Referencing a feature branch in the Jenkinsfile Invoking library methods ( vulnerability and reportsConverter ) Handling “method calls not allowed” errors Reviewing the final pipeline run and published artifacts 1. TrivyScan Groovy Script Start by creating a new Git branch and adding the TrivyScan.groovy file under vars/ . This shared library defines two methods: git checkout -b featureTrivyScan // vars/TrivyScan.groovy

def vulnerability(String imageName) {
    sh """"""
    echo ""Scanning image: ${imageName}""
    trivy image ${imageName} \
        --severity LOW,MEDIUM,HIGH \
        --exit-code 0 \
        --quiet \
        --format json \
        -o trivy-image-MEDIUM-results.json

    trivy image ${imageName} \
        --severity CRITICAL \
        --exit-code 1 \
        --quiet \
        --format json \
        -o trivy-image-CRITICAL-results.json
    """"""
}

def reportsConverter() {
    sh """"""
    trivy convert \
        --format template \
        --template ""@/usr/local/share/trivy/templates/html.tpl"" \
        --output trivy-image-MEDIUM-results.html \
        trivy-image-MEDIUM-results.json

    trivy convert \
        --format template \
        --template ""@/usr/local/share/trivy/templates/html.tpl"" \
        --output trivy-image-CRITICAL-results.html \
        trivy-image-CRITICAL-results.json

    trivy convert \
        --format template \
        --template ""@/usr/local/share/trivy/templates/junit.tpl"" \
        --output trivy-image-MEDIUM-results.xml \
        trivy-image-MEDIUM-results.json

    trivy convert \
        --format template \
        --template ""@/usr/local/share/trivy/templates/junit.tpl"" \
        --output trivy-image-CRITICAL-results.xml \
        trivy-image-CRITICAL-results.json
    """"""
} Method Name Purpose Output Files vulnerability Scan Docker image for vulnerabilities trivy-image-MEDIUM-results.json , trivy-image-CRITICAL-results.json reportsConverter Convert JSON scan reports to HTML and JUnit trivy-image-MEDIUM-results.html , trivy-image-CRITICAL-results.html , *.xml Note Branching allows you to test changes in featureTrivyScan without affecting your main pipeline. 2. Configure Global Trusted Library in Jenkins As a Jenkins administrator: Navigate to Manage Jenkins > Configure System > Global Pipeline Libraries . Add a new library: Name: dasher-trusted-shared-library Default version: main Allow default version to be overridden : Enabled Warning Enabling “default version override” lets you specify feature branches like featureTrivyScan in your Jenkinsfile . 3. Reference the Feature Branch in Your Jenkinsfile At the top of your Jenkinsfile , use the @Library annotation to load the shared library from the featureTrivyScan branch: @Library('dasher-trusted-shared-library@featureTrivyScan') _ Define the rest of your declarative pipeline: pipeline {
    agent any

    tools {
        // Define tools here if needed
    }

    environment {
        MONGO_URI          = ""mongodb+srv://.../superData""
        MONGO_DB_CREDS     = credentials('mongo-db-credentials')
        SONAR_SCANNER_HOME = tool 'sonarqube-scanner-610'
        GITEA_TOKEN        = credentials('gitea-api-token')
    }

    stages {
        stage('Install Dependencies') {
            options { timestamps() }
            steps {
                // Your dependency install steps
            }
        }

        stage('Build Docker Image') {
            steps {
                sh 'docker build -t siddharth67/solar-system:$GIT_COMMIT .'
            }
        }

        stage('Trivy Vulnerability Scanner') {
            steps {
                script {
                    trivyScan.vulnerability(""siddharth67/solar-system:$GIT_COMMIT"")
                }
            }
            post {
                always {
                    script {
                        trivyScan.reportsConverter()
                    }
                    publishHTML([
                        allowMissing: true,
                        alwaysLinkToLastBuild: true,
                        keepAll: true,
                        reportDir: './'
                    ])
                }
            }
        }

        // stage('Push Docker Image') { ... }
    }
} 4. Invoking Shared-Library Methods In declarative pipelines, all calls to shared-library methods (for example, trivyScan.vulnerability(...) ) must be wrapped inside a script block: script {
    trivyScan.vulnerability(""your/image:tag"")
} 5. Handling Common “Method Calls Not Allowed” Errors If you encounter an error like: Method calls on objects not allowed outside ""script"" blocks. ensure you’ve moved every shared-library invocation into a script { ... } section, as shown above. 6. Reviewing Pipeline Run and Artifacts Once you push your updated Jenkinsfile , your pipeline (e.g., build #8) will: Fetch your shared library from featureTrivyScan Execute Trivy vulnerability scans Convert JSON results into HTML/JUnit reports Console snippet: > git fetch ... origin/featureTrivyScan
> git checkout -f refs/remotes/origin/featureTrivyScan
...
trivy image siddharth67/solar-system:<commit> --severity LOW,MEDIUM,HIGH ...
trivy image siddharth67/solar-system:<commit> --severity CRITICAL ...
trivy convert --format template ... html.tpl ... Artifacts include JSON, HTML, and XML reports: Summary Add your Groovy methods under vars/TrivyScan.groovy . Enable “default version override” in Jenkins global libraries. Reference your feature branch with @Library . Wrap all shared-library method calls in script blocks. Use trivyScan.vulnerability(...) and trivyScan.reportsConverter() for scanning and report conversion. Publish results with publishHTML . By modularizing your vulnerability scanning logic into a shared library, you keep your Jenkinsfile clean, reusable, and easy to maintain. Links and References Trivy Documentation Jenkins Pipeline Shared Libraries publishHTML Plugin Watch Video Watch video content"
Advanced Jenkins,Refactoring Solar System Pipeline,https://notes.kodekloud.com/docs/Advanced-Jenkins/Pipeline-Enhancement-and-Caching/Refactoring-Solar-System-Pipeline,"Advanced Jenkins Pipeline Enhancement and Caching Refactoring Solar System Pipeline Building on Jenkins’ extensible agent model, this guide demonstrates how to refactor an existing Solar System Pipeline so that all Node.js–based steps run inside a single Kubernetes Pod, while Docker builds and security scans execute on the controller node. 1. Current Jenkinsfile Using agent any The legacy Jenkinsfile relies on agent any , which executes every stage on the default executor. Here’s a simplified snippet: pipeline {
  agent any

  tools {
    // e.g., nodejs 'node-18'
  }

  environment {
    MONGO_URI          = ""mongodb+srv://supercluster.d83jj.mongodb.net/superData""
    MONGO_DB_CREDS     = credentials('mongo-db-credentials')
    MONGO_USERNAME     = credentials('mongo-db-username')
    MONGO_PASSWORD     = credentials('mongo-db-password')
    SONAR_SCANNER_HOME = tool 'sonarqube-scanner-610'
    GITEA_TOKEN        = credentials('gitea-api-token')
  }

  options {
    timestamps()
    buildDiscarder(logRotator(daysToKeepStr: '7'))
  }

  stages {
    stage('Installing Dependencies') {
      options { timestamps() }
      steps {
        sh 'npm install'
      }
    }
    // ...additional stages...
  }
} This approach can lead to inconsistent environments across stages and requires pre-installed tools on every Jenkins agent. 2. Defining a Kubernetes Pod Template Create a Pod definition file named k8s-agent.yaml at the repository root. This template spins up two Node.js containers, defaulting to node-18 : apiVersion: v1
kind: Pod
spec:
  containers:
    - name: node-18
      image: node:18-alpine
      command: [ ""cat"" ]
      tty: true
    - name: node-19
      image: node:19-alpine
      command: [ ""cat"" ]
      tty: true Note The command: [""cat""] and tty: true settings keep the container alive for Jenkins to execute steps interactively. 3. Referencing the Pod in Your Jenkinsfile Update your Jenkinsfile header to leverage the Kubernetes cloud and point to the Pod template: pipeline {
  agent {
    kubernetes {
      cloud 'dasher-prod-k8s-us-east'
      yamlFile 'k8s-agent.yaml'
      defaultContainer 'node-18'
    }
  }

  tools {
    // e.g., nodejs 'node-18'
  }

  environment {
    MONGO_URI          = ""mongodb+srv://supercluster.d83jj.mongodb.net/superData""
    MONGO_DB_CREDS     = credentials('mongo-db-credentials')
    MONGO_USERNAME     = credentials('mongo-db-username')
    MONGO_PASSWORD     = credentials('mongo-db-password')
    SONAR_SCANNER_HOME = tool 'sonarqube-scanner-610'
    GITEA_TOKEN        = credentials('gitea-api-token')
  }

  options {
    timestamps()
    timeout(time: 1, unit: 'HOURS')
  }

  stages {
    // Defined next...
  }
} 4. Configuring Node.js–Based Stages With the Kubernetes Pod in place, all Node.js stages execute inside node-18 : stages {
  stage('Installing Dependencies') {
    options { timestamps() }
    steps {
      sh 'node -v'
      sh 'npm install --no-audit'
    }
  }

  stage('Dependency Scanning') {
    parallel {
      stage('NPM Dependency Audit') {
        steps {
          sh '''
            node -v
            npm audit --audit-level=critical
            echo $?
          '''
        }
      }
    }
  }

  stage('Unit Testing') {
    options { retry(2) }
    steps {
      sh 'node -v'
      sh 'npm test'
    }
  }

  stage('Code Coverage') {
    steps {
      catchError(buildResult: 'SUCCESS', message: 'Coverage issues will be fixed later') {
        sh 'node -v'
        sh 'npm run coverage'
      }
    }
  }

  stage('Build Docker Image') {
    agent any
    steps {
      sh 'printenv'
      sh 'docker build -t siddharth67/solar-system:$GIT_COMMIT .'
    }
  }

  stage('Trivy Vulnerability Scanner') {
    agent any
    steps {
      script {
        trivyScanScript.vulnerability(imageName: ""siddharth67/solar-system:$GIT_COMMIT"", severity: ""LOW"")
        trivyScanScript.vulnerability(imageName: ""siddharth67/solar-system:$GIT_COMMIT"", severity: ""MED"")
        trivyScanScript.vulnerability(imageName: ""siddharth67/solar-system:$GIT_COMMIT"", severity: ""HIGH"")
      }
    }
  }
} Push your changes to trigger the updated pipeline automatically. 5. Observing the Refactored Pipeline In Blue Ocean or the classic Jenkins UI, the run displays each Kubernetes-backed stage: Once execution completes, the full pipeline view confirms success across all stages: 6. Verifying the Console Output The Jenkins console log provides details on Pod creation and step execution: > git --version # git version 2.30.2
> git fetch --no-tags --force --progress https://gitea-server/credentials
> git checkout -f 5ea4402af016919de95183e847d60b321d2fe8d
> git rev-list --no-walk 5ea4402af016919de95183e847d60b321d2fe8d Inside the node-18 container: + node -v
v18.20.4
+ npm install --no-audit
added 358 packages in 4s

+ npm audit --audit-level=critical
# (audit results)
+ echo 0
0

+ node -v
v18.20.4
+ npm test
+ node -v
v18.20.4
+ npm run coverage
# (coverage report) All Node.js stages share an emptyDir volume, so dependencies persist across steps without reinstallation. By consolidating Node.js workloads into a single Kubernetes Pod and delegating Docker builds and vulnerability scans to the controller, this solution ensures consistency, reduces setup time, and leverages cloud-native best practices. Links and References Jenkins Kubernetes Plugin Kubernetes Pod Templates Blue Ocean User Guide Watch Video Watch video content"
Advanced Jenkins,Demo Refactor existing Jenkinsfile,https://notes.kodekloud.com/docs/Advanced-Jenkins/Shared-Libraries-in-Jenkins/Demo-Refactor-existing-Jenkinsfile,"Advanced Jenkins Shared Libraries in Jenkins Demo Refactor existing Jenkinsfile In this lesson, we’ll walk through refactoring our existing Jenkinsfile to serve as the foundation for more advanced pipeline demos. You’ll learn how to: Extract reusable helpers (e.g., Slack notifications) Simplify and structure stages for clarity Spin up a new feature branch for future enhancements Verify the pipeline in both classic and Blue Ocean views By the end, you’ll have a lean, maintainable Jenkins pipeline ready for advanced integrations. Recap: Jenkins Multibranch Pipeline Log in to your Jenkins instance and find the multibranch Organization Folder containing the solar-system repository. This setup automatically discovers and builds branches as they appear in your Git server. Within solar-system , our last active branch was feature/enabling-slack . On the Git side, the project lives under the dasher-org organization in Gitea alongside related repositories. We pushed our last updates to the feature/enabling-slack branch. Step 1: Open the Jenkinsfile in VS Code Switch your local repo to feature/enabling-slack and open Jenkinsfile in Visual Studio Code (or your preferred editor). Prerequisites Ensure you have the following installed on your Jenkins agents: Node.js & npm Docker Engine Trivy scanner Slack Notification Plugin Step 2: Extract Slack Notification Helper To avoid repeating slackSend calls and status logic, define a reusable helper function at the top of your Jenkinsfile : // Slack Notification helper
def slackNotificationMethod(String buildStatus = 'STARTED') {
    buildStatus = buildStatus ?: 'SUCCESS'
    def color = (buildStatus == 'SUCCESS') ? '#47ec05' :
                (buildStatus == 'UNSTABLE') ? '#d5eed0' : '#ec2805'
    def msg = ""${buildStatus}: ${env.JOB_NAME} #${env.BUILD_NUMBER}:\n${env.BUILD_URL}""
    slackSend(color: color, message: msg)
} This helper centralizes notification logic and can be invoked in any post block. Step 3: Simplify Pipeline Structure We’re focusing on five key stages for our advanced demos. Comment out or remove any extra stages. The new skeleton looks like this: pipeline {
    agent any

    options {
        timestamps()
        skipDefaultCheckout()
    }

    environment {
        MONGO_URI          = ""mongodb+srv://supercluster.d83jj.mongodb.net/superData""
        MONGO_DB_CRED      = credentials('mongo-db-credentials')
        SONAR_SCANNER_HOME = tool 'sonarqube-scanner-610'
        GITEA_TOKEN        = credentials('gitea-api-token')
    }

    stages {
        stage('Installing Dependencies') { steps { sh 'npm install --no-audit' } }
        stage('Dependency Scanning') {
            parallel {
                stage('NPM Dependency Audit') { steps { sh 'npm audit --json > audit.json' } }
                // OWASP Dependency Check removed for this demo
            }
        }
        stage('Unit Testing')    { steps { sh 'npm test' } }
        stage('Code Coverage')   {
            steps {
                catchError(buildResult: 'SUCCESS', message: 'Coverage issues', stageResult: 'FAILURE') {
                    sh 'npm run coverage'
                }
            }
        }
        stage('Build Docker Image') { steps { sh 'docker build -t ${env.JOB_NAME}:${env.GIT_COMMIT} .' } }
        stage('Trivy Vulnerability Scanner') {
            steps {
                sh '''
                  trivy image ${env.JOB_NAME}:${env.GIT_COMMIT} \
                    --severity LOW,MEDIUM,HIGH --exit-code 0 --format json \
                    -o trivy-image-MEDIUM-results.json
                  trivy convert --format template \
                    --template ""/usr/local/share/trivy/templates/html.tpl"" \
                    --output trivy-image-MEDIUM-results.html \
                    trivy-image-MEDIUM-results.json
                  trivy convert --format template \
                    --template ""/usr/local/share/trivy/templates/junit.tpl"" \
                    --output trivy-image-CRITICAL-results.xml \
                    trivy-image-MEDIUM-results.json
                '''
            }
            post {
                always {
                    publishHTML(
                      allowMissing: true,
                      alwaysLinkToLastBuild: true,
                      keepAll: true,
                      reportDir: './',
                      reportFiles: 'trivy-image-MEDIUM-results.html',
                      reportName: 'Trivy HTML Report'
                    )
                    publishHTML(
                      allowMissing: true,
                      alwaysLinkToLastBuild: true,
                      keepAll: true,
                      reportDir: './',
                      reportFiles: 'trivy-image-CRITICAL-results.xml',
                      reportName: 'Trivy JUnit Report'
                    )
                }
            }
        }
    }

    post {
        always {
            slackNotificationMethod(currentBuild.currentResult)
            script {
                if (fileExists('solar-system-gitops-argocd')) {
                    sh 'rm -rf solar-system-gitops-argocd'
                }
            }
            junit allowEmptyResults: true, testResults: 'test-results.xml'
            junit allowEmptyResults: true, testResults: 'dependency-check-junit.xml'
            publishHTML(
              allowMissing: true,
              alwaysLinkToLastBuild: true,
              reportDir: 'coverage/',
              reportFiles: 'lcov.info',
              reportName: 'Coverage Report'
            )
        }
    }
} Key Pipeline Stages Summary Stage Purpose Installing Dependencies Install npm modules without audit Dependency Scanning Run npm audit in parallel Unit Testing Execute unit tests Code Coverage Generate coverage report (non-blocking) Build Docker Image Build container for deployment/testing Trivy Vulnerability Scanner Scan image, export HTML & JUnit reports Step 4: Create a New Branch for Advanced Demos Instead of modifying feature/enabling-slack , spin up a fresh feature branch: git checkout -b feature/advanced-demo origin/feature/enabling-slack
git push -u origin feature/advanced-demo Jenkins Multibranch Pipeline will auto-detect and trigger a build for the new branch. Step 5: Verify in Blue Ocean Open the Blue Ocean UI to get a visual representation of your streamlined pipeline. You should now see only the core stages instead of the full, unfiltered list. With your Jenkinsfile refactored and a dedicated branch in place, you’re all set to explore advanced pipeline features in upcoming sessions. Links and References Jenkins Pipeline Syntax Slack Notification Plugin Trivy: A Simple and Comprehensive Vulnerability Scanner Blue Ocean Plugin Watch Video Watch video content"
Advanced Jenkins,Stash and Unstash,https://notes.kodekloud.com/docs/Advanced-Jenkins/Pipeline-Enhancement-and-Caching/Stash-and-Unstash,"Advanced Jenkins Pipeline Enhancement and Caching Stash and Unstash In this lesson, you’ll learn how to use Jenkins’ stash and unstash directives to preserve files between stages in the same pipeline run. By default, stashed files are discarded at the end of the run. However, with the Declarative Pipeline preserveStashes option or the Pipeline: Durable Task plugin, you can retain them across a restart. Once stashed, files can be unstashed by name in any other stage—even on a different node. Note Use unique stash names to avoid collisions when multiple modules or artifacts are stashed in the same build. When to Use Stash-and-Unstash Consider a pipeline where your first stage installs Node.js dependencies, but a later stage runs on a different agent or container that can’t simply re-run npm install due to caching permissions or filesystem errors: + npm install --no-audit --cache .
npm ERR! code ENOTEMPTY
npm ERR! syscall rename
npm ERR! path /var/lib/jenkins/workspace/lar-system_feature_advanced-demo/node_modules/chai
npm ERR! dest /var/lib/jenkins/workspace/lar-system_feature_advanced-demo/node_modules/.chai-5ivncL6v
npm ERR! errno -39
npm ERR! ENOTEMPTY: directory not empty, rename '/var/lib/jenkins/workspace/lar-system_feature_advanced-demo/node_modules/chai' -> '/var/lib/jenkins/workspace/lar-system_feature_advanced-demo/logs/...-debug-0.log'
script returned exit code 217 To work around this, stash the entire node_modules folder after installation, then unstash it in any downstream stage. Generating the stash Snippet Go to Pipeline Syntax in your Jenkins instance. Select stash from the “Steps” dropdown. Fill in a unique Name (e.g. solar-system-node-modules ) and an Includes pattern ( node_modules/ ). Click Generate Pipeline Script . The generated snippet is: stash includes: 'node_modules/', name: 'solar-system-node-modules' Adding stash to Your Jenkinsfile Here’s a simple Declarative Pipeline before and after integrating the stash step: Before pipeline {
  agent any
  stages {
    stage('Installing Dependencies') {
      steps {
        sh 'node -v'
        sh 'npm install --no-audit'
      }
    }
    stage('Dependency Scanning') {
      steps {
        // ...
      }
    }
  }
} After pipeline {
  agent any
  options {
    timestamps()
  }
  stages {
    stage('Installing Dependencies') {
      steps {
        sh 'node -v'
        sh 'npm install --no-audit'
        stash includes: 'node_modules/', name: 'solar-system-node-modules'
      }
    }
    stage('Dependency Scanning') {
      steps {
        // ...
      }
    }
  }
} When this pipeline runs, you’ll see: [Pipeline] stash
Stashed 4898 files Comparing stash and unstash Directive Purpose Example stash Saves files/folders for later stages stash includes: 'node_modules/', name: 'modules' unstash Restores previously stashed content unstash 'modules' Using unstash in a Downstream Stage In any later stage—regardless of agent or node—you can restore the stashed files instead of reinstalling: pipeline {
  agent any
  stages {
    stage('NodeJS 20') {
      agent {
        // e.g. run in a Docker container or Kubernetes pod
      }
      stages {
        stage('Install Dependencies') {
          options {
            retry(2)
          }
          steps {
            sh 'node -v'
            // Restore the stash instead of npm install
            unstash 'solar-system-node-modules'
          }
        }
        stage('Testing') {
          steps {
            // Run tests against the restored node_modules
          }
        }
      }
    }
  }
} After committing and pushing, the build log shows both operations: [Pipeline] stash
Stashed 4898 files successfully for later use

[Pipeline] unstash
Restored files from stash 'solar-system-node-modules' Both install and test stages complete successfully—even on different agents—because the node_modules directory was preserved via stash . Warning Stashes are scoped to a single pipeline run. Without preserveStashes , all stashes are discarded when the pipeline finishes. Ensure you enable the appropriate option if you need stash data after a pipeline restart. Using stash and unstash in Jenkins pipelines lets you save and reuse any files or folders across stages and agents—ideal for large dependency trees, build artifacts, or generated assets. References Jenkins Pipeline Syntax Declarative Pipeline Options Watch Video Watch video content"
Advanced Jenkins,Introduction to Shared Libraries,https://notes.kodekloud.com/docs/Advanced-Jenkins/Shared-Libraries-in-Jenkins/Introduction-to-Shared-Libraries,"Advanced Jenkins Shared Libraries in Jenkins Introduction to Shared Libraries Jenkins Shared Libraries enable teams to centralize reusable Groovy scripts and pipeline logic in a single, version-controlled repository. By defining common steps—such as notifications, build commands, or deployment tasks—you can apply the DRY principle , reduce duplication, and ensure consistency across all your CI/CD pipelines. Consider a typical Jenkinsfile for a Node.js project: pipeline {
  agent any
  stages {
    stage('Build') {
      steps {
        sh 'npm install'
        sh 'npm run build'
      }
    }
    stage('Test') {
      steps {
        sh 'npm test'
      }
    }
  }
} Without a Shared Library, you’d copy and paste this snippet into every repository—leading to: Problem Impact Duplication Hard to maintain multiple Jenkinsfiles Inconsistency Different pipelines drift over time Complexity Updates become error-prone By extracting common logic—like setup, notifications, or deployment—into a central repository , you get: Better maintainability Unified pipeline behavior Easier on-boarding for new team members Example: Centralizing a Welcome Message Imagine your DevOps team wants every pipeline to greet developers: pipeline {
  agent any
  stages {
    stage('Welcome') {
      steps {
        sh 'echo Welcome to the DevOps team from Dasher Organization'
      }
    }
    // ...
  }
} When “Dasher” rebrands to “KodeKloud,” updating dozens of Jenkinsfiles is labor-intensive. Instead, create a welcome.groovy step in your Shared Library: // vars/welcome.groovy
def call() {
  sh 'echo Welcome to the DevOps team from KodeKloud Organization'
} Then, load and invoke this function in your Jenkinsfile: @Library('kode-kloud-shared-library') _
pipeline {
  agent any
  stages {
    stage('Welcome') {
      steps {
        welcome()
      }
    }
    // ...
  }
} Now, editing one file in the Shared Library updates every pipeline automatically. Setting Up a Shared Library in Jenkins To configure a Shared Library: Create a Git repository dedicated to your shared steps and utilities. In Jenkins, go to Manage Jenkins → Configure System → Global Pipeline Libraries . Click Add and specify: Name : A unique identifier (e.g., kode-kloud-shared-library ) Default Version : Branch or tag (e.g., main ) Retrieval Method : Modern SCM → Git URL Advanced Options : Allow overriding the default version, load implicitly, etc. Note Make sure Jenkins has read access to your Git repository. If you use credentials, add them under Manage Jenkins → Credentials . Repository Structure A clear directory layout helps Jenkins locate your scripts: (root)
├── src
│   └── org
│       └── foo
│           └── Bar.groovy       # Optional: compiled Groovy classes
├── vars
│   ├── welcome.groovy            # Defines the 'welcome' pipeline step
│   └── welcome.txt               # Documentation for 'welcome'
└── resources
    └── org
        └── foo
            └── bar.json          # Optional: static resource files Directory Purpose src (Optional) Groovy/Java classes vars Required : global pipeline steps (camelCase) resources (Optional) static files accessed via libraryResource Configuring Global Pipeline Libraries Once your repo is ready: Manage Jenkins → Configure System → Global Pipeline Libraries . Add Library with: Name : kode-kloud-shared-library Default Version : main Load implicitly (optional) SCM : Git URL and credentials Using Your Shared Library In each Jenkinsfile, include the @Library annotation to load your shared code: @Library('kode-kloud-shared-library') _
pipeline {
  agent any
  stages {
    stage('Welcome') {
      steps {
        welcome()
      }
    }
    stage('Build') {
      steps {
        // your build steps...
      }
    }
  }
} The welcome() step now references vars/welcome.groovy from your Shared Library, keeping pipelines concise and maintainable. Links and References Jenkins Pipeline Shared Libraries Groovy Documentation Git SCM DRY Principle Watch Video Watch video content"
Advanced Jenkins,Types of Agents,https://notes.kodekloud.com/docs/Advanced-Jenkins/Agents-and-Nodes-in-Jenkins/Types-of-Agents,"Advanced Jenkins Agents and Nodes in Jenkins Types of Agents Jenkins agents (also known as build nodes) provide executors that run your CI/CD workloads. Configured through a Jenkinsfile, each agent specifies how a worker node connects to the controller—using SSH, JNLP, or other protocols—and includes required build tools (e.g., JDK, Node.js). When using Docker , you containerize all dependencies into an image and spin up an isolated agent for each job. This ensures consistency across builds, simplifies version management, and avoids “works on my machine” problems. Jenkins supports a variety of agent types to fit different CI/CD requirements. Below is a quick overview: Agent Type Description Use Case Permanent Agents Statically configured nodes always online, with pre-installed tools and fixed resources. Stable builds with consistent toolchains Docker Agents Ephemeral containers created from custom images, destroyed after each build. Isolation, reproducibility, version control Cloud-Based Agents On-demand VMs or Kubernetes Pods via cloud plugins (AWS, Azure, GCP). Auto-scaled per workload. Elastic scaling and cost optimization Label-Based Agents Nodes tagged with labels (e.g., java , windows ). Pipelines request labels instead of nodes. Flexible matching based on node capabilities Declarative Pipeline Examples Below are common Declarative Pipeline patterns for specifying agents in a Jenkinsfile. 1. Any Available Agent Use agent any to let Jenkins route the job to the next free executor: pipeline {
  agent any
  stages {
    stage('Build') {
      steps {
        sh 'echo Running on ${NODE_NAME}'
      }
    }
  }
} 2. Label-Based Agent Specify agent { label 'my-agent' } to target nodes matching a label: pipeline {
  agent { label 'my-agent' }
  stages {
    stage('Build') {
      steps {
        sh 'echo Executing on ${NODE_NAME} (label: my-agent)'
      }
    }
  }
} Note Ensure that your Jenkins controller has at least one node configured with the my-agent label. Otherwise, builds will remain queued. 3. Docker Agent Spin up a Docker container as your build environment. Here we mount the NPM cache for faster installs: pipeline {
  agent {
    docker {
      image 'node:latest'
      args  '-v $HOME/.npm:/root/.npm'
    }
  }
  stages {
    stage('Build') {
      steps {
        sh 'node --version && npm --version'
      }
    }
  }
} Warning Mounting host directories into containers can expose sensitive data. Validate args parameters before use. 4. Default and Stage-Specific Agents Define a default agent at pipeline level and override it per stage: pipeline {
  agent { label 'my-agent' } // Default for all stages
  stages {
    stage('Build') {
      agent {
        docker { image 'node:14-alpine' }
      }
      steps {
        sh 'echo Building in Docker: ${NODE_NAME}'
      }
    }
    stage('Test') {
      steps {
        sh 'echo Testing on default agent: ${NODE_NAME}'
      }
    }
  }
} In this setup, the Build stage runs inside a Node.js Docker container, while Test uses the static my-agent . Links and References Jenkins Documentation Jenkins Pipeline Syntax Docker Official Site Kubernetes Documentation Watch Video Watch video content"
Advanced Jenkins,Loading the Shared Library in Pipeline,https://notes.kodekloud.com/docs/Advanced-Jenkins/Shared-Libraries-in-Jenkins/Loading-the-Shared-Library-in-Pipeline,"Advanced Jenkins Shared Libraries in Jenkins Loading the Shared Library in Pipeline This guide demonstrates how to replace inline methods with a centrally managed Shared Library in Jenkins. By loading common steps—like Slack notifications—from a library, you simplify maintenance, enforce consistency, and version your pipeline logic. Prerequisites Jenkins instance with Global Pipeline Libraries configured Access to the Git repository hosting your shared library Slack Plugin installed and a valid Slack bot token Verify that your library appears under Manage Jenkins » Configure System » Global Pipeline Libraries : After you save and refresh, Jenkins connects to your Git repo and displays the commit ID from the default branch—confirming connectivity. 1. Inline Slack Notification (Before) In a typical Jenkinsfile, you might define a slackNotificationMethod inline: def slackNotificationMethod(String buildStatus = 'STARTED') {
    buildStatus = buildStatus ?: 'SUCCESS'
    def color = (buildStatus == 'SUCCESS')  ? '#47ec05' :
                (buildStatus == 'UNSTABLE') ? '#d5ee0d' :
                                              '#ec2805'
    def msg = ""${buildStatus}: '${env.JOB_NAME}' #${env.BUILD_NUMBER}\n${env.BUILD_URL}""
    slackSend(color: color, message: msg)
}

pipeline {
    agent any
    stages {
        stage('Install Dependencies') {
            steps {
                sh 'npm install --no-audit'
            }
        }
    }
    post {
        always {
            slackNotificationMethod()
        }
    }
} Drawbacks of Inline Methods Duplication across multiple Jenkinsfiles Harder to maintain and version Inconsistent behavior if modified in one pipeline only 2. Switching to the Shared Library Replace the inline function by adding the @Library annotation at the top of your Jenkinsfile: @Library('dasher-trusted-shared-library') _

pipeline {
    agent any

    tools {
        // ...
    }

    environment {
        MONGO_URI          = ""mongodb+srv://supercluster.d83jj.mongodb.net/superData""
        MONGO_DB_CREDS     = credentials('mongo-db-credentials')
        MONGO_USERNAME     = credentials('mongo-db-username')
        MONGO_PASSWORD     = credentials('mongo-db-password')
        SONAR_SCANNER_HOME = tool 'sonarqube-scanner-6'
        GITEA_TOKEN        = credentials('gitea-api-token')
    }

    options {
        // ...
    }

    stages {
        stage('Install Dependencies') {
            steps {
                sh 'npm install --no-audit'
            }
        }
    }

    post {
        always {
            // Delegates to vars/slackNotification.groovy in the shared library
            slackNotification()
        }
    }
} Note The underscore ( _ ) after @Library('…') completes the annotation syntax but doesn’t affect runtime behavior. Environment Variables Reference Variable Description Source MONGO_URI MongoDB connection string Hardcoded MONGO_DB_CREDS MongoDB credentials (ID and secret) mongo-db-credentials MONGO_USERNAME MongoDB username (secret text) mongo-db-username MONGO_PASSWORD MongoDB password (secret text) mongo-db-password SONAR_SCANNER_HOME SonarQube Scanner installation directory Jenkins Tool GITEA_TOKEN API token for Gitea gitea-api-token Warning Always store sensitive data—like database credentials or API tokens—as Jenkins Credentials . Never hardcode secrets directly in your Jenkinsfiles. 3. Shared Library Implementation In your shared library repository, under vars/slackNotification.groovy , define the call method Jenkins will load: // vars/slackNotification.groovy
def call(String buildStatus = 'STARTED') {
    buildStatus = buildStatus ?: 'SUCCESS'
    def color = (buildStatus == 'SUCCESS')  ? '#47ec05' :
                (buildStatus == 'UNSTABLE') ? '#d5ee0d' :
                                              '#ee2805'
    def msg = ""${buildStatus}: '${env.JOB_NAME}' #${env.BUILD_NUMBER}\n${env.BUILD_URL}""
    slackSend(color: color, message: msg)
} This single implementation is now reusable across any pipeline that loads the library. 4. Triggering and Verifying the Build After committing your updated Jenkinsfile on the feature/advanced-demo branch, Jenkins will detect the change and queue a new build: The pipeline runs successfully and posts a notification to Slack for build #6. 5. Confirming Library Loading in Console To see how Jenkins fetches and loads your shared library, review the classic console output and filter for library : > git ls-remote -h http://64.227.187.25:5555/dasher-org/shared-libraries # timeout=10
> git init /var/lib/jenkins/workspace/feature/advanced-demo/libs/dasher-org/shared-libraries
[Pipeline] library
Loading library dasher-trusted-shared-library @ main from http://64.227.187.25:5555/dasher-org/shared-libraries
> git fetch --no-tags --force --progress -- http://64.227.187.25:5555/dasher-org/shared-libraries +refs/heads/main:refs/remotes/dasher-org/shared-libraries/main Later in the log, you’ll see the Slack step using values defined in the library: Slack Send Pipeline step running, values are -
  baseUrl: <empty>,
  teamDomain: Jenkins,
  channel: dasher-notifications,
  color: #47ec05,
  botUser: true,
  tokenCredentialId: slack-bot-token,
  notifyCommitters: false Conclusion By centralizing common pipeline logic in Jenkins Shared Libraries, you: Reduce code duplication Simplify maintenance and updates Manage multiple library versions via Git branches or tags Now you can create additional reusable steps or even combine multiple libraries for more complex workflows. References Jenkins Shared Libraries Slack Plugin for Jenkins Jenkins Credentials Documentation Watch Video Watch video content"
Advanced Jenkins,Utilize Docker Image Agent,https://notes.kodekloud.com/docs/Advanced-Jenkins/Agents-and-Nodes-in-Jenkins/Utilize-Docker-Image-Agent,"Advanced Jenkins Agents and Nodes in Jenkins Utilize Docker Image Agent In this guide, you’ll learn how to configure Jenkins Pipeline stages to run inside Docker containers using the Docker Pipeline Plugin. By the end, you’ll be able to pull official images (e.g., Node.js 18 Alpine) or build custom images from a Dockerfile, schedule them on specific nodes, and inspect the full console output. Prerequisites Ensure you have the Docker Pipeline Plugin installed and enabled: Configuring a Docker-Based Agent Navigate to Pipeline Syntax → Agent to generate the DSL snippet. You can choose: Directive Description dockerFile Build a custom image from a local Dockerfile . docker Pull and run a prebuilt Docker image. When selecting docker , you can configure: Parameter Purpose image Name of the Docker image (e.g., node:18-alpine ). args Additional flags for docker run . label (Optional) Restrict execution to nodes with this label. customWorkspace (Optional) Override the workspace directory inside the container. registryUrl (Optional) Private registry URL. credentialsId (Optional) Credentials for private registry. alwaysPull Force image pull on each build. Note Use alwaysPull true to ensure the latest image version is used, preventing stale caches. The generated snippet might look like this: agent {
    docker {
        alwaysPull true
        image 'node:18'
    }
} Example Pipeline Below is a sample Jenkinsfile demonstrating three stages: pipeline {
    agent any
    stages {
        stage('S1-Any Agent') {
            steps {
                sh 'cat /etc/os-release'
                sh 'node -v'
                sh 'npm -v'
            }
        }
        stage('S2-Ubuntu Agent') {
            agent { label 'ubuntu-docker-jdk17-node20' }
            steps {
                sh 'cat /etc/os-release'
                sh 'node -v'
                sh 'npm -v'
            }
        }
        stage('S3-Docker Image Agent') {
            // To be configured
        }
    }
} Adding a Docker Image Agent to Stage S3 To run S3-Docker Image Agent inside the official Node.js 18 Alpine container, update that stage: pipeline {
    agent any
    stages {
        stage('S1-Any Agent') { ... }
        stage('S2-Ubuntu Agent') { ... }
        stage('S3-Docker Image Agent') {
            agent {
                docker {
                    image 'node:18-alpine'
                }
            }
            steps {
                sh 'cat /etc/os-release'
                sh 'node -v'
                sh 'npm -v'
            }
        }
    }
} Scheduling on a Specific Node If you need the Docker container to run on a labeled node (for example, one with JDK 17 and Node 20), add the label directive: pipeline {
    agent any
    stages {
        stage('S1-Any Agent') { ... }
        stage('S2-Ubuntu Agent') { ... }
        stage('S3-Docker Image Agent') {
            agent {
                docker {
                    image 'node:18-alpine'
                    label 'ubuntu-docker-jdk17-node20'
                }
            }
            steps {
                sh 'cat /etc/os-release'
                sh 'node -v'
                sh 'npm -v'
            }
        }
    }
} Commit and push this Jenkinsfile to your repository, then trigger the build in Jenkins: Build Console Output When the pipeline runs, Jenkins will: Pull the Docker image if it’s not already available. Create and start a container on the designated agent. Execute the shell steps inside that container. Stop and remove the container once finished. Example console output illustrating the pull: > docker pull node:18-alpine
18-alpine: Pulling from library/node
...
Status: Downloaded newer image for node:18-alpine
docker.io/library/node:18-alpine Within the running container: + cat /etc/os-release
NAME=""Alpine Linux""
ID=alpine
VERSION_ID=3.20.3
PRETTY_NAME=""Alpine Linux v3.20""
...
+ node -v
v18.20.4
+ npm -v
10.7.0 Cleanup at the end of the stage: > docker stop --time=1 a71ce79625ec636f...
> docker rm -f -v a71ce79625ec636f... Verifying on the Jenkins Agent After completion, confirm no containers are running: docker ps
# no containers
docker ps -a
# previous containers removed
docker images
# node:18-alpine should appear in the list Conclusion You’ve configured a Jenkins Pipeline stage to execute inside a Docker container using a prebuilt image. For more control over dependencies and environment setup, consider building custom images with a Dockerfile . Links and References Jenkins Pipeline Syntax Docker Workflow Plugin Node.js Docker Official Images Watch Video Watch video content"
Advanced Jenkins,Configure cloud instances Kubernetes,https://notes.kodekloud.com/docs/Advanced-Jenkins/Agents-and-Nodes-in-Jenkins/Configure-cloud-instances-Kubernetes,"Advanced Jenkins Agents and Nodes in Jenkins Configure cloud instances Kubernetes Integrating Jenkins with Kubernetes enables dynamic agent provisioning, improving build scalability and resource utilization. This guide walks through installing the Kubernetes plugin, setting up credentials, and configuring a cloud instance in Jenkins. 1. Install the Kubernetes Plugin In Jenkins, go to Manage Jenkins → Manage Plugins . Under the Available tab, filter for Cloud plugins. Find Kubernetes and click Install without restart . To install via CLI (pin to a specific version): jenkins-plugin-cli --plugins kubernetes:4295.v7fa_01b_309c95 To upload an .hpi manually, switch to the Advanced tab and enter: https://updates.jenkins.io/download/plugins/kubernetes/4295.v7fa_01b_309c95/kubernetes.hpi Restart Jenkins after installation. 2. Verify Plugin Installation After Jenkins restarts: Go to Manage Jenkins → Manage Plugins → Installed . Search for “Kubernetes” to confirm the plugin is active. 3. Add a Kubernetes Cloud Navigate to Manage Jenkins → Manage Nodes and Clouds → Configure Clouds . Click Add a new cloud (➕) and choose Kubernetes . Give it a name (e.g., prod-k8s-us-east ). Leave other fields blank for now; you’ll configure credentials next. 4. (Option A) Use a Full Kubeconfig File Warning Using a full kubeconfig grants admin-level access. Do not use this in production. Verify your nodes: kubectl get nodes Export the raw config: kubectl config view --raw > kubeconfig.yaml In Jenkins Credentials , add a Secret file credential and upload kubeconfig.yaml . In the cloud settings, select this credential under Kubernetes Namespace and Test Connection . After testing, remove this credential and follow the least-privilege approach below. 5. Create a Dedicated Namespace and Service Account Use the least-privilege principle: kubectl create namespace jenkins
kubectl -n jenkins create serviceaccount jenkins-service-account
kubectl -n jenkins create token jenkins-service-account --duration=9999999s In Jenkins Credentials , add a Secret text credential: Kind : Secret text Secret : Paste the token ID : jenkins-service-account-token 6. Configure the Cloud with Token Credentials In the Kubernetes cloud settings: Kubernetes URL : Your cluster endpoint (e.g., https://<cluster-endpoint> ) Credentials : jenkins-service-account-token Kubernetes Namespace : jenkins Click Test Connection . If you see certificate errors, either check Skip Certificate Verification or supply your CA certificate. On first try you might get 403 Forbidden —proceed to bind roles. 7. Grant Namespace Permissions Bind the admin role to your service account in the jenkins namespace: kubectl -n jenkins create rolebinding jenkins-admin-binding \
  --clusterrole=admin \
  --serviceaccount=jenkins:jenkins-service-account Retry Test Connection ; it should now succeed. 8. Advanced Connection Settings TCP (JNLP) Ports : Default agent communication. WebSocket : Use if TCP ports are blocked. Direct Connection : Override Jenkins URL (for proxies or gateways). Setting Use Case TCP (JNLP) Standard agent connections WebSocket In restrictive network environments Direct Connection Custom URL for Jenkins server behind a proxy or ingress 9. Define Pod Templates and Retention Set a Pod Label (e.g., organization=KodeKloudAzureArc ). Add Container Templates for your build tools and environments. Configure Pod Retention : Retention Option Behavior Never Delete pods once builds finish On failure Keep pods only if the build fails Always Never delete pods (for debugging) 10. Finalize and Save Review your settings, then click Save . Your Jenkins instance can now dynamically provision Kubernetes agents for pipelines. Now you’re ready to run CI/CD pipelines using Kubernetes-based agents. For more details, see the Jenkins Kubernetes Plugin Documentation and the Kubernetes Service Account Concepts . Watch Video Watch video content"
Advanced Jenkins,Pipeline Caching,https://notes.kodekloud.com/docs/Advanced-Jenkins/Pipeline-Enhancement-and-Caching/Pipeline-Caching,"Advanced Jenkins Pipeline Enhancement and Caching Pipeline Caching Efficient dependency caching in a CI/CD pipeline can dramatically reduce build times by reusing previously downloaded libraries and generated artifacts. In Jenkins, the Job Cacher Plugin enables you to store and restore cache items—such as node_modules —across pipeline runs, even in ephemeral environments like containers. Why Cache? Caching avoids repeated downloads and installations, leading to faster feedback loops and lower resource usage. It’s especially helpful for languages and frameworks with large dependency trees (e.g., Node.js, Python). Example: Arbitrary File Cache Configuration Below is a sample YAML definition showing how to cache an arbitrary folder. You can adapt this for different languages or tools. arbitraryFileCache:
  path: ""my-cache""
  cacheValidityDecidingFile: ""src/_src/*.generated""
  includes: ""**/*""
  excludes: ""**/*.generated"" Field Description Example path Directory to cache node_modules cacheValidityDecidingFile File that triggers cache invalidation when it changes package-lock.json includes Glob pattern to include in cache **/* excludes Glob pattern to exclude from cache **/*.generated Installing the Job Cacher Plugin Navigate to Manage Jenkins > Manage Plugins > Available . Search for Job Cacher Plugin and install it. Restart Jenkins to activate the plugin. Generating Cache Snippet with the Snippet Generator Use the Pipeline Syntax Snippet Generator to create a cache step: Go to Pipeline > Snippet Generator . Select the cache step. Fill in the fields: Path : node_modules Cache name : npm-dependency-cache Includes : **/* Excludes : (leave default) Cache validity deciding file : package-lock.json Compression : tar or zip Max cache size : 550 (MB) Generated snippet: cache(
  caches: [
    arbitraryFileCache(
      cacheName: 'npm-dependency-cache',
      cacheValidityDecidingFile: 'package-lock.json',
      includes: '**/*',
      path: 'node_modules'
    )
  ],
  maxCacheSize: 550
) {
  // your build steps here
} Integrating Caching into Your Jenkinsfile Insert the cache block around your dependency installation stage. Here’s a streamlined Jenkinsfile example: pipeline {
  agent any

  stages {
    stage('Installing Dependencies') {
      options { timestamps() }
      steps {
        cache(
          maxCacheSize: 550,
          caches: [
            arbitraryFileCache(
              cacheName: 'npm-dependency-cache',
              cacheValidityDecidingFile: 'package-lock.json',
              includes: '**/*',
              path: 'node_modules'
            )
          ]
        ) {
          sh 'node -v'
          sh 'npm install --no-audit'
          stash(includes: 'node_modules', name: 'npm-node-modules')
        }
      }
    }

    stage('Dependency Scanning') {
      steps {
        // Add your scanning logic here
      }
    }

    stage('Unit Testing') {
      parallel {
        stage('NodeJS 18') {
          options { retry(2) }
          steps {
            unstash 'npm-node-modules'
            sh 'node -v'
            sh 'npm test'
          }
        }
        stage('NodeJS 19') {
          options { retry(2) }
          steps {
            container('node-19') {
              unstash 'npm-node-modules'
              sh 'node -v'
              sh 'npm test'
            }
          }
        }
      }
    }

    stage('Code Coverage') {
      steps {
        catchError(buildResult: 'SUCCESS', message: 'Coverage step failed', stageResult: 'FAILURE') {
          unstash 'npm-node-modules'
          sh 'node -v'
          sh 'npm run coverage'
        }
      }
    }
  }
} Executing and Verifying the Pipeline Commit and push your updated Jenkinsfile . On the next build, you’ll see cache operations in the console: Searching cache in job specific caches...
Searching cache in default caches...
Skip restoring cache as no up-to-date cache exists
> node -v
v22.6.0
> npm install --no-audit
up to date in 1s
...
Stashed 4993 file(s)
Creating cache for node_modules (npm-dependency-cache) [id: 3ec03583f8eaec275c2183db769ff47]
Cache created in 164ms After success, the Blue Ocean UI highlights each stage: In the classic UI, inspect cache logs under Build Details : Console output for the Installing Dependencies stage: {cache for node_modules (npm-dependency-cache) with id ...} Searching cache in job specific caches...
{cache for node_modules (npm-dependency-cache) with id ...} got hash a47b9ef602dbc79d72ab6385105e0142 for /var/lib/jenkins/.../package-lock.json
{cache for node_modules ...} Restoring cache...
> npm install --no-audit
up to date in 1s
...
{cache for node_modules ...} Skipped saving cache as it is up to date On subsequent runs, the plugin will restore the cache automatically and skip redundant installations—saving precious build time. Next Steps Modify dependencies in package-lock.json to trigger cache invalidation. Experiment with different maxCacheSize values. Combine caching with parallel stages for maximum efficiency. References Jenkins Job Cacher Plugin Jenkins Pipeline Syntax Node.js Official Documentation Watch Video Watch video content"
Advanced Jenkins,Configure Shared Library in Jenkins,https://notes.kodekloud.com/docs/Advanced-Jenkins/Shared-Libraries-in-Jenkins/Configure-Shared-Library-in-Jenkins,"Advanced Jenkins Shared Libraries in Jenkins Configure Shared Library in Jenkins Centralize reusable Groovy scripts by registering a Jenkins Shared Library . This guide covers: Defining library functions (e.g., Slack notifications) Adding a Shared Library in Jenkins Referencing and importing library code in pipelines Fetching third-party dependencies with @Grab Bundling and retrieving resource files 1. Example: Slack Notification Script Place this Groovy function under vars/notify.groovy in your Shared Library to send build status updates to Slack: def call(String buildStatus = 'STARTED') {
    buildStatus = buildStatus ?: 'SUCCESS'
    def color = (buildStatus == 'SUCCESS') ? '#47ec05' :
                (buildStatus == 'UNSTABLE') ? '#d5ee0d' : '#ec2805'
    def msg = ""${buildStatus}: ${env.JOB_NAME} #${env.BUILD_NUMBER}\n${env.BUILD_URL}""
    slackSend(color: color, message: msg)
} Build Status Color Code SUCCESS #47ec05 UNSTABLE #d5ee0d FAILED/OTHER #ec2805 2. Register the Shared Library Go to Manage Jenkins → Configure System . Scroll to Global Pipeline Libraries . Note By default, libraries run in Jenkins’ Groovy sandbox. Disabling the sandbox allows static calls, @Grab , and advanced APIs but bypasses script-security checks. Field Description Example Name Unique identifier for the library DasherSharedLib Default version Git branch or tag used when unqualified main Load implicitly Auto-load library without @Library Off Allow version override Permit @Library('name@branch') to select alternate branches Enabled Include in job recent changes Trigger pipelines when the library code changes Opt-in Retrieval method SCM plugin to fetch the library (Git, SVN, etc.) Git Project repository Repository URL https://github.com/... Click Add under Global Pipeline Libraries . Fill out the form using the table above. Click Apply and Save . 3. Reference the Library in Your Pipeline Add the library at the top of your Jenkinsfile : @Library('DasherSharedLib@main') _
import com.mycorp.pipeline.slack.notify

pipeline {
    agent any
    stages {
        stage('Notify Start') {
            steps {
                notify('STARTED')
            }
        }
    }
} 3.1 Fetching Third-Party Dependencies Trusted Shared Libraries can include @Grab annotations to pull Maven artifacts: @Grab('org.apache.commons:commons-math3:3.4.1')
import org.apache.commons.math3.primes.Primes

void checkPrime(int count) {
    if (!Primes.isPrime(count)) {
        error ""${count} is not prime""
    }
} Retrieve packed resources (e.g., JSON templates) with libraryResource : def template = libraryResource 'com/mycorp/pipeline/somelib/request.json' Warning @Grab and libraryResource require the library to run outside the sandbox. Sandboxed mode will block these calls with security exceptions. 4. Next Steps Create a pipeline to invoke your Shared Library methods. Explore Pipeline Syntax for advanced usage. Links and References Jenkins Shared Libraries Pipeline Syntax Script Security Plugin Watch Video Watch video content"
Advanced Jenkins,Utilize Kubernetes Pod as Agent,https://notes.kodekloud.com/docs/Advanced-Jenkins/Agents-and-Nodes-in-Jenkins/Utilize-Kubernetes-Pod-as-Agent,"Advanced Jenkins Agents and Nodes in Jenkins Utilize Kubernetes Pod as Agent In this guide, you’ll learn how to run Jenkins build agents inside Kubernetes Pods using a Declarative Pipeline. We assume you have already installed and configured the Kubernetes plugin for Jenkins . Prerequisites A running Jenkins instance with the Kubernetes plugin installed Access to a Kubernetes cluster and proper credentials configured under Manage Jenkins → Configure System → Cloud → Kubernetes Note Make sure your Jenkins service account has permissions to create and delete Pods in the target namespace. 1. Create a New Pipeline Job Open the Jenkins Dashboard and click New Item . Enter a name, for example k8s-cloud-agent-demo , and select Pipeline . Scroll to the Pipeline section, choose Pipeline script , then pick the Declarative Kubernetes sample. 2. Define a Basic Kubernetes Agent The default Declarative example includes an agent block with containerTemplate . To get started quickly, simplify it to print the Pod’s hostname: pipeline {
  agent { kubernetes {
    containerTemplate {
      name 'shell'
      image 'ubuntu'
      command 'sleep'
      args 'infinity'
    }
  } }
  stages {
    stage('Print Hostname') {
      steps {
        sh 'hostname'
        sh 'sleep 120s'
      }
    }
  }
} To focus further, remove the containerTemplate and let Jenkins use defaults: pipeline {
  agent { kubernetes { } }
  stages {
    stage('Print Hostname') {
      steps {
        sh 'hostname'
        sh 'sleep 120s'
      }
    }
  }
} By default, Pods are deleted after the build completes ( podRetention: Never ). Warning With the default retention policy ( Never ), Pods are removed immediately after each build. Disable or adjust this if you need to debug post-build. 3. Inline Pod Template with YAML For more control, define your Pod in YAML directly within the pipeline: pipeline {
  agent {
    kubernetes {
      yaml '''
apiVersion: v1
kind: Pod
spec:
  containers:
  - name: ubuntu-container
    image: ubuntu
    command:
    - sleep
    args:
    - infinity
'''
    }
  }
  stages {
    stage('Print Hostname') {
      steps {
        sh 'hostname'
      }
    }
  }
} You can also reference an external Pod template file in multibranch pipelines: agent {
  kubernetes {
    yamlFile 'jenkins-pod.yaml'
  }
} Global Pod templates can be managed under Manage Jenkins → Kubernetes . 4. Run the Pipeline Save the Pipeline script. Click Build Now . Jenkins will provision a Kubernetes Pod based on your definition. In the console output, look for Pod creation logs and your hostname: [Pipeline] sh
jenkins-agent-abc123
[Pipeline] // sh 5. Default Cloud & Advanced Configuration If you don't specify a cloud in the agent block, Jenkins selects the first available Kubernetes cloud. You can view options in the Declarative Directive Generator: agent {
  kubernetes {
    // ...
  }
} Select a Specific Cloud and Namespace pipeline {
  agent {
    kubernetes {
      cloud 'dasher-prod-k8s-us-east'
      yaml '''
apiVersion: v1
kind: Pod
spec:
  containers:
  - name: ubuntu-container
    image: ubuntu
    command:
    - sleep
    args:
    - infinity
'''
    }
  }
  stages {
    stage('Print Hostname') {
      steps {
        sh 'hostname'
      }
    }
  }
} 6. Example: Multiple Containers Define multiple containers in the same Pod to run different tools per stage: pipeline {
  agent {
    kubernetes {
      cloud 'dasher-prod-k8s-us-east'
      yaml '''
apiVersion: v1
kind: Pod
spec:
  containers:
  - name: ubuntu-container
    image: ubuntu
    command:
    - sleep
    args:
    - infinity
  - name: node-container
    image: node:18-alpine
    command:
    - cat
    tty: true
'''
    }
  }
  stages {
    stage('Print Hostname') {
      steps {
        sh 'hostname'
      }
    }
    stage('Print Node Version') {
      steps {
        container('node-container') {
          sh 'node -v'
          sh 'npm -v'
        }
      }
    }
  }
} If you omit container('node-container') , the Node.js commands default to ubuntu-container and will fail. 7. Inspecting Pods and Events During a build, you can inspect Pods and events with kubectl : # List Jenkins agent Pods
kubectl -n jenkins get pods

# View Pod lifecycle events
kubectl -n jenkins get events You’ll see scheduling, image pull, start, and termination events for each container. Conclusion Leveraging Kubernetes Pods as Jenkins agents in Declarative Pipelines offers: Custom environments per stage (e.g., Ubuntu, Node.js) Ephemeral build agents that spin up and down automatically Inline YAML definitions or external templates for reusable configurations Start defining your CI/CD workloads with fine-grained control over tools and dependencies! Links and References Kubernetes Basics Jenkins Kubernetes Plugin Declarative Pipeline Syntax Watch Video Watch video content"
Advanced Jenkins,Create and Configure Node,https://notes.kodekloud.com/docs/Advanced-Jenkins/Agents-and-Nodes-in-Jenkins/Create-and-Configure-Node,"Advanced Jenkins Agents and Nodes in Jenkins Create and Configure Node This guide walks you through setting up a new Jenkins agent (node) on a dedicated Ubuntu VM— Ubuntu-Docker-JDK17-Node20 —for distributed builds. You’ll learn how to register the agent in Jenkins, launch it via JNLP, resolve common connection errors, and verify that it’s online. 1. Review Existing Nodes In Jenkins, go to Manage Jenkins → Nodes . By default, you’ll see only the built-in controller node. 2. Add a New Node Click New Node . Enter a name (for example, Ubuntu-Agent ) and choose Permanent Agent . Configure the fields below: Field Description Example Description Optional notes about this agent. “Docker + JDK17 on Ubuntu” # of executors Maximum parallel builds. 2 Remote root directory Workspace, logs, and temp files on the agent. /home/jenkins-agent Labels Tags to match pipeline stages or jobs. docker , jdk17 Usage Controls job assignment. “Only build jobs with label expressions matching this node.” Under Launch method , select Launch agent by connecting it to the controller and set Availability to keep it always online. Add any Node Properties you need—disk space monitoring, environment variables, tool locations, etc. Click Save . The new node appears offline until you launch its agent process. 3. Launch the Agent via JNLP Download agent.jar and start the agent on your VM. Here’s a quick reference for JNLP options: Option Description -url Jenkins controller URL -secret Agent’s secret for authentication -name Node name as registered in Jenkins -workDir Local workspace on the agent Sample Commands # Unix (inline secret)
curl -sO http://64.227.187.25:8080/jnlpJars/agent.jar
java -jar agent.jar \
  -url http://64.227.187.25:8080/ \
  -secret 687ec2b7…5c4a6f41 \
  -name ""ubuntu-agent"" \
  -workDir ""/home/jenkins-agent"" :: Windows (curl.exe)
curl.exe -sO http://64.227.187.25:8080/jnlpJars/agent.jar
java -jar agent.jar ^
  -url http://64.227.187.25:8080/ ^
  -secret 687ec2b7…5c4a6f41 ^
  -name ""ubuntu-agent"" ^
  -workDir ""C:\jenkins-agent"" # Unix (secret file)
echo 687ec2b7…5c4a6f41 > secret-file
curl -sO http://64.227.187.25:8080/jnlpJars/agent.jar
java -jar agent.jar \
  -url http://64.227.187.25:8080/ \
  -secret @secret-file \
  -name ""ubuntu-agent"" \
  -workDir ""/home/jenkins-agent"" On your Ubuntu VM: cd /home
curl -sO http://64.227.187.25:8080/jnlpJars/agent.jar
java -jar agent.jar \
  -url http://64.227.187.25:8080/ \
  -secret 687ec2b7…5c4a6f41 \
  -name ""ubuntu-agent"" \
  -workDir ""/home/jenkins-agent"" Note Ensure the agent’s JDK version matches the controller’s JDK to avoid compatibility issues. Common Connection Error If your agent logs show a 404 for .../jnlpAgentListener/ , it means the inbound agent TCP port is disabled. Warning Inbound agents require a TCP port. Go to Manage Jenkins → Configure Global Security and set TCP port for inbound agents to Fixed or Random . Save and re-run the JNLP command. You should see: INFO: Using /home/jenkins-agent as a remoting work directory
INFO: Setting up agent: ubuntu-agent
INFO: Locating server among [http://64.227.187.25:8080]
INFO: Connected 4. Verify the Agent in Jenkins Go back to Manage Jenkins → Nodes . Confirm that ubuntu-agent is online and ready. Click on the agent name to explore: System Information (JVM, OS, environment) JavaMelody Monitoring (threads, processes, GC stats) Agent Log (connection status, version) 5. Inspect the Agent VM On the agent VM, verify the working directory structure: root@ubuntu-docker-jdk17-node20:/home# ls
agent.jar  jenkins-agent

root@ubuntu-docker-jdk17-node20:/home# cd jenkins-agent/
root@ubuntu-docker-jdk17-node20:/home/jenkins-agent# ls
remoting As you run builds, Jenkins will create workspace , logs , and artifacts folders under /home/jenkins-agent . Links and References Jenkins Distributed Builds Managing Nodes - Jenkins User Handbook JNLP Agents Watch Video Watch video content"
Advanced Jenkins,Utilize Dockerfile Agent,https://notes.kodekloud.com/docs/Advanced-Jenkins/Agents-and-Nodes-in-Jenkins/Utilize-Dockerfile-Agent,"Advanced Jenkins Agents and Nodes in Jenkins Utilize Dockerfile Agent Overview Leveraging a Dockerfile agent in Jenkins lets you build a custom Docker image with all the CLI tools your pipeline requires. This approach ensures: Consistent environments across builds Isolation for each job Flexibility to install any SDKs, CLIs, or dependencies Prerequisites Requirement Purpose Jenkins 2.10+ Pipeline support for dockerfile agent Docker Daemon Must be available on the agent node Git Repository Contains both Jenkinsfile and Dockerfile.cowsay Note Ensure your Jenkins agent node has Docker installed and that the Jenkins user has permission to run Docker commands. Step 1: Example with the Standard Node.js Alpine Image Here’s a simple Jenkinsfile using the official node:18-alpine image: pipeline {
    agent any
    stages {
        stage('S4 - Standard Node.js Image') {
            agent {
                docker {
                    image 'node:18-alpine'
                    label 'ubuntu-docker-jdk17-node20'
                }
            }
            steps {
                sh 'node -v'
                sh 'npm -v'
                sh 'cowsay -f dragon ""Hello from Docker Container""'
            }
        }
    }
} When executed, the first two commands succeed, but cowsay is missing: /home/jenkins-agent/.../script.sh: line 1: cowsay: not found Step 2: Create a Custom Dockerfile To include cowsay , create Dockerfile.cowsay : FROM node:18-alpine

RUN apk update && \
    apk add --no-cache git perl && \
    git clone https://github.com/jasonm23/cowsay.git /tmp/cowsay && \
    cd /tmp/cowsay && \
    ./install.sh /usr/local && \
    rm -rf /tmp/cowsay This Dockerfile: Updates the Alpine package index Installs Git and Perl Clones and installs cowsay Cleans up temporary files Warning Always remove temporary directories like /tmp/cowsay to keep your image size small. Step 3: Switch to the dockerfile Agent Update your Jenkinsfile to build the custom image on the fly: pipeline {
    agent any
    stages {
        stage('S4 - Dockerfile Agent') {
            agent {
                dockerfile {
                    filename 'Dockerfile.cowsay'
                    label 'ubuntu-docker-jdk17-node20'
                }
            }
            steps {
                sh 'node -v'
                sh 'npm -v'
                sh 'cowsay -f dragon ""This is running on Docker Container""'
            }
        }
    }
} Behind the scenes, Jenkins will: Read Dockerfile.cowsay from the workspace Run: docker build -t <generated-image-id> -f ""Dockerfile.cowsay"" . Launch a container from the new image and execute your steps Step 4: Confirm Successful Build In the Jenkins console you’ll see: docker build -t 0f559f3f6a2c220b616594d407b50bd36d837f4 -f ""Dockerfile.cowsay"" .
docker inspect -f '{{.Id}}' 0f559f3f6a2c220b616594d407b50bd36d837f4
v18.20.4
10.7.0
This is running on Docker Container All commands now succeed, including cowsay . Links and References Jenkins Pipeline Agents Dockerfile Reference Node.js Official Docker Images cowsay GitHub Repository Watch Video Watch video content"
Advanced Jenkins,Utilize Agents in Jobs,https://notes.kodekloud.com/docs/Advanced-Jenkins/Agents-and-Nodes-in-Jenkins/Utilize-Agents-in-Jobs,"Advanced Jenkins Agents and Nodes in Jenkins Utilize Agents in Jobs In this guide, you’ll learn how to delegate Jenkins work to external agents. We’ll first configure a Freestyle job to run on an Ubuntu agent, then build a Declarative Pipeline that switches between the controller and the same Ubuntu node. Using an External Agent in a Freestyle Job Ensure your external Ubuntu agent is online and labeled correctly before proceeding. Note Verify the agent’s status under Manage Jenkins > Manage Nodes and confirm that SSH connectivity and labels are configured. 1. Inspect Available Nodes Navigate to Manage Jenkins > Manage Nodes to view connected agents: 2. Create a New Freestyle Job Click New Item , enter freestyle-external-agent , and select Freestyle project : In the job configuration, add a Build Step → Execute shell : Insert the following commands: cat /etc/os-release
node -v
npm -v 3. Restrict Execution to the Ubuntu Agent Copy the ubuntu-agent label: Under General , check Restrict where this project can be run and paste the label: Warning Label expressions are case-sensitive. A mismatch will cause the job to remain in the queue. 4. Save and Build After saving, trigger a build. The job will run on the external agent: Console Output Started by user siddharth
Running as SYSTEM
Building remotely on ubuntu-agent (ubuntu-docker-jdk17-node20) in workspace /home/jenkins-agent/workspace/freestyle-external-agent
+ cat /etc/os-release
PRETTY_NAME=""Ubuntu 24.04 LTS""
...
+ node -v
v22.11.0
+ npm -v
10.9.0
Finished: SUCCESS Using an External Agent in a Pipeline Job Next, we’ll build a Declarative Pipeline using both the Jenkins controller and the Ubuntu agent. 1. Clone or Migrate Your Repository Point Jenkins to your Git repo containing Jenkinsfile : 2. Add a Declarative Jenkinsfile Place this in your repository root: pipeline {
    agent any

    stages {
        stage('S1 – Any Agent') {
            steps {
                sh 'cat /etc/os-release'
                sh 'node -v'
                sh 'npm -v'
            }
        }
        stage('S2 – Ubuntu Agent') {
            agent { label 'ubuntu-docker-jdk17-node20' }
            steps {
                sh 'cat /etc/os-release'
                sh 'node -v'
                sh 'npm -v'
            }
        }
    }
} 3. Configure and Run the Pipeline In Jenkins, create a Pipeline job and point it to the main branch of your repo. Save and click Build Now . Console Output Highlights [Pipeline] Start of Pipeline
[Pipeline] node
Running on Jenkins in /var/lib/jenkins/workspace/pipeline-external-agent
[Pipeline] stage (S1 – Any Agent)
+ cat /etc/os-release
PRETTY_NAME=""Ubuntu 24.04 LTS""
+ node -v
v20.16.0
+ npm -v
10.8.1
[Pipeline] stage (S2 – Ubuntu Agent)
Building on ubuntu-docker-jdk17-node20
+ cat /etc/os-release
PRETTY_NAME=""Ubuntu 24.04 LTS""
+ node -v
v22.11.0
+ npm -v
10.9.0
[Pipeline] End of Pipeline
Finished: SUCCESS Verifying Agent Workspaces SSH into your Ubuntu agent and list workspace directories: ssh jenkins@ubuntu-agent
cd /home/jenkins-agent/workspace
ls
# Should list:
# freestyle-external-agent
# pipeline-external-agent Job and Agent Summary Job Type Agent Label Key Commands Freestyle-External-Agent ubuntu-docker-jdk17-node20 cat /etc/os-release , node -v , npm -v Pipeline Stage “S1” any (controller) cat /etc/os-release , node -v , npm -v Pipeline Stage “S2” ubuntu-docker-jdk17-node20 cat /etc/os-release , node -v , npm -v Links and References Jenkins Agents Documentation Declarative Pipeline Syntax Ubuntu Official Site Node.js Downloads npm Documentation Watch Video Watch video content"
Advanced Jenkins,Create Shared Library for Trivy Scan,https://notes.kodekloud.com/docs/Advanced-Jenkins/Shared-Libraries-in-Jenkins/Create-Shared-Library-for-Trivy-Scan,"Advanced Jenkins Shared Libraries in Jenkins Create Shared Library for Trivy Scan In this guide, you’ll learn how to extract Trivy vulnerability scanning logic into a reusable Jenkins Shared Library. By the end, you will be able to: Version and maintain your scanning scripts centrally Simplify your Jenkinsfiles across multiple repositories Generate HTML and JUnit XML reports for scan results Objectives Clone the shared-libraries repository Create a new feature branch Review the existing Trivy stage in your Jenkinsfile Implement vars/TrivyScan.groovy with two methods Commit and push your changes 1. Clone the Shared Libraries Repository First, clone the central library repository where your custom steps live. git clone http://64.227.187.25:5555/dasher-org/shared-libraries.git
cd shared-libraries
ls
# → vars Note Make sure you have the necessary Git credentials set up or an SSH key configured to avoid authentication errors. 2. Create a Feature Branch Branch off main (or your default branch) to implement the Trivy scan logic. git checkout -b feature/TrivyScan 3. Review the Existing Jenkinsfile Stage Here’s the current Trivy Vulnerability Scanner stage in the Solar System project’s Jenkinsfile : stage('Trivy Vulnerability Scanner') {
    steps {
        sh '''
            trivy image siddharth67/solar-system:$GIT_COMMIT \
                --severity LOW,MEDIUM,HIGH \
                --exit-code 0 \
                --quiet \
                --format json -o trivy-image-MEDIUM-results.json

            trivy image siddharth67/solar-system:$GIT_COMMIT \
                --severity CRITICAL \
                --exit-code 1 \
                --quiet \
                --format json -o trivy-image-CRITICAL-results.json
        '''
    }
    post {
        // conversions and reports go here
    }
} We want to remove this inlined logic and replace it with calls to our Shared Library. 4. Create vars/TrivyScan.groovy Under shared-libraries/vars , add a new file named TrivyScan.groovy with two methods: one for scanning vulnerabilities, and one for converting the JSON output into HTML and JUnit XML. // vars/TrivyScan.groovy

def vulnerability(String imageName) {
    sh """"""
        echo ""🔍 Scanning image: ${imageName}""

        trivy image ${imageName} \
            --severity LOW,MEDIUM,HIGH \
            --exit-code 0 \
            --quiet \
            --format json -o trivy-image-MEDIUM-results.json

        trivy image ${imageName} \
            --severity CRITICAL \
            --exit-code 1 \
            --quiet \
            --format json -o trivy-image-CRITICAL-results.json
    """"""
}

def reportsConverter() {
    sh """"""
        # Generate HTML reports
        trivy convert \
            --format template \
            --template ""/usr/local/share/trivy/templates/html.tpl"" \
            --output trivy-image-MEDIUM-results.html \
            trivy-image-MEDIUM-results.json

        trivy convert \
            --format template \
            --template ""/usr/local/share/trivy/templates/html.tpl"" \
            --output trivy-image-CRITICAL-results.html \
            trivy-image-CRITICAL-results.json

        # Generate JUnit XML reports
        trivy convert \
            --format template \
            --template ""/usr/local/share/trivy/templates/junit.tpl"" \
            --output trivy-image-MEDIUM-results.xml \
            trivy-image-MEDIUM-results.json

        trivy convert \
            --format template \
            --template ""/usr/local/share/trivy/templates/junit.tpl"" \
            --output trivy-image-CRITICAL-results.xml \
            trivy-image-CRITICAL-results.json
    """"""
} Shared Library Methods at a Glance Method Purpose Output Files vulnerability() Runs HIGH, MEDIUM, LOW & CRITICAL scans trivy-image-MEDIUM-results.json ,<br> trivy-image-CRITICAL-results.json reportsConverter() Converts JSON to HTML & JUnit XML report templates .html and .xml reports for both severity levels Warning Ensure your Jenkins agents have Trivy installed and the template files are accessible at /usr/local/share/trivy/templates/ . Otherwise, the conversion step will fail. 5. Commit and Push Your Changes Once your library file is in place: git add vars/TrivyScan.groovy
git commit -m ""feat(shared-library): add TrivyScan with vulnerability and reportsConverter methods""
git push -u origin feature/TrivyScan Next Steps Integrate TrivyScan.vulnerability(imageName) and TrivyScan.reportsConverter() into your project’s Jenkinsfile . Tag or merge your branch when you’re ready to release the Shared Library. Monitor the generated reports in your CI/CD pipeline and enforce policies based on exit codes. Links and References Jenkins Shared Library Documentation Trivy: A Simple and Comprehensive Vulnerability Scanner Git Basics Watch Video Watch video content"
Advanced Jenkins,Sharing a File between Containers,https://notes.kodekloud.com/docs/Advanced-Jenkins/Pipeline-Enhancement-and-Caching/Sharing-a-File-between-Containers,"Advanced Jenkins Pipeline Enhancement and Caching Sharing a File between Containers By default, all containers within a Kubernetes Pod share the same filesystem. In this tutorial, we'll demonstrate how to create a file in an Ubuntu container and then read it from a Node.js container within the same Pod using a Jenkins pipeline. Note Containers in the same Pod share volumes and file systems, allowing seamless data exchange without additional configuration. Jenkins Pipeline Configuration Paste the following pipeline definition into your Jenkinsfile to set up two containers— ubuntu-container and node-container —in a single Pod: pipeline {
  agent {
    kubernetes {
      yaml """"""
apiVersion: v1
kind: Pod
spec:
  containers:
  - name: ubuntu-container
    image: ubuntu
    command: ['sleep']
    args: ['infinity']
  - name: node-container
    image: node:18-alpine
    command: ['cat']
    tty: true
""""""
      defaultContainer 'ubuntu-container'
    }
  }

  stages {
    stage('Create File on Ubuntu') {
      steps {
        sh '''
          # Display the hostname
          hostname

          # Write data to a file
          echo important_UBUNTU_data > ubuntu-$BUILD_ID.txt

          # Confirm the file was created
          ls -ltr ubuntu-$BUILD_ID.txt
        '''
      }
    }

    stage('Access File on Node') {
      steps {
        container('node-container') {
          sh '''
            # Verify Node.js and npm versions
            node -v
            npm -v

            # List and read the file created earlier
            ls -ltr ubuntu-$BUILD_ID.txt
            cat ubuntu-$BUILD_ID.txt
          '''
        }
      }
    }
  }
} Pipeline in Action Once you commit and run this pipeline, Jenkins will create the Pod and execute both stages. The Ubuntu container writes ubuntu-$BUILD_ID.txt , and the Node.js container reads it immediately afterward. According to the Kubernetes Pod Lifecycle documentation , all containers in a Pod can share volumes and files. Console Output Example Below is an excerpt from the Jenkins console log illustrating how the file is shared between containers: [ubuntu-container] Running shell script
+ hostname
k8s-cloud-agent-demo-5-rv7b8-8frt0-n41pw
+ echo important_UBUNTU_data
+ ls -ltr ubuntu-5.txt
-rw-r--r-- 1 root root 22 Nov 10 09:47 ubuntu-5.txt

[node-container] Running shell script
+ node -v
v18.20.4
+ npm -v
8.19.2
+ ls -ltr ubuntu-5.txt
-rw-r--r-- 1 root root 22 Nov 10 09:47 ubuntu-5.txt
+ cat ubuntu-5.txt
important_UBUNTU_data Comparing Jenkins Agent Strategies Choose the right Jenkins agent model based on your infrastructure and maintenance requirements: Agent Type Description Kubernetes Cloud Agent On-demand Pods; auto-provisioned and cleaned up per job. Docker Agent Temporary containers managed by Jenkins; easy to customize. Static Node Always-on VMs or servers; requires manual OS and tool updates. Warning Static nodes demand ongoing maintenance (OS patches, software upgrades). For most CI/CD workflows, dynamic agents reduce resource waste and simplify updates. Links and References Kubernetes Pod Lifecycle Jenkins Kubernetes Plugin Docker Official Images Watch Video Watch video content"
Advanced Jenkins,Sequential Stages,https://notes.kodekloud.com/docs/Advanced-Jenkins/Pipeline-Enhancement-and-Caching/Sequential-Stages,"Advanced Jenkins Pipeline Enhancement and Caching Sequential Stages In this guide, you’ll learn how to structure sequential stages within a Jenkins pipeline to break down complex branches into clear, ordered steps. By nesting stages, you gain better visibility into each part of your build process without wading through long console logs. Note Sequential (nested) stages are supported in Declarative Pipelines . They require Jenkins 2.138.3 or later and the Pipeline: Stage Step plugin. 1. Original Pipeline Snippet Here’s a simple pipeline that runs everything in one NodeJS 20 stage. It retries on failure but doesn’t distinguish between installing dependencies and running tests: stage('NodeJS 20') {
    agent {
        docker {
            image 'node:20-alpine'
        }
    }
    options { retry(2) }
    steps {
        sh 'node -v'
        sh 'npm test'
    }
}

stage('Code Coverage') {
    // ...
} While functional, this single stage limits your insight into which specific task fails. 2. Defining Sequential (Nested) Stages To break down the work, nest a stages block inside your NodeJS 20 stage, creating two ordered steps: Install Dependencies and Testing . stage('NodeJS 20') {
    agent {
        docker {
            image 'node:20-alpine'
        }
    }
    stages {
        stage('Install Dependencies') {
            options { retry(2) }
            steps {
                sh 'node -v'
                sh 'npm install --no-audit'
            }
        }
        stage('Testing') {
            options { retry(2) }
            steps {
                sh 'node -v'
                sh 'npm test'
            }
        }
    }
}

stage('Code Coverage') {
    // ...
} Commit and push these changes to your repository. In the Jenkins UI, you’ll now see a nested structure under NodeJS 20 , clearly separating each step: 3. What Happens on Failure If Install Dependencies fails, Jenkins will: Highlight the failure in the UI. Skip the downstream Testing stage. Surface the error in your console logs. Typical error output might look like this: npm install --no-audit
npm error code EACCES
npm error syscall mkdir
npm error path /npm
npm error errno -13
npm error Your cache folder contains root-owned files...
npm ERR! code ELIFECYCLE
npm ERR! errno 243 Warning This failure is due to Node.js cache permissions, not Jenkins. You can fix it by changing the --cache directory ( npm install --cache /tmp/ ) or adjusting folder ownership ( chown -R jenkins:jenkins /npm ). 4. Benefits of Sequential Stages Benefit Description Granular Visibility See each step (install, test, package) as its own stage in the UI. Fail-Fast Behavior A failure in one nested stage blocks subsequent stages automatically. Retry Control per Step Apply options { retry(n) } individually to each nested stage for finer-grained retries. 5. Summary & Next Steps Sequential stages let you organize tasks in a strict order within each branch. Failures halt all downstream nested stages, making it clear which step broke. Visibility and retry control improve your build diagnostics. Next, we’ll explore how to share artifacts across these nested stages using the Jenkins workspace archive and copy mechanisms. Links and References Jenkins Declarative Pipeline Syntax Pipeline: Stage Step Plugin Nested Stages in Declarative Pipeline Watch Video Watch video content"
Advanced Jenkins,Utilize newContainerPerStage,https://notes.kodekloud.com/docs/Advanced-Jenkins/Agents-and-Nodes-in-Jenkins/Utilize-newContainerPerStage,"Advanced Jenkins Agents and Nodes in Jenkins Utilize newContainerPerStage In this lesson, you will learn why the newContainerPerStage directive is essential when using a Dockerfile agent in a Jenkins Declarative Pipeline. By default, Jenkins runs all stages in a single container, sharing the workspace and any generated artifacts. We’ll explore both behaviors and show how to enforce stage-level isolation. Single-Container Pipeline When you configure a pipeline with a global Dockerfile agent, every stage executes inside the same container. Artifacts created in one stage remain available in the next: pipeline {
  agent {
    dockerfile {
      filename 'Dockerfile.cowsay'
      label 'ubuntu-docker-jdk17-node20'
    }
  }
  stages {
    stage('Stage-1') {
      steps {
        sh 'cat /etc/os-release'
        sh 'node -v'
        sh 'npm -v'
        echo '#############################'
        sh ""echo $((RANDOM)) > /tmp/imp-file-$BUILD_ID""
        sh 'ls -ltr /tmp/imp-file-$BUILD_ID'
        sh 'cat /tmp/imp-file-$BUILD_ID'
        echo '#############################'
      }
    }
    stage('Stage-2') {
      steps {
        sh 'cat /etc/os-release'
        sh 'node -v'
        sh 'npm -v'
        echo 'Reading file generated in Stage-1:'
        sh 'cat /tmp/imp-file-$BUILD_ID'
      }
    }
    stage('Stage-3') {
      steps {
        sh 'cat /etc/os-release'
        sh 'node -v'
        sh 'npm -v'
      }
    }
    stage('Stage-4') {
      steps {
        sh 'node -v'
        sh 'npm -v'
        sh 'cowsay -f dragon This is running on Docker Container'
        echo 'Final file check:'
        sh 'cat /tmp/imp-file-$BUILD_ID'
        sh 'sleep 120s'
      }
    }
  }
} Execution Logs # Dockerfile build and container start
$ docker build -t pipeline-external-agent -f Dockerfile.cowsay .
...
$ docker run ...
# Stage-1
+ cat /etc/os-release
NAME=""Alpine Linux"" ...
+ node -v
v18.20.4
+ npm -v
10.7.0
+ echo $((RANDOM)) > /tmp/imp-file-7
+ ls -ltr /tmp/imp-file-7
-rw-r--r-- 1 root root 5 Nov 10 07:42 /tmp/imp-file-7
+ cat /tmp/imp-file-7
7577
# Stage-2, Stage-3, Stage-4 reuse the same container
+ cat /tmp/imp-file-7
7577 When the pipeline finishes, Jenkins stops and removes that single container. Enforcing Stage Isolation with newContainerPerStage To run each stage in its own container—so artifacts from one stage aren’t carried over—enable the newContainerPerStage() option. Note Refer to the Pipeline Syntax Reference for details on newContainerPerStage() . Updated Pipeline Configuration Insert an options block with newContainerPerStage() : pipeline {
  agent {
    dockerfile {
      filename 'Dockerfile.cowsay'
      label 'ubuntu-docker-jdk17-node20'
    }
  }
  options {
    newContainerPerStage()
  }
  stages {
    stage('Stage-1') {
      steps {
        sh 'cat /etc/os-release'
        sh 'node -v'
        sh 'npm -v'
        echo '********************'
        sh ""echo $((RANDOM)) > /tmp/imp-file-$BUILD_ID""
        sh 'ls -ltr /tmp/imp-file-$BUILD_ID'
        sh 'cat /tmp/imp-file-$BUILD_ID'
        echo '********************'
      }
    }
    stage('Stage-2') {
      steps {
        sh 'cat /etc/os-release'
        sh 'node -v'
        sh 'npm -v'
        echo 'Trying to read file from Stage-1:'
        sh 'ls -ltr /tmp/imp-file-$BUILD_ID'
        sh 'cat /tmp/imp-file-$BUILD_ID'
      }
    }
  }
} Now Jenkins rebuilds the Docker image and starts a fresh container for every stage. Stage-2 will fail because the file from Stage-1 no longer exists: # Stage-2
$ docker build -t pipeline-external-agent -f Dockerfile.cowsay .
...
# Inside new container
$ ls -ltr /tmp/imp-file-9
ls: cannot access '/tmp/imp-file-9': No such file or directory Warning Using newContainerPerStage() increases build time due to repeated image builds. Evaluate the trade-off between isolation and performance. Comparison of Pipeline Behaviors Setting Description Pros Cons Single container (default) All stages share one Docker container and workspace Fast execution No stage-level isolation newContainerPerStage() Each stage runs in a new container built from Dockerfile Full isolation per stage Longer builds due to repeated image construction Links and References Jenkins Pipeline Syntax Jenkins Docker Pipeline Pipeline Options: newContainerPerStage That’s all for now—thank you for reading! Watch Video Watch video content Practice Lab Practice lab"
Advanced Jenkins,Refactoring Unit Test Stage,https://notes.kodekloud.com/docs/Advanced-Jenkins/Pipeline-Enhancement-and-Caching/Refactoring-Unit-Test-Stage,"Advanced Jenkins Pipeline Enhancement and Caching Refactoring Unit Test Stage In this tutorial, we'll refactor the Unit Test stage of a Jenkins pipeline to run tests across multiple Node.js versions—18, 19, and 20—using Kubernetes agents and Docker containers. This approach improves reliability and ensures compatibility before merging changes. Table of Contents Pipeline Configuration with Kubernetes Agent Sequential Unit Testing and Coverage Parallel Testing Across Node.js Versions Pipeline Snippet Agent and Container Matrix Diagnosing the Node.js 20 Failure Installing Dependencies in Node.js 20 Stage Next Steps and Further Reading Pipeline Configuration with Kubernetes Agent Below is an excerpt from the original Jenkinsfile that configures a Kubernetes agent, Node.js tool, and necessary environment variables: @Library('dasher-trusted-shared-library@featureTrivyScan') _
pipeline {
  agent {
    kubernetes {
      cloud 'dasher-prod-k8s-us-east'
      yamlFile 'k8s-agent.yaml'
      defaultContainer 'node-18'
    }
  }
  tools {
    nodejs 'nodejs-22-6-0'
  }
  environment {
    MONGO_URI        = ""mongodb://srv-cluster.d83jj.mongodb.net/superData""
    MONGO_DB_CREDS   = credentials('mongo-db-credentials')
    MONGO_USERNAME   = credentials('mongo-db-username')
    MONGO_PASSWORD   = credentials('mongo-db-password')
  }
  // ...
} Note Adjust yamlFile and defaultContainer to match your Kubernetes Pod specification. Sequential Unit Testing and Coverage Originally, the pipeline ran Unit Testing and Code Coverage sequentially on Node.js 18: stage('Unit Testing') {
  options { retry(2) }
  steps {
    sh 'node -v'
    sh 'npm test'
  }
}

stage('Code Coverage') {
  steps {
    catchError(buildResult: 'SUCCESS',
               message: 'Coverage will be fixed in future releases',
               stageResult: 'UNSTABLE') {
      sh 'node -v'
      sh 'npm run coverage'
    }
  }
} While straightforward, this setup doesn’t validate compatibility with newer Node.js versions. Parallel Testing Across Node.js Versions To ensure your application works on Node.js 18, 19, and 20, we can run tests in parallel branches. This refactoring includes: Installing dependencies once on the Kubernetes agent Running dependency scanning (e.g., Snyk, Trivy) Testing in parallel across multiple Node.js versions Aggregating code coverage Pipeline Snippet pipeline {
  agent {
    kubernetes {
      cloud 'dasher-prod-k8s-us-east'
      yamlFile 'k8s-agent.yaml'
      defaultContainer 'node-18'
    }
  }
  tools {
    nodejs 'nodejs-22-6-0'
  }
  environment {
    MONGO_URI          = ""mongodb+srv://supercluster.d83jj.mongodb.net/superData""
    MONGO_DB_CREDS     = credentials('mongo-db-credentials')
    MONGO_USERNAME     = credentials('mongo-db-username')
    MONGO_PASSWORD     = credentials('mongo-db-password')
    SONAR_SCANNER_HOME = tool 'sonarqube-scanner-610'
    GITEA_TOKEN        = credentials('gitea-api-token')
  }
  options {
    disableResume()
    disableConcurrentBuilds(abortPrevious: true)
  }
  stages {
    stage('Installing Dependencies') {
      options { timestamps() }
      steps {
        sh 'node -v'
        sh 'npm install --no-audit'
      }
    }

    stage('Dependency Scanning') {
      parallel {
        // e.g., Trivy, Snyk, OWASP Dependency Check
      }
    }

    stage('Unit Testing') {
      parallel {
        stage('NodeJS 18') {
          options { retry(2) }
          steps {
            sh 'node -v'
            sh 'npm test'
          }
        }

        stage('NodeJS 19') {
          options { retry(2) }
          steps {
            container('node-19') {
              // Sleep to avoid port conflicts on shared volumes
              sh 'sleep 10s'
              sh 'node -v'
              sh 'npm test'
            }
          }
        }

        stage('NodeJS 20') {
          agent {
            docker { image 'node:20-alpine' }
          }
          options { retry(2) }
          steps {
            // npm install will be added here
            sh 'node -v'
            sh 'npm test'
          }
        }
      }
    }

    stage('Code Coverage') {
      steps {
        catchError(buildResult: 'SUCCESS',
                   message: 'Coverage will be fixed in future releases',
                   stageResult: 'UNSTABLE') {
          sh 'node -v'
          sh 'npm run coverage'
        }
      }
    }
  }
} Agent and Container Matrix Stage Environment Container/Agent Purpose Dependencies Kubernetes Pod node-18 Install node_modules via npm install Scanning Kubernetes Pod node-18 Security and license scanning NodeJS 18 Kubernetes Pod node-18 Default Node.js NodeJS 19 Kubernetes Pod node-19 container Verify compatibility with v19 NodeJS 20 Docker-in-Docker Pod node:20-alpine Docker Validate on latest LTS Coverage Kubernetes Pod node-18 Generate and publish code coverage Diagnosing the Node.js 20 Failure After pushing the refactored Jenkinsfile , the NodeJS 20 branch failed: + node -v
v20.18.0
+ npm test
> mocha app-test.js --timeout 10000 --reporter mocha-junit-reporter --exit
sh: mocha: not found
script returned exit code 127 Warning The Docker container node:20-alpine starts with a clean filesystem—it doesn’t have your node_modules . Always install dependencies inside each container or agent before running tests. Installing Dependencies in Node.js 20 Stage Option 1: Inline npm install Add npm install directly before running tests in the NodeJS 20 branch: stage('NodeJS 20') {
  agent {
    docker { image 'node:20-alpine' }
  }
  options { retry(2) }
  steps {
    sh 'npm install'
    sh 'node -v'
    sh 'npm test'
  }
} Option 2: Separate Install Step in Parallel Stage Alternatively, include installation as part of the Unit Testing parallel stage: stage('Unit Testing') {
  parallel {
    // NodeJS 18 and 19 as before...

    stage('NodeJS 20') {
      agent {
        docker { image 'node:20-alpine' }
      }
      options { retry(2) }
      steps {
        sh 'npm install'
        sh 'node -v'
        sh 'npm test'
      }
    }
  }
} Both ensure that each parallel branch has its own node_modules prior to execution. Next Steps and Further Reading Explore adding test result archiving with junit plugin. Integrate SonarQube for code quality reporting. Consider dynamic matrix stages for other runtimes (e.g., Node.js 21+). Links and References Jenkins Kubernetes Plugin Node.js Docker Official Images Jenkins Pipeline Syntax Mocha Testing Framework That completes the refactoring of the Unit Test stage to support parallel testing across multiple Node.js versions. Watch Video Watch video content"
Advanced Jenkins,Invalidate Cache,https://notes.kodekloud.com/docs/Advanced-Jenkins/Pipeline-Enhancement-and-Caching/Invalidate-Cache,"Advanced Jenkins Pipeline Enhancement and Caching Invalidate Cache When you add or remove dependencies in your package.json , it’s crucial to invalidate the existing cache so Jenkins reinstalls packages based on the updated lock file. Below is a step-by-step guide to demonstrate how cache invalidation works in a Jenkins pipeline using the Job Cache plugin’s cacheValidityDecidingFile option. 1. Navigate to Your Repository Switch back to your Solar System project: cd ~/solar-system Verify you’re on the correct feature branch: git status
# On branch feature/advanced-demo 2. Add a New Dependency Install a new package (e.g., localtunnel ) to trigger a change in package-lock.json : npm install localtunnel This command updates both package.json and package-lock.json , altering the lock file’s hash. 3. Commit & Push Stage and commit the updated dependency files: git add package.json package-lock.json
git commit -m ""chore: install localtunnel to trigger cache invalidation""
git push Pushing these changes starts a new Jenkins build. The install stage will reveal the cache behavior. 4. Observe Cache Invalidation in Jenkins In the pipeline’s console output, look for entries related to the cache plugin: 17:06:25  + node -v
17:06:25  v22.6.0
17:06:26  + npm install --no-audit
17:06:27  added 7 packages in 2s
17:06:27  45 packages are looking for funding
17:06:27  run `npm fund` for details
17:06:30  Stashed 5131 file(s)

17:06:25  {cache for node_modules (npm-dependency-cache) with id 3ec03583f8eace275cb2183db769ff47}
17:06:25  got hash 5a159d4c8ba0a6882fd4b8ef16c2 for cacheValidityDecidingFile(s): /var/lib/jenkins/workspace/lar-system_feature_advanced-demo/package-lock.json
17:06:25  cache outdated; skipping restore
17:06:30  creating cache...
17:06:32  cache created in 2179ms Key observations: Jenkins computes a new hash for package-lock.json . Since the hash differs from the previous build, the old cache is skipped. A fresh cache is created based on the updated dependencies. Note On subsequent runs, if package-lock.json remains unchanged, Jenkins will restore the cache instead of reinstalling all packages. 5. Cache Invalidation Logic Condition Action package-lock.json has changed Invalidate old cache & recreate package-lock.json is unchanged Restore existing cache By specifying cacheValidityDecidingFile: package-lock.json in your Job Cache plugin configuration, you ensure your Node.js dependencies are always in sync with the lock file. Links and References Jenkins Job Cache Plugin npm install documentation package-lock.json specification Watch Video Watch video content Practice Lab Practice lab"
Ansible Advanced Course,Core Components Introduction,https://notes.kodekloud.com/docs/Ansible-Advanced-Course/Core-Components/Core-Components-Introduction,"Ansible Advanced Course Core Components Core Components Introduction In this article, we take a deep dive into the core components of our system. In our previous discussions, we covered topics such as inventories, modules, variables, plays, and playbooks. This time, our focus shifts to exploring facts and configuration files. Note Before you begin, please be aware that the code examples provided throughout this article are designed to illustrate playbook syntax and structure. Some examples have been simplified for clarity. For the complete versions used in our labs, please refer to the GitHub repository or the lab instructions. Happy learning! Watch Video Watch video content"
Ansible Advanced Course,Course Pre Requisites,https://notes.kodekloud.com/docs/Ansible-Advanced-Course/Introduction/Course-Pre-Requisites,"Ansible Advanced Course Introduction Course Pre Requisites Before diving into the content of this article, it's essential to ensure that you meet a few prerequisites. This guide assumes that you have some prior exposure to Ansible and Linux-based systems. If you're completely new to Ansible, we highly recommend starting with the Ansible for the Absolute Beginners course. This introductory course covers the essentials such as setting up a lab environment using VirtualBox, working with YAML, and grasping key Ansible concepts including inventory files, playbooks, variables, modules, and loops. Mastering these fundamentals will provide you with a firm foundation to fully benefit from this article. Note If you are an absolute beginner to automation tools or scripting languages, taking a step-by-step course designed for beginners is a great way to build confidence before progressing to more advanced topics. For the purposes of this article, live lab access will be provided, eliminating the need for manual lab setup. Every aspect of Ansible required to prepare for the exam will be covered in a comprehensive and structured manner. However, you should have prior experience working with Linux systems—Red Hat Enterprise Linux or CentOS is preferred—since the automation tasks will be executed on a Red Hat Enterprise Linux environment. A solid understanding of basic Linux security concepts is also recommended. This includes, but is not limited to: Configuring SSH keys Managing users, groups, and file permissions Utilizing package managers and managing software installations Handling services and configuring cron jobs Managing file systems and logical volume managers Working with firewalls and archiving tools While some of these topics will be reviewed during the module discussions, independently brushing up on them will provide a significant advantage. Watch Video Watch video content"
Ansible Advanced Course,Playbook run options,https://notes.kodekloud.com/docs/Ansible-Advanced-Course/Core-Components/Playbook-run-options,"Ansible Advanced Course Core Components Playbook run options In this guide, we explore several additional options available when executing an Ansible playbook. These options enable features such as dry runs, starting at a specific task, and running or skipping tasks based on tags, thereby providing more control over your automation process. Dry Run Option Before applying any changes on your managed nodes, you might want to simulate the playbook run to understand what changes would be made. The --check option performs this dry run, allowing you to preview the operations without making any modifications. Consider the following playbook example that installs the httpd package: ---
- name: Install httpd
  hosts: all
  tasks:
    - yum:
        name: httpd
        state: installed To execute this playbook in dry-run mode: $ ansible-playbook playbook.yml --check Dry Run Tip Using --check is a valuable way to verify your playbook changes in a safe environment before enforcing them on production systems. Start at a Specific Task Sometimes it is beneficial to run a playbook starting from a particular task, bypassing all previous tasks. The --start-at-task option lets you specify the task name from which the execution should begin. For instance, given the following playbook: ---
- name: Install httpd
  hosts: all
  tasks:
    - name: Install httpd
      yum:
        name: httpd
        state: installed

    - name: Start httpd service
      service:
        name: httpd
        state: started You can commence execution from the task ""Start httpd service"" by running: $ ansible-playbook playbook.yml --start-at-task ""Start httpd service"" Using Tags to Select or Skip Tasks Tagging is an effective method to organize your playbook by grouping plays or individual tasks, enabling you to selectively run or skip parts of the playbook. You can tag tasks and plays with specific labels so that when you execute the playbook, only the tagged tasks are considered. Below is a sample playbook that applies tags to both the play and the individual tasks: ---
- name: Install httpd
  tags: [install, start]
  hosts: all
  tasks:
    - yum:
        name: httpd
        state: installed
      tags: install
    - service:
        name: httpd
        state: started
      tags: ""start httpd service"" To run only tasks tagged with ""install"", execute: $ ansible-playbook playbook.yml --tags ""install"" Alternatively, to skip the tasks associated with the ""install"" tag: $ ansible-playbook playbook.yml --skip-tags ""install"" Tagging Best Practice Using tags effectively organizes your playbook, which helps in isolating specific parts of your automation workflow and simplifies debugging. That concludes an overview of the additional options available for running your Ansible playbook. These options enhance your control over execution, enabling safer and more efficient automation. Stay tuned as we explore even more advanced features in our next lesson. For a deeper dive into Ansible, visit the Ansible Documentation . Watch Video Watch video content"
Ansible Advanced Course,Privilege Escalation,https://notes.kodekloud.com/docs/Ansible-Advanced-Course/Install-and-Configure/Privilege-Escalation,"Ansible Advanced Course Install and Configure Privilege Escalation In this guide, you'll learn how to configure privilege escalation on managed nodes using Ansible. Privilege escalation is the process of temporarily obtaining elevated permissions (typically root or administrative privileges) to perform system tasks that a regular user cannot. Understanding User Privileges Every user on a system is assigned a set of permissions. Consider a development server for a web application where various user accounts exist: root: Full system privileges (often locked for direct login). Administrator/Developer: Limited privileges necessary for everyday tasks. Dedicated Users: Specific accounts for tools or applications like Nginx, databases, and monitoring tools. A typical workflow in such an environment involves an administrator logging in (using a password or SSH key), installing packages that require elevated privileges, and then configuring applications under designated user accounts. Example: Installing a Package Without Escalation For instance, if an administrator logs in to install a package: ssh -i id_ras admin@server1
su
yum install nginx
# Permission Denied The command fails because installing packages requires root privileges. By configuring the admin user to use the sudo utility (or a similar mechanism), the admin can escalate privileges and perform the installation successfully. Example: Switching User Accounts The process of switching to another user to configure an application is also a form of privilege escalation. For example: ssh -i id_ras admin@server1
sudo yum install nginx
su nginx
su mysql
# Configure MySQL In scenarios like this, privilege escalation is essential for transitioning between different user roles during the configuration process. Configuring Privilege Escalation in Ansible Ansible allows you to replicate the behavior of privilege escalation that you manually perform on the command line. Basic Inventory and Playbook Without Privilege Escalation Consider an inventory file that connects to a lamp server with the admin user. Although it's often a good practice to create a dedicated user for Ansible tasks, this example uses the admin user: # inventory file
lamp-dev1 ansible_host=172.20.1.100 ansible_user=admin

# playbook file
---
- name: Install nginx
  hosts: all
  tasks:
    - yum:
        name: nginx
        state: latest Running the playbook without privilege escalation will fail because the admin user lacks the required permissions. Enabling Privilege Escalation with the become Directive To fix the issue, add the ""become"" directive to the playbook. This instructs Ansible to perform tasks with elevated privileges, similar to using the sudo command: # inventory file
lamp-dev1 ansible_host=172.20.1.100 ansible_user=admin

# playbook file
---
- name: Install nginx
  become: yes
  hosts: all
  tasks:
    - yum:
        name: nginx
        state: latest With the ""become"" directive, Ansible runs the tasks with elevated permissions, successfully installing Nginx. Using Alternative Privilege Escalation Methods By default, Ansible uses sudo for privilege escalation. However, if you prefer another method such as ""doas"" or ""pfexec,"" you can specify it using the ""become_method"" option: # inventory file
lamp-dev1 ansible_host=172.20.1.100 ansible_user=admin

# playbook file
---
- name: Install nginx
  become: yes
  become_method: doas
  hosts: all
  tasks:
    - yum:
        name: nginx
        state: latest Targeting a Specific User With become_user You can also designate a specific target user (e.g., the nginx user) using the ""become_user"" directive. This tells Ansible to switch to a particular user before executing tasks. You can define these settings in multiple locations: In the Ansible configuration file (/etc/ansible/ansible.cfg) In the inventory file as host parameters (prefixed with ""ansible_"") Directly in the playbook Via command-line arguments In the Ansible Configuration File # /etc/ansible/ansible.cfg
become              = True
become_method       = doas
become_user         = nginx In the Inventory File # inventory file
lamp-dev1 ansible_host=172.20.1.100 ansible_user=admin ansible_become=yes ansible_become_user=nginx In the Playbook # playbook file
---
- name: Install nginx
  hosts: all
  tasks:
    - yum:
        name: nginx
        state: latest Keep in mind that any values specified directly in the playbook override those in the inventory file, and command-line parameters have the highest precedence, while settings in the default configuration file have the lowest. Tip For complex environments, consider consolidating privilege escalation settings in the Ansible configuration file to simplify management. Prompting for a Privilege Escalation Password Sometimes, escalating privileges may require a password (similar to using sudo). Ansible can prompt you for this password by using the ""--ask-become-pass"" option on the command line: $ ansible-playbook --become --become-method=doas --become-user=nginx --ask-become-pass When you run the playbook with this option, Ansible will prompt you to enter the appropriate password for privilege escalation. Summary This article covered how to configure privilege escalation in Ansible by: Understanding different user roles and privileges. Using the ""become"" directive to execute tasks with elevated permissions. Configuring alternative methods with ""become_method"" and targeting specific users with ""become_user."" Overriding settings in the inventory file, playbook, and command-line. Prompting for privilege escalation passwords when needed. Review the available practice exercises to apply and further solidify your understanding of these concepts. For more detailed information, refer to the Ansible Documentation . Happy automating! Watch Video Watch video content"
Ansible Advanced Course,Facts,https://notes.kodekloud.com/docs/Ansible-Advanced-Course/Core-Components/Facts,"Ansible Advanced Course Core Components Facts In this article, we explore Ansible facts—vital details automatically collected during playbook execution. When Ansible connects to a target machine, it gathers a wide range of system and network information. This data, including system architecture, OS version, processor details, memory statistics, network interfaces, IP addresses, FQDN, MAC addresses, disks, volumes, mounts, and even date/time settings, is stored in a collective variable called “facts.” Ansible leverages the setup module to gather these facts. This module runs automatically before any playbook tasks, even if it is not explicitly specified. Tip Even if you do not include the setup module in your playbook, facts are gathered by default. Basic Playbook Example Below is a simple playbook that prints a hello message. Notice that besides displaying the message, Ansible first collects facts using the setup module: - name: Print hello message
  hosts: all
  tasks:
    - debug:
        msg: ""Hello from Ansible!"" When you run this playbook, the output includes two tasks—the fact gathering task and the debug message display. An example output is: PLAY [Print hello message] ***************************************************

TASK [Gathering Facts] *******************************************************
ok: [web2]
ok: [web1]

TASK [debug] *****************************************************************
ok: [web1] => {
    ""msg"": ""Hello from Ansible!""
}
ok: [web2] => {
    ""msg"": ""Hello from Ansible!""
} Displaying Gathered Facts All the system details gathered by Ansible are stored in a variable named ansible_facts . To display these facts, modify your playbook as shown below: - name: Print all gathered facts
  hosts: all
  tasks:
    - debug:
        var: ansible_facts The resulting output contains a comprehensive list of system information, including IP addresses, system architecture, operating system details, DNS configurations, network interfaces, memory allocation, processor information, and more: PLAY [Print all gathered facts] ********************************************

TASK [Gathering Facts] **************************************************
ok: [web2]
ok: [web1]

TASK [debug] **************************************************************
ok: [web1] => {
    ""ansible_facts"": {
        ""all_ipv4_addresses"": [
            ""172.20.1.100""
        ],
        ""architecture"": ""x86_64"",
        ""date_time"": {
            ""date"": ""2019-09-07""
        },
        ""distribution"": ""Ubuntu"",
        ""distribution_file_variety"": ""Debian"",
        ""distribution_major_version"": ""16"",
        ""distribution_release"": ""xenial"",
        ""distribution_version"": ""16.04"",
        ""dns"": {
            ""nameservers"": [
                ""127.0.0.11""
            ]
        },
        ""fqdn"": ""web1"",
        ""hostname"": ""web1"",
        ""interfaces"": [
            ""lo"",
            ""eth0""
        ],
        ""machine"": ""x86_64"",
        ""memfree_mb"": 72,
        ""memory_mb"": {
            ""real"": {
                ""free"": 72,
                ""total"": 985,
                ""used"": 913
            }
        }
    }
} This information can be particularly useful when configuring devices, managing logical volumes, or making decisions based on disk space or system resources. Disabling Fact Gathering There are scenarios where fact gathering may be unnecessary or even undesired. You can disable automatic fact gathering by setting the gather_facts parameter to no in your playbook: - name: Print hello message without gathering facts
  hosts: all
  gather_facts: no
  tasks:
    - debug:
        msg: ""Hello from Ansible!"" When fact gathering is disabled, only the explicitly defined tasks run. Keep in mind that the Ansible configuration file (typically found at /etc/ansible/ansible.cfg ) also controls this behavior. The gathering setting in the configuration file can be specified as follows: smart : Gathers facts by default, but avoids re-gathering if the data already exists. implicit : Automatically gathers facts; you can disable this with gather_facts: False in your playbook. explicit : Does not gather facts by default; you must explicitly set gather_facts: True in the playbook when needed. An example snippet from the configuration file is: /etc/ansible/ansible.cfg
# Plays will gather facts by default, containing system details.
# smart - gather facts by default, but don't re-gather if already gathered.
# implicit - gather facts automatically; turn off with gather_facts: False in the playbook.
# explicit - do not gather facts by default; must specify gather_facts: True in the playbook.
gathering = implicit To illustrate the effect of disabling fact gathering, consider the following playbook. It shows that the ansible_facts variable remains empty: - name: Print facts disabled
  hosts: all
  gather_facts: no
  tasks:
    - debug:
        var: ansible_facts And the output will be: PLAY [Print facts disabled] ***********************************************

TASK [debug] ***************************************************************
ok: [web1] => {
    ""ansible_facts"": {}
}
ok: [web2] => {
    ""ansible_facts"": {}
} Important Disabling fact gathering can improve playbook performance if you do not need the collected information, but be cautious when your playbook logic depends on those facts. Targeting Specific Hosts Ansible gathers facts only for hosts that are targeted in the playbook. For instance, if your inventory includes two hosts ( web1 and web2 ) but the playbook targets only web1 , facts will be gathered only for web1 . This behavior is demonstrated in the example below: Playbook Targeting a Single Host - name: Print facts for a specific host
  hosts: web1
  tasks:
    - debug:
        var: ansible_facts Example Inventory File (/etc/ansible/hosts) web1
web2 If you intend to reference facts from hosts not explicitly targeted by your playbook, ensure that they are included in the host specification. Conclusion Ansible facts are an essential aspect of playbook execution, providing detailed insights into the target systems. They can help tailor your automation tasks based on system-specific data. In the subsequent sections, we will explore deeper into parsing these facts and incorporating them into variables and Jinja2 templates, further enhancing your playbook capabilities. For further reading, check out the following resources: Ansible Documentation Ansible Setup Module Jinja2 Templating in Ansible Watch Video Watch video content Practice Lab Practice lab"
Ansible Advanced Course,Configuration files,https://notes.kodekloud.com/docs/Ansible-Advanced-Course/Core-Components/Configuration-files,"Ansible Advanced Course Core Components Configuration files In this lesson, we will explore configuration files in Ansible and understand how they influence Ansible's behavior. When you install Ansible, a default configuration file is generated. On Windows, for example, it might be located at C:\ansible\ansible.cfg , while on Linux the default file is commonly found at /etc/ansible/ansible.cfg . This file regulates Ansible's default behavior by splitting settings into several sections. The primary section is usually at the top (commonly named [defaults] ), followed by sections for inventory, privilege escalation, SSH connection, colors, and others. Each section contains various options with corresponding values. For example, a typical configuration file may appear as follows: /etc/ansible/ansible.cfg
[default]
[inventory]
[privilege_escalation]
[paramiko_connection]
[ssh_connection]
[persistent_connection]
[colors] These sections define settings such as the default inventory file location, log file path, directories for modules, roles, or plugins, as well as options like fact gathering and SSH connection timeouts. Consider the following sample configuration: [defaults]
inventory          = /etc/ansible/hosts
log_path           = /var/log/ansible.log
library            = /usr/share/my_modules/
roles_path         = /etc/ansible/roles
action_plugins     = /usr/share/ansible/plugins/action
gathering          = implicit
# SSH timeout
timeout            = 10
forks              = 5

[inventory]
enable_plugins     = host_list, virtualbox, yaml, constructed This configuration sets essential parameters: Inventory location Log file location Module library and roles path Behavior controlling fact gathering Connection timeout and the number of hosts to target simultaneously when executing playbooks Under the [inventory] section, options are provided to enable specific inventory plugins. Tip For more detailed configuration options and explanations, refer to the Ansible Configuration Documentation . Overriding Configuration Settings The values used in the default configuration file apply whenever you run playbooks from any location on your control machine. For instance, if you have multiple playbooks—each for web, database, or network tasks—you might require tailored settings for each. Examples include: Web playbooks: Disable fact gathering. Database playbooks: Enable fact gathering but disable colored output. Network playbooks: Extend the SSH timeout to 20 seconds. To handle these scenarios, you can copy the default configuration file into each playbook's directory and modify only the necessary parameters. When running a playbook, Ansible first searches for an ansible.cfg in the current directory; if it is found, that file is used. Otherwise, Ansible defaults to /etc/ansible/ansible.cfg . What if you want to use a configuration file stored in an alternative location (e.g., /opt/ansible-web.cfg ) for multiple playbooks? You can specify the location of this configuration file through the ANSIBLE_CONFIG environment variable before running your playbook. For example: ANSIBLE_CONFIG=/opt/ansible-web.cfg Ansible determines which configuration file to use based on the following order of precedence: Priority Configuration File Source 1 The file specified by the ANSIBLE_CONFIG environment variable. 2 The ansible.cfg file in the current directory. 3 The ansible.cfg file in the user's home directory. 4 The default ansible.cfg file in /etc/ansible/ . Note: These files do not have to include every parameter; only the parameters you wish to override need to be specified. Parameters not defined in the highest-priority file will inherit their values from the next available file in the priority chain. For example, if your storage playbooks should use the default configuration except for modifying the fact-gathering behavior, you don't need to copy the entire file. Instead, you can override just the necessary parameter. If the default configuration has: /etc/ansible/ansible.cfg
gathering = implicit You can change it to explicit by setting: ANSIBLE_GATHERING=explicit Setting Environment Variables Ansible supports determining configuration parameters based on environment variables. Generally, you can convert the parameter name to uppercase and prefix it with ANSIBLE_ . In this case, gathering becomes ANSIBLE_GATHERING . There are various ways to set this environment variable: Inline with the playbook command: ANSIBLE_GATHERING=explicit ansible-playbook playbook.yml Export for the duration of a shell session: export ANSIBLE_GATHERING=explicit
ansible-playbook playbook.yml If you require persistent configuration across sessions or for multiple users, it is recommended to create a local copy of the configuration file within your playbook directory, update the necessary parameter, and check this file into your version control system. Warning Avoid modifying environment variables without verifying their impact on other playbooks. Always test configuration changes in a controlled environment. Exploring Configuration Options To explore the available configuration options and view their corresponding environment variables, you can utilize the following commands: $ ansible-config list  # Lists all configurations and their default values
$ ansible-config view  # Shows the configuration file currently active If you are unsure which setting is active, the ansible-config dump command can help you. It displays a comprehensive list of all settings that Ansible has picked up, along with their source: $ ansible-config dump  # Shows the current settings
$ export ANSIBLE_GATHERING=explicit
$ ansible-config dump | grep GATHERING
DEFAULT_GATHERING(env: ANSIBLE_GATHERING) = explicit The output above confirms that the gathering parameter is set to explicit and shows that it was derived from the environment variable ANSIBLE_GATHERING . This feature is particularly useful for troubleshooting configuration issues. That’s it for this lesson. You can now apply your knowledge by practicing with Ansible configuration files and enhancing your skills. The configuration file is well documented and self-explanatory, allowing you to identify and modify the necessary options. We will be testing some of these configuration skills during future challenges. Happy configuring! Watch Video Watch video content Practice Lab Practice lab"
Ansible Advanced Course,Project Introduction,https://notes.kodekloud.com/docs/Ansible-Advanced-Course/Ansible-Modules/Project-Introduction,"Ansible Advanced Course Ansible Modules Project Introduction In this article, we introduce the project that you will work on throughout this course. You will develop Ansible playbooks to deploy the KodeKloud ecommerce website—a fictional online store selling electronic devices. This project is divided into stages, starting with setting up a lab environment and creating simple playbooks, and then progressing to advanced best practices using includes and roles. The KodeKloud ecommerce website uses a LAMP stack architecture: Linux as the operating system Apache HTTP Server for web services MariaDB as the database (a community fork of MySQL) PHP for server-side scripting Note The focus of this project is on automating the deployment process with Ansible, rather than making changes to the application code itself. Before you automate the deployment, it is important to be familiar with the manual configuration steps for setting up each component. This lesson reviews these essential tasks so you understand how each piece fits into the overall process. The tasks include: Choosing the deployment system (we use a CentOS Linux machine). Installing and configuring the Apache HTTP server, then enabling and starting the service. Installing and configuring the MariaDB database, then enabling and starting the service. Installing and configuring PHP. Downloading and setting up the application code such that it correctly connects to Apache and PHP. Configuring the system further by setting up the firewall and creating the necessary rules. For better logical flow, the guide begins by setting up and configuring the database before moving on to Apache and PHP. Step 1: Setting Up the Firewall First, install firewalld on your CentOS system. Run the following commands to install, start, and enable the firewall service: sudo yum install firewalld
sudo service firewalld start
sudo systemctl enable firewalld Step 2: Configuring the MariaDB Database Begin by installing the MariaDB server. Next, update the /etc/my.cnf file to change port settings if needed (remember that although the file is named my.cnf , it is also used by MariaDB). Then, start and enable the MariaDB service. sudo yum install mariadb-server
sudo vi /etc/my.cnf  # Adjust port settings as required
sudo service mariadb start
sudo systemctl enable mariadb After starting the database service, add the necessary firewall rules to allow external access on port 3306: sudo firewall-cmd --permanent --zone=public --add-port=3306/tcp
sudo firewall-cmd --reload Next, access the database with the MySQL client to create the database, user, and assign privileges. Use the following SQL commands: MariaDB > CREATE DATABASE ecomdb;
MariaDB > CREATE USER 'ecomuser'@'localhost' IDENTIFIED BY 'ecompassword';
MariaDB > GRANT ALL PRIVILEGES ON *.* TO 'ecomuser'@'localhost';
MariaDB > FLUSH PRIVILEGES; Finally, load the inventory data for the products with the provided database load script. Reminder Make sure the database credentials and port settings in your configuration match those specified in your Ansible playbooks. Step 3: Configuring the Web Server This step involves installing Apache, PHP, and PHP-MySQL to enable database connectivity. Then, update the firewall rules to allow HTTP traffic and modify Apache’s configuration to use index.php as the default file. Install the necessary packages: sudo yum install -y httpd php php-mysql Configure the firewall for HTTP traffic: sudo firewall-cmd --permanent --zone=public --add-port=80/tcp
sudo firewall-cmd --reload Edit the Apache configuration file to prioritize index.php : sudo vi /etc/httpd/conf/httpd.conf  # Set DirectoryIndex to use index.php instead of index.html After saving the changes, start and enable the Apache service: sudo service httpd start
sudo systemctl enable httpd Step 4: Deploying the Application Code Clone the repository containing the KodeKloud ecommerce application code. If Git isn't installed, install it first: sudo yum install -y git
git clone https://github.com/<application>.git /var/www/html/ Before testing, update the index.php file with the correct database details (address, name, user ID, and password). Finally, verify the deployment with a simple test: curl http://localhost This setup demonstrates how to deploy the LAMP stack on a single node where the database, Apache, and PHP all reside on the same system. In a multi-node configuration, the components are distributed on separate systems. Although the steps remain similar, connectivity details must be adjusted: Update index.php on the web server with the database server's IP address. In the database server, specify the web server's IP address when configuring user access. This ensures that only the authorized web server can connect. For example, on the database server, use the following configuration: MariaDB > CREATE DATABASE ecomdb;
MariaDB > CREATE USER 'ecomuser'@'172.20.1.102' IDENTIFIED BY 'ecompassword';
MariaDB > GRANT ALL PRIVILEGES ON *.* TO 'ecomuser'@'172.20.1.102';
MariaDB > FLUSH PRIVILEGES; And, update the PHP connection code on the web server accordingly: $link = mysqli_connect('172.20.1.101', 'ecomuser', 'ecompassword');
if ($link) {
    $res = mysqli_query($link, ""SELECT * FROM products;"");
    while ($row = mysqli_fetch_assoc($res)) {
        // Process each row
    }
} Security Warning Always ensure that your database user permissions and firewall settings are secured in both single and multi-node setups to prevent unauthorized access. Step 5: Reviewing the PHP Connection in the Application The primary file to focus on within the application is index.php , which contains the database connection details. The critical line in the file is: $link = mysqli_connect('172.20.1.101', 'ecomuser', 'ecompassword', 'ecomdb'); This line specifies the IP address of the database server, the database name, the user ID, and the password. You will modify this connection string as needed throughout the project. Demo and Next Steps After reviewing this demo: Set up your project environment. Create Ansible playbooks to automate the deployment. Practice deploying the KodeKloud ecommerce application following the steps provided. For further reading on Kubernetes concepts and container orchestration (if relevant to your deployment automation), visit the Kubernetes Documentation . Happy automating! Watch Video Watch video content"
Ansible Advanced Course,Validate a working configuration using ad hoc Ansible commands,https://notes.kodekloud.com/docs/Ansible-Advanced-Course/Install-and-Configure/Validate-a-working-configuration-using-ad-hoc-Ansible-commands,"Ansible Advanced Course Install and Configure Validate a working configuration using ad hoc Ansible commands In this article, you will learn how to run ad hoc commands with Ansible. Although YAML-based playbooks are the recommended method for automating tasks—enabling reusability, version control with Git, and easy sharing—ad hoc commands provide a quick and efficient alternative for testing modules, verifying connectivity, or gathering one-off information from multiple servers. Note The Ansible ping module is used to verify SSH connectivity to target machines using the configured credentials, not for performing an ICMP ping. Using a YAML Playbook for Connectivity Testing For administrators managing virtual machines, a simple playbook using the ping module is an excellent way to test connectivity. Below is an example playbook that pings all target servers: ---
- name: Ping Servers
  hosts: all
  tasks:
    - ping: To run the playbook, execute the following command in your terminal: ansible-playbook playbook.yml Executing Ad-Hoc Commands Directly If you prefer not to create a playbook, you can achieve the same result using an ad hoc command. Use the -m option to specify the module and designate the target hosts. The command below pings all servers: ansible -m ping all When executed, the output appears in JSON format for each host, similar to the following: web2 | SUCCESS => {
    ""changed"": false,
    ""ping"": ""pong""
}
web1 | SUCCESS => {
    ""changed"": false,
    ""ping"": ""pong""
} Running Arbitrary Commands To run an arbitrary command on all hosts, use the -a option to pass the command directly. For instance, to display the contents of the /etc/hosts file, run: ansible -a 'cat /etc/hosts' all The output will be similar to this: web1 | CHANGED | rc=0 >>
127.0.0.1 localhost
::1 localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
172.20.1.100 web1
web2 | CHANGED | rc=0 >>
127.0.0.1 localhost
::1 localhost ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
172.20.1.100 web1 Privilege Escalation with Ad-Hoc Commands You can also include privilege escalation options (such as become or become_user ) with these ad hoc commands, just as you would when using a playbook. This flexibility makes ad hoc commands a powerful tool for managing your infrastructure on the fly. Continue exploring additional methods to effectively utilize ad hoc commands in Ansible for efficient infrastructure management. Watch Video Watch video content Practice Lab Practice lab"
Ansible Advanced Course,Frequently Asked Questions,https://notes.kodekloud.com/docs/Ansible-Advanced-Course/Install-and-Configure/Frequently-Asked-Questions,"Ansible Advanced Course Install and Configure Frequently Asked Questions This article addresses common questions related to YAML syntax and Ansible playbooks. Explore our examples to understand the flexibility of YAML Boolean values, the purpose of document start markers, best practices for variable interpolation, and the difference between legacy and updated Ansible SSH connection variables. Boolean Values in YAML YAML allows for various representations of Boolean values. In Ansible, options can be set using yes/true/TRUE/True as well as no/false/FALSE/False. All these forms are interpreted the same way. For instance, the following playbook tasks are functionally equivalent: - name: Gather facts using various boolean forms (true)
  gather_facts: yes
  gather_facts: true
  gather_facts: TRUE
  gather_facts: True

- name: Gather facts using various boolean forms (false)
  gather_facts: no
  gather_facts: false
  gather_facts: FALSE
  gather_facts: False Note Ansible does not differentiate between the different representations of Boolean values. Choose the style that best fits your project conventions. The YAML Document Separator (---) The three dashes (---) at the beginning of a YAML file mark the start of a document. While this marker is optional for single-document files, it is especially useful when merging multiple YAML files into one document. Consider the following example: ---
- name: Print DNS server
  hosts: all
  tasks:
    - debug:
        msg: Hello For most playbooks, including or omitting the YAML document separator will not affect Ansible’s behavior. Using Double Curly Braces and Variable Interpolation Ansible playbooks process variables using Jinja2 templates, and variable interpolation is typically done with double curly braces (e.g., {{ variable_name }} ). However, there are certain nuances to keep in mind: Options that explicitly expect a variable name (such as the var parameter of the debug module or within the when condition) should be used without double curly braces. When combining text with variables or when a variable is the sole content of a field, enclose it in quotes with double curly braces. Consider the example below: - name: Print DNS server information
  hosts: all
  tasks:
    - debug:
        msg: ""{{ dns_server_ip }}""
        var: dns_server_ip
        when: ansible_host != 'web'
        with_items: ""{{ db_servers }}"" Key Points: The msg field outputs the value of dns_server_ip using double curly braces. The var parameter and the when condition expect the variable names directly without interpolation. The with_items directive uses double curly braces within quotes to indicate a list variable. Important When constructing playbooks, always assess the context in which a variable is used. Using or omitting double curly braces incorrectly may lead to unexpected behavior. Ansible SSH Connection Variables A common source of confusion is whether to use ansible_ssh_pass or ansible_password for SSH passwords. Although both work, ansible_ssh_pass is considered a legacy option. The updated and recommended variable is ansible_password . Here are two inventory file examples: Legacy approach: /etc/ansible/hosts
web1 ansible_host=172.20.1.100 ansible_ssh_pass=Passw0rd
web2 ansible_host=172.20.1.101 ansible_ssh_pass=Passw0rd Recommended approach: /etc/ansible/hosts
web1 ansible_host=172.20.1.100 ansible_password=Passw0rd
web2 ansible_host=172.20.1.101 ansible_password=Passw0rd Tip For new playbooks and configurations, always use ansible_password to ensure compatibility with current best practices. Preparing for a Hands-On Coding Exercise In the upcoming coding exercise, you will work with multiple playbooks to apply these concepts and troubleshoot common YAML and Ansible syntax errors. The objective is to enhance your proficiency in writing correct and efficient YAML-based playbooks. During the exercise, you will be given playbooks with intentional errors. Your task is to diagnose and resolve these issues. This hands-on troubleshooting will reinforce your understanding of YAML syntax and Ansible best practices while ensuring your work doesn't impact any managed systems. Happy learning and troubleshooting! Watch Video Watch video content Practice Lab Practice lab"
Ansible Advanced Course,Create simple shell scripts that run ad hoc Ansible commands,https://notes.kodekloud.com/docs/Ansible-Advanced-Course/Install-and-Configure/Create-simple-shell-scripts-that-run-ad-hoc-Ansible-commands,"Ansible Advanced Course Install and Configure Create simple shell scripts that run ad hoc Ansible commands In this guide, you'll learn how to create efficient shell scripts that execute ad-hoc Ansible commands. These scripts are especially useful for automating multiple tasks sequentially – for example, running a command to ping all nodes and then printing the contents of the /etc/hosts file on each node. Tip Embedding Ansible configuration parameters as environment variables directly in your shell script ensures that your setup is consistently applied every time you run your commands. Setting Up Your Environment Previously, we discussed configuring Ansible parameters through environment variables. Instead of setting these variables manually each time, you can include them in your shell script. This guarantees that the required configurations are always in place before any command execution. Below is an example of a shell script that sets an environment variable and then runs several Ansible commands: export ANSIBLE_GATHERING=explicit
ansible -m ping all
ansible -a 'cat /etc/hosts' all
ansible-playbook playbook.yml Running Your Shell Script There are two convenient methods to execute your shell script: Method Command Example Using the sh command sh script_name.sh Making the script directly executable chmod 755 script_name.sh followed by ./script_name.sh Choose the method that best suits your environment for running the script. Security Warning Before running any shell script, ensure you review its contents for security. Running untrusted scripts, especially with elevated privileges, may put your system at risk. Conclusion Embedding environment variables within your shell scripts simplifies the execution of Ansible playbooks and ad-hoc commands, minimizing the risk of misconfiguration and saving time. For further study, consider revisiting related exercises on shell scripting and Ansible commands to reinforce your knowledge. For additional resources, check out the Ansible Documentation to enhance your understanding of best practices and advanced usage tips. Watch Video Watch video content Practice Lab Practice lab"
Ansible Advanced Course,Additional Modules,https://notes.kodekloud.com/docs/Ansible-Advanced-Course/Ansible-Modules/Additional-Modules,"Ansible Advanced Course Ansible Modules Additional Modules In this lesson, we introduce several additional Ansible modules designed to extend your automation capabilities. In the upcoming labs, you will gain hands-on experience with these modules while exploring their various options. For detailed reference on each module, please check the respective module documentation. Let's get started! Package Module To install packages on a target host, use the module that corresponds to the host's package manager. For instance, use the yum module for CentOS or Red Hat Enterprise Linux and the apt module for Ubuntu. Starting with Ansible 2.0, you can also use the unified package module which automatically selects the correct package manager based on the host. However, keep in mind that package names might differ between operating systems—for example, the web server package is known as ""httpd"" on CentOS but ""apache2"" on Ubuntu. In such scenarios, managing package names with variables or conditionals becomes crucial. Note Using the package module with a single package name (e.g., httpd ) might not work across different operating systems. Consider advanced techniques such as variables and conditionals to handle such variations. Below is an example playbook that demonstrates three approaches: ---
- name: Install web server packages on multiple platforms
  hosts: all
  tasks:
    - name: Install web server on CentOS
      yum:
        name: httpd
        state: installed

    - name: Install web server on Ubuntu
      apt:
        name: apache2
        state: installed

    - name: Install web server on any host using package module
      package:
        name: httpd
        state: installed Service Module The service module is used to manage system services. Whether you need to start, stop, or restart a service, or ensure it is enabled to start at boot time, this module has you covered. ---
- name: Ensure httpd service is running and enabled
  hosts: all
  tasks:
    - service:
        name: httpd
        state: started
        enabled: yes Firewalld Module For firewall configuration on CentOS or Red Hat systems, the firewalld module is the go-to tool. Configure rules based on port, protocol, service, or source address, and specify the appropriate zone. By default, Ansible applies changes immediately; however, if you need rules to persist after reboots, set the permanent option to yes . Keep in mind that with permanent enabled, rules will not take immediate effect unless the immediate option is also set to yes . ---
- name: Add firewalld rule to allow HTTP traffic on port 8080
  hosts: all
  tasks:
    - firewalld:
        port: 8080/tcp
        service: http
        source: 192.0.0.0/24
        zone: public
        state: enabled
        permanent: yes
        immediate: yes LVM Modules The LVM modules allow you to manage Logical Volume Management efficiently. Start by creating a volume group using the lvg module, then create logical volumes within the group using the lvol module. The example below creates a volume group named ""vg1"" on specified physical devices, followed by the creation of a logical volume ""lvol1"" with a size of 2GB. ---
- hosts: all
  tasks:
    - name: Create LVM Volume Group ""vg1""
      lvg:
        vg: vg1
        pvs: /dev/sdb1,/dev/sdb2

    - name: Create Logical Volume ""lvol1"" with 2GB size
      lvol:
        vg: vg1
        lv: lvol1
        size: 2g File System and File Modules The filesystem and file management modules are essential for creating filesystems, mounting devices, and managing files or directories. Use the filesystem module to create filesystems on devices and the mount module for mounting. The file module comes in handy for creating files, directories, and symbolic links with specific permissions and ownership settings. For example, the following playbook creates an application directory and an empty HTML file with defined ownership and permissions: ---
- hosts: all
  tasks:
    - name: Create application directory
      file:
        path: /opt/app/web
        state: directory

    - name: Create index.html file with proper ownership and mode
      file:
        path: /opt/app/web/index.html
        state: touch
        owner: app-owner
        group: app-owner
        mode: '0644' Archive and Unarchive Modules The archive module is useful for compressing files or directories. By default, it creates a GZ archive, though you can specify different formats using the format option. Conversely, the unarchive module decompresses files and can transfer archives from the Ansible controller to target systems. When working with files already existing on the remote host, set remote_src: yes . ---
- hosts: all
  tasks:
    - name: Compress the /opt/app/web folder
      archive:
        path: /opt/app/web
        dest: /tmp/web.gz
        format: gz

    - name: Uncompress the archive on the remote host
      unarchive:
        src: /tmp/web.gz
        dest: /opt/app/web
        remote_src: yes Cron Module The cron module automates the configuration of scheduled tasks (cron jobs) on managed nodes. With this module, you can specify details such as the job name, command, and exact schedule parameters like month, day, hour, minute, and even weekday. Example: Specific Date and Time Schedule a job to execute on February 19 at 08:10. ---
- hosts: all
  tasks:
    - name: Create a scheduled task for February 19 at 08:10
      cron:
        name: Run daily health report
        job: sh /opt/scripts/health.sh
        month: 2
        day: 19
        hour: 8
        minute: 10 Example: Using Wildcards Run the task every day at 08:10 using wildcards for the month and day values. ---
- hosts: all
  tasks:
    - name: Create a scheduled daily task at 08:10
      cron:
        name: Run daily health report
        job: sh /opt/scripts/health.sh
        month: ""*""
        day: ""*""
        hour: 8
        minute: 10
        weekday: ""*"" This schedule corresponds to the following cron syntax: 10 8 * * * Example: Using Step Values Configure a task to run every two minutes using step values in the minute field: ---
- hosts: all
  tasks:
    - name: Create a scheduled task to run every two minutes
      cron:
        name: Run daily health report
        job: sh /opt/scripts/health.sh
        month: ""*""
        day: ""*""
        hour: ""*""
        minute: ""*/2""
        weekday: ""*"" Which is equivalent to the cron entry: */2 * * * * User, Group, and Authorized Keys Modules Managing user accounts and groups is made simple with the user and group modules. The user module allows you to set properties such as user ID, default group, login shell, and more. Additionally, the group module is used to create or manage user groups. ---
- hosts: all
  tasks:
    - name: Create a new user ""maria""
      user:
        name: maria
        uid: 1001
        group: developers
        shell: /bin/bash

    - name: Create the ""developers"" group
      group:
        name: developers To manage SSH keys for user accounts, use the authorized_keys module. This module distributes public SSH keys to specified user accounts on managed nodes. ---
- hosts: all
  tasks:
    - name: Configure SSH keys for user ""maria""
      authorized_keys:
        user: maria
        state: present
        key: |
          ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABQC4WKn4K2G3iWg9HdCGo34gh+......root@97a1b9c3a That concludes the overview of these additional Ansible modules. In upcoming labs, you'll have plenty of practical exercises to deepen your understanding and enhance your skills with these modules. Happy automating! Watch Video Watch video content Practice Lab Practice lab"
Ansible Advanced Course,Project Demo,https://notes.kodekloud.com/docs/Ansible-Advanced-Course/Ansible-Modules/Project-Demo,"Ansible Advanced Course Ansible Modules Project Demo In this article, we demonstrate how to deploy the KodeKloud ecommerce application manually on a CentOS machine—without using automation tools like Ansible. You can use any available CentOS machine or quickly access a CentOS playground online. The application's source code is hosted on GitHub in the learning-app-ecommerce repository maintained by the KodeKloud organization. The repository includes all necessary deployment files along with a detailed README file divided into three sections: • Deploying prerequisites • Deploying and configuring the database • Deploying and configuring the web server Follow the steps below sequentially to complete the manual deployment. 1. Deploying Prerequisites Installing and Configuring firewalld Begin by installing firewalld, then start and enable the service to run on system boot: sudo yum install -y firewalld
sudo systemctl start firewalld
sudo systemctl enable firewalld If you need to troubleshoot, verify the service status with: sudo systemctl status firewalld When adding firewall rules later in the process, you can list all active rules with: sudo firewall-cmd --list-all A sample troubleshooting session might resemble: [root@eb29eab4499 ~]# sudo systemctl start firewalld
Redirecting to /bin/systemctl start firewalld.service
[root@eb29eab4499 ~]# sudo systemctl enable firewalld
Created symlink from /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service
    to /usr/lib/systemd/system/firewalld.service.
Created symlink from /etc/systemd/system/multi-user.target.wants/firewalld.service
    to /usr/lib/systemd/system/firewalld.service. 2. Deploying and Configuring the Database Installing MariaDB Server Install the MariaDB server package using the following command: sudo yum install -y mariadb-server After installation, you can review or modify default settings in the configuration file /etc/my.cnf . Below is an example configuration; unless you need to change the default port or other settings, the provided configuration remains suitable: [mysqld]
datadir=/var/lib/mysql
socket=/var/lib/mysql/mysql.sock
# Disabling symbolic-links is recommended to prevent assorted security risks
symbolic-links=0
# Settings user and group are ignored when systemd is used.
# If you need to run mysqld under a different user or group,
# customize your systemd unit file for mariadb according to the
[mysqld_safe]
log-error=/var/log/mariadb/mariadb.log
pid-file=/var/run/mariadb/mariadb.pid
#
# include all files from the config directory
#
!includedir /etc/my.cnf.d Start and enable MariaDB: sudo systemctl start mariadb
sudo systemctl enable mariadb A sample session to verify these steps: [root@eb29eab44d99 ~]# vi /etc/my.cnf
[root@eb29eab44d99 ~]# sudo systemctl start mariadb
Redirecting to /bin/systemctl start mariadb.service
[root@eb29eab44d99 ~]# sudo systemctl enable mariadb
Created symlink from /etc/systemd/system/multi-user.target.wants/mariadb.service
    to /usr/lib/systemd/system/mariadb.service. Adding the Firewall Rule for MariaDB Open port 3306 (the default MySQL port) by executing: sudo firewall-cmd --permanent --zone=public --add-port=3306/tcp
sudo firewall-cmd --reload Confirm that the new rule is active: sudo firewall-cmd --list-all Configuring the Database Access the MariaDB monitor to create the database, add a new user, and grant the necessary privileges. :::note Database Setup In this example, we create a database named ecomdb and a user ecomuser with the password ecompassword . ::: Create the Database: MariaDB [(none)]> CREATE DATABASE ecomdb;
Query OK, 1 row affected (0.00 sec) Verify the Database: If a typo occurs, such as show database; , correct it using: MariaDB [(none)]> SHOW DATABASES;
+--------------------+
| Database           |
+--------------------+
| information_schema |
| ecomdb             |
| mysql              |
| performance_schema |
| test               |
+--------------------+
5 rows in set (0.00 sec) Create a Database User and Grant Privileges: MariaDB [(none)]> CREATE USER 'ecomuser'@'localhost' IDENTIFIED BY 'ecompassword';
Query OK, 0 rows affected (0.00 sec)

MariaDB [(none)]> GRANT ALL PRIVILEGES ON *.* TO 'ecomuser'@'localhost';
Query OK, 0 rows affected (0.00 sec)

MariaDB [(none)]> FLUSH PRIVILEGES; Loading Inventory Data The GitHub repository includes a SQL script ( DB-load-script.sql in the assets directory) that creates a products table with sample data. Create the SQL Script File: Run the command below to create the file and paste the content: cat > db-load-script.sql Sample Contents of db-load-script.sql: USE ecomdb;
CREATE TABLE products (
  id mediumint(8) unsigned NOT NULL auto_increment,
  Name varchar(255) DEFAULT NULL,
  Price varchar(255) DEFAULT NULL,
  ImageUrl varchar(255) DEFAULT NULL,
  PRIMARY KEY (id) AUTO_INCREMENT=1
);

INSERT INTO products (Name, Price, ImageUrl) VALUES
  (""Laptop"", ""100"", ""c-1.png""),
  (""Drone"", ""20"", ""c-2.png""),
  (""VR"", ""300"", ""c-3.png""),
  (""Tablet"", ""50"", ""c-5.png""),
  (""Watch"", ""90"", ""c-6.png""),
  (""Phone Covers"", ""20"", ""c-7.png""),
  (""Phone"", ""80"", ""c-8.png""),
  (""Laptop"", ""150"", ""c-4.png""); Load the Data: Execute the script with: mysql < db-load-script.sql Verification: Log in to the MariaDB monitor and switch to the ecomdb database to confirm the data was loaded: mysql
Welcome to the MariaDB monitor.  Commands end with ; or \g.
Your MariaDB connection id is 4
Server version: 5.5.64-MariaDB MariaDB Server

MariaDB [(none)]> USE ecomdb;
Database changed
MariaDB [ecomdb]> SELECT * FROM products;
+----+---------------+-------+----------+
| id | Name          | Price | ImageUrl |
+----+---------------+-------+----------+
|  1 | Laptop        | 100   | c-1.png  |
|  2 | Drone         | 20    | c-2.png  |
|  3 | VR            | 300   | c-3.png  |
|  4 | Tablet        | 50    | c-5.png  |
|  5 | Watch         | 90    | c-6.png  |
|  6 | Phone Covers  | 20    | c-7.png  |
|  7 | Phone         | 80    | c-8.png  |
|  8 | Laptop        | 150   | c-4.png  |
+----+---------------+-------+----------+
8 rows in set (0.00 sec) At this point, the database is configured and pre-populated with sample data. 3. Deploying and Configuring the Web Application Installing HTTPD, PHP, and MySQL Extensions Install the required web server packages along with PHP and its MySQL extension: sudo yum install -y httpd php php-mysql Add the firewall rule for HTTP (port 80) and reload firewalld: sudo firewall-cmd --permanent --zone=public --add-port=80/tcp
sudo firewall-cmd --reload Configuring HTTPD To configure Apache, open the configuration file and modify the DirectoryIndex if needed. For example, change the default file to index.php by editing the file: sudo vi /etc/httpd/conf/httpd.conf Locate and adjust the DirectoryIndex section from: <IfModule dir_module>
    DirectoryIndex index.html
</IfModule> to: <IfModule dir_module>
    DirectoryIndex index.php
</IfModule> After saving the changes, restart HTTPD: sudo systemctl start httpd
sudo systemctl enable httpd
sudo systemctl restart httpd Downloading the Application Code Ensure Git is installed, then clone the ecommerce repository into the web server’s document root: sudo yum install -y git
git clone https://github.com/kodekloudhub/learning-app-ecommerce.git /var/www/html/ Verify the files were cloned successfully: ls /var/www/html/
# Expected output: assets  css  fonts  img  index.php  js  README.md  scss  vendors Updating Database Connection Settings By default, the application’s index.php file is configured to connect to an IP address (e.g., 172.20.1.101). Since this is an all-in-one setup, update the database connection details to use localhost . Open index.php and locate the mysqli_connect call. Change the connection parameters from: $link = mysqli_connect('172.20.1.101', 'ecommerce', 'password', 'ecommerce'); to: $link = mysqli_connect('localhost', 'ecomuser', 'ecompassword', 'ecomdb'); Save the file and refresh your web page. You can also test the server response with: curl http://localhost If the connection is correctly configured, you will see a list of products fetched from the database. Troubleshooting Default Page Issues If an index.html file exists alongside index.php , Apache might serve index.html by default. Ensure that the DirectoryIndex configuration in /etc/httpd/conf/httpd.conf prioritizes index.php . If necessary, remove or rename the default index.html . For example, to create a temporary index file: cat > index.html
Hello there! This is a sample index file.
^C After adjusting the DirectoryIndex value, restart HTTPD: sudo systemctl restart httpd Now, accessing your site should display the ecommerce application (i.e., index.php ) rather than the sample index.html . Conclusion Following these steps, you have successfully deployed the KodeKloud ecommerce application manually on a CentOS machine. The process included: • Installing and configuring firewalld and MariaDB • Setting up the database by creating the database, adding a user, and loading sample data • Installing and configuring HTTPD , PHP, and cloning the application code • Updating the database connection settings in index.php This demonstration highlights how all components work together in a manual deployment scenario. Enjoy exploring the KodeKloud ecommerce application and refer to the KodeKloud GitHub repository for further information and updates. Watch Video Watch video content Practice Lab Practice lab"
Ansible Advanced Course,Create and distribute SSH keys to managed nodes,https://notes.kodekloud.com/docs/Ansible-Advanced-Course/Install-and-Configure/Create-and-distribute-SSH-keys-to-managed-nodes,"Ansible Advanced Course Install and Configure Create and distribute SSH keys to managed nodes In this article, learn how to create a static inventory hosts file and configure SSH key-based authentication for your managed nodes. Managed nodes are target machines—whether they are web servers, database servers, or other systems—that you manage with Ansible. While using username and password authentication (as shown in the example below) may be acceptable in learning environments, it is not recommended for production. Instead, SSH key-based authentication enhances security and is the preferred method for production deployments. Below is an example inventory file using password-based authentication: /etc/ansible/hosts
web1 ansible_host=172.20.1.100 ansible_ssh_pass=Passw0rd
web2 ansible_host=172.20.1.101 ansible_ssh_pass=Passw0rd For improved security, we will now set up SSH key-based authentication. What are SSH Keys? SSH keys provide a secure method of authenticating without using passwords. You generate a pair of keys—a private key (which you keep secure) and a public key (which you share with remote systems). The public key functions like a lock on the remote machine, while your private key acts as the key that unlocks it. Generating SSH Keys If you are new to SSH keys on Linux, here is a brief refresher. Assume you are using your local computer (laptop or virtual machine) to connect to a remote system. In environments where password-based authentication is disabled for security reasons, you rely on SSH keys. To create a pair of SSH keys, run the following command: ssh-keygen After execution, two files will be generated: id_rsa – your private key, which must remain securely on your system. id_rsa.pub – your public key, which can be shared with remote systems. These files form a ""key and lock"" pair, where the public key (lock) is placed on the remote system and the private key (key) remains with you. Configuring Passwordless SSH Login Copy the contents of your public key ( id_rsa.pub ) into the ~/.ssh/authorized_keys file on the remote system. Once the public key has been added, you can establish a connection using your private key with the -i flag, as shown below: cat ~/.ssh/authorized_keys
ssh-rsa AAAAB3NzaC1yc...KhtUBfoTzlBqRV1NThrOo4opzEwRQ01mWx user1

ssh -i id_rsa user1@server1 If the connection is successful, you will see a message similar to: Successfully Logged In! For environments with multiple virtual machines (VMs), copy the same public key to the authorized_keys file on each server. Remember, SSH keys are specific to user accounts, so ensure you use the same user when connecting to different servers. In many cases, you may start by transferring the SSH keys using password-based authentication; after confirming that passwordless access works, disable password-based authentication for added security. SSH Keys in an Ansible Environment The process in an Ansible environment involves: Generating a pair of SSH keys on the Ansible control node. Transferring the public key to each target VM. A handy tool for automating this transfer is ssh-copy-id . For example, to copy your public key to a remote server, run: ssh-copy-id -i id_rsa user1@server1 A sample output may be: Number of key(s) added: 1 Test the connection with: ssh -i id_rsa user1@server1 Successful authentication will display: Successfully Logged In! Updating the Ansible Inventory File With SSH key-based authentication established, update your Ansible inventory file. By default, Ansible assumes the user is root . If you are using a different user, specify that in your inventory file. If your private key is in the default location under the user's home directory, Ansible will detect it automatically. If the key is stored in a custom path, include the ansible_ssh_private_key_file parameter to inform Ansible of its location. /etc/ansible/hosts
web1 ansible_host=172.20.1.100 ansible_user=user1 ansible_ssh_private_key_file=/some-path/private-key
web2 ansible_host=172.20.1.101 ansible_user=user1 ansible_ssh_private_key_file=/some-path/private-key With these configurations, you have successfully set up passwordless, SSH key-based authentication for your managed nodes within an Ansible environment. Enhance your skills further by practicing SSH key-based authentication on your servers. Next Steps Continue exploring Ansible's capabilities by integrating other security best practices and advanced configurations to optimize your environment. Watch Video Watch video content Practice Lab Practice lab"
Ansible Advanced Course,Install required packages create static host inventory file create config file,https://notes.kodekloud.com/docs/Ansible-Advanced-Course/Install-and-Configure/Install-required-packages-create-static-host-inventory-file-create-config-file,"Ansible Advanced Course Install and Configure Install required packages create static host inventory file create config file In this lesson, you'll learn how to install the Ansible control machine, set up a static host inventory file, and configure Ansible using a custom configuration file. The control node hosts the core Ansible software and stores all your playbooks. Remember that Ansible must be installed on a Linux machine. Although you can run Ansible from a Linux VM on Windows, it cannot be installed directly on Windows. However, Windows machines can be managed as target nodes in your Ansible environment. Note Ansible can be installed using various methods. Use Linux package managers (yum, dnf, apt-get) for a quick setup, or use pip for the latest version and greater flexibility. Installing Ansible You have two main options for installing Ansible: using your system’s package manager or the Python package manager pip. Using Package Managers For systems based on different distributions, execute the following commands: For Red Hat or CentOS: $ sudo yum install ansible For Fedora: $ sudo dnf install ansible For Ubuntu or Debian: $ sudo apt-get install ansible Using pip If you already have Python installed, pip allows you to install or upgrade Ansible. On enterprise Linux, you may need to install the extra EPEL-release packages first: $ sudo yum install epel-release
$ sudo yum install python-pip
$ sudo pip install ansible To upgrade Ansible: $ sudo pip install --upgrade ansible To install a specific version, such as 2.4: $ sudo pip install ansible==2.4 Creating an Inventory File When Ansible is installed via a package manager, a default inventory file is created at /etc/ansible/hosts . If no other inventory file is specified, Ansible will use this file by default. However, you can create your own static inventory file anywhere, such as alongside your playbooks. Default Inventory File Example /etc/ansible/hosts

# This is the default Ansible 'hosts' file.
#
# Location: /etc/ansible/hosts
#
# - Comments start with the '#' character
# - Blank lines are ignored
# - Groups of hosts are defined using [group_name] headers
# - Hostnames or IP addresses can be specified
## Example 1: Ungrouped hosts (specify before any group headers).

## green.example.com
## blue.example.com
## 192.168.100.1
## 192.168.100.10

## [webservers]
## alpha.example.org
## beta.example.org
## 192.168.1.100
## 192.168.1.110 Custom Inventory File Example /opt/my-playbook/hosts
web1 ansible_host=192.168.1.100
web2 ansible_host=192.168.1.101 When executing a playbook, you can specify the path to your custom inventory file with the -i option. Creating and Overriding the Configuration File Ansible uses a default configuration file located at /etc/ansible/ansible.cfg when installed via a package manager. This file defines the default parameters and behaviors. You can modify these defaults directly or create a custom configuration file within your playbook directory with only the settings you wish to override. Default Configuration File Example /etc/ansible/ansible.cfg
[defaults]
inventory       = /etc/ansible/hosts
log_path        = /var/log/ansible.log
library         = /usr/share/my_modules/
roles_path      = /etc/ansible/roles
action_plugins  = /usr/share/ansible/plugins/action
gathering       = implicit
# SSH timeout
timeout         = 10
display_skipped_hosts = True
nocolor         = 1
forks           = 5 Custom Configuration File Example /opt/my-playbook/ansible.cfg
[defaults]
gathering       = explicit Important When installing Ansible via pip, the default inventory and configuration files are not created automatically. You will need to create these files manually. That’s it for this lesson. Feel free to explore installing Ansible in the hands-on labs and experiment with different configuration settings. Good luck, and see you in the next lesson! Quick Reference Task Command Example Install Ansible on CentOS/Red Hat sudo yum install ansible Install Ansible on Fedora sudo dnf install ansible Install Ansible on Ubuntu/Debian sudo apt-get install ansible Install Ansible with pip sudo pip install ansible Upgrade Ansible with pip sudo pip install --upgrade ansible Install Specific Version (2.4) sudo pip install ansible==2.4 For further details, check out the Ansible Documentation . Watch Video Watch video content Practice Lab Practice lab"
AWS Solutions Architect Associate Certification,Why take this course,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Introduction/Why-take-this-course,"AWS Solutions Architect Associate Certification Introduction Why take this course Hello, future Solutions Architects! I'm Michael Forrester, and welcome to the AWS Solutions Architect Associate certification course. In this course, you'll discover the benefits of certification and how it can help you propel your career forward. Let’s dive into the compelling reasons to join this course. Career Progression Opportunities A 2023 survey by Jefferson Frank revealed that 61% of certification holders experienced enhanced career progression compared to their non-certified peers. This certification not only opens doors for new job opportunities but also paves the way for promotions in your current role. Holding this certification demonstrates to employers that you're committed to your professional development, giving you a competitive edge in today's job market. Enhancing Industry Knowledge and Skills This course is meticulously designed to expand your AWS expertise and sharpen your design skills. You will learn how to create scalable, secure, and efficient solutions on AWS, building a robust foundation in AWS architectural principles—even if you are starting your journey as a Solutions Architect. Pay Raises and Employer Support According to the Jefferson Frank survey, 73% of certification holders received a pay raise after earning their AWS certification. Additionally, many employers offer partial or full reimbursement for the cost of certification. While these benefits can vary by organization, the certification is a proven way to not only increase your value in the marketplace but also to potentially boost your salary. Building Foundational AWS Design Skills This course will arm you with the fundamental skills needed to design architectures on AWS that are secure, reliable, efficient, and cost-optimized. These best practices align with the four pillars of the AWS Well-Architected Framework. Although this is an introductory course rather than an advanced architect masterclass, it provides a solid starting point for anyone eager to design with AWS. Note Remember, a strong foundation in AWS design principles is essential for any aspiring Solutions Architect. This course is tailored to provide that groundwork effectively. Meet Your Instructors I, Michael Forrester, will lead you through the design concepts of AWS architecture. My colleague, Sanjeev Thiyagarajan, will complement the learning experience by covering the essential skills section. Throughout the course, you'll see us transition between topics seamlessly. If you have any questions or need further clarification, please reach out via the KodeKloud arena on Slack—we’re here to help! Summary If you already possess foundational Cloud Practitioner knowledge and are ready to delve into designing AWS solutions, this course is the perfect next step for you. It not only prepares you for the AWS Solutions Architect Associate exam but also empowers you to make strategic AWS service choices. Key benefits include: Accelerated career progression and promotion opportunities Enhanced industry knowledge and technical skills Increased market value and potential for pay raises A competitive edge in a challenging job market Final Thoughts This course is more than just exam preparation—it’s a step towards mastering AWS design principles and advancing your career as a Solutions Architect. Let's embark on this journey together! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Course Introduction prerequistites details audience outcomes structure and more,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Introduction/Course-Introduction-prerequistites-details-audience-outcomes-structure-and-more,"AWS Solutions Architect Associate Certification Introduction Course Introduction prerequistites details audience outcomes structure and more Welcome future Solutions Architects, Associates, and Certified Solutions Architects! I'm Michael Forrester, and in this lesson, we'll explore the course introduction and layout. Course Objectives This course is designed to help you master designing for security, resilience, performance adaptation, and cost optimization on AWS. You’ll gain a solid understanding of core AWS services and design principles, which will prepare you for the AWS Solutions Architect exam. For instance, you will learn how to enhance security for EC2 instances, improve resiliency for Direct Connect, and optimize performance for DynamoDB. Throughout the lesson, we cover the basics of core AWS services. It is recommended that you take detailed notes—consider dedicating a separate sheet for each service in your notebook to collect key points and insights. The course equips you with the dials and levers necessary to optimize security, resiliency, performance, and cost efficiency. It assumes that you have prior experience as a Solutions Architect on AWS, though even newcomers will acquire the skills needed for both passing the exam and excelling in the role. Note If you have prior AWS hands-on experience, you'll find it easier to follow along and absorb the concepts presented in this course. If you aim to maximize your exam success, hands-on practice is recommended. However, even if you haven't worked in this role before, this course is structured to help you pass the exam while developing job-ready skills. Target Audience This lesson is ideal for individuals who have already completed the Cloud Practitioner exam or who possess foundational cloud computing knowledge. If you're new to this area, we highly recommend taking the Cloud Practitioner course first. Sanjeev Thiyagarajan and I co-teach the Cloud Practitioner course , so you'll benefit from diverse perspectives. Once you’ve finished the Cloud Practitioner material, return here fully prepared. Assessments and Course Structure Before diving into the course materials, assessments are available for both Cloud Practitioner and Solutions Architect Associate levels. Passing the Cloud Practitioner assessment indicates that you already have the foundational knowledge needed for this course. If not, consider revisiting that material before proceeding further. The course is divided into two major sections: AWS Services Section In this section, you will learn about various AWS services through labs, quizzes, and presentations. Design for AWS Section This section focuses on core design fundamentals for AWS services. It includes quizzes, interactive questions, and a design challenge that tests your ability to optimize for security, reliability, performance, and cost management. Throughout the course, gamified elements and hands-on labs are integrated into the learning experience. The journey begins with an introductory section, followed by detailed content sections, and finally concludes with a closing session summarizing course takeaways and outlining the next steps in your AWS certification journey. In addition to pre- and post-assessments, the course offers a Solutions Architect Associate mock exam. For example, the mock exam consists of 65 questions to be completed in 35 minutes. A strong performance on this initial assessment may mean that you require only supplementary practice exams. Otherwise, you will benefit from the course materials that steadily build your skills from the ground up. Detailed Course Breakdown Meet Your Services After the course introduction, the first major section is titled ""Meet Your Services."" Here, you will dive into various service categories such as Networking, Storage, Compute, and Database. While it is not necessary to memorize every service detail, organizing your notes by category (for example, all networking services grouped together) can be very helpful. By the end of this section, you might be familiar with between 100 and 150 AWS services. Each service category concludes with a quiz to test your understanding. Design for X The second major section, ""Design for X,"" focuses on enhancing key design elements such as security, reliability, performance, and cost optimization. This section reinforces each design concept with quizzes, followed by an ultimate design challenge. In this challenge, you will analyze diagrams to identify service modifications that enhance security, reliability, performance, or cost efficiency. At the design challenge, integrate all four design elements into a cohesive solution. Study Tips and Final Thoughts This is a comprehensive course, and it's normal to feel overwhelmed at first. Here are some tips to help you study effectively: Study one small section at a time with a systematic approach. Establish a regular study schedule—even dedicating just 15 minutes a day can lead to steady progress. Engage actively with hands-on labs and AWS playgrounds; however, be mindful to manage costs and avoid unnecessary expenses. Experiment with different note-taking techniques, such as mind maps or sketches, to reinforce your understanding. Stay focused and patient, and try not to let distractions interrupt your study routine. Effective Study Tip Breaking down the material and taking consistent notes will significantly improve your retention and understanding. Even small, regular study sessions can cumulatively lead to a successful exam performance. In summary, this course is geared toward learners with foundational Cloud Practitioner knowledge. It is divided into two main parts: ""Meet Your Services"" and ""Design for X."" With pre- and post-assessments, quizzes, labs, and interactive challenges, the course provides a robust, hands-on learning experience. Diligent note-taking and a disciplined study schedule will equip you for the Solutions Architect exam. If you have any questions, please reach out to us on Slack under AWS-courses or Cloud Courses. Thank you for joining this lesson. Sanjeev Thiyagarajan and I wish you the very best on your AWS journey, and we look forward to seeing you in the next lesson! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Why AWS Solutions Architecture Certification and what is a solutions architect,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Introduction/Why-AWS-Solutions-Architecture-Certification-and-what-is-a-solutions-architect,"AWS Solutions Architect Associate Certification Introduction Why AWS Solutions Architecture Certification and what is a solutions architect Welcome back, future AWS Solutions Architect Associate professionals. In this article, we explore the benefits of the AWS Solutions Architect Associate (SAA) certification and clarify the role of a Solutions Architect. This discussion focuses on the Associate-level certification and details how the role differentiates itself from other IT and cloud positions. We will cover: • The rationale for the SAA certification • An overview of the Solutions Architect role • Sample exam questions you might encounter AWS Exam Landscape Overview The AWS exam progression starts from the AWS Cloud Practitioner (CLF-C02) foundational level—depicted as a black hexagon—and moves upward through Associate-level exams. The Solutions Architect exam, the focus of this article, is one of these key tracks. Other Associate-level certifications include the AWS Certified Developer - Associate exam and certifications addressing operational roles, eventually leading to Professional and Specialty exams. The difficulty scaling moves from black to blue, then to teal green, and finally to purple for deep-dive specialties. Exam Retirement Notice One of the current exams is scheduled for retirement in April of this year. Although it is included for historical context, AWS will soon replace it with a new exam format. While the foundational level no longer demands prior cloud experience, it is recommended that candidates have one to two years of hands-on experience in roles such as Solutions Architect, Developer, or SysOps Administrator. AWS suggests that for some certifications, three to five years of relevant experience may be beneficial. AWS Solutions Architect Associate Certification at a Glance The AWS Solutions Architect Associate certification emphasizes a solid understanding of AWS services without requiring coding or operational tasks. Its primary focus is on evaluating how solutions can enhance security, optimize performance, and improve cost-efficiency within AWS environments. This certification is uniquely designed to ensure that you can design solutions which integrate AWS services effectively. It is important to note that the Solutions Architect Associate role is distinct from positions such as Enterprise Architect or other specialized architect roles. Who Should Consider the SAA Certification? If you are seeking comprehensive exposure to core AWS services—whether you work in development, operations, machine learning, or security—this certification is an excellent starting point. It covers a broad spectrum of topics including compute, networking, storage, databases, deployment management, and migration services. Mastery of the AWS Well-Architected Framework and a solid understanding of the AWS global infrastructure are key to your success. What Is a Solutions Architect? A Solutions Architect designs technical solutions that meet detailed business requirements by integrating multiple AWS services. This role acts as a bridge between enterprise-level strategy and hands-on technical details. For instance, while a Python software developer concentrates on application development, or an enterprise architect focuses on policy and strategic governance, a Solutions Architect ensures cohesive integration across systems and services. In contrast: An Enterprise Architect holds a strategic outlook across an organization’s entire IT ecosystem. An Application Architect is focused on a singular product or service. Solutions Architects work closely with both roles to ensure that technical designs align with broader business objectives. Exam Format and Example Questions The AWS Solutions Architect Associate exam (SAA-C03) comprises about 65 scenario-based questions over a two-hour duration. The exam evaluates your ability to design multi-service solutions rather than merely testing memorized facts. Below are sample questions that reflect the scenario-based problem-solving you can expect: Example Question 1 A company operates a public-facing three-tier web application in a VPC spanning multiple availability zones. The application tier’s Amazon EC2 instances are in private subnets and need to download software patches from the internet while remaining inaccessible directly from the internet. Which actions should be taken? (Select two.) To meet these requirements: Configure a NAT gateway in a public subnet, as NAT gateways must reside in public subnets to function as internet proxies. Create a custom route table for the private subnets with a route that directs outbound traffic to the NAT gateway. Using an Elastic IP or placing a NAT instance in a private subnet would not fulfill the requirements. The correct answers are options A and B. Example Question 2 A Solutions Architect is tasked with devising a cost-saving strategy for Amazon EC2 instances that do not need to operate during a two-week company shutdown. The instances store data in memory, which must be preserved during downtime. Which feature of EC2 allows this? Hibernation is the feature that preserves instance memory during shutdowns. The recommended solution is to use instance hibernation, making option C the correct answer. Example Question 3 A company needs to run a monitoring application on an EC2 instance within a VPC. The instance is accessed via its private IPv4 address. The challenge is to quickly redirect traffic to a standby EC2 instance if the primary application fails. Which solution best supports an active/passive setup? Two solutions include: • Attaching a secondary Elastic Network Interface (ENI) with the private IP address to the primary instance and transferring the ENI to the standby instance upon failure. • Associating an Elastic IP address with the primary instance’s network interface and re-associating it with the standby when necessary. While both options are viable, the ENI solution (option C) is preferred as it maintains the configuration within the private subnet, avoiding exposure of a public IP address. Example Question 4 An analytics company is setting up a web analytics service that requires each user's webpage to include a JavaScript script for making authenticated GET requests to the company’s S3 bucket. To ensure the script functions correctly across domains, what is the optimal solution? The recommended approach is to provide users with a pre-signed URL. This URL includes authentication tokens with an expiration timer, allowing secure, authenticated GET requests without additional configuration. Summary The AWS Solutions Architect Associate certification stands out as the most popular AWS Associate-level credential. It prepares you to build comprehensive, multi-service technical solutions that align with business needs. Unlike the enterprise architect, who maintains a broad strategic view, or the application architect, focused solely on a single product, a Solutions Architect bridges these roles by integrating multiple services seamlessly. Key exam topics include: Designing custom route tables for VPCs Exploring connectivity options like VPC peering and transit gateways Implementing best practices across various AWS services By mastering these concepts, you will be better prepared to succeed on the AWS Solutions Architect Associate exam and excel as a Solutions Architect—the technical professional who bridges enterprise strategy with detailed system implementation. I'm Michael Forrester. Thank you for reading. If you have any questions, please join our AWS courses Slack channel. Otherwise, I look forward to seeing you in the next lesson. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Solutions Archicture Exam guide What to focus on,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Introduction/Solutions-Archicture-Exam-guide-What-to-focus-on,"AWS Solutions Architect Associate Certification Introduction Solutions Archicture Exam guide What to focus on Welcome, Solutions Architect Associates! In this guide, we review the exam content for the AWS Certified Solutions Architect – Associate (SAA-C03) exam. The exam objectives have not only shaped our comprehensive course content but also mirror AWS’s latest detailed guidelines. For direct access to the SAA-C03 exam guide, please click the link provided on this page or simply search for “SAA-C03 exam guide” on Google. One of the exam’s main focuses is on designing solutions as well as assessing existing architectures for improvement. The four key areas of concentration are: Security Resiliency Performance Cost Our course starts with an introduction to essential AWS services and then guides you step-by-step on designing solutions that optimize these areas. Tip If you’re new to AWS, consider building a project portfolio during this course to gain practical experience. Sharing your portfolio with peers can lead to valuable insights. This exam is tailored for candidates with at least one year of hands-on AWS experience. However, if you’re building your expertise, practicing course concepts through projects is a great way to build your portfolio. Look out for an upcoming lesson on portfolio development that perfectly complements this course. Exam Format and Scoring The exam comprises two types of questions: Multiple Choice: Only one correct answer is available. Multiple Response: Two or more correct answers may be selected. There are 65 questions in total. According to the exam guide, 50 of these are scored while the remaining 15 are used to evaluate new questions without impacting your final score. Hence, it’s wise to put equal effort into answering all 65 questions. You will have 130 minutes to complete the exam, and to pass, you typically need a score of at least 720 out of 1000 (approximately 72%). However, aiming for around 90% can provide additional confidence. Exam Domains and Weightings The exam is structured into four domains as outlined below: Domain Percentage Weighting Designing Secure Architectures 30% Designing Resilient Architectures 26% Designing High-Performing Architectures 24% Designing Cost-Optimized Architectures 20% This breakdown signifies that about one-third of the questions center on security, more than a quarter focus on resiliency, slightly less than a quarter on high performance, and roughly one-fifth on cost optimization. Although cost optimization has a lower weighting compared to previous versions, it remains a critical area of focus. Course Alignment with Exam Objectives Each domain in the exam guide outlines specific knowledge and skills. Every section of this course is designed to directly address these objectives. For example: Learn how to design secure access to AWS resources. Understand how to create fault-tolerant architectures. A notable update in the current exam is within task statement five of the High-Performing Architectures domain, which now includes data ingestion and transformation solutions. This update emphasizes the growing importance of building robust data ingestion pipelines, even when your primary focus is not machine learning. As you navigate the exam guide, you'll see extensive task statements and subtasks for each domain. Rest assured, our course covers every knowledge area mentioned so you can build the necessary theoretical understanding as well as practical skills. Key Task Statements and Updates Task Statement Two: Emphasizes designing highly available or fault-tolerant architectures. Task Statement Five (High-Performing Architectures): Now includes data ingestion and transformation solutions, preparing you for designing comprehensive AWS solutions. Towards the end of the exam guide, you will find an extensive list of AWS services that could be covered on the exam along with services that are generally not included. For instance, services like Lightsail are mentioned to help differentiate them from EC2. Our course content is rigorously aligned with the exam guide to ensure you engage with all essential topics and avoid extraneous material. Final Thoughts Every section of our course has been mapped directly to the objectives laid out in the exam guide. For more detailed information, please refer to the full exam guide via the provided link. Thank you for taking the time to review this guide. I'm Michael Forrester, and I hope this overview helps set you on the right path to success in your AWS Certified Solutions Architect – Associate exam. If you have any questions or would like to discuss AWS architecture in more detail, please feel free to reach out. Happy learning and best of luck with your exam! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Default VPC Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Default-VPC-Demo,"AWS Solutions Architect Associate Certification Services Networking Default VPC Demo In this article, we explore the default Virtual Private Cloud (VPC) settings that AWS automatically configures when you create a new account. We will guide you through the VPC dashboard, inspect default configurations, and demonstrate how these settings enable internet connectivity for your AWS resources. Accessing the Default VPC Begin by navigating to the VPC section in the AWS Management Console. You can access this by selecting ""VPC"" from the list of services or by searching for ""VPC."" Although the console may display a specific region (e.g., Northern Virginia), note that the default VPC configuration remains consistent across all AWS regions. For new AWS accounts, you will observe that there is only one default VPC created, even though additional security groups might be visible that are not part of the default configuration. Inspecting the Default VPC Details Click on the default VPC to view its detailed configuration. Here are the key elements you will notice: The VPC state is ""available."" The CIDR block is set to 172.31.0.0/16. It is explicitly marked as the default VPC. Additional details such as route tables and network ACLs are also available, but these primary indicators confirm that this is your default VPC. Exploring Different Regions To verify consistency across regions, switch to another region, such as Ohio, and confirm that the default VPC setup is identical. In any region, you will find one VPC with the same CIDR block (172.31.0.0/16) and the default designation. Returning to the Northern Virginia region, you can now dive deeper into the VPC resources. Examining the Default Subnets Under the default VPC, navigate to the ""Subnets"" section. Here are some important observations: The VPC ID is displayed in a truncated format (for example, ending in ACB5). There are six subnets, each associated with the default VPC. Each subnet corresponds to a different availability zone (e.g., 1A, 1B, 1C, 1D, 1E, 1F), ensuring comprehensive coverage across the region. For a clearer view of the network topology, return to the VPC resource map page to see all six subnets mapped to their respective availability zones. Route Tables, Network ACLs, and Internet Gateway The default VPC includes a route table that manages the traffic between the subnets. By clicking on the route table, you can review its configuration, which appropriately directs subnet traffic. While we are not covering the route table in detail here, it plays a crucial role in routing network traffic. Note An Internet Gateway is automatically created and attached to the default VPC. This gateway allows resources within the VPC to access the internet. With the correct routing settings, any instance launched within a default subnet will have an auto-assigned public IPv4 address. Deploying an EC2 Instance to Validate Internet Connectivity To demonstrate the functionality of the default VPC, follow these steps to deploy an EC2 instance: Open the EC2 section in the AWS Management Console. Launch a new instance using the default settings (for example, choose Amazon Linux and a T2 micro instance). During network configuration, confirm that the instance is launched in the default VPC with the CIDR block 172.31.0.0/16 and select one of the default subnets. Ensure the instance is set to auto-assign a public IPv4 address for immediate internet access. When configuring the instance, double-check that the selected VPC is indeed the default VPC. You may also choose a specific subnet corresponding to an availability zone (e.g., US East 1B) if needed. Once launched, navigate to the ""Instances"" section to verify the instance's status. The key details to verify are: The instance is in a running state. It has a private IP address from the subnet. It has automatically been assigned a public IP address. Connecting to the EC2 Instance To confirm internet connectivity, establish an SSH connection to the EC2 instance. If you have your PEM file ready, use the following command: C:\Users\sanje\Documents\scratch\aws-demo>ssh -i aws-demo.pem [email protected] The authenticity of host '34.201.6.109 (34.201.6.109)' can't be established.
ECDSA key fingerprint is SHA256:fa0CPuuMP2Fvn9aHeAw56Eei94znaTnFefIDRg1mE.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '34.201.6.109' (ECDSA) to the list of known hosts. After connecting, test the connectivity by pinging an external address (for example, the Google DNS server) to ensure that the instance has proper internet access. Conclusion This article demonstrated how AWS pre-configures a default VPC to simplify resource deployment with built-in internet connectivity. By creating a default VPC, setting up subnets across all availability zones, and integrating route tables and an Internet Gateway, AWS enables you to quickly launch and manage EC2 instances with minimal networking configuration. Happy cloud computing! For further reading, check out these resources: AWS VPC Documentation AWS EC2 Documentation Watch Video Watch video content"
AWS Solutions Architect Associate Certification,VPC Overview,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/VPC-Overview,"AWS Solutions Architect Associate Certification Services Networking VPC Overview In this article, we dive deep into the AWS Virtual Private Cloud (VPC) concept, a cornerstone of secure and isolated networking in the cloud. Understanding VPCs is essential for anyone looking to master AWS networking, whether you are preparing for the Solutions Architect exam or managing production cloud services. What is a Virtual Private Cloud (VPC)? A Virtual Private Cloud (VPC) provides a secure, isolated section of AWS where you can launch AWS resources in a virtual network that you define. VPCs enable you to isolate resources, ensuring that one customer's data remains separate from another's—even within the same AWS account—and allow you to securely segment different applications. Within a VPC, you have full control over your networking environment. You can: Define custom IP addressing through subnetting. Configure routing tables to control packet flow within your account. Use security features such as Security Groups and Network Access Control Lists (NACLs) to manage traffic flow. Customize inbound and outbound traffic using various types of gateways. Note Managing your VPC in AWS is similar to managing a traditional data center with routers and switches, but the AWS Management Console streamlines and simplifies the process. Regional Isolation and VPC Deployment When you create a VPC, you must specify a single AWS region for its deployment. For instance, you might have VPC1 in the US East 1 region and VPC2 in US East 2. Each VPC exists solely within its designated region, preventing cross-region communication unless specifically configured. By design, resources within one VPC are isolated from those in another. To enable communication with the internet or between VPCs, you must explicitly configure the necessary settings, adding an extra layer of security. The Role of CIDR Blocks in VPCs Each VPC is assigned a range of IP addresses via its Classless Inter-Domain Routing (CIDR) block. For example, a VPC with a CIDR block of 192.168.0.0/16 can assign any IP address within that range to its resources. The allowed block sizes vary from a /16 (65,536 addresses) to a /28. In addition to the primary CIDR block, you can enable secondary IPv4 CIDR blocks or add up to five IPv6 CIDR blocks per VPC (each providing a /56 block), enhancing the flexibility and scalability of your network configuration. Types of VPCs When working with VPCs, you typically encounter two types: Default VPC Custom VPC Default VPC A default VPC is automatically created by AWS when a new account is set up. Each region comes with a default VPC, pre-configured to allow immediate internet connectivity for your instances. This ready-to-use configuration lets you deploy servers without investing time in complex networking setups. Custom VPC Custom VPCs are created and fully configured by you. When you set up a custom VPC, you define: The CIDR block Subnets and their IP addressing Routing configurations Network access rules using security groups and NACLs Default VPC Configuration AWS provides several default configurations for a default VPC to ease network setup. Below is an overview of these settings: Feature Default Configuration Description CIDR Block 172.31.0.0/16 Provides 65,536 IP addresses Default Subnets One subnet per availability zone (typically /20 CIDR each) For example, one zone may have 172.31.16.0/20 and another 172.31.32.0/20 Internet Gateway Attached with a default route (0.0.0.0/0) Enables internet connectivity for instances Default Security Group Configured to allow outbound traffic Protects instances by default Default Network ACL (NACL) Allows both inbound and outbound traffic Provides an additional layer of security Note AWS configures a default VPC in every region. This configuration is designed to help you get started quickly, but for production environments, you might consider creating custom VPCs tailored to your specific security and performance requirements. Summary To summarize the key points: VPCs are a foundational element of AWS networking, providing isolated environments for deploying resources. Each VPC exists within a single AWS region, enhancing security and network segmentation. A VPC’s network is defined by its CIDR block, which restricts the range of assignable IP addresses. AWS provides a default VPC per region with pre-configured subnets, an internet gateway, a security group, and a NACL. The default VPC uses the CIDR block 172.31.0.0/16, with each availability zone having one default subnet. This comprehensive overview should serve as a valuable resource for understanding the design and functionality of AWS Virtual Private Clouds (VPCs). For further reading, consider exploring the AWS Documentation and AWS VPC FAQs . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Custom VPC Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Custom-VPC-Demo,"AWS Solutions Architect Associate Certification Services Networking Custom VPC Demo In this lesson, you will learn how to create and delete a Virtual Private Cloud (VPC) in AWS. This step-by-step demonstration covers creating a new VPC, reviewing its configuration, and then deleting it. Future lessons will explore additional features and capabilities of VPCs in more depth. Note Remember that VPCs are region-specific. Ensure you select the appropriate AWS region before proceeding. Step 1: Select Your AWS Region First, log in to your AWS Management Console and choose the region where you want to deploy your VPC. For example, if you select the US East (N. Virginia) region, your VPC will be created there. If your deployment requires a different region, simply select the one that fits your needs. Step 2: Navigate to the VPC Dashboard Use the search bar at the top of the AWS console to search for ""VPC."" This action will direct you to the VPC page where you can view all VPC-related networking resources. To see an overview of your VPCs, click on ""Your VPCs"" in the left-hand navigation panel. If your AWS account is new, you might see one default VPC. You can identify it by scrolling horizontally until you find the ""default VPC"" label. AWS provides a default VPC to help you get started quickly; however, for this demonstration, we will create a custom VPC to explore the process without using the default configuration. Step 3: Create a Custom VPC Click the Create VPC button. AWS now provides two options: Create a standalone VPC Deploy a VPC along with a full set of networking resources (subnets, route tables, Internet Gateways, and NAT Gateways) For this demo, we will select the VPC-only option for simplicity. In the configuration panel: Name Tag: Enter a descriptive name for your VPC (e.g., ""demo VPC""). IPv4 CIDR Block: Set the CIDR block to 10.0.0.0/16 for this demonstration. IPv6 CIDR Block: Optionally, add an IPv6 CIDR block. You can either use one that you own or let AWS auto-provide one. For this lesson, the default settings for IPv6 will be used. Tenancy: Keep the tenancy option at its default value unless you have specific requirements. After reviewing your settings and tags, which should now include your ""demo VPC"" name tag, click Create VPC . The deployment process is typically quick, and a green status bar will indicate when it is complete. Step 4: Review the VPC Details Once the custom VPC is created, inspect its details on the VPC information page. Here you'll find valuable information such as: VPC ID Current State: Usually marked as ""available"" DNS Settings IPv4 and IPv6 CIDR Blocks Tenancy Status AWS also creates a default route table for the VPC to manage traffic routing. Additionally, explore sections for network ACLs (which control access by permitting or blocking traffic) and the resource map that lists all resources associated with the VPC. This demonstration highlights the simplicity of managing a custom VPC: select your region, define your CIDR block, and modify other settings as required. Step 5: Delete the Custom VPC When you are ready to clean up, you can delete your custom VPC by following these steps: Select your custom VPC from the ""Your VPCs"" list. Click the Actions button. Choose Delete VPC . Confirm the deletion by typing the word ""delete."" This action will permanently remove the custom VPC from your account. Reminder Deleting a VPC is permanent. Ensure that you have backed up any necessary configurations or data before proceeding. Conclusion This quick demo has shown you how easy it is to create and delete a custom VPC in AWS. By carefully selecting your region, configuring your CIDR block, and reviewing key settings, you can establish a custom network environment that suits your needs. For additional insights on networking and AWS best practices, explore more tutorials and official documentation. For more details, check out these resources: AWS VPC Documentation AWS Management Console Help Happy networking! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Routing in VPC,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Routing-in-VPC,"AWS Solutions Architect Associate Certification Services Networking Routing in VPC In this article, we explore the mechanics of routing within an Amazon Virtual Private Cloud (VPC). Every VPC includes a router that is critical in directing traffic both among subnets and between the VPC and external networks. VPC Router and Subnet Interfaces Every VPC router is assigned an interface in each subnet, making it accessible within that specific network segment. For example, if a subnet is defined as 192.168.1.0/24, the VPC router’s interface for that subnet is set to 192.168.1.1. This dedicated one-address-per-subnet assignment ensures efficient traffic management, whether it's inter-subnet communication or routing traffic to external destinations. Key Point Remember that the designated interface IP (e.g., 192.168.1.1) is automatically assigned for each subnet within your VPC, ensuring consistent internal routing. Route Tables and Routing Rules Routing in a VPC is governed by a route table—a collection of routing rules that directs how network traffic should be forwarded. Each rule in this table is referred to as a route. The router inspects the destination IP address of each outbound packet and then matches it against the routes defined in the table. For instance, consider an IPv6 route as depicted in the diagram below. Although the example demonstrates IPv6, the routing principles apply equally to IPv4 addresses. When a packet's destination IP falls within a specific prefix range (e.g., 10.16.0.0/16), the router selects the matching route. In scenarios where multiple routes can apply—such as an overlapping 10.16.1.0/24 alongside the broader 10.16.0.0/16—the router prioritizes the route with the largest (most specific) prefix length. Route Targets and Default Routes After identifying a matching route, the VPC router forwards the packet to the specified target. While many routes point to ""local"" (ensuring internal routing within the VPC), the target can alternatively be an IP address, a gateway, or even another EC2 instance. Essentially, the target is the destination endpoint where the packet should be sent. Every VPC is initialized with a default route table, which includes a mandatory local route. This local route ensures that traffic destined for other devices within the same VPC (as defined by the VPC's CIDR block) is routed internally. Additionally, if IPv6 is enabled, there is a corresponding local route for the IPv6 CIDR block. Subnet Associations and Multiple Route Tables Each subnet within a VPC is associated with a single route table. By default, newly created subnets are linked to the default route table. However, you can change this association by linking a subnet to a different route table if your networking requirements demand distinct routing behaviors. Multiple subnets can share the same route table when they adhere to identical routing rules, a useful approach for segregating network traffic between private and public subnets. Tip Consider grouping subnets with similar security or access requirements to simplify management by associating them with the same route table. Summary Amazon VPC routing revolves around a dedicated router that facilitates both internal and external traffic flows. The main considerations include: The router is equipped with an interface in every subnet, using a unique one-address-per-subnet assignment. A route table comprising multiple rules governs the routing process based solely on the packet’s destination IP address. When overlapping routes exist, the router chooses the one with the most specific prefix. A default route table automatically provides a local route for internal VPC communications, with the flexibility to associate specific subnets with custom route tables when required. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Route Table Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Route-Table-Demo,"AWS Solutions Architect Associate Certification Services Networking Route Table Demo In this guide, we explore how AWS route tables work within a Virtual Private Cloud (VPC). You will learn how packets are routed inside a VPC by inspecting the default route table, associating subnets, and modifying routing rules. Although AWS route tables support advanced customizations, this tutorial focuses on fundamental operations to help you get started. Creating a Demo VPC and Subnets Begin by creating a demo VPC using your preferred CIDR block. Ensure that IPv6 support is enabled. Then, create two subnets within this VPC with the following configurations: Subnet One: IPv4 CIDR: 10.0.1.0/24 Provide an appropriate IPv6 CIDR block. Subnet Two: IPv4 CIDR: 10.0.2.0/24 Provide an appropriate IPv6 CIDR block. (You can let AWS automatically select the availability zone.) After provisioning the VPC and subnets, AWS automatically associates them with the main (default) route table created for your VPC. Inspecting the Default Route Table To review the AWS-generated main route table, navigate to the VPC console and click on your newly created VPC. Then, open the route table in a new tab to inspect its configuration. The default route table typically includes the following routes: Local IPv4 Route: Any packet destined for an IP address within the VPC CIDR block is routed locally. Local IPv6 Route: Similarly, traffic destined for any IP within the VPC's IPv6 range is handled locally. Even though the ""Subnet Associations"" section may not list explicit subnet assignments, both subnets automatically inherit the routes defined in the main route table. In practice, when an EC2 instance in any associated subnet sends traffic, the route table evaluates the destination IP against its rules to determine the correct forwarding path. Creating and Associating a Custom Route Table For greater control over your network, you can define custom route tables. Follow these steps to create and associate a custom route table: Navigate to the ""Route Tables"" section in the VPC console. Click ""Create Route Table"", provide a name such as ""Route Table One"", and select your demo VPC. After creation, view the ""Subnet Associations"" tab. By default, no subnets are explicitly linked. Click ""Edit"" and associate the route table with Subnet One. With this association in place, traffic from Subnet One follows the rules defined in ""Route Table One"". If needed, you can create an additional custom route table and assign it to Subnet Two to segregate routing rules. This setup is particularly useful when configuring different network behaviors for public and private subnets. Tip For more details on AWS VPCs and route tables, refer to the AWS VPC Documentation . Modifying Routes in a Route Table Customizing routes is essential for directing network traffic efficiently. To add or modify routes, follow these steps: Open the desired route table from the VPC console. In the ""Routes"" section, click ""Edit Routes"". To add a new route, input the destination CIDR. For example, configuring a destination CIDR of 0.0.0.0/0 establishes a default route that captures all unmatched traffic. Select an appropriate target for this route, such as an Internet Gateway, NAT Gateway, or another designated target. Click ""Save Changes"" to apply your modifications. This process demonstrates the manner in which AWS routes incoming packets: by inspecting the destination IP, matching it against the configured rules, and subsequently forwarding the traffic along the approved path. Cleaning Up Resources After completing your demo, it is important to clean up your resources to avoid incurring unwanted charges. To do this: Delete the VPC you created. AWS will automatically remove all associated subnets, route tables, and other related resources. Resource Cleanup Reminder Always verify that you have terminated or deleted all temporary resources to prevent unexpected service charges. This demo has provided an overview of AWS route tables, including how to inspect default settings, configure custom route tables, associate subnets appropriately, and modify routing rules. For further reading and advanced configurations, consider exploring additional AWS networking resources and documentation. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Where are you now Take an Assessment to know what you dont know,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Introduction/Where-are-you-now-Take-an-Assessment-to-know-what-you-dont-know,"AWS Solutions Architect Associate Certification Introduction Where are you now Take an Assessment to know what you dont know Welcome back, Solutions Architect Associate learners! In this article, we explain the purpose and value of the assessment you are about to take and how it can benefit you depending on your AWS background. Assessing Your AWS Knowledge There are two primary scenarios: New to AWS If you are new to AWS—for example, you have recently completed the AWS Cloud Practitioner (CLF-C02) certification but do not yet have extensive hands-on experience—the assessment is optional. It serves as a checkpoint to gauge your current understanding while you continue building your foundational AWS knowledge. Experienced with AWS If you have been working with AWS for around six months or more and are familiar with core concepts such as the global infrastructure or compute services like EC2 , ECS , and EKS , then this assessment is especially valuable. It will help you pinpoint both your strengths and areas where additional learning is needed. For example, if you are already comfortable with compute services, you might breeze through that section and instead focus on other topics to deepen your overall expertise. Note A lower score on this assessment is not a setback—it highlights areas for growth and improvement. Conversely, a high score indicates a strong foundation, though we still recommend progressing through the entire course to ensure you cover any hidden gaps. No matter your background, use this assessment as a tool to understand your current level of AWS proficiency. It is particularly useful for those with prior AWS experience, enabling you to dive into the material with targeted focus and confidence. Happy learning! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Subnets,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Subnets,"AWS Solutions Architect Associate Certification Services Networking Subnets In this lesson, we explore subnets and their function within a Virtual Private Cloud (VPC). Subnets are defined as groups of IP addresses within your VPC, allowing you to deploy resources into isolated segments. When you launch an AWS resource—such as an EC2 instance—it must be assigned to a specific subnet. Availability Zones and Subnet Placement Every subnet is associated with a single availability zone. For instance, if you have: Subnet One in Availability Zone 1, any resource deployed within this subnet will reside in that zone. Subnet Two in Availability Zone 2, resources will similarly be placed in the second zone. By selecting the appropriate subnet for your resource deployment, you are effectively choosing the availability zone where your resource will reside. Public vs. Private Subnets Subnets can be designated as either public or private based on the intended use: Public Subnets: Ideal for resources that need direct internet connectivity, such as web servers or public applications. Public subnets have a direct route to an Internet Gateway. Private Subnets: Best suited for resources that do not require exposure to the internet, such as databases or internal applications. Internet-bound traffic in private subnets typically routes via a NAT Gateway. CIDR Range and Subnet Configuration A key aspect of subnet configuration is the CIDR range. For example, if your VPC is assigned a CIDR of 192.168.0.0/16, every subnet you create must have an IP address range within this block. Examples include: A subnet defined as 192.168.10.0/24 is valid. A subnet defined as 10.100.1.0/24 is outside the VPC's CIDR range and would trigger a configuration error. Further, the block size of any subnet must be between /16 and /28. Note the following reserved IP address rules within each subnet: Reserved IP Addresses The first four IP addresses in every subnet are reserved: The first address serves as the network address. The second address is typically used by the VPC router. The third address is reserved for DNS. The fourth address is held for future use. Additionally, the last IP address in a subnet (e.g., 192.168.10.255 in a /24 subnet) is reserved as the broadcast address. Avoiding Overlapping Subnets When creating subnets within the same VPC, ensure their IP address ranges do not overlap. For example: Subnet A: 10.16.0.0/24 Subnet B: 10.16.0.128/25 This configuration is invalid because the ranges overlap. However, overlapping subnets in different VPCs are permitted. IPv6 Support and Internal Routing Subnets can also be assigned an IPv6 CIDR block (commonly a /56) or even be configured to use exclusively IPv6 addresses. By default, resources in subnets can communicate with one another within the same VPC without requiring additional routing configurations in route tables. Auto-Assigning Public IP Addresses When configuring a subnet, you can enable the option to auto-assign a public IPv4 or IPv6 address to a resource upon deployment. Typically, instances receive a private IP address by default. Enabling this feature is particularly useful for resources in public subnets that require internet connectivity. When this setting is enabled: A public IP address is automatically assigned alongside the private IP. This ensures that resources, such as internet-facing web servers, are reachable by external users. Summary Subnets are integral to defining IP address ranges within a VPC and are confined to a single availability zone, which in turn determines resource placement. They can be configured as public or private depending on your requirements for internet connectivity. All subnets must comply with the VPC's CIDR range, and specific IP addresses within each block are reserved for essential network functions. Remember that while overlapping IP ranges are not allowed within a single VPC, such overlaps are permissible across different VPCs. Finally, subnet settings offer advanced options, such as IPv6 addressing and automatic public IP assignment, to help you build a scalable and robust AWS network architecture. For additional details, explore AWS VPC Documentation and other related resources for best practices in network design. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,The KodeKloud AWS Playgrounds,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Introduction/The-KodeKloud-AWS-Playgrounds,"AWS Solutions Architect Associate Certification Introduction The KodeKloud AWS Playgrounds Welcome, Future Solutions Architects! In this lesson, we explore the powerful features of the KodeKloud AWS Playground. Whether you’re refining your cloud skills or preparing for your AWS Solutions Architect exam, this guide will help you navigate and utilize the AWS Sandbox effectively. When you log in with your subscription, you gain access to multiple environments via the ""Playgrounds"" section on KodeKloud. Visit KodeKloud Playgrounds to discover a hidden gem that includes: Cloud Playgrounds Linux Playgrounds Kubernetes Environments and related tools Container Playgrounds HashiCorp Playgrounds Machine Learning Playground Observability Playgrounds While not every option directly relates to AWS, selecting AWS reveals the specific services and configurations available in your AWS Sandbox. Exploring AWS Services After clicking on AWS, you will see a detailed list of available services in your AWS playground. These include features to: Run virtual machines Configure object storage Manage RDS Deploy EKS Utilize services such as DocumentDB, ECR, SNS, and KMS Note AWS services related to code management (e.g., CodeStar, CodePipeline, and CodeDeploy) are generally aligned with operations exams rather than the AWS Solutions Architecture exam. This organized service list eliminates permission errors, letting you experiment freely with every feature. Starting Your Lab To start using the playground, simply click the Launch Now button. You will be redirected to a page where you can select Start Lab . The lab might already be running if you’ve visited previously. Here, you can also access: A list of supported services A troubleshooting guide Links to community support via Slack or forums Once the lab starts, you’ll be presented with a console that provides you with the necessary credentials and a link to access the AWS console. Copy the link into your browser and log in with the provided username and password. This setup connects you to a pre-configured IAM user account. At the IAM sign-in page, the account ID is pre-filled. Simply input your IAM username and password, then click Sign in . You may be prompted to save these details via your password manager. Enjoy full access to the AWS console in a secure sandbox environment that lets you experiment without risk or cost. Launching an EC2 Instance To launch an EC2 instance, navigate to the EC2 dashboard. Use the region dropdown at the top of the page to select your desired region (e.g., US East 1, US West 2, Europe, or AP Southeast 1 in Singapore). Steps to Launch an EC2 Instance Click the ""Launch Instance"" Button. Name Your Instance: For example, ""my-test-instance."" Choose an Amazon Linux 2023 AMI: This AMI is optimized for cloud environments (Ubuntu or RHEL are valid alternatives). Select an Instance Type: T2 Micro is a good starting point; you can later switch to a T3 or another instance type as needed. When configuring your instance, review the offered virtual machine types in the playground. Although some settings might reference other cloud platforms (like Azure), ensure you select the AWS-specific settings. If you need more resources, adjust the instance type and storage allocation (for example, change storage from 8GB to 12GB on a GP2 volume). Creating a Key Pair for Secure Access Before launching the instance, create a key pair for secure SSH access. AWS uses key pairs in place of traditional passwords for EC2 instances. Follow these steps: Create a new key pair (e.g., ""test-instance""). Choose the PEM file format. The key pair file will automatically download to your browser. After configuring storage, selecting the proper instance type, and setting up your key pair, proceed to launch your instance. If you receive a notification about missing networking settings (such as a VPC or subnet), follow the prompts to create a new default VPC. Refresh the page to view available subnets, enable automatic IP address assignment, and then complete the instance launch process. Accessing and Experimenting with AWS Services Once your EC2 instance is running, use your downloaded key pair to log in. Besides EC2, the AWS Playground allows you to experiment with other AWS services such as RDS and SNS. Remember that some code-specific services are not required for the Solutions Architecture exam, but exploring the entire range enriches your overall understanding. Tip If any service is missing or if you need additional practice, revisit the playground and experiment until you feel confident with the platform’s capabilities. This lesson provided an overview of the AWS services available through the KodeKloud sandbox playground. We hope you enjoy this hands-on experience and build a solid foundation for your future in cloud architecture. Thank you for reading, and see you in the next lesson! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,RegisteringTaking an exam for the first time What to know Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Introduction/RegisteringTaking-an-exam-for-the-first-time-What-to-know-Demo,"AWS Solutions Architect Associate Certification Introduction RegisteringTaking an exam for the first time What to know Demo Welcome to this detailed walkthrough of registering for your first AWS Solutions Architect – Associate exam. I'm Michael Forrester, and in this lesson, I'll guide you through each step of the registration process, ensuring that you’re well-prepared for exam day. If you’ve already become familiar with the AWS Cloud Practitioner (CLF-C02) process , you’ll notice several similarities. Otherwise, congratulations on taking this crucial step in advancing your AWS certification journey! Overview of the Exam Registration Process Start by searching for “AWS Certified Solutions Architect” on Google. This will lead you to a comprehensive certification page providing exam details and scheduling options. After arriving on the training and certification overview page, click the ""schedule an exam"" button. If you’re not signed in, the page may display differently. I was already signed in, which allowed me to proceed seamlessly to the next step. Logging In and Navigating the AWS Portal When you click ""sign in"" , you will be prompted to log in. I chose ""log in with Amazon"" because I maintain a dedicated Amazon account for AWS training separate from my retail account. If your account has multi-factor authentication (MFA) enabled, be prepared to verify your credentials. After logging in, you’ll either return to the AWS training page or be redirected to the CertMetrics page where you can view your exam history and certifications. Scroll down on the CertMetrics page until you locate the SAA C03 exam, which is the most current version of the Solutions Architect Associate exam. Here, you can schedule your exam with Pearson VUE. From CertMetrics to Pearson VUE At this point, AWS redirects you from its website to CertMetrics and then to the Pearson VUE portal. Pearson VUE provides two primary exam delivery options: Testing Center : Schedule an exam at a physical testing location. Online Proctored Exam : Take your exam remotely from your preferred location. In this demonstration, I selected the online exam option, which involves additional steps compared to an in-person exam. Online Exam Setup On the Online Exam Options page, you will find detailed instructions regarding: Performing system tests Meeting computer requirements Adhering to testing space rules Presenting acceptable IDs Exam Environment Restrictions Ensure you follow these guidelines strictly: no breaks, no electronics allowed nearby (other than the approved computer), no personal items within reach, and no external communication. Violating these rules may lead to exam cancellation or further disciplinary actions. Proceed by clicking ""Next"" and then select your preferred exam language. The Solutions Architect Associate exam is available in multiple languages including Chinese, English, and French. For this demo, I selected English. Reviewing Policies and Exam Instructions After selecting your language, review and agree to several policies covering: Facial recognition and data processing Online proctoring parameters Make sure you read each agreement carefully. Next, go through the exam instructions page. Here, guidelines include the rescheduling policy (only two reschedules are allowed; a third will require cancellation and re-registration) and the strict exam day protocols. Additional details cover cancellation and conduct policies. The strict exam environment rules are outlined clearly: no personal items within arm’s reach, no standing or taking breaks, no scratch paper, and absolutely no audible conversations—even self-talking is discouraged. Breaching these rules can lead to exam cancellation or revocation of your certification if any third-party assistance is detected. Before the exam begins, you will also select the spoken language for your proctor. Options include Japanese, Mandarin, Latin American Spanish, and English. For this demonstration, I chose English. Scheduling Your Exam Appointment The next step is choosing an appointment time for your exam. Your time zone is automatically detected, but adjustments can be made if necessary. I opted for a Saturday appointment at 9:45 a.m.—using the 24-hour format can reduce the risk of scheduling confusion. Review the available time slots and confirm your selection. After selecting your preferred time, proceed to book your appointment. The checkout page will display all exam details and the total registration cost (in my case, $150.00). If you have a voucher or promo code, enter it on the billing page. Once applied, if the voucher covers the full cost, your total will show as $0. If not, provide the required billing information to complete your order. After verifying your billing details, enter your credit card information and confirm the order. In my example, my system autofilled the details using a virtual card. Once your payment is processed, you will receive confirmation emails. Be sure to mark the exam date and time in your calendar immediately. Completing the Registration with a System Test The final step involves running a system test, which confirms that your computer complies with the exam environment requirements. During this process, you will download a binary and receive an access code. For instance: 587-210-689 This access code, along with other important exam details such as receipts and policy reminders, will appear on your confirmation page. After confirming your registration and completing the system test, you’re ready for exam day. Expect to receive several emails with your appointment details and exam policies, all of which you should review carefully. Need Help or More Information? If you have any questions, feel free to join our Slack channel or participate in our forums under AWS courses. I'm frequently available to help and ensure you have a smooth exam experience. Enjoy the course and best of luck on your exam! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Subnets Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Subnets-Demo,"AWS Solutions Architect Associate Certification Services Networking Subnets Demo In this lesson, you will learn how to create subnets within a Virtual Private Cloud (VPC) and deploy an EC2 instance into a specific subnet. We'll start by creating a VPC and then proceed to set up two subnets within it. Step 1: Create a VPC Begin by navigating to the VPC section in the AWS Management Console. Select the ""VPC only"" option and configure your VPC with the following details: Name: demo VPC CIDR block: 10.0.0.0/16 (Optional) Enable the Amazon provided IPv6 CIDR block. This configuration is all you need to create your VPC. Once the VPC is created, you will be taken to the VPC creation interface. Step 2: Create Subnets in the VPC Create the First Subnet Navigate to the subnets section and click on Create Subnet . Select the custom VPC you just created. Name: subnet one Availability Zone: Choose one among the available zones from the Northern Virginia (US East) region. For this example, select US East 1D. CIDR Block: It is crucial that the subnet's CIDR block falls within the VPC's CIDR block. Therefore, set the CIDR block for ""subnet one"" to 10.0.1.0/24. Warning Using a CIDR block outside of the range 10.0.0.0/16 (like 192.168.1.0/24) will result in an error. After configuring the details, click Create Subnet . Create the Second Subnet To create another subnet within the same VPC: Click Create Subnet again. Select the custom VPC. Name: subnet two Availability Zone: Choose a different zone; for example, select US East 1A. CIDR Block: Set the CIDR block to a range within 10.0.0.0/16, such as 10.0.5.0/24. Once you have configured ""subnet two,"" click Create Subnet . After creating both subnets, clear any applied filters on your VPC dashboard. You can also filter by your specific VPC to display only the subnets you created. Step 3: Deploy an EC2 Instance in a Specific Subnet To demonstrate subnet usage, you can launch an EC2 instance in one of your subnets: Go to the EC2 launch interface and select the VPC you created. Under the networking section, you will see both subnets available. For this example, choose subnet two (US East 1A) so that the instance will be deployed with an IP address within the 10.0.5.0/24 range. Select your desired Amazon Machine Image (AMI), choose an existing key pair, and keep the default security group settings. Launch the instance. AWS will automatically assign a private IP address from subnet two . After launching the instance, navigate to the Instances section in the console to verify the deployment. Even if the instance is still booting, you should see it has been assigned a private IP address (e.g., 10.0.5.113), confirming its placement in subnet two . Step 4: Clean Up Resources When you no longer need the instance or the VPC, it is important to terminate and delete them properly: Delete the EC2 instance from the Instances section. Confirm that it has been terminated. Return to your demo VPC and delete it. Note that deleting the VPC will also remove all associated subnets. Note If an error occurs during deletion because the instance is still shutting down, wait a few seconds and try again. Ensure that all instances and network interfaces have been terminated before deleting the VPC. Summary of Steps Step Number Action Details 1 Create a VPC Name: demo VPC, CIDR: 10.0.0.0/16 2 Create Subnets Subnet one: 10.0.1.0/24 (US East 1D); Subnet two: 10.0.5.0/24 (US East 1A) 3 Launch an EC2 Instance Deploy in subnet two; assigned IP in 10.0.5.0/24 4 Clean Up Terminate the instance and delete the VPC This lesson provided a step-by-step guide to setting up a VPC, configuring subnets, launching an EC2 instance in a specific subnet, and finally cleaning up the resources. For additional guidance and resources on AWS networking, consider exploring the AWS Documentation and AWS VPC User Guide . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,DNS VPC,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/DNS-VPC,"AWS Solutions Architect Associate Certification Services Networking DNS VPC In this lesson, we explore how DNS works within an AWS Virtual Private Cloud (VPC) and examine how domain names are resolved for instances in both private and public subnets. When you deploy an EC2 instance within a subnet, it automatically receives a private IP address regardless of the subnet type. Every private IP address assigned to an instance is also given a corresponding DNS entry by default. For example, if an instance in a public subnet is assigned the IP address 10.0.100.10, AWS automatically creates a domain name that incorporates this IP address. Resources within your VPC can access an instance either using its IP address (e.g., 10.0.100.10) or its assigned domain name. Remember, the automatic DNS assignment is applied exclusively to private IP addresses by default. DNS Query Methods To resolve these domain names, AWS provides dedicated DNS servers. Each resource in the VPC can query these servers using one of two methods: Query the special IP 169.254.169.253, which is accessible by all resources in the VPC. Query the second IP address in your VPC CIDR block. For instance, if your VPC uses the CIDR block 10.10.0.0/16, use 10.10.0.2 as the DNS server. Similarly, for a VPC with CIDR block 10.20.0.0/16, the DNS server is located at 10.20.0.2. When creating a custom VPC, pay attention to the following two settings that directly impact DNS functionality: Enable DNS Hostnames: By default, only private IP addresses receive a DNS entry. To assign public DNS hostnames to instances with public IP addresses, ensure the ""enable DNS hostnames"" option is activated during VPC creation. This option is crucial for instances that need to be accessed publicly. Enable DNS Support: This setting determines whether the VPC supports DNS resolution using Amazon-provided DNS servers. When enabled, DNS queries sent to the AWS DNS servers (either via the second IP in the VPC CIDR block or the special IP 169.254.169.253) will be resolved successfully. Disabling this option prevents DNS queries from reaching these servers. Summary of DNS Key Points Automatic DNS Entries: Private IP addresses are automatically mapped to DNS entries. AWS DNS Server Access: The DNS servers can be accessed at the second IP in the VPC CIDR block or via 169.254.169.253. Enable DNS Hostnames: Necessary to assign public DNS hostnames to instances with public IP addresses. Enable DNS Support: Must be active for the VPC to resolve DNS queries using AWS-provided servers. For more information on AWS networking and DNS configurations, refer to the AWS Documentation and the Understanding VPC DNS guide. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Elastic IP Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Elastic-IP-Demo,"AWS Solutions Architect Associate Certification Services Networking Elastic IP Demo In this guide, we demonstrate how to use Elastic IPs with an AWS EC2 instance. Before you begin, ensure that you have the following pre-configured in your AWS environment: A Virtual Private Cloud (VPC) A public subnet (configured with an attached gateway) A server deployed to the public subnet named ""my server"" that initially receives a dynamic public IP address Initial Setup Make sure that your server, ""my server"", is running on a public subnet with an automatically assigned IP address before proceeding. This demo assumes that the initial public IP is 52.90.159.117. Demonstrating Dynamic IP Behavior Before configuring an Elastic IP, it's important to understand how dynamic public IP addresses behave. Follow these steps: Navigate to the EC2 dashboard. Stop the instance and wait until it shows a ""stopped"" state (this may take a few minutes). Notice that the public IP address is no longer displayed. Start the instance again. Observe that the new public IP address is different and may begin with a ""3"" rather than a ""5"". This behavior indicates that the public IP is dynamic. Every time the instance is restarted, it receives a new public IP, which can disrupt access if the IP is hardcoded in client configurations. Introducing Elastic IPs AWS offers Elastic IPs as a solution to the issue of changing public IPs. An Elastic IP is a static public IP address that remains associated with your account regardless of the instance's state. Whether the server is stopped or restarted, the Elastic IP remains constant, ensuring uninterrupted access. Allocating an Elastic IP To reserve an Elastic IP, complete the following steps: Navigate to the EC2 dashboard. Click on ""Elastic IPs."" Select ""Allocate Elastic IP address."" AWS will automatically choose an IP address from the Amazon pool of IPv4 addresses and assign it to your account. After the allocation, assume the assigned Elastic IP is 35.173.92.86. This static IP is now reserved exclusively for your account. Associating the Elastic IP with an Instance Once you have allocated an Elastic IP, associate it with your instance by following these steps: In the EC2 dashboard, select the Elastic IP and click on “Actions.” Choose ""Associate Elastic IP address."" Specify the instance (or network interface) that the IP should be associated with. If the instance has multiple private IP addresses, select the appropriate one (in this demo, there is only one). On successful association, the instance now displays the Elastic IP (35.173.92.86) as its public IP address. Verifying Connectivity To test that your Elastic IP is functioning correctly, run a ping test from your terminal: C:\Users\sanje\Documents\scratch\aws-demo>clear
'clear' is not recognized as an internal or external command,
operable program or batch file.

C:\Users\sanje\Documents\scratch\aws-demo>ping 35.173.92.86

Pinging 35.173.92.86 with 32 bytes of data:
Reply from 35.173.92.86: bytes=32 time=22ms TTL=112
Reply from 35.173.92.86: bytes=32 time=21ms TTL=112
Reply from 35.173.92.86: bytes=32 time=15ms TTL=112
Reply from 35.173.92.86: bytes=32 time=19ms TTL=112

Ping statistics for 35.173.92.86:
    Packets: Sent = 4, Received = 4, Lost = 0 (0% loss),
Approximate round trip times in milli-seconds:
    Minimum = 15ms, Maximum = 22ms, Average = 19ms The successful ping confirms that the Elastic IP (35.173.92.86) is correctly routing traffic to your instance. Ensuring Elastic IP Persistence To confirm that the Elastic IP remains persistent across instance reboots: Stop the instance via the EC2 dashboard. Restart the instance. Verify that the public IP continues to be 35.173.92.86. This persistence guarantees uninterrupted connectivity even when the instance is restarted. Releasing an Elastic IP If you no longer need the reserved Elastic IP and wish to avoid unnecessary charges, follow these steps to release it: In the EC2 dashboard, select your Elastic IP. Under ""Actions,"" choose ""Release Elastic IP."" If you receive a prompt indicating that the IP cannot be released due to an active association, first disassociate the IP. After disassociating, release the Elastic IP address. The IP is now removed from your account and will no longer incur charges. Release Caution Before releasing your Elastic IP, ensure that no critical services rely on this IP address. Releasing an Elastic IP might lead to service interruptions. Summary Table Below is a quick reference table summarizing the key steps for working with Elastic IPs: Step Action Expected Outcome Dynamic IP Demonstration Stop and restart the instance Public IP changes from 52.90.159.117 to a new value Allocating an Elastic IP Allocate a new Elastic IP in EC2 dashboard A static IP (e.g., 35.173.92.86) is assigned to your account Associating the Elastic IP Associate the Elastic IP with your instance The instance now consistently uses the Elastic IP Verifying Connectivity Ping the Elastic IP Successful replies confirm connectivity Ensuring Elastic IP Persistence Reboot the instance Elastic IP remains unchanged Releasing the Elastic IP Disassociate and release the Elastic IP Elastic IP is removed from your account By following these steps, you ensure your server uses a fixed public IP address, mitigating potential disruptions from dynamic IP changes. For more detailed information about AWS EC2 and Elastic IPs, explore the AWS Documentation and AWS Elastic IP FAQs . This concludes our demonstration on using Elastic IPs with AWS EC2 instances. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,NACLs Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/NACLs-Demo,"AWS Solutions Architect Associate Certification Services Networking NACLs Demo Welcome to this technical lesson on Network Access Control Lists (NACLs). In this guide, we will explore the key differences between stateful security groups and stateless NACLs, as well as demonstrate how to configure NACLs to control traffic at the subnet level. Before you begin testing NACLs, ensure that your EC2 instances have security groups configured correctly to avoid interference. Security groups are stateful and typically control traffic for individual instances, whereas NACLs filter traffic for the entire subnet. In our scenario, we configure our security groups to allow all traffic so that NACLs become the primary filter. Step 1: Verifying Security Group Settings Begin by verifying the security groups associated with your servers. For example, one server might be using the ""webserver-sg"" security group: If a server shows no security group initially, refresh the console to load the updated details: Update the security group on every instance to ""webserver-sg."" This configuration ensures that security groups allow all traffic, so that subsequent tests focus on NACL behavior. For example, update server two as shown below: Next, change the security group settings for the instance: Then, open the ""webserver-sg"" settings and modify its rules to allow all inbound and outbound traffic: Step 2: Confirming Subnet Consistency Ensure that both instances reside on the same subnet by checking the networking details from the EC2 console. For example, inspect server one to verify that it is in the correct subnet (e.g., subnet 168.3): Then, navigate to the VPC dashboard. Under the ""Security Network ACLs"" section, select the default NACL for your VPC (VPC A): Examine the inbound rules. You will notice a rule numbered 100 allowing all traffic. Since NACL rules are processed sequentially, any following rule (such as a deny rule) will not take effect if rule 100 already permits the traffic. Step 3: Testing Default NACL Settings To confirm that your configuration is correct, attempt to SSH into an instance. You should be able to establish a connection because the security groups are not restricting any traffic. Below is an example output from a successful ping test and SSH session: 64 bytes from 8.8.8.8: icmp_seq=1 ttl=53 time=1.58 ms
64 bytes from 8.8.8.8: icmp_seq=2 ttl=53 time=1.61 ms
64 bytes from 8.8.8.8: icmp_seq=3 ttl=53 time=1.61 ms
64 bytes from 8.8.8.8: icmp_seq=4 ttl=53 time=1.91 ms
64 bytes from 8.8.8.8: icmp_seq=5 ttl=53 time=1.62 ms
64 bytes from 8.8.8.8: icmp_seq=6 ttl=53 time=1.65 ms
64 bytes from 8.8.8.8: icmp_seq=7 ttl=53 time=1.57 ms
--- 8.8.8.8 ping statistics ---
7 packets transmitted, 7 received, 0% packet loss, time 6011ms
rtt min/avg/max/mdev = 1.572/1.651/1.909/0.107 ms
[ec2-user@ip-10-1-1-82 ~]$ exit
logout
Connection to 3.82.5.183 closed.

C:\Users\sanje\Documents\scratch\aws-demo> ssh -i main.pem [email protected] #        
  ~  #####
  ~ ######
   #/   ___ 
  ~W~ https://aws.amazon.com/linux/amazon-linux-2023
  ~~~
   _m/
Last login: Sat Aug 26 04:37:31 2023 from 169.150.196.66
[ec2-user@ip-10-1-1-82 ~]$ Additionally, refresh your web browser to confirm that the hosted website is accessible. Step 4: Modifying NACL Rules Now, modify the NACL rules to showcase their stateless behavior. Edit the inbound rules, and change rule 100 so that it allows only SSH access. Configure rule 100 to permit SSH (port 22) from any IP address, and then save the changes. Despite security groups still allowing all traffic, this modification will restrict inbound traffic to SSH only, and a subsequent deny rule will block other protocols. To test this configuration, SSH into the instance using server two: Connect using a command similar to: ssh -i main.pem [email protected] After establishing the SSH connection, refresh the website in your browser. The page should hang due to the absence of HTTP access, confirming that the NACL is now enforcing SSH-only inbound traffic. Step 5: Allowing HTTP and HTTPS Traffic via NACLs To enable web traffic, update the NACL by adding new inbound rules: Add a rule (for example, rule 101 or 110) that allows HTTP (port 80). Add another rule (for example, rule 120) to allow HTTPS (port 443). After applying these changes, your inbound rules should now include entries permitting SSH, HTTP, and HTTPS traffic. Refreshing the website should now display properly on both EC2 instances since they share the same subnet. At this point, you may observe that server two does not have NGINX installed, causing the website to remain unresponsive. To remedy this: SSH into server two: ssh -i main.pem [email protected] Install NGINX using the Yum package manager: [ec2-user@ip-10-1-1-13 ~]$ sudo yum install nginx -y Note If the package downloads hang, it is due to the stateless nature of NACLs. NACLs require explicit inbound rules for the response traffic (return packets) when an outbound HTTP request is made. To temporarily allow the returning traffic needed to download NGINX, add a new inbound rule (for example, rule 130) permitting all traffic. Once NGINX is successfully installed, remove this broad rule. After installation, start NGINX: [ec2-user@ip-10-1-1-13 ~]$ sudo systemctl start nginx Refresh the website to verify accessibility, then revert the temporary rule so that only the rules for SSH, HTTP, and HTTPS remain. Step 6: Implementing Explicit Deny Rules One of the advantages of using NACLs over security groups is the ability to explicitly block traffic with deny rules. For instance, you can allow SSH by default but insert a high-priority deny rule for specific IP addresses. To demonstrate this, add a deny rule with a lower rule number (e.g., rule 90) that blocks SSH from a defined IP range. This rule will override the general SSH allow rule for that specific range, while SSH from all other addresses will still be permitted. Summary In this lesson, we covered: The differences between stateful security groups and stateless NACLs. How NACLs operate at the subnet level and require both inbound and outbound rules. Configuring NACL rules to allow or block specific types of traffic. Demonstrating the practical application by restricting traffic to SSH only and then enabling HTTP/HTTPS traffic. Using explicit deny rules to block traffic from specific IP ranges. By following these steps, you now have a clear understanding of how to leverage NACLs to manage network traffic effectively. For more detailed information on AWS networking, consider visiting the AWS Documentation or exploring related topics such as Networking & Content Delivery . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,NAT Gateways VPC Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/NAT-Gateways-VPC-Demo,"AWS Solutions Architect Associate Certification Services Networking NAT Gateways VPC Demo In this article, we demonstrate how to configure NAT gateways in AWS to allow an EC2 instance to access the internet securely. This setup ensures that only outbound connections initiated from within AWS are allowed. The EC2 instance makes internet requests via the NAT gateway, while external sources cannot initiate a connection to it. We create a dummy Virtual Private Cloud (VPC) using the CIDR block 10.0.0.0/16. Step 1: Create the Private Subnet Next, create a private subnet where your EC2 instance will reside. For this demo, configure the subnet with the CIDR block 10.0.1.0/24. Step 2: Launch an EC2 Instance After setting up the VPC and private subnet, navigate to the EC2 console to launch an instance in the private subnet. Name the instance ""private-server"" and use the default Amazon Linux image. During the network configuration step while launching the instance, select the VPC (""VPC demo"") and choose the private subnet. Do not assign a public IP address since internet access will be provided via the NAT gateway. After the instance is launched, you will notice that no public IP address is assigned. This means the EC2 instance is not directly accessible from the internet. To manage or reach the instance, you should use a VPN connection or another secure method to access your private network. Step 3: Configure the Internet Gateway and Public Subnet To enable internet access for the private instance via a NAT gateway, start by attaching an internet gateway to your VPC. Follow these steps: Create an internet gateway. Attach the internet gateway to your VPC. Next, create a public subnet by specifying the CIDR block 10.0.2.0/24. The availability zone selection is arbitrary for this demo. With both subnets in place, create two route tables: Public Route Table: Associate this route table with the VPC. Add a default route (0.0.0.0/0) pointing to the internet gateway. Associate the public route table with the public subnet. Private Route Table: This route table will later include a default route that points to the NAT gateway. Associate this route table with the private subnet. Step 4: Deploy the NAT Gateway Deploy a NAT gateway in the public subnet by following these steps: Create a NAT gateway in the public subnet. Assign a name to the NAT gateway. Allocate an Elastic IP to ensure the gateway has a fixed public IP address. After deploying the NAT gateway, update the private route table by adding a default route that points to the NAT gateway. Save these changes to complete the routing configuration for the private subnet. Note It is normal for the NAT gateway to show a pending status during initial setup—this indicates that the gateway is initializing. Considerations for High Availability NAT gateways are deployed within a specific subnet and are only resilient within their respective availability zones. For example, if a NAT gateway is deployed in us-east-1b and that availability zone experiences an outage, instances relying on that gateway will lose internet connectivity. In production environments, deploy multiple NAT gateways across different availability zones (e.g., us-east-1a, us-east-1b) and update your route tables accordingly to ensure redundancy and high availability. For this demo, only a single NAT gateway is used, so high availability configuration is not implemented. Conclusion This demo has shown how to set up a secure environment in AWS using a NAT gateway. By following these steps, you can enable internet access for private EC2 instances while maintaining a secure network architecture. For more detailed information on AWS networking and NAT gateways, refer to the AWS Documentation . Happy configuring! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Private and Public Subnets,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Private-and-Public-Subnets,"AWS Solutions Architect Associate Certification Services Networking Private and Public Subnets Understanding the differences between private and public subnets is fundamental when designing AWS environments. Knowing when to use each type is key to maintaining secure and scalable infrastructure. When deciding whether a subnet should be public or private, ask yourself: Should internet devices interact directly with the resources deployed on the subnet? If the answer is yes, that subnet should be public; if no, it should be private. For instance, consider a web server that serves content to internet users. This web server should reside in a public subnet. In contrast, a backend database that stores sensitive data must not be directly accessible from the internet. Instead, the database should be placed in a private subnet, where only trusted resources—like the aforementioned web server—can access it. Key Design Principle Place internet-facing resources, such as web servers, in public subnets while keeping sensitive backend services like databases in private subnets to ensure enhanced security. In practice, this configuration means that end users interact with a public-facing web server, which in turn securely communicates with a private database. This setup prevents direct internet access to the database, significantly reducing potential attack vectors. Another common scenario involves extending a private on-premises data center into AWS. In such cases, the cloud resources are treated as an extension of your existing private network and are typically deployed in private subnets. A secure VPN connection links your on-premises data center to the AWS infrastructure, thereby eliminating the need to expose these resources to the internet. Design Summary Resources in public subnets are designed to be accessible from the internet, while resources in private subnets remain isolated from direct external access. Gateways and routing configurations play a crucial role in defining these access levels. Watch Video Watch video content Practice Lab Practice lab"
AWS Solutions Architect Associate Certification,Optional Do you meet the pre requisites A Cloud Practitioner Mock Exam,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Introduction/Optional-Do-you-meet-the-pre-requisites-A-Cloud-Practitioner-Mock-Exam,"AWS Solutions Architect Associate Certification Introduction Optional Do you meet the pre requisites A Cloud Practitioner Mock Exam Welcome to this lesson! In this article, we help you determine whether you have the necessary knowledge to advance to the AWS Solutions Architect Associate Certification course. This assessment is based on your understanding of the AWS Cloud Practitioner (CLF-C02) material. Below, you will find a link to a mock exam specifically designed for the AWS Cloud Practitioner (CLF-C02) certification. Although you might already be enrolled in a different course, this optional exam acts as a prerequisite check: Exam Requirement If you have not yet achieved the AWS Cloud Practitioner (CLF-C02) certification or completed the introductory course, it is strongly recommended that you take this mock exam first. A passing score of 70% or above indicates you are ready to move forward with the AWS Solutions Architect Associate Certification course. Should your score fall below 70%, consider completing the AWS Cloud Practitioner (CLF-C02) course first to ensure you have a solid foundation before tackling the more advanced material. We hope you excel on the exam and build the confidence needed to succeed in the Solutions Architect Associate course. Good luck, and we look forward to seeing you in the next lesson! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Internet Gateways VPC,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Internet-Gateways-VPC,"AWS Solutions Architect Associate Certification Services Networking Internet Gateways VPC In this guide, you'll learn how Internet Gateways enable public connectivity for subnets within a Virtual Private Cloud (VPC). Understanding this concept is crucial for configuring your AWS environment for both private and public communication. By default, subnets in a VPC are created as private. Devices within these subnets cannot access the Internet, and external resources cannot reach them. To convert a subnet into a public subnet, you must attach an Internet Gateway to your VPC. Important Reminders Each VPC can have only one Internet Gateway attached. An Internet Gateway can only be attached to one VPC at a time. An Internet Gateway is a horizontally scaled, redundant, and highly available component that spans all Availability Zones within a region. It provides the essential communication bridge between resources inside your VPC and the Internet. Without an Internet Gateway, all subnets in your VPC remain private by default. Converting a Private Subnet to a Public Subnet To transform a private subnet into a public one, follow these essential steps: Create an Internet Gateway. Attach the Internet Gateway to your VPC. Create a custom route table. Configure a default route in the route table that points to the Internet Gateway. This default route ensures that any traffic without a more specific route is forwarded to the Internet Gateway. Associate the desired subnet with the custom route table. This association enables all resources within that subnet to access the Internet. Public and Private IP Addressing When you deploy resources in a public subnet, they automatically receive a private IP address. To allow Internet-facing communication, you must enable the assignment of public IP addresses. For example: A resource might have a private IP like 192.168.1.1. Additionally, it will receive a public IP, such as 1.1.1.1, which external clients can use to reach the resource. It's important to note that the resource itself only recognizes its private IP. AWS manages the translation between the public and private IP addresses. When an external request is directed to the public IP, AWS forwards it to the corresponding private IP. Each network interface on a resource can have its own pair of public and private IP addresses. AWS seamlessly manages the mapping between these IP addresses to ensure smooth communication. Summary To recap the key points about Internet Gateways and VPC connectivity: Internet Gateways provide necessary public connectivity for VPC resources. They offer regional resilience by spanning all Availability Zones. Each VPC is limited to one Internet Gateway, and an Internet Gateway can be attached to only a single VPC. A subnet is converted to a public subnet when its route table includes a default route pointing to the Internet Gateway. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Load Balancers,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Load-Balancers,"AWS Solutions Architect Associate Certification Services Networking Load Balancers In this lesson, we dive into AWS Elastic Load Balancers (ELBs) and explore the common challenges they resolve. Imagine you have a web application running on a single EC2 instance with an IP address of 1.1.1.1. End users can access the application by sending requests to that IP. However, if that instance fails, your application becomes inaccessible. To enhance reliability, you might deploy the application across multiple EC2 instances in different availability zones to mitigate zone-specific failures. Now, consider running the application on three servers with different public IP addresses—for example, 3.3.3.3, 1.1.1.1, and 2.2.2.2. Expecting end users to manually choose or switch between these IPs when one fails is not a viable design. Instead, you can use a dedicated device with its own IP address that sits between end users and your EC2 instances. This device, known as a load balancer, accepts all incoming requests on its single IP and efficiently distributes the traffic to your backend instances. By centralizing the entry point to your application, a load balancer abstracts away the complexity of managing multiple endpoints. AWS provides built-in load balancing services under the Elastic Load Balancer umbrella, which directs traffic to various backend resources efficiently. Key Benefit ELBs enable you to manage your application's traffic seamlessly by routing requests to healthy EC2 instances, thus ensuring high availability. There are three types of load balancers offered by AWS: Classic Load Balancer (CLB) Application Load Balancer (ALB) Network Load Balancer (NLB) Let's explore each type in detail. Classic Load Balancer The Classic Load Balancer was AWS's inaugural load balancing solution. Over time, its limited feature set has rendered it less favorable compared to modern alternatives. For instance, it supports only a single SSL certificate. If you need to manage two separate applications each with its own SSL certificate behind the same load balancer, the Classic Load Balancer falls short. For this reason, new projects are generally built using either the Application or Network Load Balancer. Application Load Balancer Application Load Balancers are optimized for web-based applications that rely on HTTP, HTTPS, and WebSockets. Operating at the application layer (Layer 7), ALBs inspect and forward requests based on URL paths, host domains, HTTP methods, query parameters, and other HTTP-specific attributes. This additional intelligence facilitates features such as HTTP redirects, custom responses, and detailed health checks to ensure traffic only reaches healthy backend resources. When a client sends an HTTP or HTTPS request, the load balancer terminates the connection. For HTTPS, this includes SSL/TLS termination—meaning the SSL certificates are managed on the ALB. After decryption, the ALB typically forwards the request over HTTP to the backend. If end-to-end encryption is required, secure connections can also be established at the instance level by configuring HTTPS on the backend targets. Network Load Balancer The Network Load Balancer works at the transport layer (Layer 4), handling TCP and UDP traffic. Unlike the ALB, the NLB does not inspect higher-level protocols like HTTP or HTTPS. This makes it ideal for applications that require support for non-HTTP/HTTPS protocols or scenarios where high throughput and low latency are critical. Since the NLB does not terminate traffic, TCP sessions persist end-to-end between the client and the EC2 instance. When you require HTTPS with an NLB, SSL certificates must be managed on the server side, as the SSL/TLS handshake occurs directly between the client and the target. Health checks are conducted using TCP or ICMP to confirm the accessibility of targets at the transport layer. Elastic Load Balancer Architecture in a VPC When deploying an Elastic Load Balancer within an Amazon Virtual Private Cloud (VPC), you must designate the availability zones by selecting proper subnets. AWS automatically deploys load balancer nodes—the physical resources that handle traffic distribution—across the chosen subnets. These nodes receive traffic through a DNS record associated with the ELB, ensuring that traffic is balanced evenly across multiple nodes. Each load balancer node then forwards incoming traffic to the appropriate EC2 instances within its respective availability zone. An important feature is cross-zone load balancing. Without it, a load balancer node will only route traffic to EC2 instances within its own availability zone. This might lead to uneven distribution if one zone receives a higher proportion of traffic but has fewer instances. Cross-zone load balancing allows nodes to distribute traffic across instances in different zones, ensuring a more balanced load. Public vs. Private Load Balancers Elastic Load Balancers can be deployed as either public or private, depending on your application's requirements: Public Load Balancer: Deployed within a public subnet, it is accessible from the internet. This is ideal for web applications, APIs, or any service that needs to serve external users. Private Load Balancer: Deployed within a private subnet, it is only accessible from your internal network. This configuration is beneficial for back-end services, such as database servers, that should not be directly exposed to the internet. Example: Layered Application Architecture with ELB Consider an application with a multi-layered architecture: API Layer: This layer serves external API requests. A public Elastic Load Balancer distributes incoming traffic to several EC2 instances running the API services. Database Layer: This layer consists of EC2 instances handling database operations. To secure communication between the API layer and the database, a private Elastic Load Balancer is placed in front of the database instances. The public API layer routes requests to this private load balancer, which evenly distributes traffic among the database servers. This design not only optimizes traffic distribution but also enhances security by preventing direct external access to the database. Listeners and Target Groups When configuring an Elastic Load Balancer, two key components that define its behavior are listeners and target groups: Listeners: A listener is a process that checks for incoming connection requests based on a specified protocol and port. For example, if you want to handle requests for ""app1.com"", a listener can be configured to detect those requests and forward them appropriately. Target Groups: A target group comprises a set of registered targets—such as EC2 instances, ECS containers, or Lambda functions—that receive the forwarded traffic according to the listener's rules. You can configure health checks for each target group to ensure only healthy targets receive traffic. For instance, one target group might manage two EC2 instances for ""app1.com"", while separate target groups could serve endpoints like ""app2.com/auth"" for ECS containers or ""app2.com/cart"" for Lambda functions. Listener Rules Listeners use defined rules to match incoming requests and route them to the appropriate target group. Health checks ensure that traffic is only sent to targets that are operational. Summary AWS Elastic Load Balancers automatically distribute incoming traffic across multiple targets and availability zones, ensuring high availability and improved performance of your applications. Here are the key points to remember: Classic Load Balancers: Outdated and feature-limited; better alternatives are the ALB or NLB. Application Load Balancers (ALB): Operate at the application layer (Layer 7), support advanced routing based on HTTP/HTTPS attributes, and handle SSL/TLS termination. Network Load Balancers (NLB): Function at the transport layer (Layer 4) and are ideal for non-HTTP/HTTPS protocols while providing high-performance TCP/UDP traffic forwarding. Cross-Zone Load Balancing: Ensures even traffic distribution across instances in multiple availability zones. Listeners and Target Groups: Define how incoming traffic is inspected, routed, and health-checked across various backend resources. In summary, Elastic Load Balancers in AWS provide a robust, scalable mechanism to distribute traffic across your infrastructure, ensuring high availability, improved performance, and enhanced security for your applications. For further details on AWS load balancing and related cloud architecture best practices, refer to the AWS Documentation and explore additional resources on AWS Elastic Load Balancing . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Setting up your own AWS Account A walkthrough,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Introduction/Setting-up-your-own-AWS-Account-A-walkthrough,"AWS Solutions Architect Associate Certification Introduction Setting up your own AWS Account A walkthrough Welcome, future solutions architects! In this lesson, we will guide you step-by-step through the process of registering for your very own AWS account. While KodeKloud provides AWS playgrounds for cost-free experimentation, setting up your own account enables you to explore additional services not available in the playgrounds. Cost Management Reminder AWS services may incur charges if left running (e.g., NAT gateways, virtual machines, volumes, network firewalls). Always remember to create and delete your resources as needed. Let's dive into the registration process: Step 1: Sign Up for an AWS Account Start by navigating to the AWS homepage and clicking on the ""Sign up for a free AWS account"" button. When prompted, enter your email address. You can use a plus sign with an alias (e.g., if your email is [email protected] , you could use [email protected] ). Although it appears unique to AWS, emails will still be delivered to your main inbox. Next, choose an account name, such as ""Michael's demo KK account,"" and verify your email. Step 2: Email Verification AWS will send a verification code to your email. Once you receive that code, enter it and click ""Verify."" After verification, you'll be asked to create a password for your root account. Choose a strong, memorable password that satisfies AWS's complexity requirements. Step 3: Entering Personal and Contact Information Provide your personal details such as your full name, usage type (e.g., business or personal), and phone number. Additionally, fill in your billing address. For demonstration purposes, personal details may appear blurred for privacy. Step 4: Credit Card and Identity Verification Next, you'll be asked to provide a credit card for identity verification. AWS might place a temporary hold (typically up to $1 USD) on your card during this process. Rest assured, this is standard procedure, and your actual credit card information will remain secure. After entering your credit card details and proceeding, AWS will prompt you for an additional round of identity verification via phone call or text message. Once you receive the verification code, simply enter it. When prompted, you will also be asked to choose between support plans. For most users, the Basic (free) support plan is sufficient as it offers access to comprehensive online support resources. 0061 This code block represents the verification code you will enter. (In practice, you will receive a unique code specific to your registration.) Step 5: Completing Registration and Accessing the AWS Management Console Once the identity verification process is complete, AWS will process your account setup. Global service activation may take up to 24 hours; you will receive an email notification once your account is fully activated. After activation, log in using your root account credentials (e.g., [email protected] with your chosen password). You should see your account name displayed in the upper right-hand corner, such as ""Michael Forrester's KodeKloud demo."" Final Recommendations After setting up your AWS account, consider these best practices: Create an IAM user for everyday activities instead of using your root account. Set up AWS Budgets by searching for ""AWS Budgets"" in the AWS Management Console. This tool will notify you if your spending exceeds a specified limit (e.g., $5 or $10). Thank you for following along! You now have your own AWS account and are ready to explore the wide array of services AWS has to offer. Always manage your resources carefully to avoid unexpected charges. I'm Michael Forrester—see you in the next lesson! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,DNS VPC Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/DNS-VPC-Demo,"AWS Solutions Architect Associate Certification Services Networking DNS VPC Demo In this guide, we explore the various DNS options available for a Virtual Private Cloud (VPC) in AWS. In our example, a custom VPC is created with default settings, preconfigured with the standard DNS configuration. This VPC also includes an attached internet gateway, enabling public subnets. When selecting the VPC, navigating to ""Actions"" and then ""Edit VPC settings"" will display two critical configuration options: Enable DNS Resolution – This option allows the AWS provided DNS server to resolve host names. Enable DNS Hostnames – When enabled, it assigns a domain name to an instance's public IP address. In this demonstration, the ""Enable DNS Hostnames"" option is initially disabled so that we can focus on testing the DNS resolution feature. Launching an EC2 Instance The next step involves launching an EC2 instance from the AWS EC2 console. Follow these steps: Launch an instance named ""DNS demo."" Select one of your key pairs. Change the VPC to the VPC in focus. Enable ""Auto-assign Public IP."" Additionally, configure a security group rule that allows all ICMP traffic for ping testing. Ensure the rule allows traffic from 0.0.0.0/0 before launching the instance. After the instance is launched, check its details. The private IP address is assigned an internal DNS name: The internal (private) DNS name ensures that other servers within the VPC can communicate with this instance. However, since the ""Enable DNS Hostnames"" is disabled, the public IPv4 address does not have an associated DNS name. Enabling DNS Hostnames To assign a DNS name to the public IP address, follow these steps: Return to ""Actions"" > ""Edit VPC settings."" Enable the ""DNS Hostnames"" option. Click Save. After refreshing the instance details, you will observe that the public IP now has an associated DNS name. This simplifies accessing the instance via its domain name rather than its IP address. To verify, copy the public DNS name and run a ping command. The response should resolve to the public IP (for example, 35.173.226.213). Testing DNS Resolution from the Instance To verify the DNS resolution setting, connect to your EC2 instance using SSH: ssh -i /path/to/your-key.pem ec2-user@<public-ip-address> Once connected, view the DNS configuration by displaying the contents of the resolv.conf file: cat /etc/resolv.conf You should see an output similar to: nameserver 10.0.0.2
search ec2.internal This output confirms that the AWS DNS server at the second IP in the VPC subnet (10.0.0.2 for a CIDR block of 10.0.0.0/16) is active. Next, test external domain resolution using NS lookup: nslookup google.com Expected output: Server:         10.0.0.2
Address:        10.0.0.2#53

Non-authoritative answer:
Name:   google.com
Address: 142.251.163.100
Name:   google.com
Address: 142.251.163.101
Name:   google.com
Address: 142.251.163.102
Name:   google.com
Address: 142.251.163.113
Name:   google.com
Address: 142.251.163.138
Name:   google.com
Address: 142.251.163.139
Name:   google.com
Address: 2607:f8b0:4004:c08::71
Name:   google.com
Address: 2607:f8b0:4004:c08::64
Name:   google.com
Address: 2607:f8b0:4004:c08::65
Name:   google.com
Address: 2607:f8b0:4004:c08::66 This confirms that external domains are successfully resolved due to the enabled DNS resolution setting in the VPC. Tip For improved troubleshooting, verify your security group settings to ensure that ICMP traffic is permitted, as this is essential for successful ping tests. Disabling DNS Resolution If you choose to disable the DNS resolution option in the VPC settings, AWS will no longer answer DNS queries from your EC2 instances. To test this behavior: Disable the DNS resolution setting in the VPC configuration. Connect to your instance via SSH. Run an NS lookup for a domain such as: nslookup youtube.com Since the instance continues to direct DNS queries to the AWS DNS server at 10.0.0.2 (which is no longer configured to respond), the lookup will fail. In this scenario, you must specify an alternative DNS server (for example, Google's DNS at 8.8.8.8) or use an internally managed DNS server accessible to the instance. Warning Disabling DNS resolution can disrupt connectivity for your applications and services. Ensure you have alternative DNS servers configured to avoid outages. Final Thoughts This demonstration clarifies how modifying DNS settings within a VPC affects internal and external name resolution for your EC2 instances. This knowledge is especially valuable for those preparing for the AWS Solutions Architect Associate Certification exam and for understanding DNS behavior in AWS environments. For more resources on AWS and DNS configuration best practices, consider exploring: AWS Documentation Kubernetes Basics Docker Hub Happy cloud computing! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Internet Gateways VPC Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Internet-Gateways-VPC-Demo,"AWS Solutions Architect Associate Certification Services Networking Internet Gateways VPC Demo In this guide, we demonstrate how to create a public subnet in AWS. By starting from scratch—with the creation of a VPC and its subnet, followed by attaching an Internet Gateway—you’ll learn how to enable public connectivity for your EC2 instances. Step 1: Create a VPC and a Public Subnet Begin by navigating to the VPC dashboard in the AWS Console: Create a new VPC with a CIDR block of 10.0.0.0/16. You can optionally assign an IPv6 CIDR block if needed. Add a subnet to your VPC using the CIDR block 10.0.1.0/24 and give it a descriptive name such as ""public subnet"". There is no need to specify an availability zone unless you have a preference. With these steps, your basic VPC configuration is ready. Step 2: Launch an EC2 Instance for Connectivity Testing Next, switch to the EC2 dashboard and launch an EC2 instance using the following guidelines: Select an Amazon Linux AMI. Choose the T2 Micro instance type (eligible for the free tier). Use your pre-existing key pair (for example, aws-demo.pem) to allow secure SSH access. Under network settings, select the VPC you just created along with the corresponding public subnet. Ensure that the auto-assign public IP option is enabled so that the instance receives a public IP address. A security group is automatically created to allow SSH access (port 22). You can also add additional rules (such as for ICMP) if required. After launching the instance, allow a few moments for it to initialize. Verify that the instance is running and note its public IP address. Open your terminal and test connectivity using the public IP. For example, execute: ping 54.159.89.36 You can also attempt an SSH connection: ssh -i aws-demo.pem [email protected] Expected Behavior If the ping or SSH commands hang or time out, it is because the subnet is still private by default—even though the instance has a public IP assigned. Step 3: Attach an Internet Gateway To provide Internet access to your subnet, follow these steps to attach an Internet Gateway (IGW): In the VPC dashboard, select ""Internet Gateway"" and click to create a new Internet Gateway. Give it a meaningful name, such as ""my Internet Gateway"". Once created, attach the Internet Gateway to your VPC by selecting your VPC from the available list. After attaching the internet gateway, re-test connectivity with the ping command. If the instance is still unreachable, the subnet’s route table likely requires an update. Step 4: Update the Route Table To route Internet-bound traffic, update the route table associated with your public subnet: In the VPC console, locate the route table associated with your public subnet. If your subnet still uses the default route table, consider creating a custom route table for clarity. Create a new route table for your demo VPC and name it (for example, ""public route table""). Associate your public subnet with this new route table. Edit the route table to add a default route with a destination of 0.0.0.0/0 and set the target to your Internet Gateway. Save the changes. After these modifications, your subnet is configured as public, and all traffic destined for the Internet will be directed through the attached Internet Gateway. Step 5: Verify Internet Connectivity Once the route table is updated, verify that your EC2 instance is publicly accessible: Ping the public IP address again: ping 54.159.89.36 Attempt an SSH connection: ssh -i aws-demo.pem [email protected] A successful response (e.g., a ping reply with “Reply from 54.159.89.36: bytes=32 time=27ms TTL=112”) confirms that your instance is now accessible over the Internet. An example output might look like: Ping statistics for 54.159.89.36:
    Packets: Sent = 4, Received = 4, Lost = 0 (0% loss) Success Confirmation The successful ping and SSH connection indicate that attaching the Internet Gateway and updating the route table have effectively transformed your private subnet into a public subnet. At this point, any resources deployed in the configured subnet are publicly accessible over the Internet, provided that the necessary security group and network ACL settings permit the desired traffic. For more details on setting up secure and scalable AWS architectures, consider reviewing the following resources: AWS VPC Documentation Amazon EC2 User Guide Watch Video Watch video content"
AWS Solutions Architect Associate Certification,NAT Gateways VPC,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/NAT-Gateways-VPC,"AWS Solutions Architect Associate Certification Services Networking NAT Gateways VPC In this article, we explore how NAT Gateways enable secure communication between private subnets and the internet. NAT Gateways allow instances within private subnets to initiate outbound connections while blocking unsolicited inbound traffic, thereby keeping internal servers shielded from external threats. Consider a scenario where a server hosted in a private subnet requires internet access to download updates and security patches. One might think of attaching an Internet Gateway to the VPC and creating a route for the subnet. However, this would expose the server to the internet, making it public and vulnerable. Instead, the solution is to configure a NAT Gateway so that the server can access the internet for outbound requests without accepting inbound connections initiated from the internet. How NAT Gateways Work NAT Gateways offer a secure channel for outbound internet requests from private subnets while preventing unsolicited inbound traffic. Essentially, the NAT Gateway allows instances to initiate connections but blocks external entities from starting a conversation with them. To set up a NAT Gateway, you need the following components: Internet Gateway: Attach an Internet Gateway to your VPC. Public Subnet: Create a public subnet with a default route pointing to the Internet Gateway. NAT Gateway Deployment: Deploy the NAT Gateway in the public subnet so that it functions as a mediator for outbound internet requests. After deploying the NAT Gateway, update the route table for your private subnet with a default route that points to the NAT Gateway. This ensures that when the private server sends a request, the packets are directed from the private subnet to the NAT Gateway. The NAT then forwards these packets to the Internet Gateway, facilitating internet communication while keeping the private server off the public internet. The following diagram illustrates a typical NAT Gateway setup within a VPC, including public and private subnets, route tables, and internet connectivity: NAT Gateway Configuration and Operation Key aspects of NAT Gateway configurations include: Dependency on an Internet Gateway: Even though a NAT Gateway is used, an Internet Gateway is necessary for external connectivity. Deployment on Public Subnets: NAT Gateways are deployed within public subnets and thus receive a public IP address. This setup is essential for routing outbound traffic to the internet. Routing Setup: Ensure that the private subnet’s route table directs all outbound traffic to the NAT Gateway. Fully Managed Service: Being a managed service by AWS, NAT Gateways require minimal ongoing management, with AWS handling maintenance and scaling. Billing Considerations: NAT Gateway usage is billed based on hourly usage and the volume of data processed. High Availability Note: For maintaining high availability, it is recommended to deploy NAT Gateways across multiple Availability Zones. Since NAT Gateways are not region-resilient and are tied to specific Availability Zones, using multiple gateways mitigates the risk of service disruption in case of an outage in any single zone. The diagram below demonstrates the deployment of one NAT Gateway per Availability Zone, illustrating the redundancy required for high availability: Summary of NAT Gateway Features NAT Gateways provide a secure method for routing outbound internet traffic from private subnets without exposing your instances to inbound traffic. Below is a concise summary of their features: Deployed on public subnets and require an Internet Gateway for internet communication. Utilize Elastic IPs (details of which are covered in further documentation). Must be deployed individually for each Availability Zone to ensure redundancy. Private subnet route tables must direct outbound traffic to the NAT Gateway. Operate as fully managed services provided by AWS, with billing based on uptime and data processing volume. Designed to automatically handle bursts of high traffic, typically supporting up to 10 Gbps throughput. The following diagram offers a summary of NAT Gateway features, highlighting their deployment characteristics and operational capabilities: Further, the NAT Gateway’s routing configuration and AWS management model ensure transparent operation once deployed. This allows private instances to access the internet securely while remaining safeguarded against external unsolicited connections. Conclusion NAT Gateways in a VPC provide an essential solution for securely managing outbound internet access from private subnets. By ensuring that internal services can communicate externally without exposure to direct inbound internet traffic, NAT Gateways help maintain the security and integrity of your network architecture. Whether you're scaling up for high throughput or ensuring availability across multiple Availability Zones, NAT Gateways offer a robust and managed solution for modern cloud environments. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Security Groups Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Security-Groups-Demo,"AWS Solutions Architect Associate Certification Services Networking Security Groups Demo In this lesson, you'll learn how to leverage AWS security groups and network ACLs (NACLs) to control traffic to and from your AWS resources. We begin by launching an Amazon Elastic Compute Cloud (EC2) instance and applying security groups to efficiently manage access. This method applies equally to other resources such as load balancers and databases, as they use similar configuration processes. Launching an EC2 Instance with a Security Group First, create an EC2 instance using the default Linux image. For demonstration, name the instance ""server one"" and launch it within an existing Virtual Private Cloud (VPC). During setup, you will have the option to configure a security group. If you do not select an existing security group, AWS will automatically create one for you. By default, this security group contains an inbound rule that permits SSH (TCP port 22) access from any IP address (0.0.0.0/0). The default rule uses the TCP protocol, essential for SSH connections. However, you can customize this rule to use a different protocol or port, such as HTTP on port 80. For this demo, we keep the default SSH configuration. Once the instance is launched, navigate to it in the EC2 console. Although the status may initially show as ""initializing,"" it will quickly change to ""running."" To verify the security group settings, select the instance and check the ""Security"" tab. You will see an inbound rule allowing SSH (port 22) and an outbound rule permitting all traffic. To test connectivity, use the following SSH command: ssh -i main.pem [email protected] If port 22 is open in the security group, the connection will be successful. Otherwise, the connection attempt will fail. Modifying Security Group Rules Now, let's modify the security group rules to restrict access. Open the security group page in a new tab and click ""Edit inbound rules."" Remove the existing SSH inbound rule so that the instance has no inbound rules, effectively blocking all traffic. When you attempt to SSH into the instance again, the connection will hang because no inbound rules permit access: C:\Users\sanje\Documents\scratch\aws-demo>ssh -i main.pem [email protected] - launch-wizard-20
The authenticity of host '3.82.5.183 (3.82.5.183)' can't be established.
ECDSA key fingerprint is SHA256:8kdgYdu32+mHW5/8xQ49708P4ChRwRBn+oSeBmM.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '3.82.5.183' (ECDSA) to the list of known hosts.
Amazon Linux 2023 And similarly: C:\Users\sanje\Documents\scratch\aws-demo>ssh -i main.pem [email protected] The authenticity of host '3.82.5.183 (3.82.5.183)' can't be established.
ECDSA key fingerprint is SHA256:BDxdgWyu32+mHHW5/8xQ409708P4CHRwRBn+oSeBmM.
Are you sure you want to continue connecting (yes/no/[fingerprint])? yes
Warning: Permanently added '3.82.5.183' (ECDSA) to the list of known hosts.
             #
          ~  #####
        ~~~~~ #####
       ~~~~~~  ###|#
      ~~~~~~~~ #/__
   ~~~~~~~~~~~ V\'-._
  ~~~~~~~~~~~~  _.-' 
                /m/
[ec2-user@ip-10-1-1-82 ~]$ client_loop: send disconnect: Connection reset
C:\Users\sanje\Documents\scratch\aws-demo>ssh -i main.pem [email protected] Note Since there are no inbound rules, remote connections cannot be established. Stop this test connection to prevent unintended lockouts. Creating and Applying a New Security Group To restore connectivity, create a new security group for the server. Name it ""web server security group"" (or a similar descriptive name such as ""web applications"") and ensure you select the correct VPC where the server exists. Define an inbound rule that allows SSH traffic (port 22) from any IP address (0.0.0.0/0), keeping the outbound rule to allow all traffic. This newly created security group mirrors the initial default rule but is manually set up. After creation, reattach it to your EC2 instance (""server one""): In the EC2 console, select the instance. Click on ""Actions"" > ""Security"" > ""Change Security Groups."" Remove the old security group and add the new ""web server security group."" Click ""Add"" and then ""Save."" After attaching the correct security group, you should be able to SSH into the instance as it now permits SSH traffic per the new rules. Installing and Testing a Web Server (nginx) With SSH access restored, install the nginx web server on your EC2 instance by running: [ec2-user@ip-10-1-1-82 ~]$ sudo yum install nginx Next, start the nginx service with: [ec2-user@ip-10-1-1-82 ~]$ sudo systemctl start nginx To verify that nginx is running correctly, execute a local cURL command: [ec2-user@ip-10-1-1-82 ~]$ curl localhost
<!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p>
<p>For online documentation and support please refer to <a href=""http://nginx.org"">nginx.org</a>.<br/> Commercial support is available at <a href=""http://nginx.com"">nginx.com</a>.</p>
<p><em>Thank you for using nginx.</em></p>
</body>
</html> Once confirmed, open a browser and navigate to the instance's public IP address. If the web page fails to load and hangs, it is likely due to the security group allowing only SSH traffic. Warning Ensure that you update the ""web server security group"" to allow HTTP (port 80) and HTTPS (port 443) traffic for full web server functionality. To add these rules: Open the security group in the EC2 console. Click ""Edit inbound rules."" Add an HTTP rule (automatically sets protocol to TCP and port to 80) and an HTTPS rule (TCP port 443). Set the source to 0.0.0.0/0 for both rules and save your changes. Now, reloading the public IP in your browser should display the nginx welcome page, confirming that the web server is accessible. Outbound Rules and Stateful Behavior Security groups in AWS are stateful. This means that even if the default outbound rule allows all traffic, AWS automatically permits return traffic for any inbound connection. For example, if an inbound request is received on port 80, the outbound response is allowed even in the absence of an explicit outbound rule. You can test outbound connectivity by pinging an external server: [ec2-user@ip-10-1-1-82 ~]$ ping 8.8.8.8
PING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.
64 bytes from 8.8.8.8: icmp_seq=1 ttl=53 time=1.58 ms
64 bytes from 8.8.8.8: icmp_seq=2 ttl=53 time=1.61 ms
64 bytes from 8.8.8.8: icmp_seq=3 ttl=53 time=1.91 ms
64 bytes from 8.8.8.8: icmp_seq=4 ttl=53 time=1.62 ms
64 bytes from 8.8.8.8: icmp_seq=5 ttl=53 time=1.65 ms
64 bytes from 8.8.8.8: icmp_seq=6 ttl=53 time=1.57 ms
--- 8.8.8.8 ping statistics ---
7 packets transmitted, 7 received, 0% packet loss, time 601ms
rtt min/avg/max/mdev = 1.572/1.651/1.909/0.107 ms If outbound traffic is blocked, the ping test will fail. Reinstate the outbound rule allowing all traffic to enable responses from your EC2 instance. Combining Multiple Security Groups AWS allows you to attach multiple security groups to a single EC2 instance, providing a modular approach to access control. For demonstration, create the following two security groups: Allow SSH Security Group – contains an inbound rule for SSH (port 22) from any IP. Allow HTTP Security Group – contains an inbound rule for HTTP (port 80) from any IP. After creating these security groups, update the instance (""server one"") by following these steps: In the EC2 console, select your instance. Go to ""Actions"" > ""Security"" > ""Change Security Groups."" Remove the previous security group. Attach both the SSH and HTTP security groups. Save your changes. The rules of both security groups will merge, allowing both SSH and HTTP traffic. You can verify the updated configuration by checking the instance details in the EC2 instances list: This modular approach simplifies management across instances. You can apply the same security groups to another instance (e.g., server two) if similar access requirements exist. Creating a Database Security Group Lastly, create a security group specifically for a database. Name it ""database security group"" and add an inbound rule for your database port (for example, port 5432 for PostgreSQL). Although you could set the source to 0.0.0.0/0 for universal access, it is best practice in production to restrict access to only your web servers. One advanced option is to use the security group of your web servers (e.g., the ""Allow HTTP Security Group"") as the source. This ensures that the database only accepts connections from trusted sources, and any changes to web server configurations automatically update the access rules. Conclusion In this lesson, you learned how to: Launch an EC2 instance with a default security group. Modify security group rules to restrict access. Create and attach a new security group that enables SSH, HTTP, and HTTPS access. Combine multiple security groups to modularize and simplify access control. Create a dedicated database security group with restricted access linked to another security group. Next, we will explore filtering traffic at the subnet level using NACLs. Happy cloud computing! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Security Groups amp NACLs,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Security-Groups-amp-NACLs,"AWS Solutions Architect Associate Certification Services Networking Security Groups amp NACLs In this lesson, we explore AWS firewalls by understanding security groups and network access control lists (NACLs), key components used to control network traffic. Learn the difference between stateful and stateless firewalls, and discover how AWS implements these solutions through security groups and NACLs. Overview of Firewalls Consider a web server listening on port 443 (HTTPS). When a client sends a request, the firewall controls the flow of data by applying a set of predefined inbound (incoming traffic) and outbound (outgoing traffic) rules. There are two primary types of firewalls: Stateless Firewalls: Evaluate each packet individually. Rules must be configured for both inbound and outbound traffic as there is no tracking of connection states. Stateful Firewalls: Monitor active connections. When an inbound request is allowed, the corresponding outbound response is automatically permitted without the need for an explicit rule. Configuring a Stateless Firewall For a stateless firewall, imagine your web server that listens on port 443. You must configure rules separately for both directions: Inbound Rules: Allow incoming traffic on port 443. Outbound Rules: Allow outbound traffic to the ephemeral port range (typically 1024 to 65535) since client source ports fall within this range. Additionally, if the server initiates a connection (for example, pulling updates from a server via port 80), you need to allow outbound traffic on port 80 and specify corresponding inbound rules to accept responses on the ephemeral port range. Key Concept Stateless firewalls treat inbound and outbound traffic separately, requiring manual configuration for both sides. Configuring a Stateful Firewall Using the same web server example for a stateful firewall, the configuration simplifies due to the system’s ability to track connections: Inbound Rules: Permit incoming traffic on port 443. Outbound Rules: No explicit outbound rule is required for returning response traffic as the firewall automatically recognizes established connections. For additional outbound connections (e.g., connecting to port 80 on a different server), only the rule to allow the outbound request is required. Stateful Advantage Stateful firewalls eliminate the need for duplicate rules by automatically permitting response traffic once a valid connection is established. AWS Network Access Control Lists (NACLs) AWS NACLs provide subnet-level filtering in a Virtual Private Cloud (VPC): Subnet-Level Filtering: NACLs are associated with subnets rather than individual resources. Resources within a subnet are subject to the NACL’s rules, while traffic between resources in the same subnet is not filtered. Stateless Nature: Being stateless, NACLs require explicit rules for both inbound and outbound traffic. AWS Security Groups Security groups act as virtual firewalls for individual AWS resources, such as EC2 instances, load balancers, and RDS databases. They differ from NACLs in the following ways: Resource-Level Protection: Security groups protect individual resources rather than an entire subnet. Stateful Behavior: Only the initial inbound traffic (or outbound traffic) needs to be explicitly allowed since the return traffic is automatically permitted. Allow Rules Only: Security groups only contain rules that allow traffic. There is no provision to create explicit deny rules; any traffic not expressly permitted is blocked. Configuring Security Group Rules When configuring security group rules in the AWS console, you will encounter separate sections for inbound and outbound traffic. Here’s a breakdown of the typical fields within a security group rule: Name: A descriptive name that helps identify the rule. Rule ID: A unique identifier for the rule. IP Version: Indicates whether the rule applies to IPv4 or IPv6 traffic. Type, Protocol, and Port Range: For example, selecting ""HTTP"" sets the rule to allow TCP traffic on port 80. Source/Destination: Specifies the IP address or range permitted by the rule. For instance, 0.0.0.0/0 allows traffic from any IP, while specifying an IP like 1.1.1.1 restricts access. Description: Provides an explanation of the rule’s purpose. Consider these examples: HTTP Traffic Rule: Permits HTTP (TCP port 80) traffic from any IP address. Custom TCP Rule: Configured as a ""custom TCP"" rule, it allows traffic on port 200 only from IP address 1.1.1.1. If a range (e.g., ports 200 through 300) is required, the rule can be configured accordingly. For outbound traffic, the configuration is similar. By default, security groups permit all outbound traffic, though customization is possible. Security Group Reminder If no rules are defined, all traffic is blocked by default. Also, when multiple security groups are attached to a resource, their rules merge to form a consolidated security policy. AWS Network ACL Rules Unlike security groups, NACL rules offer greater control by allowing both allow and deny actions: Rule Order: Each rule is assigned a number, and lower numbered rules are evaluated before higher ones. Allow and Deny Actions: NACLs can explicitly allow or block traffic based on type, protocol, port range, and source address, offering granular control over subnet traffic. Multiple Security Groups AWS resources can be associated with multiple security groups simultaneously. When applying more than one security group (e.g., one for web access and another for management), their rules are merged. The final effective security policy is the combination of all allowed traffic flows from these groups. Final Notes on VPC and NACLs Every subnet in a Virtual Private Cloud (VPC) must be associated with a network ACL. A single network ACL can be linked to multiple subnets, but each subnet can only have one network ACL at a time. Certain traffic types (such as communication with Amazon's DNS, DHCP, EC2 instance metadata, Windows license activation, and time synchronization services) are exempt from NACL filtering. Summary Stateless Firewalls: Require explicit configuration of both inbound and outbound traffic rules. Stateful Firewalls: Automatically allow return traffic for approved connections, simplifying rule management. Network ACLs: Function as stateless firewalls for subnets, allowing detailed allow/deny rules and processing rules in numeric order. Security Groups: Provide stateful protection at the resource level, merging multiple groups' rules to secure AWS resources like EC2 instances. By understanding these key differences and implementations in AWS, you can design a more secure and efficient network architecture in your AWS environment. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Elastic IP,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Elastic-IP,"AWS Solutions Architect Associate Certification Services Networking Elastic IP In this article, we explore AWS Elastic IPs and their critical role in managing EC2 instances. Understanding the differences between dynamic public IPs and static Elastic IPs can help maintain a reliable connection for your applications. When an EC2 instance is deployed in a public subnet, it automatically receives a public IP address (for example, 1.1.1.1). However, this public IP is temporary; AWS does not assign permanent ownership to you. Consequently, if the instance is rebooted, it might receive a different public IP—resulting in potential disruption for applications relying on a consistent backend address. Static vs Dynamic IPs A static Elastic IP remains constant, ensuring that even if your EC2 instance is rebooted or relocated to a different physical server, the IP address stays the same. What Are Elastic IPs? Elastic IP addresses are static IPv4 addresses allocated to your AWS account. Once allocated, the address is reserved exclusively for your use and cannot be allocated to others. You can then associate an Elastic IP with an EC2 instance so that it keeps the same IP address, regardless of reboots or host migration. Key Benefits Consistent Connectivity: End users always connect to the same IP, improving reliability. Flexibility in Maintenance: You can easily reassign the Elastic IP from one server to another during scheduled maintenance or unexpected server issues. The diagram below shows how an Elastic IP (1.1.1.1) is reassigned from Server A to Server B, ensuring uninterrupted service: Pricing Structure Elastic IPs assigned to a running instance incur no additional cost. However, additional considerations include: Associating extra Elastic IPs with an instance. Reserving an Elastic IP without associating it. Both scenarios will result in a small hourly fee. The following diagram outlines the pricing model, showing that additional or unused reserved Elastic IPs are billed per hour: Cost Management Ensure you manage and monitor your Elastic IP usage to avoid unexpected charges, especially for unused or extra reserved IPs. Regional Specificity and IP Origins Elastic IPs are tied to specific AWS regions and cannot be transferred across regions. They can be associated only with EC2 instances within the same region. Additionally, these IP addresses are sourced either from AWS’s pool of IPv4 addresses or from a custom pool you bring to your account. The infographic below reinforces these points: How to Use an Elastic IP To take advantage of Elastic IPs: Allocate an Elastic IP to your AWS account. Associate the allocated IP with your EC2 instance or network interface. In summary, while public IP addresses assigned to EC2 instances are dynamic and can change, Elastic IPs offer a static solution that ensures a consistent point of entry for your applications. Remember, Elastic IPs are region-specific and must be allocated and associated within the same region. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Load Balancers Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Load-Balancers-Demo,"AWS Solutions Architect Associate Certification Services Networking Load Balancers Demo In this article, we walk through the process of configuring an AWS Application Load Balancer to distribute network requests across multiple EC2 instances. The demo uses two pre-provisioned web servers located in different Availability Zones (US East 1A and US East 1B) and leverages an existing VPC with an Internet Gateway and proper route configurations. Each web server is running an Nginx web server that displays a simple page with an H1 tag identifying which server processed the request. For example, when you enter the IP address of web server one in your browser (using ""http://""), you should see a page indicating that the request was handled by server one: Likewise, accessing web server two directly shows: And when viewing server two details: Additionally, a quick look at the VPC reveals that each instance resides in a different public subnet: Web server one is in subnet 10.0.201.0/24 (US East 1A). Web server two is in subnet 10.0.202.0/24 (US East 1B). This separation encourages redundancy, so if one zone experiences issues, the other continues serving requests. Load Balancer Overview The main goal here is to configure a load balancer that provides a unified DNS name, distributing incoming traffic evenly between both web servers. Creating Dedicated Subnets for the Load Balancer Before configuring the load balancer, you need to create dedicated subnets for its nodes. Since the load balancer will operate in both US East 1A and US East 1B, create one subnet in each Availability Zone: Create a subnet named ""LB"" in US East 1A with the CIDR block 10.0.101.0/24. Create another subnet named ""LB"" in US East 1B with the CIDR block 10.0.102.0/24. After completing these steps, your VPC includes four subnets: two for the web servers and two dedicated LB subnets. Configuring the Load Balancer For a website, the load balancer must be publicly accessible. Since the LB subnets are configured as public (with default routes to the Internet Gateway), navigate to the EC2 console and access the Load Balancers section: Click on ""Create Load Balancer"" and choose the ""Application Load Balancer"" option. Enter the following details: Name: ""web load balancer"" Scheme: ""internet-facing"" IP address type: IPv4 (or dual stack if IPv6 is also required) VPC: Select the VPC (e.g., ""demo"") For the Availability Zones, select the dedicated LB subnets (one in US East 1A and one in US East 1B) instead of the web server subnets. Configure a preexisting security group that allows web traffic on ports 80 and 443. Security Group Reminder Ensure that your security group permits HTTP (port 80) and HTTPS (port 443) traffic so that the load balancer can communicate with end users effectively. Setting Up Listeners and Target Groups The listener configuration instructs the load balancer on which port to monitor. For this demo, port 80 (HTTP) is used. Next, create a target group that includes the two EC2 web server instances: Target type: Instances Name: ""web"" Protocol: HTTP Port: 80 Optionally configure health checks (using the default root path “/” or a custom path if required) to monitor the health of your instances. Register both web server instances (web server one and web server two) to this target group. Once your target group is configured, return to the Application Load Balancer setup to assign this group to the listener on port 80. Although additional listeners (e.g., HTTPS) could be set up, this example remains focused on HTTP traffic. Finalize the configuration and create the load balancer. The provisioning process may take a few minutes. Testing the Load Balancer Once your load balancer is active, select it in the console to view its details and locate its DNS name. This DNS name is the point of contact for end users. Open a new browser tab, paste the DNS name, and hit refresh multiple times; you should see the following behavior: One refresh may display ""this is server one."" Another refresh may show ""this is server two."" This confirms that traffic is being distributed evenly between the two servers. Security Considerations In production environments, direct public access to backend EC2 instances is discouraged. Instead, restrict access to ensure all traffic flows through the load balancer. Consider the following best practices: Configure the web server security groups to only allow traffic from the load balancer. Deploy backend servers in private subnets, ensuring the public load balancer is the sole point of access. This strategy enhances overall security by limiting direct exposure of backend resources. Conclusion This guide demonstrated how to set up an AWS Application Load Balancer that efficiently distributes traffic across EC2 instances in different Availability Zones. By creating dedicated LB subnets, configuring listeners and target groups, and implementing best security practices, you can ensure a resilient and highly available web application architecture. Happy load balancing! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Direct Connect,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Direct-Connect,"AWS Solutions Architect Associate Certification Services Networking Direct Connect In this lesson, we explore Direct Connect—a service that provides a dedicated physical connection to AWS. By bypassing the public internet, Direct Connect delivers a high-speed, stable, and reliable connection, eliminating the latency and potential instability of VPN and IPSec tunnels. What is Direct Connect? Direct Connect enables a direct connection between your on-premise network and AWS through a dedicated link. This connection is available in various speeds, such as 1 Gbps, 10 Gbps, or even 100 Gbps, making it ideal for organizations that require high throughput, low latency, and enhanced reliability. How Direct Connect Works Direct Connect's architecture involves three primary components: Your On-Premise Network: This can be a data center, corporate office, or any facility with a dedicated edge router or firewall. The router used here may serve both your internet connection and Direct Connect, or it can be exclusively dedicated to Direct Connect. Direct Connect Location: Typically a regional data center where AWS installs its routers. This is not a full AWS data center but rather a location where the physical connection is established. AWS's Direct Connect Router: This router is located on the AWS side of the connection. When you purchase Direct Connect, you lease a port on the AWS router. Your on-premise router then forms a ""cross connect"" with AWS’s router at the Direct Connect location. Below is a diagram illustrating the overall Direct Connect architecture: The connection flow is as follows: Your on-premise network connects to your customer gateway. The customer gateway interfaces with the customer router at the Direct Connect location. A cross connect at the Direct Connect location links the customer router to the AWS Direct Connect router. From the AWS router, traffic is routed directly to AWS, allowing access to private resources (via a VPN Gateway) or public services such as Amazon S3. Note If you need to connect to private VPC resources, configure a VPN Gateway and establish a connection between the cross connect and the VPN Gateway. For public services, traffic is routed directly through the AWS router, leveraging the AWS backbone network for improved performance. The following diagram offers additional insight into the connection from your on-premise network to AWS: Direct Connect Pricing Direct Connect pricing is determined by two main factors: Port Hours: You are charged based on the number of hours the port is active. Outbound Data Transfer: Charges apply for the data transferred out of AWS. This pricing model ensures that you only pay for the actual resources you use while enjoying a dedicated, high-speed, secure connection to AWS. The diagram below summarizes the pricing structure for Direct Connect: Summary Direct Connect provides a robust and reliable alternative to traditional VPN solutions by offering a dedicated physical connection from your on-premise network to AWS. This results in: Greater throughput and improved performance Enhanced security by bypassing the public internet Reduced latency and increased reliability With a straightforward pricing model based on port hours and outbound data transfer, Direct Connect is an excellent choice for organizations that demand a high-performance, secure network connection to AWS. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,VPN,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/VPN,"AWS Solutions Architect Associate Certification Services Networking VPN In this article, we explore Virtual Private Networks (VPNs) and their integration with AWS, including their purpose, deployment, routing methods, pricing, and performance limits. You’ll also find architectural diagrams that clarify key components of VPN setups in AWS. VPN Purpose and Use Case Imagine you have an AWS Virtual Private Cloud (VPC) containing private subnets that host various resources without public IP addresses. These resources require secure connectivity to an on-premises data center. A VPN provides this secure link by establishing an encrypted IPsec tunnel between the two environments, ensuring that communication remains private and protected. VPN Architecture in AWS Consider a VPC with a CIDR block of 10.0.0.0/16 where your resources are hosted in private subnets. To connect to an on-premises data center (for example, using 192.168.0.0/16), two critical components are involved: VPN Gateway (VGW): Located on the AWS side, it terminates the VPN connection. Customer Gateway (CGW): Located on your on-premises network, it terminates the VPN connection on the customer side and possesses a public IP address. For instance, if the Customer Gateway is assigned the public IP 1.1.1.1 and the VPN Gateway uses 2.2.2.2, an IPsec tunnel is established over the internet between these endpoints. In this setup, the Customer Gateway represents your on-premises side, while the VPN Gateway is deployed on the AWS side. The encrypted IPsec tunnel ensures that all data transmitted across the public internet remains secure. Routing Between AWS and On-Premises Networks To facilitate communication between the on-premises network (192.168.0.0/16) and the AWS VPC (10.0.0.0/16), you must configure routing appropriately. Packets destined for the on-premises network should be directed through the VPN Gateway. There are two routing approaches: Static Routing: Manually add a route in the VPC routing table that directs traffic for 192.168.0.0/16 to the VPN Gateway. Dynamic Routing: Use a routing protocol like Border Gateway Protocol (BGP) to automatically exchange routes between the VPN Gateway and the Customer Gateway. This dynamic method allows AWS to learn the on-premises routes automatically. Note Using dynamic routing with BGP simplifies route management and provides improved resiliency by automatically adapting to route changes. VPN Pricing AWS charges for VPN gateways in two main ways: A fee for each hour that the VPN connection is available. Additional charges for data transfer out (egress traffic) from Amazon EC2 over the VPN. Warning Monitor your outbound data transfer closely to manage costs, as VPN egress charges can accumulate quickly. VPN Performance Limits When deploying VPN gateways, consider these performance limits: Bandwidth: Up to 1.25 Gbps per VPN tunnel. Packets Per Second: Capable of handling up to 140,000 packets per second. MTU (Maximum Transmission Unit): Limited to 1,466 bytes. If a single tunnel does not satisfy performance requirements, utilize Equal-Cost Multi-Path (ECMP) routing by establishing additional VPN tunnels to distribute the traffic load. Summary To summarize: VPNs provide secure connectivity between AWS VPCs and on-premises data centers. The VPN Gateway (AWS side) and Customer Gateway (on-premises side) serve as the endpoints for the encrypted IPsec tunnel. Routing can be managed either statically or dynamically using BGP to ensure proper packet flow. AWS charges for VPN usage based on connection uptime and data egress, and VPN tunnels have defined performance limits. For further reading, consider exploring the AWS VPN Documentation and general VPN Concepts . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,VPC Peering,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/VPC-Peering,"AWS Solutions Architect Associate Certification Services Networking VPC Peering VPC peering is an essential component in cloud networking, enabling direct communication between Virtual Private Clouds (VPCs) by connecting them through a private network. In this lesson, we will explore how VPC peering works, its benefits, and considerations for configuration. For additional context, refer to the AWS Cloud Practitioner (CLF-C02) exam materials. Overview of VPC Isolation and Peering By default, resources hosted in different VPCs are isolated because each VPC creates its own network boundary. This isolation prevents instances in separate VPCs from communicating with each other unless explicitly configured. VPC peering addresses this challenge by creating a direct network connection between two VPCs, allowing instances in each VPC to interact as if they were on the same network. VPC peering supports connections under various scenarios: Between VPCs in the same region. Across different regions. Between VPCs in different AWS accounts. How VPC Peering Works Consider an example with two VPCs: VPC One : CIDR block 10.1.0.0/16 VPC Two : CIDR block 10.2.0.0/16 One VPC initiates a peering request to the other. If the VPCs belong to different AWS accounts, the owner of the target VPC must accept the request. In the same account, the request is automatically approved. Once accepted, the peering connection is established. After establishing the connection, you need to manually update the route tables in both VPCs. For instance, in VPC One, add a route that directs traffic destined for the 10.2.0.0/16 CIDR block to the peering connection. Similarly, in VPC Two, create a route for the 10.1.0.0/16 CIDR block pointing to the same connection. Routing Note Remember to update the route tables in both VPCs after establishing the peering connection. Without these changes, instances will not be able to communicate. Key Considerations for VPC Peering One critical aspect of VPC peering is its non-transitive nature. For example, if you have three VPCs (VPC One, VPC Two, and VPC Three) and establish peering between VPC One & VPC Two and between VPC Two & VPC Three, VPC One will not automatically communicate with VPC Three. Each communication pair requires a dedicated peering connection. Non-Transitivity Warning VPC peering does not support transitive routing. Ensure that each pair of VPCs needing communication has its own peering connection. Pricing Considerations VPC peering connections themselves incur no additional charges. However, data transfer costs apply in specific scenarios: Data transferred within the same availability zone via a VPC peering connection is free. Data transferred between different availability zones is billed. Summary VPC peering offers a secure and efficient mechanism to connect two VPCs, allowing direct routing of traffic as if they are part of the same network. The key benefits include: Feature Description Direct Connectivity Enables private communication between VPC resources Cross-Region Support Works across regions and between AWS accounts Cost Efficiency No additional charge for peering connections; only data transfer is billed (if applicable) By understanding and implementing VPC peering, you can effectively design and manage your cloud network, ensuring secure and efficient resource communication across different VPCs. For more configuration details and best practices, refer to the AWS documentation and guidelines on VPC peering. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Privatelink,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Privatelink,"AWS Solutions Architect Associate Certification Services Networking Privatelink PrivateLink is a secure and efficient method that allows your Virtual Private Cloud (VPC) to connect directly to AWS services (like S3, CloudWatch, etc.) and even third-party services hosted in other VPCs using private IP addresses. This direct connectivity eliminates the need for routing traffic through the public Internet, reducing exposure and potential vulnerabilities. Why Use PrivateLink? Consider a scenario where an EC2 instance located in a private subnet needs access to an S3 bucket. Traditionally, you might attach an Internet Gateway or a NAT Gateway to provide the necessary connectivity. However, doing so grants the instance full Internet access, which increases its exposure to threats. PrivateLink addresses this by ensuring that the EC2 instance can communicate directly with the S3 bucket without any additional external exposure. Key Benefits Enhanced security through direct connectivity. Reduced risk by eliminating unnecessary Internet exposure. Simplified network architecture for AWS services. How PrivateLink Works PrivateLink uses VPC endpoints to facilitate seamless, private access to AWS services and third-party services hosted on other VPCs. With these endpoints in place, private links make external services appear as if they are part of your own VPC network. This approach not only improves security by minimizing external exposure but also streamlines connectivity, ensuring that only the required communications occur within your secured network boundaries. Practical Applications By integrating PrivateLink, you can: Use Case Benefit Example Scenario Access to AWS S3 Secure, direct connectivity without Internet access An EC2 instance in a private subnet accesses S3 Connection to Third-Party Services Maintain security while interacting with external services Directly connecting to a vendor's service hosted in another VPC This capability is particularly valuable when you need to restrict direct Internet exposure yet require internal communication across services. In Summary PrivateLink enhances your VPC's connectivity options by allowing secretive, internal access to AWS and external services. This setup simplifies your network architecture while maintaining robust security standards. For further reading on related topics, consider visiting the following links: AWS PrivateLink Documentation AWS VPC Endpoints By leveraging PrivateLink, your infrastructure benefits from a secure, optimized approach to accessing essential cloud services without compromising on safety or efficiency. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,LambdaEdge,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/LambdaEdge,"AWS Solutions Architect Associate Certification Services Networking LambdaEdge In this lesson, we explore two powerful services offered by AWS—CloudFront functions and Lambda@Edge—that enable you to run code at Amazon CloudFront edge locations. These services allow you to process requests and modify responses right at the edge, reducing the delay caused by routing traffic back to your origin servers. With these functions, developers can manipulate incoming requests and outgoing responses. For example, you could implement basic authentication, perform authorization checks, or even generate an entire response directly at the edge. This capability brings complex backend logic closer to users, thereby reducing latency and improving performance. When Do These Functions Run? It is crucial to understand that CloudFront functions and Lambda@Edge functions are designed for different scenarios. Here’s when each type executes: CloudFront Functions run: Immediately when a viewer sends a request to an edge location. Just before CloudFront returns a response to the viewer. Lambda@Edge Functions run: When CloudFront receives a request from a viewer. When CloudFront forwards a request to the origin, especially for content not cached at the edge. When a response is received from the origin, before delivering it back to the viewer. Use Cases and Choosing the Right Service CloudFront Functions CloudFront functions are perfect for lightweight, short-running tasks. Common use cases include: Cache Key Normalization: Transform HTTP request attributes to create an optimal cache key, which improves the cache hit ratio. Header Manipulation: Insert, modify, or delete HTTP headers in requests or responses. For instance, you might add a header with the true client IP. URL Redirects or Rewrites: Redirect viewers based on request details or rewrite URLs. This allows for dynamic request management. Request Authorization: Inspect authorization headers or metadata to validate tokens like JSON Web Tokens (JWT). Pro Tip For extremely short execution times (sub-millisecond), choose CloudFront functions to take full advantage of their speed and efficiency. Lambda@Edge Functions Lambda@Edge functions are ideal for scenarios that require extended execution times or additional capabilities, such as: Running operations that take several milliseconds or more. Handling tasks with adjustable CPU and memory configurations. Integrating with other AWS services using third-party libraries like the AWS SDK. Performing functions that depend on network access to external services. Accessing the request body or file system when needed. A detailed comparison chart below outlines the differences between CloudFront functions and Lambda@Edge. This chart covers programming languages, event triggers, scaling, function duration, and capabilities like network and file system access. Choosing the Right Tool For logic that requires network access, integration with external systems, or longer running times, Lambda@Edge is the recommended service. Summary Both Lambda@Edge and CloudFront functions allow you to run code as close to your users as possible at the edge. They provide flexible options for manipulating requests and responses in Amazon CloudFront: CloudFront Functions: Best for lightweight tasks such as header manipulation, URL redirects/rewrites, and authorization on sub-millisecond executions. Lambda@Edge Functions: Suitable for operations requiring extended execution times, network or file system access, and deeper integrations with other AWS services. Understanding the differences between these two services is essential for selecting the right approach for your specific use case. For more detailed information, explore the AWS Documentation . Happy building at the edge! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,CloudFront,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/CloudFront,"AWS Solutions Architect Associate Certification Services Networking CloudFront In this lesson, we explore Amazon CloudFront—a powerful Content Delivery Network (CDN) that ensures your users receive web content quickly and reliably. We also revisit key concepts from the AWS Cloud Practitioner (CLF-C02) course that cover global content delivery. When delivering content globally, your web server may be situated in one region (for example, North America). If a user in India sends a request, the data must travel a long distance, resulting in high latency. To mitigate this, AWS uses edge locations: smaller, geographically dispersed sites that cache data from your origin (such as a web server or an S3 bucket). Users receive content from the nearest edge location, ensuring minimal delay. For instance, if users in Australia access your application, they connect to a local edge location instead of the distant origin server. This proximity reduces the interaction time with your application, making it more responsive. Essentially, Amazon CloudFront improves performance by caching content closer to users. What is CloudFront? CloudFront is a web service that accelerates the distribution of both static and dynamic content—including HTML, CSS, JavaScript, images, videos, and music—by delivering it from the closest edge location. This global network of edge locations minimizes latency by serving cached copies of your content rather than always querying a centralized server. CloudFront Architecture CloudFront's architecture is built on several core components that work together to deliver content efficiently. The following sections explain each component in detail. Origin The origin is the source of the content that CloudFront caches. This can be an S3 bucket storing images and files, or a custom origin like a load balancer or an HTTP server running on an EC2 instance. Once CloudFront fetches the content from the origin, it caches the data at nearby edge locations. Distribution A distribution in CloudFront is a configuration block where you define the origin settings. CloudFront generates a unique domain name (e.g., xyz.cloudfront.net) that users leverage to access cached content. Upon receiving a request, CloudFront checks for a cached version at the closest edge location. If available, it serves the file immediately; otherwise, it retrieves the content from the origin, caches it, and then delivers it to the user. Time to Live (TTL) The Time to Live (TTL) defines how long content remains cached at an edge location. By default, the TTL is set to 24 hours, meaning content is served from the cache for that duration before it is considered stale. If a user requests the content after the TTL expires, CloudFront must fetch a fresh copy from the origin and update its cache. You have the flexibility to modify the TTL based on your content freshness requirements. Cache Invalidation Sometimes you need to update content before the TTL expires. CloudFront enables manual cache invalidation to remove outdated content from all edge locations. For example, if you update a file (from version one to version two), users might still retrieve the outdated version until the TTL lapses. By initiating a cache invalidation, CloudFront removes the old version so that the new version is fetched on subsequent requests. Invalidations can be applied at the distribution level using a wildcard (e.g., ""/*""), specific directories, or individual file paths. Integration with Other AWS Services CloudFront integrates seamlessly with several AWS services to enhance its functionality: SSL/TLS: HTTPS is enabled by default. AWS provides a default SSL certificate (e.g., *.cloudfront.net), and you can use AWS Certificate Manager to customize certificates for your domains. CloudWatch: CloudFront automatically pushes operational metrics to CloudWatch, allowing you to monitor performance. You can also opt for additional metrics at an extra cost. Use Cases for CloudFront CloudFront is versatile and supports a wide range of real-world use cases: Use Case Description Example Static Websites Ideal for hosting websites without dynamic server-side logic. Delivering HTML, CSS, and JavaScript files. Video on Demand Efficiently caches and delivers video content for on-demand streaming. Streaming pre-recorded video content to users. Media File Delivery Ensures fast distribution of images, documents, and other media libraries. Distributing downloadable files such as PDFs or images. Summary Amazon CloudFront leverages a global network of edge locations to deliver content with reduced latency by caching files closer to users. The core components include: Origin: The source of your content. Distribution: The configuration that defines how content is delivered. TTL: The duration for which cached content remains valid (default: 24 hours). Cache Invalidation: The process to manually refresh outdated content. Key Takeaway Understanding CloudFront's components and how they interact is essential for configuring a robust content delivery strategy that improves performance and reliability. By leveraging CloudFront, you can ensure that your content is served quickly and securely to users around the globe, enhancing their experience and optimizing your web application's performance. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,EFS,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/EFS,"AWS Solutions Architect Associate Certification Services Storage EFS In this lesson, we cover file system storage with a focus on Amazon's Elastic File System (EFS). Amazon EFS is one of the two primary file system storage services provided by AWS—the other being Amazon FSx. It supports the Network File System (NFS) protocol, allowing any NFS-dependent application to work seamlessly with the EFS service. Once you create an EFS file system within your Virtual Private Cloud (VPC), your Linux-based EC2 instances and other compute services can mount it remotely. Note that EFS is not supported on Windows-based EC2 instances. Key Benefit One of the major advantages of EFS is its ability to be mounted on multiple EC2 instances simultaneously. This feature allows efficient data sharing across instances. When deploying EFS, you associate it with your VPC by creating mount targets in selected subnets. A mount target is assigned its own IP address, which EC2 instances use to connect to the file system. For high availability, it is highly recommended to create mount targets in multiple availability zones. This ensures that if one mount target becomes unavailable, an alternative is readily available. EFS Storage Classes Amazon EFS offers different storage classes to address a variety of use cases: Standard Storage Classes : EFS Standard EFS Standard Infrequent Access These storage classes provide multi-AZ resilience with high durability and availability. One Zone Storage Classes : EFS One Zone EFS One Zone Infrequent Access These options help reduce costs by storing data in a single availability zone. Performance Modes in EFS EFS provides various performance modes tailored to different throughput, IOPS, and latency requirements: General Purpose Performance Mode : Ideal for latency-sensitive environments such as web applications, content management systems, home directories, and typical file serving operations. Elastic Throughput Mode : Automatically adjusts throughput performance based on your workload requirements. Max I/O Performance Mode : Supports extremely high aggregate throughput and a large number of operations per second, though it may exhibit higher latencies for certain operations. Provisioned Throughput Mode : Allows specification of a throughput level independent of the file system size. Bursting Throughput Mode : Scales throughput with the amount of storage and supports burst performance for up to 12 hours per day. Setting Up EFS on an EC2 Linux Instance This section outlines the process for mounting an EFS file system on an Amazon EC2 Linux instance. Step 1: Install Amazon EFS Utilities Depending on your package manager, install the Amazon EFS utilities using one of the following commands: For systems that use DNF: sudo dnf -y install amazon-efs-utils For systems using APT: sudo apt install amazon-efs-utils For systems using YUM: sudo yum install amazon-efs-utils Tip Ensure that you have appropriate permissions for installing packages on your EC2 instance. Step 2: Mount the EFS File System After installing the utilities, mount your EFS file system to a directory. Replace efs:id with your actual EFS file system ID (available from the AWS Console) and /directory with your chosen mount point: sudo mount.efs efs:id /directory This command mounts the EFS file system similarly to other file system types, with the configuration necessary for EFS connectivity. Summary Feature Details File System Service Managed service provided by AWS Protocol Supports NFS for seamless integration with NFS-based applications Supported Instances Linux-based EC2 instances only Multi-Instance Mounting Can be mounted on multiple EC2 instances simultaneously Mount Targets Provide an IP address for connecting to the EFS file system within a VPC Storage Classes - Standard (multi-AZ resilience)<br>- One Zone (cost-saving, single-AZ option) Performance Modes General Purpose, Elastic Throughput, Max I/O, Provisioned Throughput, and Bursting Throughput Boot Storage Limitation Cannot be used as boot storage for operating systems; EBS is recommended for boot volumes Important Remember, EFS is ideal for shared file systems used by Linux-based applications but is not a substitute for block storage solutions like EBS, especially when used for booting operating systems. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,CloudFront Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/CloudFront-Demo,"AWS Solutions Architect Associate Certification Services Networking CloudFront Demo In this lesson, you'll learn how to use CloudFront to cache static content for faster delivery. We'll configure an Amazon S3 bucket as the origin to store an image and then set up a CloudFront distribution to cache that image at edge locations. This approach ensures that users receive content from a nearby location rather than directly from the S3 bucket (for example, hosted in Northern Virginia). Setting Up the Origin (S3 Bucket) Before configuring CloudFront, we need an origin to store our files. Although any web server, load balancer, or API endpoint can be used as an origin, this demo uses an Amazon S3 bucket for simplicity. Create a New S3 Bucket Open the Amazon S3 console and create a new bucket. For this demonstration, name the bucket kodeklouddemo123 and leave the default region settings. Ensure that the bucket is configured to allow internet access. Upload an Image File Open the created bucket and upload the file car.jpg (a blue car image) by dragging and dropping it into the bucket. Verify Object Access After uploading, click on the file to view the object URL. If you encounter an ""Access Denied"" error as shown below, it indicates that the bucket policy hasn’t been configured for public access. <Error>
    <Code>AccessDenied</Code>
    <Message>Access Denied</Message>
    <RequestId>LEGJQIT1HU0Z8N1X2P1F6X1H15/Requests15</RequestId>
    <HostId>86u4cayk5wB7dEFlCkNFnKf2dC5D1E7UY2c0/38Vnty6toGvYwXlP8iM8WnDZe</HostId>
</Error> Note The error occurs because the S3 bucket policy restricts public access by default. Configure the Bucket Policy To allow public access, navigate to the bucket's permissions and add the following JSON policy. (Remember to update the bucket name if needed.) {
  ""version"": ""2012-10-17"",
  ""statement"": [
    {
      ""sid"": ""PublicReadGetObject"",
      ""Effect"": ""Allow"",
      ""Principal"": ""*"",
      ""Action"": [
        ""s3:GetObject""
      ],
      ""Resource"": [
        ""arn:aws:s3:::kodeklouddemo123/*""
      ]
    }
  ]
} Confirm Public Accessibility After applying the policy, clicking the object URL should display the image successfully. Configuring CloudFront With the origin set up, the next step is to configure CloudFront to cache the image at edge locations for enhanced performance. Create a CloudFront Distribution In the CloudFront console, create a new distribution. Under ""Origin Domain,"" select your S3 bucket ( kodeklouddemo123 ). If you wanted to cache a specific folder (for example, /images ), you could enter that in ""Origin Path."" For this demo, leave the origin path blank to cache all objects. Adjust Distribution Settings Update the following settings as needed: Origin Access: Set to public if you want users to access the S3 URL directly, or configure origin access control for enhanced security by limiting access exclusively through CloudFront. Compress Objects Automatically: Set this option to Yes for performance improvements. Allowed Protocols: Enable both HTTP and HTTPS. For a production environment, it's recommended to enforce HTTPS only. Allowed HTTP Methods: For static content, GET is sufficient. Additional methods (PUT, POST, PATCH) can be enabled if required. Edge Locations: CloudFront uses all edge locations by default; you can modify this to restrict caching to specific regions if necessary. Other settings like AWS Certificate Manager for certificates and IPv6 support can remain at their default values for this demonstration. Deploy the Distribution Once configured, create the distribution. Deployment may take a few minutes. When it’s complete, the distribution shows as enabled and displays a domain name you can use to access the cached files. Access the Cached Image To test the configuration, enter your distribution's domain name in the browser followed by /car.jpg . Note that accessing the domain root will not work since the S3 bucket is configured for static objects without an index. At this stage, your S3-hosted image should load via CloudFront’s edge locations, delivering faster content to users. Demonstrating Cache Behavior with Invalidation After verifying that CloudFront is serving the cached blue car image, we will update the object in the S3 bucket to demonstrate CloudFront's caching behavior and invalidation process. Update the Image in S3 Delete the existing car.jpg from the S3 bucket. Upload a new image (a red car) with the same file name ( car.jpg ). When accessing the direct S3 URL, you should now see the red car image. Observe Cache Persistence Refresh the CloudFront distribution URL for /car.jpg . You may still see the blue car image because it is cached with a default TTL (Time to Live) of 86,400 seconds (24 hours). Caching Behavior The cached content persists until the TTL expires. If immediate updates are required, you must invalidate the cache. Invalidate the Cache To force CloudFront to fetch the updated image before the TTL elapses, create an invalidation request: In the CloudFront console, select your distribution and go to the ""Invalidations"" tab. Create a new invalidation. To invalidate a specific file, enter /car.jpg . Alternatively, to invalidate all objects, use /* . You can also invalidate a folder using a pattern like /images/* . Verify the Invalidation Once the invalidation process is complete, refresh the CloudFront URL for /car.jpg . The red car image should now appear as CloudFront fetches the updated object from the S3 bucket. Review TTL and Caching Policy For further insights, check the TTL and caching policy by navigating to the ""Behaviors"" tab in your CloudFront distribution settings and clicking ""Edit"" on the appropriate behavior. Conclusion In this lesson, we demonstrated how to set up an Amazon S3 bucket as the origin for static content and configure an AWS CloudFront distribution to cache that content at edge locations. We also covered how to perform cache invalidation to ensure that updates propagate before the default TTL expires. This flexible setup is also ideal for hosting static websites, ensuring quick and reliable content delivery. Happy caching, and enjoy building faster web experiences! For more information on AWS CloudFront and S3, visit the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Transit Gateway,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Transit-Gateway,"AWS Solutions Architect Associate Certification Services Networking Transit Gateway In this article, we explore the AWS Transit Gateway service, which simplifies connectivity and routing between multiple VPCs and on-premises environments. AWS Transit Gateway offers a scalable alternative to managing a complex web of peering connections. Background By default, VPCs cannot communicate with one another. For example, resources in VPC 1 cannot interact with resources in VPC 2 unless a VPC peering connection is configured. While VPC peering allows direct communication between two VPCs, it does not support transitive routing. This means if VPC 1 is peered with VPC 2, and VPC 2 is peered with VPC 3, then VPC 1 and VPC 3 cannot communicate unless a direct peering is established between them. When you scale out to include more VPCs, the number of required peering connections increases significantly. For instance, with four VPCs, a full mesh of peering is required, meaning every VPC must connect to every other VPC. This quickly becomes both complex and hard to manage—especially when integrating on-premises networks through multiple VPN connections. Note For environments with multiple VPCs, AWS Transit Gateway provides a simplified and more scalable approach compared to traditional VPC peering. How Transit Gateway Works AWS Transit Gateway acts as a centralized router, allowing you to attach each VPC to the gateway instead of wiring a full mesh of peerings. In a scenario with four VPCs, each VPC requires only a single connection to the Transit Gateway, which then efficiently routes traffic between them. A key configuration requirement is to designate one subnet from each availability zone within a VPC to be used by the Transit Gateway. For example, if your VPC spans three availability zones, create a dedicated subnet in each zone. This setup ensures that the Transit Gateway has sufficient network interfaces for effective traffic routing. Connecting On-Premises Environments Transit Gateway also significantly simplifies on-premises connectivity. Traditionally, connecting an on-premises data center to multiple VPCs required establishing separate VPN connections for each VPC. With Transit Gateway, a single VPN connection to the gateway suffices, as it routes traffic to all connected VPCs. Additionally, AWS Direct Connect can be integrated with Transit Gateway to offer enhanced connectivity options for high-performance and low-latency needs. Advanced Connectivity Options A standout feature of Transit Gateways is their ability to peer with other Transit Gateways. This enables inter-region connectivity as well as connectivity across different AWS accounts. For example, if you need to link resources in two regions or manage networks across separate AWS accounts, establishing a peering relationship between Transit Gateways simplifies these scenarios considerably. Summary AWS Transit Gateway is a powerful tool that streamlines network connectivity. It eliminates the complexities of setting up a full mesh of VPC-to-VPC peerings by enabling transitive routing. Key takeaways include: Each VPC requires a designated subnet per availability zone for Transit Gateway connectivity. Transit Gateway reduces the number of VPN connections needed for on-premises connectivity. Integration with both VPN and AWS Direct Connect enhances on-premises connection options. Peering capabilities allow Transit Gateways to connect across regions and AWS accounts. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,EBS Demo part 1,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/EBS-Demo-part-1,"AWS Solutions Architect Associate Certification Services Storage EBS Demo part 1 In this lesson, you will learn how to work with Elastic Block Store (EBS) volumes in AWS. We will cover the process of creating an EBS volume in a specific Availability Zone, attaching it to an EC2 instance, verifying and formatting the volume, mounting it, and eventually detaching it so it can be reattached to another instance—all while ensuring that the volume remains in the same Availability Zone as the target instance. Before starting, note that several EC2 instances have already been pre-configured. Servers one through three are deployed in the US East (N. Virginia) region, though they span different Availability Zones. Servers one and two reside in the same zone, while server three is located in a different zone. Server four is in an entirely separate region. This setup demonstrates that for an EBS volume to be movable between instances, both the volume and the target instance must reside within the same Availability Zone. Creating and Attaching an EBS Volume Switch your AWS console to the US East 1 region and follow these steps to create an EBS volume for server one: Identify the Availability Zone of server one (for example, US East 1a). Navigate to the Volumes page on the AWS console and create a new volume: Choose the desired volume type (default is GP2). Set the volume size (we will use 10 GB for this demo). Ensure the volume is created in the same Availability Zone as server one. Leave encryption disabled for this demonstration. Once created, the volume's status will be ""available."" You may assign a descriptive name to the volume—such as ""demo volume."" After naming it, select Actions and choose Attach Volume . The console will display EC2 instances in the same Availability Zone (server one and server two). Select server one and note that the device is designated as /dev/sdf . On newer Linux kernels, this device might be automatically renamed (for example, to /dev/xvdf ). After confirming the attachment, the volume becomes connected to the selected instance. Verifying and Formatting the Volume on Server One Open a terminal connected to server one. List the block devices using: lsblk You should see the root device (commonly listed as xvda ) and the new 10 GB device (likely listed as xvdf ). Verify if a filesystem exists on the new device with: sudo file -s /dev/xvdf If the output shows data , no filesystem exists yet, which is expected. Create an XFS filesystem on the device: sudo mkfs -t xfs /dev/xvdf Verify that the filesystem has been created successfully by running: sudo file -s /dev/xvdf The expected output should mention ""SGI XFS filesystem data (blksz 4096, inosz 512, v2 dirs)."" Below is a sample terminal session demonstrating these steps: $ lsblk
NAME     MAJ:MIN RM   SIZE RO TYPE MOUNTPOINT
xvda     202:0    0     8G  0 disk 
├─xvda1  202:1    0     8G  0 part /
├─xvda127 259:0   0     1M  0 part 
├─xvda128 259:1   0    10M  0 part 
xvdf     202:80   0    10G  0 disk

$ sudo file -s /dev/xvdf
/dev/xvdf: data

$ sudo mkfs -t xfs /dev/xvdf
meta-data=/dev/xvdf              isize=512  agcount=4, agsize=655360 blks
         =                     sectsz=512  attr=2, projid32bit=1
         =                      finobt=1, sparse=1, rmapbt=0
data     =                       bsize=4096  blocks=2621440, imaxpct=25
         =                      sunid=0 blks
naming   =                    version 2
log      =              internal log     bsize=4096  blocks=16384, version=2
realtime =                      none    extsz=4096  blocks=0, rtexts=0

$ sudo file -s /dev/xvdf
/dev/xvdf: SGI XFS filesystem data (blksz 4096, inosz 512, v2 dirs) Mounting the Filesystem Temporary Mount The mount operation demonstrated below is temporary. If the instance is rebooted, the mount will be lost unless it is added to /etc/fstab for persistence. Create a directory to serve as the mount point (for this demo, we will use /ebsdemo ): sudo mkdir /ebsdemo
sudo mount /dev/xvdf /ebsdemo Confirm the mount with the command: df -k This output confirms that /dev/xvdf is mounted at /ebsdemo . Updating /etc/fstab for Persistent Mounting To ensure the volume stays mounted across reboots, follow these steps: Retrieve the UUID of the EBS volume: sudo blkid Look for the entry corresponding to /dev/xvdf . For example, you might see: /dev/xvdf: UUID=""04fdc8e-3441-4518-986c-a32254c0e925"" TYPE=""xfs"" Edit the /etc/fstab file: sudo vi /etc/fstab Add the following line to the file, making sure to replace the UUID with your own: UUID=04fdc8e-3441-4518-986c-a32254c0e925  /ebsdemo  xfs  defaults,nofail  0  0 To apply the changes without rebooting, run: sudo mount -a Verify the mount and the volume’s UUID: df -k
sudo blkid Testing the Mount with File Operations To confirm that the EBS volume is working correctly: Change to the mount directory and create a test file: cd /ebsdemo/
echo ""I made this file on server1"" | sudo tee demo.txt
sudo vi demo.txt  # (Edit the file as needed)
ls -l demo.txt This confirms write access to the volume, ensuring that the data is stored on the EBS volume rather than on the instance’s root storage. Detaching and Re-Attaching the EBS Volume To demonstrate the detachable nature of an EBS volume, perform these steps: Unmount the EBS volume from server one: cd ~
sudo umount /ebsdemo Verify the unmount by running: df -k The /ebsdemo mount point should no longer be listed. In the AWS console, navigate to the list of EBS volumes. Select the volume, then choose Actions > Detach Volume . Confirm the detachment using the dialog provided: Reattach the volume to server two by selecting Attach Volume from the Actions menu and specifying server two as the target. Verifying the Volume on Server Two On server two, perform the following steps to ensure that the reattached volume is functioning as expected: Check the list of block devices: lsblk
sudo file -s /dev/xvdf The device /dev/xvdf should still show the SGI XFS filesystem. Create the mount point and mount the volume: sudo mkdir /ebsdemo
sudo mount /dev/xvdf /ebsdemo Navigate to the mount directory, list its contents, and check the test file: cd /ebsdemo/
ls
cat demo.txt If the file demo.txt exists and displays the expected content (""I made this file on server1""), the persistent data has been successfully preserved on the EBS volume independently of a particular EC2 instance. Conclusion This demonstration has shown you how to create, attach, verify, format, and mount an EBS volume in AWS. It also illustrated how to detach an EBS volume from one instance and reattach it to another, all while ensuring that the volume remains in the correct Availability Zone. These techniques are essential for maintenance and migration tasks without risking data loss. In the next lesson, we will explore how to move an EBS volume to an EC2 instance located in another Availability Zone. For more information on AWS and EBS, you can refer to the official AWS documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Route 53 Application Recovery Controller,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Route-53-Application-Recovery-Controller,"AWS Solutions Architect Associate Certification Services Networking Route 53 Application Recovery Controller In this article, we explore the Amazon Route 53 Application Recovery Controller and its role in simplifying and automating application recovery processes. This service continuously monitors your application’s health and readiness, ensuring that your system is prepared for any failure. Amazon Route 53 Application Recovery Controller eliminates the need for manual intervention during failovers by automating continuous checks on your backup or standby site. In the event of a failure in the primary deployment, the controller redirects traffic seamlessly to the standby environment. How It Works The controller monitors the primary deployment against predefined criteria. For example, if an application's performance degrades beyond a 5% threshold or during scheduled maintenance, the controller can automatically or manually divert traffic to the standby site. Architecture Overview A typical architecture in the US East-1 region includes the following components for the active site: A load balancer EC2 instances within an availability zone A DynamoDB table accessed by the EC2 instances The backup site in another region is set up identically, with its own load balancer and EC2 instances that connect to the same global DynamoDB table. Route 53, our DNS management service, initially directs traffic to the active site’s load balancer. When a failover is needed, Route 53 updates the DNS record to point to the standby site. Below is a diagram illustrating the Application Recovery Controller architecture, emphasizing how traffic routing is managed between active and standby sites: Key Concepts Understanding the core elements of the Application Recovery Controller is essential: Concept Description Cells Groups of AWS resources required to operate an application independently (e.g., a load balancer and EC2 instances). Recovery Group A collection of cells representing an application or group of applications monitored for failover readiness. Resource Set A collection of AWS resources that may span multiple accounts or cells, such as a global DynamoDB table shared across deployments. The controller continuously performs readiness checks on the standby deployment, ensuring every resource is operational and available for failover. Additionally, its routing control feature enables manual traffic redirection to the standby site, facilitating a full failover when necessary. Failover Consideration When a failover is triggered, Route 53 updates the DNS configuration to route traffic to the standby site. This feature minimizes downtime, but ensure that your standby environment is regularly tested and maintained for uninterrupted availability. Below is a summary diagram highlighting the key features of the Route 53 Application Recovery Controller, including its monitoring capabilities, simplified failover process, and resource optimization for recovery readiness: Conclusion Amazon Route 53 Application Recovery Controller is an indispensable service for ensuring application resiliency. By automating continuous health checks and enabling rapid failover, it simplifies the recovery process and helps maintain high availability with minimal manual intervention. For more details on setting up and managing failover scenarios, refer to the Amazon Route 53 Application Recovery Controller documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,VPC Peering DEMO,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/VPC-Peering-DEMO,"AWS Solutions Architect Associate Certification Services Networking VPC Peering DEMO In this lesson, we demonstrate how to set up VPC peering between two Virtual Private Clouds (VPCs) to allow resources in one VPC to securely communicate with resources in another without traversing the public internet. Overview We start with two pre-configured VPCs: VPC-A: CIDR block 10.1.0.0/16 Contains an EC2 instance with a private IP address of 10.1.1.13. VPC-B: CIDR block 10.2.0.0/16 Contains an EC2 instance with a private IP address of 10.2.1.139. Initial Connectivity Test Before establishing a VPC peering connection, attempt to ping the EC2 instance in VPC-B from VPC-A. Log in to server one (in VPC-A) and run: [ec2-user@ip-10-1-1-13 ~]$ ping 10.2.1.139
PING 10.2.1.139 (10.2.1.139) 56(84) bytes of data. The ping fails because VPCs are isolated by default, even if all security groups and NACLs are configured to allow traffic. This failure is expected until a VPC peering connection is established. Creating the VPC Peering Connection Follow these steps to configure the VPC peering connection: Open the VPC peering connections section in the AWS console and click Create Peering Connection . Name the connection (for example, ""VPC A to VPC B"") for clarity. Select the local VPC (the requester) and specify the VPC to peer with. You can create peering connections between VPCs in the same AWS account or across different accounts. Since both VPCs are located in the US East 1 region, choose that region and select VPC-B as the accepter. Verify that the CIDR blocks do not overlap. VPC peering requires distinct CIDR blocks for proper routing. Click Create Peering Connection . Once the request is sent, the peering connection appears in a “pending acceptance” state. Since both VPCs are in the same account, you need to manually accept the request: Select the pending peering connection. Click Actions and choose Accept Request . Note After accepting the peering request, the connection is established but resources still cannot communicate until the routing tables are correctly configured. Configuring Route Tables Even with an active peering connection, neither VPC knows how to route traffic for the other, as explained below: VPC-A: Has a local route for 10.1.0.0/16 and an Internet Gateway route, but lacks a route for 10.2.0.0/16. VPC-B: Lacks a route for 10.1.0.0/16. To enable communication: In VPC A’s route table, add a route with the destination CIDR 10.2.0.0/16 and set its target to the VPC peering connection. In VPC B’s route table, add a route with the destination CIDR 10.1.0.0/16 and set its target to the same peering connection. Warning Ensure that the routing changes are applied to the correct route tables associated with the subnets containing your EC2 instances. Misconfigurations can prevent successful connectivity. Testing Connectivity After updating the route tables, test connectivity again from server one: [ec2-user@ip-10-1-1-13 ~]$ ping 10.2.1.139
PING 10.2.1.139 (10.2.1.139) 56(84) bytes of data.
64 bytes from 10.2.1.139: icmp_seq=1 ttl=127 time=1.88 ms
64 bytes from 10.2.1.139: icmp_seq=2 ttl=127 time=1.43 ms
64 bytes from 10.2.1.139: icmp_seq=3 ttl=127 time=1.38 ms
...
--- 10.2.1.139 ping statistics ---
17 packets transmitted, 17 received, 0% packet loss, time 16027ms
rtt min/avg/max/mdev = 1.321/1.457/1.882/0.120 ms The successful ping confirms that the VPC peering connection and routing configurations are properly set up. Traffic between the two servers now flows privately within the AWS infrastructure. Summary Setting up VPC peering involves: Sending a peering request from one VPC and accepting it in the other. Configuring the route tables of both VPCs to enable proper routing between them. By following these steps, you can establish secure, private connectivity between resources in different VPCs within your AWS environment without exposing traffic to the public internet. For additional details on VPC peering and AWS network architecture, please refer to the official AWS documentation . Watch Video Watch video content Practice Lab Practice lab"
AWS Solutions Architect Associate Certification,EBS,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/EBS,"AWS Solutions Architect Associate Certification Services Storage EBS In this article, we explore AWS Elastic Block Store (EBS) and the fundamentals of block storage. Block storage divides data into separate blocks, each uniquely identified and distributed across multiple physical devices. These blocks can be presented to an operating system as a volume, allowing you to create a file system on top of it. Alternatively, the block device can be bootable, enabling you to install an operating system directly on it. This flexibility—being both mountable and bootable—is especially crucial for the Solutions Architect exam. The diagram below illustrates a block storage system where data blocks are spread across two storage units and managed by a computer interface: What is AWS Elastic Block Store (EBS)? Amazon EBS is a block-level storage service designed specifically for EC2 instances. When attached to an EC2 instance, these volumes appear as block storage devices. You can format these volumes with file systems such as XFS, ext3, or ext4. Key Advantage An important benefit of EBS is its independence from EC2 instances. You can detach an EBS volume from one instance and reattach it to another without data loss. Note that while most EBS volumes attach to only one EC2 instance, some volume types support multi-attach. If using multi-attach, ensure that your application effectively manages simultaneous writes to prevent data corruption. EBS volumes are provisioned within a single availability zone (AZ), ensuring built-in redundancy within that zone. If one physical device fails, the volume remains operational. However, if the entire AZ experiences an outage, the data on the volume can be lost. This means that an EC2 instance and its attached EBS volume must reside in the same AZ. EBS Operation Workflow Imagine deploying EBS in multiple availability zones: Create an EBS Volume: Provision your volume within the desired AZ. Attach to an EC2 Instance: The volume is then presented as a block device. Data Migration within the Same AZ: Detach the volume from one instance and attach it to another within the same AZ if needed. For migrating data across AZs, you can use snapshots. Taking a snapshot creates an image of your volume stored in Amazon S3. You can then use this snapshot to create a new volume in a different AZ and attach it to an EC2 instance, effectively copying your data from one zone to another. The following diagram shows the architecture of EBS within a region, including two availability zones, EBS volumes, and EC2 instances. It also demonstrates the process of creating a volume from a snapshot: For data migration between regions, the process is analogous: Take a snapshot (stored in an S3 bucket in the source region). Copy the snapshot to an S3 bucket in the destination region. Create a new volume from the copied snapshot and attach it to an EC2 instance. The next diagram depicts the process of copying an EBS snapshot from one region to another and creating a volume from that snapshot: EBS Volume Types Amazon EBS offers various volume types designed to satisfy differing performance and cost requirements. Below is an overview of each: 1. General Purpose SSD (gp2 and gp3) General Purpose SSD Volumes: Backed by solid state drives (SSDs), these volumes deliver a balanced price/performance ratio ideal for a wide range of workloads such as virtual desktops, medium-sized databases, and interactive applications. GP3: Offers predictable performance with independently scalable IOPS from capacity at a 20% lower price per gigabyte compared to GP2. GP2: The default volume type where performance scales with the volume size. 2. Provisioned IOPS SSD (io1, io2, and io2 Block Express) High-Performance SSD Volumes: Designed for I/O-intensive and low-latency workloads. IO1/IO2: Both offer high performance, with IO2 delivering higher durability (99.999% compared to approximately 99.8%–99.9% for IO1). IO2 Block Express: Supports extreme workloads with maximum volume sizes up to 64 TB, IOPS up to 256,000, and throughput up to 4000 MB/s. The image below summarizes the specifications of different SSD volume types, including durability, volume limits, IOPS, throughput, and features like multi-attach support and boot volume compatibility: 3. Hard Disk Drive (HDD) Volumes HDD-Based Volumes: Suitable for less frequently accessed data, HDD volumes are available in two types: Throughput Optimized HDD (st1): Tailored for throughput-intensive workloads with frequently accessed large datasets. Cold HDD (sc1): Designed for infrequently accessed data where lower storage cost is a priority. The diagram below compares throughput optimized HDDs and cold HDDs, highlighting performance and cost differences: 4. Magnetic Volumes Previous Generation Magnetic Volumes: Ideal for workloads with small, infrequently accessed datasets where high performance is not essential. Typically, these volumes deliver around 100 IOPS on average, with burst capabilities to a few hundred IOPS, and range in size from 1 GB to 1 TB. The table below (depicted in the image) details the specifications of magnetic volumes: EBS Pricing With EBS, you only pay for the storage you provision. Pricing is charged on a per-gigabyte, per-month basis, varying by the selected volume type—meaning that higher performance volumes are priced at a premium. Similarly, EBS snapshots are billed per gigabyte per month. Note that snapshots are full copies of the volumes (stored in Amazon S3), not incremental backups. Summary Block storage divides data into distinct blocks that are uniquely identified, enabling them to be aggregated into volumes that an operating system can use both as storage and as a boot device. AWS Elastic Block Store (EBS) provides robust block-level storage for EC2 instances, ensuring that volumes are provisioned within a specific availability zone. To migrate data between zones or regions, you can create snapshots and build new volumes from them. With a range of volume types available, EBS allows you to optimize performance and cost based on your specific needs. The summary diagram below encapsulates the key block storage concepts, including how data is divided into blocks, presented as volumes, and managed with snapshots for data migration: Additionally, the final diagram summarizes the essential features of EBS volumes—from provisioning in availability zones to snapshot-based data migration and the diversity of volume types offered: Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Route 53,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Route-53,"AWS Solutions Architect Associate Certification Services Networking Route 53 In this lesson, we explore AWS Route 53, a fully managed domain name service (DNS) provided by AWS. What Is Route 53? AWS Route 53 is a robust and scalable DNS service that facilitates several critical functions: It acts as a domain registrar, allowing you to purchase domain names much like popular services such as GoDaddy and Namecheap. For example, you can register a domain like KodeKloud.com directly through Route 53. It manages all DNS records for your domains, so once you register a domain, you can easily configure its DNS settings within Route 53. Being a global service, Route 53 is not limited to a single region, ensuring worldwide accessibility. Hosted Zones A fundamental concept in Route 53 is the hosted zone. A hosted zone is a container for all the DNS records associated with a specific domain (e.g., KodeKloud.com). When you create a hosted zone: AWS reserves four dedicated name servers for that domain. For any additional domain, such as fastcars.com, AWS allocates another distinct set of four name servers to manage its DNS records and rules. Key Highlights AWS Route 53 functions as both a DNS service and a domain registrar. It delivers global coverage and is not confined to any specific region. Hosted zones provide a structured environment where each domain receives four dedicated name servers for optimal DNS management. Summary AWS Route 53 is a fully managed DNS service and domain registrar. It operates at a global level. Hosted zones are collections of DNS records for specific domains, with each hosted zone having four dedicated name servers reserved by AWS. In the next section, we will provide a demonstration to illustrate how to set up and manage hosted zones effectively. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Global Accelerator,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Global-Accelerator,"AWS Solutions Architect Associate Certification Services Networking Global Accelerator In this lesson, we dive into AWS Global Accelerator and explore how it enhances the performance and reliability of your globally distributed applications. Why Use Global Accelerator? Imagine deploying an application in North America while users from all around the world access it. Typically, their requests traverse the public internet via local service providers, which may not offer the most direct or efficient path. This routing can introduce higher latency and less reliable connections, ultimately degrading the user experience. AWS Global Accelerator overcomes these challenges by connecting users to the nearest edge location and routing their traffic over a dedicated AWS backbone network. This approach not only reduces latency but also improves overall network reliability for global applications. How Global Accelerator Optimizes Connectivity When Global Accelerator is enabled, user traffic is efficiently managed through the following steps: A user sends a request from their location. The request is routed to the closest Global Accelerator edge location. From the edge location, the request travels over the AWS backbone network—a dedicated infrastructure built for high speed, enhanced security, and reliability. Finally, the request reaches the target application hosted within an AWS region or data center. Key Benefit By bypassing the unpredictable public internet and leveraging the AWS backbone, Global Accelerator ensures your users experience faster and more consistent connections. The diagram above demonstrates how user traffic is swiftly routed onto the AWS backbone network, making the connection significantly faster compared to traditional internet routing. CloudFront vs. Global Accelerator While both CloudFront and Global Accelerator make use of edge locations, they serve distinct purposes: Service Primary Focus How It Works CloudFront Caching static content Delivers cached content from nearby edge locations to end users Global Accelerator Optimized network routing Routes user traffic to the nearest edge, then over the AWS backbone for enhanced speed and reliability Further Reading Explore more about AWS networking solutions and best practices by visiting the AWS Documentation . Global Accelerator leverages the robust AWS global network to provide secure, fast, and efficient network paths, ensuring your application performs effectively regardless of user location. Conclusion AWS Global Accelerator improves the end-user experience by: Routing traffic over a dedicated, high-performance AWS backbone. Reducing latency by connecting users to the nearest edge location. Enhancing the reliability and security of global network communication. By integrating Global Accelerator into your infrastructure, you ensure that your global applications are accessible quickly and reliably—delivering a superior experience to your users. For more information, check out these resources: AWS Global Accelerator AWS Networking Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Demo Route 53,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Networking/Demo-Route-53,"AWS Solutions Architect Associate Certification Services Networking Demo Route 53 In this lesson, we demonstrate AWS Route 53, a managed DNS service that allows you to both register domain names and define DNS records—all within the AWS ecosystem. By leveraging Route 53, you can register domains directly without relying on third-party registrars like GoDaddy or Namecheap . Domain Registration First, search for ""Route 53"" in the AWS console and open the Route 53 service. In the Domains section, select ""Registered domains."" Since no domains are registered in your account yet, you can choose to either transfer an existing domain or register a new one. In this demonstration, we will register a new domain. Check Domain Availability: Enter a domain name to see if it is available. For demonstration purposes, we will use a dummy name such as ""KodeKloudDemo123.com"" (alternatively, ""KodeKloudDemo123.link""). In this example, we select the .com option, priced at $13 per year. Click on the domain and then click ""Proceed"". Checkout Process: At checkout, you'll see an option to enable auto renew, which automatically renews your domain after one year, ensuring uninterrupted ownership. Note If auto renew is deselected, you must manually renew the domain—a process that can easily be overlooked. Provide Contact Information: Fill out the form with your details and be sure to enable privacy protection for all contacts associated with your domain. Review & Submit: Click ""Next"" to review your registration details. On the recap page, you'll see the purchase summary along with a note on a small hosted zone management fee charged in addition to the registration fee. Scroll down, verify your contact information, agree to the terms and conditions, and click ""Submit."" Domain registration may take some time; you will receive an email with the registration status for each domain. Be sure to monitor your email and later check the ""Registered domains"" section in Route 53. Post-Registration Details: Once registered, your domain listing will display details such as the registration date, expiration date (typically one year out), and the auto renew setting. Clicking on your domain provides additional info including the four name servers assigned by AWS. These name servers identify the hosted zone where your DNS records are stored. DNS Management After domain registration, manage your DNS records by navigating to the ""Hosted zones"" section. Here you will find a hosted zone for your newly registered domain. Clicking on the hosted zone reveals details like the reserved DNS servers and the default DNS records configured by AWS. To add a new DNS record, click on ""Create records."" For instance, if you have a public web server with an IPv4 address and you want your domain name to point to that server, you can create an A record. Follow these steps: In the ""Create record"" menu, select the A record type. Specify whether the record applies to the root domain (e.g., ""kodeklouddemo123.com"") or a subdomain (e.g., ""www.kodeklouddemo123.com""). Paste the server's IP address, set the desired TTL (Time To Live), and create the record. Route 53 will propagate the new DNS record to all authoritative DNS servers within approximately 60 seconds. You can check the propagation status by using the ""View Status"" button. Once propagation is complete, you can open your browser and enter your domain name (e.g., ""kodeklouddemo123.com"") to access your web server by its domain rather than its IP address. Summary This article demonstrated how AWS Route 53 allows you to register a domain and create DNS records to route traffic to a web server. Understanding these procedures is essential for the AWS Solutions Architect Associate Certification exam, as Route 53 is a fundamental part of managing domain names and DNS entries in the AWS cloud. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,EBS Demo part 2,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/EBS-Demo-part-2,"AWS Solutions Architect Associate Certification Services Storage EBS Demo part 2 Welcome back to the second part of our EBS demo. In this lesson, you'll learn how to transfer an EBS volume between servers—first within the same availability zone and then across different availability zones. Previously, we moved our EBS volume from Server One to Server Two seamlessly because both resided in the same availability zone. Since an EBS volume is zonal, it can be attached to any EC2 instance within its current zone. Now, we'll move the EBS volume to Server Three, which is located in a different availability zone (us-east-1b). Pro Tip Before moving volumes, always ensure that any active file systems are unmounted to avoid data corruption. Unmounting the Volume On the current server where the volume is in use, unmount it by executing: sudo umount /ebsdemo After unmounting, verify that no file systems are mounted by listing block devices: lsblk Expected output: NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS xvda 202:0 0 8G 0 disk ├─xvda1 202:1 0 8G 0 part / ├─xvda2 259:0 0 0 0 part └─xvda128 259:1 0 10M 0 part xvdf 202:80 0 10G 0 disk Verify the file system type on the volume: sudo file -s /dev/xvdf You should see output similar to: /dev/xvdf: SGI XFS filesystem data (blksz 4096, inosz 512, v2 dirs) Next, create a mount point and mount the volume to verify the data inside: sudo mkdir /ebsdemo
sudo mount /dev/xvdf /ebsdemo
cd /ebsdemo
ls
cat demo.txt The file ""demo.txt"" should display: I made this on server Finally, unmount the volume again: cd ..
sudo umount /ebsdemo Confirm that the volume is not mounted by checking: df -k Detaching and Moving the Volume With the volume unmounted, log into your AWS console and navigate to the EBS Volumes section. Select the volume, and ensure it is detached or in the process of detaching. Wait until its status changes to ""available."" Refresh the page if necessary. Next, click the ""Attach volume"" button to try attaching the volume to Server Three. Note that only servers in the same availability zone (Server One and Server Two) will appear in the instance selection dropdown. Creating a Snapshot to Overcome Availability Zone Limitations Because you cannot directly attach an EBS volume to an instance in a different availability zone, the solution is to create a snapshot of your volume. Follow these steps: Select the volume. Choose ""Actions"" > ""Create snapshot."" (Optional) Add a description (e.g., “my snapshot”) and confirm. Creating a snapshot produces an exact copy of your volume's data. Monitor the snapshot’s progress in the Snapshots section. It may initially show a status like ""pending"" or a progress percentage until it turns ""available."" Creating a New Volume in a Different Availability Zone Once the snapshot is available, follow these steps to create a new volume in the target availability zone: Select the snapshot. From the ""Actions"" dropdown, choose ""Create volume from snapshot."" Change the availability zone to the target zone (e.g., US East 1B for Server Three). You can adjust volume type, size, and other settings as needed. (Optional) Add tags (for example, name the volume ""EBS clone""). After creating the new volume, confirm in the Volumes section that the clone appears in the correct availability zone. Now, attach the new volume to Server Three: Verifying on Server Three Log into Server Three via your terminal and run the following commands to verify that the block device is attached and to confirm the file system: lsblk Expected output should look similar to this: NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINTS xvda 202:0 0 8G 0 disk ├─xvda1 202:1 0 8G 0 part / ├─xvda127 259:0 0 1M 0 part └─xvda128 259:1 0 10M 0 part xvdf 202:80 0 10G 0 disk Check the filesystem on the new volume: sudo file -s /dev/xvdf Expected output: /dev/xvdf: SGI XFS filesystem data (blksz 4096, inosz 512, v2 dirs) Then, create the mount point and mount the volume: sudo mkdir /ebsdemo
sudo mount /dev/xvdf /ebsdemo Change directory to the mount point and verify the transferred data: cd /ebsdemo/
ls
cat demo.txt The file ""demo.txt"" should display: I made this on server This confirms that the original volume's data has been successfully cloned and attached to Server Three in a different availability zone. Moving Volumes Across Regions If you need to migrate the volume to a completely different AWS region (for example, from Northern Virginia to Ohio), you cannot directly create a volume from a snapshot in another region. Instead, you must: Copy the snapshot from the original region to the desired region. Create a volume from the copied snapshot in the target region. To copy the snapshot: Select the snapshot. Click ""Actions"" and select ""Copy snapshot."" Provide a description such as “copy of my snapshot.” Specify the destination region (e.g., US East 2 for Ohio). Once the snapshot copy is complete, switch to the target region (e.g., us-east-2), locate the snapshot in the Snapshots section, and then click ""Create volume from snapshot."" Choose the correct availability zone (e.g., us-east-2a) for your EC2 instance. After creation, the new volume will be available. Finally, attach the newly created volume to your target EC2 instance: This completes the process of migrating an EBS volume across different availability zones and regions. Happy configuring! Watch Video Watch video content Practice Lab Practice lab"
AWS Solutions Architect Associate Certification,Instance Store,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/Instance-Store,"AWS Solutions Architect Associate Certification Services Storage Instance Store In this article, we discuss the concept of instance stores and their relationship to EC2 instances. Instance stores offer temporary block-level storage directly attached to the host that runs your EC2 instance, making them ideal for scratch data or rapidly changing temporary datasets. What Is an Instance Store? Instance stores provide temporary storage that is physically attached to the host machine. Unlike EBS volumes—which reside on separate machines and are accessed via protocols like iSCSI—instance stores enable your EC2 instance to access a local physical drive directly on the host. Their temporary nature means that the stored data is lost if the instance is moved to a different host. How Instance Stores Work Consider a scenario where multiple EC2 instances run on the same host machine. When an EC2 instance utilizes an instance store: The instance directly accesses a physical drive on that host. Provided the instance remains on the same host, even after a reboot, it retains access to its instance store data. However, when an instance is shut down and later restarted on a different host, it loses access to the original instance store and all associated data because the new host has a different storage configuration. Warning Moving an EC2 instance to a different host results in the loss of any data stored on the original instance store. Use instance stores only for data that can be safely discarded. Use Cases and Limitations Instance stores are best suited for applications where temporary data storage is acceptable and data persistence is not required. Common use cases include: Caching Buffering Temporary file storage When designing systems, keep in mind that instance stores are not reliable for storing persistent data, as data loss will occur if the instance is migrated. Note For persistent data storage, consider using Amazon EBS volumes, which offer durability and the ability to retain data independently of the EC2 instance lifecycle. Summary To summarize: Instance stores provide block-level, temporary storage for EC2 instances. Data on an instance store is lost if the instance migrates to a different host. They are ideal for transient data such as caches or scratch files, but not for persistent storage. By understanding the inherent limitations and ideal use cases for instance stores, you can better architect your cloud infrastructure to maximize performance and efficiency while safeguarding critical data. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Instance Store Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/Instance-Store-Demo,"AWS Solutions Architect Associate Certification Services Storage Instance Store Demo In this article, we demonstrate how to work with instance store volumes on AWS EC2. Remember that instance store data is temporary—if your EC2 instance moves from one physical host to another, the instance store is replaced with a new, empty volume. Therefore, use instance stores only for temporary or scratch data that does not need to persist. Launching an Instance with an Instance Store To begin, navigate to the EC2 console and launch a new instance. For this demonstration, name your instance ""instance store demo"" and select the Amazon Linux 64-bit AMI. Note that not all EC2 instance types support instance stores; for example, t2.micro and other free tier instances lack instance store volumes. Select an instance type that includes instance store support—even if it comes at a small cost for this short demo. After choosing an appropriate instance type, select any key pair of your preference. Review the configuration details. You will notice that the default root volume is 8 GB, and further down, you will see an instance store volume (approximately 75 GB, reported as 69.8 GB). The device name for the instance store is typically something like /dev/nvme0n1 or /dev/nvme1n1 . Also, ensure that ""Auto-assign Public IP"" is enabled so you can easily connect to your instance. Once your instance is deployed, return to the EC2 console's instance tab. You should now see the instance with its assigned public IP. Connecting and Configuring the Instance Store After noting the public IP, SSH into your instance. Once connected, run the following command to verify the block devices: lsblk You'll see output similar to: NAME        MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS
nvme1n1    259:0    0  69.8G  0 disk 
nvme0n1    259:1    0    8G  0 disk 
├─nvme0n1p1 259:2    0    8G  0 part /
└─nvme0n1p128 259:4    0   10M  0 part Here, nvme0n1 is your root volume (with partition nvme0n1p1 mounted as / ), and nvme1n1 is the instance store volume. Verify if a file system exists on the instance store volume by running: sudo file -s /dev/nvme1n1 If the output shows ""data,"" no file system exists. Create an XFS file system on the volume as follows: sudo mkfs -t xfs /dev/nvme1n1 Then, validate the file system creation by running: sudo file -s /dev/nvme1n1 You should now see that an SGI XFS filesystem is present on the device. Next, create a directory to serve as the mount point and mount the instance store with these commands: sudo mkdir /instance-demo
sudo mount /dev/nvme1n1 /instance-demo/ Confirm that the volume is mounted by executing: df -k Now, navigate into the mount directory and create a test file to verify that data is being written to the instance store: cd /instance-demo
echo ""test"" | sudo tee test The file ""test"" is now stored on your instance store volume. Tip Keep in mind that the instance store is optimized for fast, temporary storage. It is ideal for cache, buffers, or temporary files, but not for data that requires persistence. Demonstrating the Ephemeral Nature of Instance Store Data Data on an instance store is ephemeral. A simple reboot will not change the physical host, so your instance store volume and its data remain intact. You can verify this by rebooting the instance from the EC2 console (right-click the instance and select ""Reboot""). After the reboot, the public IP address will remain the same. However, when you stop and then start your instance, it is relocated to a different physical host. This process replaces the instance store with a new, empty volume. To observe this behavior: Record the current public IP address. Stop the instance using the EC2 console. Wait a few minutes, then start the instance again. After the instance restarts, note the change in the public IP address, which confirms it now runs on a different physical host. SSH into the instance again and run: lsblk You will notice the instance store volume ( nvme1n1 ) is still present. However, if you check the file system with: sudo file -s /dev/nvme1n1 and list the contents of /instance-demo , you will find that the test file created earlier is missing. This confirms that data from the previous instance store has been lost due to the instance migration. Warning Do not use instance stores for storing critical data. Always rely on persistent storage solutions like Amazon EBS for data that must be retained. Conclusion This article illustrates that instance store volumes on AWS EC2 are temporary. While a simple reboot preserves the data, stopping and starting the instance moves it to a different physical host and clears the instance store. For AWS environments, it’s crucial to use instance stores only for temporary data needs. Happy learning, and see you in the next article! Quick Reference Table Command/Action Description lsblk List block devices sudo file -s /dev/nvme1n1 Check for a file system on the instance store volume sudo mkfs -t xfs /dev/nvme1n1 Create an XFS file system on the instance store sudo mkdir /instance-demo Create a mount directory sudo mount /dev/nvme1n1 /instance-demo/ Mount the instance store df -k Confirm the mount status `echo ""test"" sudo tee test` For more information on AWS EC2, please refer to the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,FSx for WindowsLustreNetAppOpenZFS,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/FSx-for-WindowsLustreNetAppOpenZFS,"AWS Solutions Architect Associate Certification Services Storage FSx for WindowsLustreNetAppOpenZFS Amazon FSx is a fully managed service that provides high-performance file storage for a wide range of workloads. By automating complex management tasks—such as provisioning, replication, patching, hardware maintenance, and backups—Amazon FSx allows businesses to focus on core activities rather than manual file server management. Unlike Amazon EFS, which supports only Linux environments, Amazon FSx is designed to work seamlessly with both Windows and other operating systems. This flexibility enables you to deploy file storage solutions across diverse environments while AWS manages the underlying infrastructure. Key Features and Benefits Amazon FSx offers multiple features tailored to modern storage needs: Storage and Management: Enjoy secure, reliable storage while AWS handles server setup and maintenance. Scalability: Easily adjust your storage capacity to match changing data requirements. Shared Access: Enable multiple users or systems to access data concurrently, ideal for collaborative projects. Automated Backups: Benefit from automatic backups to safeguard your critical data. FSx Flavors Amazon FSx is available in several variants, each designed for specific workloads. Amazon FSx for Windows File Server Optimized for Windows environments, this service leverages Windows Server and the SMB protocol to deliver a seamless file server experience. Key features include: Native integration with Microsoft Active Directory Support for data deduplication and quota management Amazon FSx for Lustre Designed for high-performance parallel file processing, FSx for Lustre is ideal for: Scientific computing Machine learning Data analytics Built on the Lustre file system, it delivers low-latency, high-throughput data access. Integration with AWS services such as Amazon S3, AWS DataSync, and AWS Batch ensures that your file system can scale efficiently with workload demands. Amazon FSx for NetApp ONTAP Powered by NetApp's ONTAP file system, this version offers high-performance storage that is accessible from Linux, Windows, and macOS. It supports multiple protocols, including: NFS SMB iSCSI Additional capabilities like dynamic scalability, snapshots, clones, and replication make it a robust choice for enterprise environments. Amazon FSx for OpenZFS This variant is built on the open-source OpenZFS file system and provides: Cross-platform support for Linux, Windows, and macOS via the NFS protocol Powerful features including data compression, snapshots, and data cloning Built-in data protection and security mechanisms Note Amazon FSx for OpenZFS is an excellent choice for organizations looking for advanced data management features paired with cross-platform compatibility. Deployment Options and Service Comparison FSx for Windows, NetApp ONTAP, and OpenZFS support both single Availability Zone (AZ) and multi-AZ deployments, whereas FSx for Lustre is available only in single AZ deployments. For a detailed side-by-side comparison of client compatibility, protocol support, latency, throughput, and maximum file system size, refer to the diagram below: Summary Amazon FSx streamlines the deployment and management of high-performance file storage in the cloud. Its various options are tailored to meet diverse workload requirements: FSx for Windows: A fully managed Windows file server using the SMB protocol with seamless Active Directory integration. FSx for Lustre: Tailored for high-performance, parallel file processing tasks, making it suitable for fields such as scientific computing and machine learning. FSx for NetApp ONTAP: A versatile solution built on NetApp's ONTAP, supporting multiple protocols and delivering robust scalability. FSx for OpenZFS: Leveraging the open-source OpenZFS file system, it offers advanced data management features and cross-platform accessibility. Key Takeaway Amazon FSx reduces the overhead of managing on-premises file systems by offering a range of flexible, scalable, and reliable cloud storage solutions that cater to different operating systems and workload requirements. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,S3 Access Points Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/S3-Access-Points-Demo,"AWS Solutions Architect Associate Certification Services Storage S3 Access Points Demo In this lesson, we explore how to work with AWS S3 Access Points. You will learn how to create a demo bucket named ""KodeKloud access point"", upload a test file, and configure multiple access points with custom policies to manage access for different users. Creating the Demo Bucket Begin by creating a new bucket in the AWS S3 console and naming it ""KodeKloud access point"". Retain all the default settings. After the bucket is created, upload a demo file for testing purposes. Click the ""Open"" button within the bucket to view the file as the bucket owner. Next, inspect the details of the object (e.g., ""beach.jpg""): Testing Access with Multiple Users To simulate different access permissions, open several browser tabs representing different users within the same AWS account. For demonstration purposes: Blue Tab: Represents user one (the bucket owner). Green Tab: Represents user two. Yellow Tab: Represents user three. Verify user permissions in the AWS IAM Management Console. You’ll notice that while user one has full S3 access as the bucket owner, user two and user three have limited access (primarily to Cloud Shell) without direct S3 permissions. Using Cloud Shell (which already includes the AWS CLI), run the following test commands. As a general user (not the bucket owner), attempting to copy the object produces a 403 Forbidden error: [cloudshell:user@ip-10-2-30-244 ~]$ aws s3 ls
2023-04-07 02:26:33 kk-access-point
[cloudshell:user@ip-10-2-30-244 ~]$ aws s3 cp s3://kk-access-point/beach.jpg .
fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden Similarly, testing from user two’s Cloud Shell session shows restricted access: [cloudshell-user@ip-10-2-30-244 ~]$ aws s3 ls
2023-04-07 07:26:13 kk-access-point
[cloudshell-user@ip-10-2-30-244 ~]$ aws s3 cp s3://kk-access-point/beach.jpg .
2023-04-07 07:27:37  2073914 beach.jpg
[cloudshell-user@ip-10-2-30-244 ~]$ aws s3 cp s3://kk-access-point/beach.jpg .
fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden This confirms that only user one (the bucket owner) currently has full access to the file. Switch back to the main user (user one) and test again: [cloudshell-user@ip-10-2-30-244 ~]$ aws s3 ls
2023-04-07 02:36:13 kk-access-point
[cloudshell-user@ip-10-2-30-244 ~]$ aws s3 cp s3://kk-access-point/beach.jpg .
fatal error: An error occurred (403) when calling the HeadObject operation: Forbidden Creating and Configuring Access Points Setting Up Access Points We now create access points to delegate specific access permissions for different groups. First, set up an access point for developers (user two). During creation: Assign a name (e.g., ""developers""). Choose the bucket ""KodeKloud access point"". Enable the option to allow requests from the Internet. For now, skip setting an access point policy; you can modify it later as needed. After creation, the developers access point appears alongside the bucket's access points: Repeat the process to create an additional access point for user three (finance team). Use open Internet access with default policy settings. Access Point Policies and Bucket Delegation Access point policies control requests made through an access point. For example, to allow a specific user (Jane) to perform object operations, you might use a policy like this: {
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Effect"": ""Allow"",
            ""Principal"": {
                ""AWS"": ""arn:aws:iam::123456789012:user/Jane""
            },
            ""Action"": [
                ""s3:GetObject"",
                ""s3:PutObject""
            ],
            ""Resource"": ""arn:aws:s3:::accesspoint:my-access-point/object/Jane/*""
        }
    ]
} Note that the resource field in the policy references the access point ARN, which functions similarly to a traditional S3 bucket ARN during operations. Important Remember that permissions in an access point policy are only effective if the underlying bucket also permits the same access. You can achieve this by either: Adding the same policy to the bucket policy, or Delegating access control from the bucket to the access point. The recommended approach is delegation. To delegate access, adjust the bucket policy to allow access points to manage access control. An example delegation bucket policy is: {
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Principal"": { ""AWS"": ""*"" },
      ""Action"": [ ""Bucket ARN"", ""Bucket ARN/*"" ],
      ""Resource"": [ ""Bucket ARN"", ""Bucket ARN/*"" ],
      ""Condition"": { ""StringEquals"": { ""s3:DataAccessPointAccount"": ""Bucket owner's account ID"" } }
    }
  ]
} Replace ""Bucket ARN"" and ""Bucket owner's account ID"" with your bucket's specific values. Once updated, save the bucket policy to allow seamless operation of all access points. A sample bucket policy after replacing placeholders might appear as: {
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Effect"": ""Allow"",
            ""Principal"": {
                ""AWS"": [
                    ""arn:aws:s3:::kk-access-point"",
                    ""arn:aws:s3:::kk-access-point/*""
                ]
            },
            ""Action"": ""*"",
            ""Resource"": ""arn:aws:s3:kk-access-point"",
            ""Condition"": {
                ""StringEquals"": {
                    ""s3:DataAccessPointAccount"": ""bucket owner's account id""
                }
            }
        }
    ]
} After saving, the bucket now delegates management to its access points. Review the current access point settings for the bucket: Switch to the ""Permissions"" tab for the ""developers"" access point: Configuring the Access Point Policy Next, define and refine an access point policy. Refer to the AWS documentation on S3 Access Points for complete details. For instance, a sample policy allowing user John to perform GetObject operations through an access point is: {
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""AWS"": ""arn:aws:iam::123456789012:user/John""
      },
      ""Action"": ""s3:GetObject"",
      ""Resource"": ""arn:aws:s3:us-west-2:123456789012:accesspoint/my-access-point/object/John/*""
    }
  ]
} Similarly, a variant policy allowing user Jane both GetObject and PutObject operations looks like: {
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Effect"": ""Allow"",
            ""Principal"": {
                ""AWS"": ""arn:aws:iam::123456789012:user/jane""
            },
            ""Action"": [
                ""s3:GetObject"",
                ""s3:PutObject""
            ],
            ""Resource"": ""arn:aws:s3:::us-west-2:access-point:my-access-point:object/jane/*""
        }
    ]
} For this demo, modify the policy for the developers access point (user two). For example, allowing user two to list, get, and put objects: {
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""AWS"": ""arn:aws:iam::184186092733:user/user2""
      },
      ""Action"": [
        ""s3:GetObject"",
        ""s3:PutObject"",
        ""s3:ListBucket""
      ],
      ""Resource"": [
        ""arn:aws:s3:us-east-1:184186092733:accesspoint/developers/*""
      ]
    }
  ]
} Remember, the access point ARN format is: arn:aws:s3:<region>:<account-id>:accesspoint/<access-point-name> To permit access to all objects, include the object path by appending a slash and a wildcard. For example: {
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Effect"": ""Allow"",
            ""Principal"": {
                ""AWS"": ""arn:aws:iam::841860927337:user/user2""
            },
            ""Action"": [
                ""s3:GetObject"",
                ""s3:PutObject"",
                ""s3:ListBucket""
            ],
            ""Resource"": [
                ""arn:aws:s3:us-east-1:841860927337:accesspoint/developers/object/*"",
                ""arn:aws:s3:us-east-1:841860927337:accesspoint/developers""
            ]
        }
    ]
} This policy permits user two to list bucket contents and perform object-level operations via the ""developers"" access point. Test the configuration by listing contents using the access point ARN: aws s3 ls s3://arn:aws:s3:us-east-1:841860927337:accesspoint/developers You should see ""beach.jpg"" in the listing. To copy the file locally, run: [cloudshell-user@ip-10-2-30-244 ~]$ aws s3 cp s3://arn:aws:s3:us-east-1:841860927337:accesspoint/developers/beach.jpg .
download: s3://arn:aws:s3:us-east-1:841860927337:accesspoint/developers/beach.jpg to ./beach.jpg This confirms that operations through the access point work identically to direct bucket operations. Configuring the Finance Access Point A similar process applies for the finance team (user three). Start by copying and adjusting the developers' policy for the finance access point. Update the principal for user three accordingly. For example, a sample policy for the finance access point might look like: {
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Effect"": ""Allow"",
            ""Principal"": {
                ""AWS"": ""arn:aws:iam::184186092733:user/users""
            },
            ""Action"": [
                ""s3:GetObject"",
                ""s3:PutObject""
            ],
            ""Resource"": [
                ""arn:aws:s3:us-east-1:184186092733:accesspoint/developer/objects/*"",
                ""arn:aws:s3:us-east-1:184186092733:accesspoint/developers""
            ]
        }
    ]
} After updating the policy to reference the correct access point (e.g., ""finance"") and account details, save the changes. Test by listing contents and copying the file through the finance access point: [cloudshell-user@ip-10-14-113-192 ~]$ aws s3 ls s3://arn:aws:s3:us-east-1:841860927337:accesspoint/finance
2023-03-07 07:39:25  2879431 beach.jpg
[cloudshell-user@ip-10-14-113-192 ~]$ aws s3 cp s3://arn:aws:s3:us-east-1:841860927337:accesspoint/finance/beach.jpg .
download: s3://arn:aws:s3:us-east-1:841860927337:accesspoint/finance/beach.jpg to ./beach.jpg You can also test uploading a new file (e.g., “test1”) to confirm that the permissions are correctly applied through the access point. Conclusion This lesson demonstrated how to create and configure S3 Access Points to delegate and manage access policies for different user groups. By assigning separate access point policies rather than relying solely on a bucket policy, you gain granular control of S3 operations. Both the developers (user two) and finance (user three) teams can interact with the same S3 bucket using their dedicated access points, simplifying access management and enhancing security. Using S3 Access Points not only streamlines policy administration but also provides each team with a tailored “window” into the underlying S3 bucket for optimized and secure data access. Watch Video Watch video content Practice Lab Practice lab"
AWS Solutions Architect Associate Certification,S3 Pres Signed URLs,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/S3-Pres-Signed-URLs,"AWS Solutions Architect Associate Certification Services Storage S3 Pres Signed URLs This article explores a powerful AWS S3 feature called pre-signed URLs and explains how they provide secure, time-limited access to private S3 objects. Motivation and Use Case Imagine you have an AWS account with a private S3 bucket. As an authenticated IAM user with the necessary permissions, you can retrieve or upload objects to that bucket. However, if you need to share a specific file with someone who does not have an AWS account, the bucket's private settings prevent public access. There are a couple of approaches to manage this situation: Provide the public user with AWS account credentials. This approach is not scalable, particularly in an organizational context. Make the bucket public, which exposes all files to everyone—a clearly undesired outcome if you only wish to share select files. This is where pre-signed URLs shine. They allow you to grant temporary, secured access to a private S3 bucket on a per-user basis by embedding your authentication credentials directly into the URL. How Pre-Signed URLs Work Even though your S3 bucket remains private, you as an authenticated IAM user can generate a pre-signed URL using an API call to S3. The URL incorporates your full credentials, making S3 believe the request is coming from you. Sharing this URL with a public user enables them to access the specific object without opening up your entire bucket. Key Insight Only the holder of the pre-signed URL can access the specific content, based on the embedded credentials. Unintended access is prevented, ensuring secure sharing. Real-World Use Case: Video Streaming Consider a video hosting website similar to Netflix, where a vast library of videos is stored in an S3 bucket. Instead of hosting videos on a web server, you can significantly reduce server load and storage costs by leveraging S3. When a paying customer requests a video, the server generates a unique pre-signed URL for that particular video (for instance, tied to UserX) and returns it to the customer. S3 then processes the request using the permissions of UserX, enabling the video to be streamed directly. Conversely, non-paying or unauthenticated users cannot access these videos because they do not receive a valid pre-signed URL. Pre-Signed URLs for Uploads Pre-signed URLs are not limited to downloads; they are equally effective for uploads. For example, when users update their profile pictures on a website, traditionally the image would be sent to an API hosted on an EC2 instance, which then forwards the file to the S3 bucket, increasing backend load. Using a pre-signed URL, the workflow simplifies: once the API server generates and returns the URL, the user uploads the file directly to S3. This approach bypasses the backend server, thereby reducing load and improving performance. Expiration and Permissions When creating a pre-signed URL, you must specify an expiration time to limit its validity. Typically, when using an IAM user's credentials, the maximum expiration period is seven days. This duration can be adjusted based on the requirements of your application. Important Remember, a pre-signed URL does not grant new permissions. It merely allows a request to be executed using the IAM user's current permissions. If the user lacks access to a particular object, then any pre-signed URL they generate will also fail to access it. Summary Pre-signed URLs offer secure, time-limited access to objects within a private S3 bucket by embedding authentication credentials into the URL. When the URL is accessed, the request is executed using the permissions of the IAM user who originally generated it. If that user lacks access to a specific object, the URL will also be invalid for accessing that object. Understanding these concepts is essential for integrating pre-signed URLs into your AWS architecture, whether you're handling secure downloads or enabling direct-to-S3 uploads. For further information on AWS S3 and related topics, consider exploring: AWS S3 Documentation AWS Security Best Practices Understanding IAM Policies Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Storage Gateway,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/Storage-Gateway,"AWS Solutions Architect Associate Certification Services Storage Storage Gateway This article provides an in-depth overview of AWS Storage Gateway, a hybrid cloud storage service that seamlessly integrates your on-premises environment with AWS Cloud storage. With scalable storage resources, AWS Storage Gateway bridges the gap between local applications and the cloud, offering solutions for extending storage capacity, facilitating gradual migration, enabling cost-effective backups, and supporting disaster recovery. Key Benefits Extend your on-premises storage capacity without expensive hardware upgrades. Simplify data migration by replicating on-premises data to the cloud. Reduce backup costs using AWS storage solutions. Enhance disaster recovery with rapid data replication to AWS. AWS Storage Gateway can be deployed either as a virtual machine or as a physical appliance in your data center. Depending on your storage protocols, you can choose between three modes: Volume, File, and Tape. For instance, if you use iSCSI for block storage in your network-attached storage (NAS), the Volume Gateway is the ideal choice, whereas NFS or SMB protocols are best suited for the File Gateway. Below, we delve into the specifics of each operating mode, starting with the Volume Gateway. Volume Gateway The Volume Gateway operates in two sub-modes: Stored Mode and Cached Mode. Both modes present block storage volumes over iSCSI to your servers, but they differ in where the primary data storage occurs. Volume Stored Mode In Volume Stored Mode, the Storage Gateway appliance is installed in your on-premises data center and connected to your servers via iSCSI, much like a traditional NAS system. In this configuration, the appliance stores data locally on its physical disks before asynchronously replicating it to AWS S3 as EBS snapshots. These snapshots can later be used to create EBS volumes, making them ideal for backup and disaster recovery scenarios. Key features of Volume Stored Mode include: Local storage of data on the appliance’s physical disks. Asynchronous replication of data to AWS S3 in the form of EBS snapshots. Simplified backup and disaster recovery; however, on-premises storage capacity remains limited to the appliance's physical disks. Volume Cached Mode Volume Cached Mode is designed to extend your on-premises storage using the virtually unlimited capacity of AWS Cloud storage. In this mode, the Storage Gateway appliance does not maintain a full copy of your data on-premises. Instead, it stores frequently accessed data locally as a cache while keeping the entire data set in AWS S3. This configuration not only reduces your dependency on local storage capacity but also improves performance by keeping hot data close to your applications. Advantages of Volume Cached Mode: Elimination of the need for large on-premises storage infrastructure. Fast access to frequently used data through local caching. Seamless extension of your data center capacity by leveraging AWS S3. File Gateway The File Gateway provides an optimized solution for file-based storage, allowing servers to connect using standard protocols such as NFS or SMB. Deployed as either a virtual machine or a physical appliance, the File Gateway functions as a file server in your on-premises environment. When a server writes a file (for example, /media/pic1.jpg), the gateway does not store the file locally apart from minimal caching for performance. Instead, it converts the file into an object stored in AWS S3, with the object's key reflecting its original file system path. This approach offers you virtually unlimited storage capacity while maintaining a simple, file-based interface. Tape Gateway For organizations that have traditionally relied on physical tape-based backup systems, the Tape Gateway offers a modern alternative. This mode emulates a tape backup system using the Storage Gateway appliance, which communicates over iSCSI to produce virtual tape libraries (VTL) within AWS S3. When data is written via the Tape Gateway, it behaves much like a traditional tape library, enabling you to maintain familiarity with legacy backup processes. Over time, data can be migrated from the VTL to a Virtual Tape Shelf that utilizes AWS Glacier for long-term archival storage. Virtual tapes in the VTL can range from 100GB to 5TB in size. The primary differences between File and Tape Gateways are: File Gateway uses NFS/SMB protocols for direct file storage, converting files into objects stored in AWS S3. Tape Gateway emulates a traditional tape library over iSCSI, storing backup data in a VTL and archiving it to AWS Glacier for long-term retention. In summary, AWS Storage Gateway offers versatile modes—Volume (Stored and Cached), File, and Tape—to meet diverse storage needs such as backup, data migration, capacity extension, and disaster recovery. Choose the configuration that best aligns with your current storage protocols and operational requirements to harness the benefits of a hybrid cloud storage solution. For further reading on AWS Storage Gateway and other hybrid cloud solutions, check out the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,S3 Storage Classes,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/S3-Storage-Classes,"AWS Solutions Architect Associate Certification Services Storage S3 Storage Classes In this article, we explore the various AWS S3 storage classes and explain how each one is optimized for different access patterns, resiliency requirements, and cost considerations. AWS offers a range of storage classes so that you can avoid overpaying by selecting the most suitable option based on your data access frequency and performance needs. Everyone has unique data storage requirements, whether for large-scale data or less frequently accessed information. AWS addresses these needs by providing multiple pricing schemes and performance features tailored to distinct usage patterns. S3 Standard S3 Standard is the default storage class for AWS S3. When you upload a file without specifying a storage class, it is automatically stored as S3 Standard. In this class, objects are automatically replicated across at least three Availability Zones (AZs), enabling the system to survive up to two simultaneous AZ failures. This design ensures an impressive durability of 99.999999999% (11 nines). Key features of S3 Standard include: Billed per gigabyte per month. Low latency access with files available in milliseconds. Support for public access (ideal for web application media files). Charges for data egress; uploading data (ingress) is free. S3 Standard-IA (Infrequent Access) S3 Standard-IA is designed for data that is accessed less frequently while still requiring rapid access when needed. Similar to S3 Standard, it replicates data across at least three Availability Zones to ensure high durability. However, it offers a lower storage cost for infrequently accessed data. Consider these important points for S3 Standard-IA: Lower per gigabyte storage rates compared to S3 Standard. Maintains low latency for rapid data access. Applies a retrieval fee on each data request along with a per-gigabyte egress charge. Enforces a minimum duration charge of 30 days per object. If an object is deleted before 30 days, you are still charged for the full period. Uses a minimum object size charge of 128 kilobytes (even a 1 KB file is billed as 128 KB). S3 One Zone-IA (One Zone Infrequent Access) S3 One Zone-IA is similar to S3 Standard-IA, but stores data in a single Availability Zone instead of multiple zones, resulting in lower storage costs. This makes it an attractive option for data that is infrequently accessed and does not require the added resiliency of multiple AZ replication. Key details about S3 One Zone-IA: Provides immediate, low latency access. Supports public access similar to other S3 storage classes. Applies charges for egress, retrieval, a minimum duration of 30 days, and a minimum size charge of 128 kilobytes. Lacks multi-AZ replication, meaning that in the event of an AZ failure, your data could become inaccessible. Warning Since S3 One Zone-IA stores data in only one Availability Zone, consider the risk of data unavailability during an AZ outage. S3 Glacier Instant S3 Glacier Instant is part of the Glacier family and is designed for archiving data that is rarely accessed while providing rapid data retrieval. It maintains high durability through replication across multiple Availability Zones, similar to S3 Standard. Key features of S3 Glacier Instant include: Low-cost storage optimized for archival data. Retrieval of data in milliseconds. Lower per gigabyte charges compared to S3 Standard. Data egress fees and additional retrieval fees apply. A minimum duration charge of 90 days and a minimum size charge of 128 kilobytes per object. S3 Glacier Flexible S3 Glacier Flexible is intended for data that is archived and does not require immediate access. Unlike S3 Glacier Instant, objects stored in Glacier Flexible are not available instantly; a retrieval process similar to a ""cold start"" is necessary. Consequently, direct public access to these objects is not supported. Important details for S3 Glacier Flexible: Lower monthly storage costs compared to S3 Standard-IA. Retrieval fees and a minimum duration charge of 90 days apply. Enforces a minimum size charge of 40 kilobytes per object. Offers three retrieval options based on speed: Expedited (1–5 minutes) Standard (3–5 hours) Bulk (5–12 hours) During the retrieval process, objects are temporarily stored in S3 Standard-IA. S3 Glacier Deep Archive S3 Glacier Deep Archive provides the most cost-effective storage solution available for AWS S3. Like Glacier Flexible, objects stored in this class are not immediately available and must go through a retrieval process. This class is ideal for data that is rarely needed and can tolerate longer retrieval times. Key Characteristics: Lowest storage cost among all S3 storage classes. Charges include a per-gigabyte egress fee and a retrieval fee. A minimum duration charge of 180 days applies, along with a minimum size charge of 40 kilobytes per object. Retrieval options include: Standard (within 12 hours) Bulk (within 48 hours; the more cost-effective choice if immediate access isn’t critical) Retrieved data is temporarily stored in S3 Standard-IA. S3 Intelligent Tiering S3 Intelligent Tiering is a smart storage option for workloads with unpredictable access patterns. This class automatically moves data between tiers to optimize storage costs based on changing usage patterns. An additional monitoring and automation fee is applied per 1,000 objects alongside the underlying storage cost. Learn more about AWS S3 Intelligent Tiering . Choosing the Right Storage Class Selecting the appropriate storage class depends on your performance requirements and budget considerations. Use the following decision-making process to choose the correct AWS S3 storage class: Determine if immediate access (within milliseconds) is necessary. If yes, consider: Frequent access: Choose S3 Standard. Infrequent access: Multi-AZ resilience needed: Choose S3 Standard-IA. Single-AZ storage is sufficient: Choose S3 One Zone-IA. If immediate access is not required: For very rarely accessed data: Choose S3 Glacier Deep Archive. For archival data with occasional access needs: Choose S3 Glacier Flexible. Note To specify a storage class for an S3 object, include the x-amz-storage-class request header when uploading the object. In summary, AWS S3 storage classes provide a variety of options to help you optimize costs while ensuring that data access and durability requirements are met. Choosing the right storage class avoids unnecessary expenses and maintains the performance specifications needed for your applications. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,S3 Overview,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/S3-Overview,"AWS Solutions Architect Associate Certification Services Storage S3 Overview S3 (Simple Storage Service) is a highly scalable, available, secure, and high-performance object storage service provided by Amazon Web Services (AWS). Despite its name including three S's, it is primarily recognized as S3, a robust solution for storing and managing your data in the cloud. In simple terms, S3 functions like a cloud-based storage service similar to Dropbox or Google Drive, but with the extensive capabilities and integration of AWS. As a core component of the AWS ecosystem, S3 works seamlessly with services like EC2, Lambda, and IAM. This integration allows you to control access via robust IAM policies and roles. Additionally, S3 can be managed through various interfaces, including the AWS Management Console, CLI, SDK, or REST API—providing versatility regardless of your preferred method of interaction. S3 is an object-based storage system, meaning it stores complete objects (files) with their metadata. Rather than using a traditional directory structure, S3 employs a flat namespace. The folder-like view in the AWS console is simply a visual convenience created by using prefixes in object names. In summary, S3 is designed exclusively for object storage. Use Amazon's Elastic File System (EFS) for file-based storage and Amazon Elastic Block Store (EBS) for block storage. Common Use Cases for S3 S3 is ideal for storing: Application log files Media assets such as images, videos, and audio files Artifacts generated from CI/CD pipelines Note S3's versatility makes it suitable for a broad range of deployment architectures beyond the typical use cases mentioned above. S3 in a Web Application Deployment Consider a traditional web application hosting scenario where a web server stores HTML, CSS, JavaScript, and media files. For media-intensive sites such as YouTube or Netflix, storing all assets on the same server can become inefficient and costly. A common deployment strategy with S3 involves: Hosting HTML, CSS, and JavaScript files on the web server. Storing media files (videos, images, audio) in an S3 bucket. Embedding URLs in the HTML that point directly to media stored in S3. Offloading media requests to S3, which ensures efficient, scalable, and cost-effective delivery of large data volumes. This architecture reduces the load on your web server and leverages S3's superior storage capabilities. Key Terminology and Concepts Buckets Buckets are the containers in S3 where objects (files) are stored. Think of a bucket as a folder that groups related files. You can create multiple buckets to organize data by application, use case, or data type (e.g., one for logs and another for media). Objects An object in S3 consists of the file data and its metadata. Each object includes: A key: a unique name for the file. A value: the actual data of the file. Additional properties such as version ID and metadata (if versioning is enabled). Flat File Structure S3 utilizes a flat file structure, meaning there is no inherent concept of nested directories. While the AWS console may visually display folder-like groupings using prefixes, the underlying storage remains flat. For example, uploading files with keys like ""music/song_one"", ""music/song_two"", and ""music/song_three"" creates the illusion of a ""music"" folder. Data Durability and Availability When you upload a file to S3, AWS replicates it across multiple servers and availability zones. This replication ensures high durability and availability, protecting your data even if a server or entire availability zone fails. Unique Bucket Names Each S3 bucket must have a globally unique name across all AWS accounts. This name is part of the bucket's URL. If the chosen name is already in use, you must select a different one. S3 Restrictions When using S3, keep the following restrictions in mind: S3 can store an unlimited number of objects. The maximum size for a single object is 5 terabytes. By default, each AWS account can create up to 100 buckets. This limit can be increased up to 1,000 by requesting a service limit increase. Warning Always ensure that your bucket names are unique and comply with AWS naming conventions to avoid conflicts during deployment. Conclusion S3 offers unmatched scalability, data availability, security, and performance as an object storage service. Its design is ideal for use cases such as storing media files, log files, CI/CD artifacts, and hosting static websites. Remember, S3 employs a flat file structure where files are stored as objects with keys, values, and optional metadata rather than through traditional directories. Effective management of S3 buckets is essential. With globally unique bucket names and the ability to handle an unlimited number of objects (up to 5 TB per object), S3 is engineered for large-scale storage and high performance. Enhanced features such as multi-part uploads further optimize the handling of large files. This article provided an in-depth overview of AWS S3, covering its fundamental concepts, common use cases, and key restrictions. Understanding these principles is crucial for anyone working with AWS or preparing for the AWS Solutions Architect exam. For more detailed insights into AWS services, consider exploring the AWS Documentation and Kubernetes Basics . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,S3 Versioning,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/S3-Versioning,"AWS Solutions Architect Associate Certification Services Storage S3 Versioning Amazon S3 versioning provides a powerful mechanism to safeguard your data by preserving, retrieving, and restoring different versions of your objects. This guide explains how versioning works within S3, its benefits, and the best practices to manage your data safely. In S3, versioning is disabled by default. Consider an S3 bucket containing five files: file1, file2, file3, file4, and file5. When versioning is disabled, if you delete file1, it is permanently removed and cannot be recovered. Moreover, uploading a new file with the same key (e.g., file5.txt) will permanently overwrite the existing object, leading to potential data loss. Tip Enabling versioning mitigates risks of accidental deletion or unwanted overwrites by preserving older versions of your objects. Versioning is a bucket-level setting—it applies to every object stored in the bucket. A bucket can be in one of three states: Unversioned (Default State) – Versioning is disabled. Versioning Enabled – New versions of objects are recorded. Versioning Suspended – Existing versions are kept, but new uploads do not receive version IDs. Once versioning is activated, you cannot disable it; you can only suspend it. When suspended, all existing object versions are maintained, but new uploads will have a null version ID, functioning as if versioning is turned off. How Versioning Works Under the Hood When versioning is enabled, each uploaded object is assigned a unique version ID. For example, if the first version of an object is assigned ""1"" (in practice, S3 uses a long unique string), subsequent uploads with the same key create new versions with distinct IDs (e.g., ""2"", ""3"", etc.). The most recent version is always considered the current version. If you access the object without specifying a version ID, the latest version is returned. Within the S3 console, object uploads such as file1.txt are listed with their corresponding version IDs and metadata, including modification dates. This allows you to track changes over time. If versioning is disabled, the version ID associated with each object remains null. Deleting Objects with Versioning When you delete an object without specifying a version ID while versioning is enabled, S3 adds a ""delete marker"" instead of permanently removing older versions. This delete marker makes it appear as if the file is deleted, while previous versions remain intact. Removing the delete marker in the S3 console will restore the most recent previous version as the current version. Alternatively, if you delete a specific version by specifying its version ID (for example, version ID 2 of file1.txt), that version is permanently removed, and the latest available version subsequently becomes the current version. Versioning and Pricing Keep in mind that when versioning is enabled, you are billed for each stored version of an object. For example, if file1.txt comprises two versions (one of 10 GB and another of 15 GB), you will incur charges for a cumulative 25 GB. This reinforces the importance of periodically cleaning up outdated versions, especially for large files. Suspended Versioning When you suspend versioning, the following behavior applies: All previous versions remain stored in the bucket. New uploads receive a null version ID, effectively behaving as if versioning is disabled. If you upload a new object with an existing key, it permanently replaces the current version while preserving the historical versions. MFA Delete MFA Delete adds an extra layer of security to your bucket's versioning operations. When enabled, any changes to the bucket's versioning configuration or deletions of specific versions require multi-factor authentication (MFA). This feature ensures that such critical actions are executed only with proper verification. Note that MFA Delete can only be enabled using the AWS CLI . Summary Versioning in S3 lets you protect your data against accidental modifications and deletions by maintaining historical versions of your objects. Key points include: Default State : Versioning is disabled by default and must be explicitly enabled at the bucket level. Irreversible Activation : Once enabled, versioning cannot be completely turned off; it can only be suspended. Storage Costs : All versions of an object are stored and billed, so it's important to manage versions to optimize costs. Deletion Mechanics : Deleting an object without specifying a version ID adds a delete marker rather than erasing previous versions. Deleting a specified version permanently removes that version. Enhanced Security : MFA Delete requires multi-factor authentication to change versioning settings or delete versions, adding an extra layer of security. By understanding and implementing S3 versioning, you can ensure robust data protection against accidental deletions and overwrites while maintaining control over your storage costs through efficient version management. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,S3 Pres Signed URLs Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/S3-Pres-Signed-URLs-Demo,"AWS Solutions Architect Associate Certification Services Storage S3 Pres Signed URLs Demo In this lesson, we demonstrate how pre-signed URLs work with Amazon S3 . Pre-signed URLs allow you to grant temporary access to private S3 objects without making them public, ensuring controlled and secure sharing. Creating the Bucket Begin by navigating to the S3 console and selecting ""Create bucket."" Configure the bucket with default settings and ensure that public access is blocked. This setup guarantees that only you (as the root user) or users with explicit permissions can access your bucket. After the bucket is created, you will see a bucket similar to the one displayed below. Notice that both the bucket and its objects remain private. Uploading an Object Next, open your pre-signed demo bucket and upload an object. Authenticated users can view the object normally, but unauthenticated users will encounter an ""Access Denied"" error when trying to access it. A quick review of the bucket permissions confirms that public access is blocked and no policies allow anonymous access. Generating a Pre-Signed URL Imagine you want to share this image with a friend who does not have an AWS account—without making the image publicly available. Pre-signed URLs serve this purpose perfectly. To generate one, follow these steps: Navigate to the object's details page and click ""Share with a pre-signed URL."" Specify the duration for which the URL will remain active (for example, 30 minutes). Generate the pre-signed URL, which will be copied to your clipboard. If it isn’t automatically copied, click the URL to copy it manually. When accessed in your browser, the URL includes all necessary authentication data, allowing temporary access to the object for the specified period (30 minutes, in this case). Even though the URL can be used by anyone with the link during this timeframe, the object remains private for any access attempts outside the allowed window. Programmatic URL Generation While the AWS Console offers a simple method for generating pre-signed URLs, production environments typically generate them programmatically using the AWS SDK or AWS CLI. Demonstrating User Permissions and URL Behavior Next, let's explore the behavior of pre-signed URLs when generated by a user with limited permissions. Return to your original account and open the IAM console. Here, you will see another account named user2. User2 has been granted a policy that allows bucket listing but restricts object retrieval and deletion. Below is the policy applied to user2: {
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Sid"": ""VisualEditor0"",
      ""Effect"": ""Allow"",
      ""Action"": [
        ""s3:ListAllMyBuckets"",
        ""s3:ListBucket""
      ],
      ""Resource"": ""*""
    }
  ]
} This policy permits user2 to list all buckets and enumerate the contents of the pre-signed demo bucket. However, when user2 attempts to open an object, access is denied because the policy does not include the ""s3:GetObject"" permission. Even with limited permissions, user2 can still generate a pre-signed URL for the object. If they share this URL, any public attempt to access the object will also result in an ""Access Denied"" error. This is because the pre-signed URL only operates under the permissions of the user who generated it. Since user2 lacks sufficient permission to retrieve the object, the URL cannot override this restriction. Below is an example error response when attempting to access the object via a pre-signed URL generated by a user without the proper permissions: <Error>
    <Code>AccessDenied</Code>
    <Message>Access Denied</Message>
    <RequestId>348F7B3F19D916A7V</RequestId>
    <HostId>AHmIbdweQpZyPcoVo2t5fMcp4jSywbWtlAkYdLXoADepHMfM1v1Zk9aCKBW0=</HostId>
</Error> Important Security Note Remember: A pre-signed URL does not grant additional privileges beyond those allowed by the generating user's IAM permissions. Conclusion Pre-signed URLs offer a secure and temporary method for granting access to private objects stored in Amazon S3. The essential point to remember is that the URL inherits the permissions of the user who created it. Even if the URL is shared publicly, it cannot bypass the underlying IAM permissions. That’s all for this lesson on pre-signed URLs. For further details on AWS best practices and permissions, consider exploring additional Amazon S3 documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,S3 Versioning Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/S3-Versioning-Demo,"AWS Solutions Architect Associate Certification Services Storage S3 Versioning Demo In this tutorial, we'll explore how versioning works in Amazon S3. First, we'll create an S3 bucket with versioning disabled to review the default behavior. Later, we'll enable versioning and compare the differences. 1. Creating the Bucket with Versioning Disabled Begin by navigating to the AWS S3 console and create a new bucket. For this example, name the bucket ""versioning demo"" and retain all default settings. Ensure that bucket versioning is set to disabled. 2. Uploading the First Version of the File Open the newly created bucket and upload a dummy text file named ""file1.txt"" containing the following content: this is version 1 This simple content makes it easy to track changes in later versions. After uploading the file, click ""Close"". Once uploaded, open the file from the bucket to verify it displays ""this is version 1"". Next, select the file and attempt to delete it. Since versioning is disabled, Amazon S3 will prompt you to permanently delete the file: Warning When versioning is disabled, deleting a file permanently removes it from your bucket with no recovery option. 3. Re-uploading and Overwriting the File without Versioning To continue the demonstration, re-upload ""file1.txt"" ensuring it still contains ""this is version 1"". Then, modify the file content to the following: this is version 2 When you upload the updated file using the same key, Amazon S3 overwrites the existing file. Opening ""file1.txt"" will now display ""this is version 2"". Note that the original version is lost because versioning is not enabled. 4. Enabling Bucket Versioning Now, let's enable versioning for the bucket. Navigate to the bucket's properties, select ""Versioning"", and click ""Edit"". Then, enable versioning and save your changes. The interface will now display a ""show versions"" option when you view your bucket's contents. After enabling versioning, upload ""file1.txt"" again. Even though the UI looks similar, the file now receives a version ID. Open the file to confirm it contains ""this is version 1"". Then, update the file content to: this is version 2 Upload the file using the same key. Click on ""show versions"" to reveal both versions. The older version will have an earlier timestamp, while the current version shows the updated content. Review both file versions by opening them, ensuring that each version retains its specific content. 5. Adding Additional Versions Next, update ""file1.txt"" again by changing its content to: this is version 3 Upload the file, and the bucket will now contain three versions, with the most recent version reflecting the update. With versioning enabled, each distinct version is preserved using its unique version ID. When you delete a file now, Amazon S3 places a delete marker instead of permanently removing the object. Select the file and click ""Delete"". Notice that the prompt simply asks for confirmation without requiring ""permanently delete"": After deletion, the file might not be visible until you enable ""show versions"". Then all versions, including the delete marker, can be viewed. To restore the file, simply delete the delete marker: You can also permanently delete individual versions. For example, permanently removing version two will leave only versions one and three in your bucket. 6. Suspending Bucket Versioning Once enabled, versioning cannot be fully disabled; it can only be suspended. To suspend versioning, go to the bucket properties and edit the versioning settings: When versioning is suspended, existing versions remain intact. However, new uploads for an existing key will receive a version ID of ""null"". Upload an updated ""file1.txt"" with the following content: this is version 4 Even though versioning is suspended, you can still view all historical versions. The newly uploaded file will have a version ID of ""null"". For further demonstration, upload another update with: this is version 5 The bucket continues using ""null"" as the version ID for subsequent changes. This behavior indicates that only one live version is maintained for each key, although previous versions are still stored. If required, you can manually delete older versions. 7. Demonstrating Versioning with a New Object Key To illustrate versioning behavior for a new object key, create and upload a new file called ""file2.txt"" with the initial content: this is version 5 After uploading ""file2.txt"", enabling ""show versions"" will display that the file has a version ID of ""null"". Updating the file (for example, changing the content to ""this is version 2"") will overwrite the existing object, and the new upload will also have a version ID of ""null"" because versioning is suspended. 8. Multi-Factor Authentication (MFA) Delete Another key feature is MFA delete. When enabled in the versioning configuration, MFA delete requires multi-factor authentication to make changes to the versioning status. Note that MFA delete can only be enabled via the AWS CLI or SDK, not through the console. For detailed setup instructions, please refer to the AWS Documentation . Note MFA delete adds an additional layer of security by ensuring that changes to your versioning configuration require multi-factor authentication. 9. Cleaning Up: Deleting Objects and the Bucket After completing the demonstration, it's important to clean up. Delete the objects and then the bucket. When you delete objects with multiple versions, ensure that you review all versions and effectively remove them as needed. Finally, delete the bucket to finish the cleanup process. This concludes our detailed demonstration of Amazon S3 versioning. Happy cloud computing! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,S3 Static Website Hosting,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/S3-Static-Website-Hosting,"AWS Solutions Architect Associate Certification Services Storage S3 Static Website Hosting In this lesson, we explore how to host a static website on Amazon S3. With this service, you can transform an S3 bucket into a web server that serves static files such as HTML, CSS, JavaScript, and media assets. What Is a Website? When a user enters a URL in a browser, an HTTP GET request is sent to a web server. The server then responds with an HTML file that the browser renders as a website. While an HTML file forms the core of any website, its look and functionality are enhanced by additional components: HTML : Provides the document structure and content. CSS : Improves visual styling with colors, layouts, and fonts. JavaScript : Adds dynamic behavior and interactivity. Assets : Includes images, videos, audio, and other media files. All these elements can be stored in an S3 bucket. Since S3 is designed for object storage, it can efficiently house and serve all files necessary for a static website. Enabling Static Website Hosting on S3 Amazon S3 supports static website hosting by offering a configuration option to serve all files (HTML, CSS, and JavaScript) over HTTP. Once you enable this option, S3 provides you with a URL to access your website. Note Static website hosting on S3 is ideal for sites without server-side processing. If you require dynamic functionality, consider services like Amazon EC2 , Amazon ECS , or AWS Lambda . Warning If you plan to use a custom domain (e.g., bestcars.com), ensure that your S3 bucket is named exactly as your custom domain. Pricing Considerations When hosting a static website on S3, you incur two main types of costs: Storage and Data Transfer Fees : Charged per gigabyte stored and for data transferred out. HTTP Request Fees : A minor fee is applied for each HTTP request, such as GET operations. For instance, if you're using S3 Standard Infrequent Access (IA) and reviewing GET request pricing, you might pay around $0.0004 per thousand GET requests. It’s essential to consider these costs as part of your overall budget for hosting your static website on AWS. Accessing Your Static Website After enabling static website hosting, S3 generates a URL you can use to access your site. The URL follows this format: http://bucketname.s3-website-<region-name>.amazonaws.com Replace ""bucketname"" with your actual S3 bucket name and ""<region-name>"" with the AWS region where your bucket is hosted. By simply visiting this URL, users can access the hosted static website. If you prefer to use a custom domain, you can integrate Amazon Route 53 with your S3 bucket to map your domain (for example, http://bestcars.com) directly to your hosted site. Summary S3 for Static Content : Ideal for hosting static files such as HTML, CSS, JavaScript, and media. Cost Factors : Incur charges for storage, data transfer, and HTTP requests. URL Structure : When enabled, S3 provides a URL in the format: http://bucketname.s3-website-<region-name>.amazonaws.com Custom Domain Setup : To use your own domain (e.g., example.com), ensure your S3 bucket's name matches your domain name exactly. In summary, Amazon S3 static website hosting provides a cost-effective and scalable solution for serving static content. By understanding its configuration options, pricing models, and domain requirements, you can efficiently host and manage your static website on AWS. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Elastic Disaster Recovery,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/Elastic-Disaster-Recovery,"AWS Solutions Architect Associate Certification Services Storage Elastic Disaster Recovery Elastic Disaster Recovery (DRS) is a fully managed solution that ensures fast, reliable recovery for both on-premise and cloud-based applications. This service leverages affordable storage options, minimal compute resources, and point-in-time recovery to simplify complex disaster recovery processes. Key Benefit Elastic Disaster Recovery minimizes capital expenditure by eliminating the need for an extra on-premise backup environment. Instead of maintaining redundant infrastructure and paying for idle servers, costs are incurred only when resources are actively used. Elastic Disaster Recovery manages disaster recovery for physical, virtual, and cloud servers. Traditionally, disaster recovery setups require an additional backup environment—which means provisioning extra servers, renting extra data center space, and shouldering continuous maintenance. With Elastic Disaster Recovery, AWS handles these complexities, letting you use its robust infrastructure as a recovery site. The service continuously replicates your operating system, applications, and databases with 24/7 monitoring. This constant replication ensures that during a disaster, a simple click deploys an up-to-date recovery infrastructure on AWS. When a disaster strikes, notify AWS and witness your recovery resources come online within minutes. Elastic Disaster Recovery supports various failover scenarios including: On-premises to AWS One cloud platform to AWS One AWS region to another How Elastic Disaster Recovery Works Identifying Source Servers Determine the source servers whose data needs protection. Install the AWS replication agent on these servers to enable continuous monitoring and replication. Configuring Replication Settings Set up the replication process by defining a staging area—a dedicated subnet in AWS where EC2 instances capture and archive your replicated data. This process involves three key components: Component Description On-premises System The environment where the replication agent collects server data. Staging Area The AWS subnet that receives and archives the replicated data. Production Subnet The AWS subnet where recovery servers are launched during a failover. Defining Launch Settings Configure the recovery environment by defining launch settings. These settings include EC2 instance specifications such as instance types, sizes, regions, subnets, and security groups. When a disaster occurs, these parameters guide the quick deployment of recovery servers on AWS. In summary, Elastic Disaster Recovery enables organizations to leverage AWS infrastructure for disaster recovery, eliminating the need for costly, redundant on-premise systems. Its pay-as-you-go model allows rapid scaling during emergencies, ensuring business continuity with a minimal cost footprint. The essential components of Elastic Disaster Recovery include: Source Servers: On-premises servers and data slated for replication. Staging Area: AWS subnet designated for receiving and archiving replicated data. Launch Template: Predefined configurations (instance type, subnet, security groups) for recovery server deployment. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,S3 ACL and Resource Policies Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/S3-ACL-and-Resource-Policies-Demo,"AWS Solutions Architect Associate Certification Services Storage S3 ACL and Resource Policies Demo In this lesson, we explore how to control access to an S3 bucket by defining resource policies. You will learn how to grant varying permissions to different users—including users within the same AWS account, users from different accounts, and even anonymous (public) access—by configuring precise policies. Overview The demonstration involves three different user contexts, represented by distinct colored tabs: The blue tab represents Account One, User One (the bucket creator). The green tab represents Account One, User Two. The yellow tab represents Account Two, User Admin (a user from a different account). Below is the AWS Management Console home page overview: Creating the S3 Bucket Using Account One, User One (blue tab), we create a demo S3 bucket with default settings. The configuration disables legacy NACLs, versioning remains off, and public access is blocked by default. Once the bucket is created, several files are uploaded. Bucket Creation Process Creating the Bucket : A zoomed view of the bucket creation screen illustrates options for bucket name, region, object ownership, and public access settings. Post-creation and Upload : After the bucket is created, you open it to upload files. The following image confirms the successful creation of a bucket named ""kk-resource-policies"" in the US East (N. Virginia) region, with the uploaded objects remaining non-public. Verifying Access : When accessing an object while authenticated as User One, permission is granted. However, accessing the object URL publicly results in an ""Access Denied"" error. Testing IAM Policies for User Two Before switching to User Two (green tab), we review the IAM policy assigned to User Two. The policy permits only listing of all buckets and bucket contents: {
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Sid"": ""VisualEditor0"",
            ""Effect"": ""Allow"",
            ""Action"": [
                ""s3:ListAllMyBuckets"",
                ""s3:ListBucket""
            ],
            ""Resource"": ""*""
        }
    ]
} With this policy, User Two can view the bucket in the S3 console but cannot open objects directly. Trying to open a file (e.g., file1.txt) will trigger an ""Access Denied"" error. Creating a Resource Policy for Specific Folder Access Returning to User One (blue tab), we now create a resource policy that permits User Two to access only the ""logs"" folder within the bucket. Steps to Configure Folder-Specific Access Modify the Bucket Policy : Navigate to the Permissions tab of the bucket and edit the bucket policy using the AWS built-in wizard. Start with the default template shown below: {
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Sid"": ""Statement1"",
      ""Principal"": {},
      ""Effect"": ""Allow"",
      ""Action"": [],
      ""Resource"": []
    }
  ]
} Policy Implementation : Name the policy ""user2AllowLogs"" and specify the principal using the AWS ARN for User Two. For example: {
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Sid"": ""user2AllowLogs"",
      ""Principal"": {
        ""AWS"": [""arn:aws:iam::<ACCOUNT_NUMBER>:user/user2""]
      },
      ""Effect"": ""Allow"",
      ""Action"": ""s3:GetObject"",
      ""Resource"": ""arn:aws:s3:::kk-resource-policies/logs/*""
    }
  ]
} Replace <ACCOUNT_NUMBER> with the actual account number. Note This resource policy grants GET access on all objects within the ""logs"" folder, ensuring that User Two can only view the content within that directory. Verification : After saving the policy, switch to User Two (green tab). Test by navigating to the ""logs"" folder and opening a log file; the file should download successfully. Attempting access to files outside this folder (e.g., file1.txt) will correctly result in ""Access Denied."" For additional reference on S3 actions and resource policy construction, refer to the AWS S3 bucket policy documentation: Granting Additional Permissions Using a Combined Policy Next, we enhance the bucket policy to allow User Two the ability to delete objects from the ""traces"" folder, while maintaining GET access to the ""logs"" folder. Updated Combined Policy The revised bucket policy is as follows: {
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Sid"": ""userAllowLogs"",
            ""Effect"": ""Allow"",
            ""Principal"": {
                ""AWS"": ""arn:aws:iam::<ACCOUNT_NUMBER>:user/user2""
            },
            ""Action"": ""s3:GetObject"",
            ""Resource"": ""arn:aws:s3:::kk-resource-policies/logs/*""
        },
        {
            ""Sid"": ""user2AllowDelete"",
            ""Effect"": ""Allow"",
            ""Principal"": {
                ""AWS"": ""arn:aws:iam::<ACCOUNT_NUMBER>:user/user2""
            },
            ""Action"": ""s3:DeleteObject"",
            ""Resource"": ""arn:aws:s3:::kk-resource-policies/traces/*""
        }
    ]
} Note This configuration differentiates between GET and DELETE actions by applying them to specified folders using separate policy statements. After applying this policy, when User Two attempts to delete an object in the ""traces"" folder (e.g., ""trace1""), the deletion is successful, while deletion attempts in other folders are rejected. Combining Permissions in a Single Statement At times, you may wish to combine permissions for different resource types in a single statement. However, combining object-level operations (like ""s3:GetObject"") with bucket-level operations (like ""s3:DeleteBucket"") in one action can cause errors if applied incorrectly. To resolve this, list both resource ARNs in an array as demonstrated below: {
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Sid"": ""user2AllObject"",
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""AWS"": ""arn:aws:iam::<ACCOUNT_NUMBER>:user/user2""
      },
      ""Action"": [
        ""s3:GetObject"",
        ""s3:DeleteBucket""
      ],
      ""Resource"": [
        ""arn:aws:s3:::kk-resource-policies"",
        ""arn:aws:s3:::kk-resource-policies/*""
      ]
    },
    {
      ""Sid"": ""user2AllowDelete"",
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""AWS"": ""arn:aws:iam::<ACCOUNT_NUMBER>:user/user2""
      },
      ""Action"": ""s3:DeleteObject"",
      ""Resource"": ""arn:aws:s3:::kk-resource-policies/traces/*""
    }
  ]
} Warning Ensure that actions are applied to the correct resource types by segregating bucket-level and object-level operations in your policies. Allowing Public Access to Specific Folders To demonstrate controlled public access, a policy statement is added that allows anonymous GET requests for a designated folder, such as ""media."" Before doing so, disable the ""Block Public Access"" feature for the bucket as required. Public Access Policy Example {
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Sid"": ""AllowPublic"",
            ""Principal"": ""*"",
            ""Effect"": ""Allow"",
            ""Action"": ""s3:GetObject"",
            ""Resource"": ""arn:aws:s3:::kk-resource-policies/media/*""
        }
    ]
} After the policy update, objects stored in the ""media"" folder become publicly accessible via their URL, while objects outside this folder continue to be restricted. Granting Access to a User in a Different AWS Account Finally, we demonstrate how to grant permissions to an external user (Account Two, yellow tab). Initially, when the external user attempts to list the bucket contents using Cloud Shell, the output is as follows: [cloudshell-user@ip-10-6-180-191 ~]$ aws s3 ls s3://kk-resource-policies
An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied Updating the Bucket Policy for External Access To allow access, we add new policy statements to permit Account Two's ""admin"" user the ability to list the bucket and delete objects from the ""logs"" folder. The updated policy snippet is: {
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Sid"": ""AllowAccount2UserAdmin"",
      ""Principal"": {
        ""AWS"": ""arn:aws:iam::<ACCOUNT_TWO_NUMBER>:user/admin""
      },
      ""Effect"": ""Allow"",
      ""Action"": ""s3:ListBucket"",
      ""Resource"": ""arn:aws:s3:::kk-resource-policies""
    },
    {
      ""Sid"": ""AllowAccount2UserAdminDelete"",
      ""Principal"": {
        ""AWS"": ""arn:aws:iam::<ACCOUNT_TWO_NUMBER>:user/admin""
      },
      ""Effect"": ""Allow"",
      ""Action"": ""s3:DeleteObject"",
      ""Resource"": ""arn:aws:s3:::kk-resource-policies/logs/*""
    }
  ]
} Replace <ACCOUNT_TWO_NUMBER> with the appropriate account number. After saving the change, the external user is able to list the bucket contents and delete objects within the ""logs"" folder. For example, a Cloud Shell session might display: [cloudshell-user@ip-10-6-180-191 ~]$ aws s3 ls s3://kk-resource-policies
2023-04-06 08:22:04    PRE media/
2023-04-06 08:22:04    PRE traces/
2023-04-06 08:22:05       7 file1.txt
2023-04-06 08:22:05       0 file2.txt
2023-04-06 08:22:05       0 secondfile.txt Deletion of files outside the permitted folder (e.g., file1.txt) remains restricted and will return an ""Access Denied"" error. Conclusion In this lesson, we demonstrated how to: Create an S3 bucket and upload objects. Use IAM policies to restrict a user's actions. Define granular resource policies to grant GET and DELETE access to specific folders. Implement public (anonymous) access for a subset of objects. Grant permissions to users in other AWS accounts. By following these techniques, you can achieve flexible and secure access control for your S3 buckets while ensuring efficient resource management. For more details, visit the AWS Documentation and explore further use cases for resource policies. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,AWS Backup,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/AWS-Backup,"AWS Solutions Architect Associate Certification Services Storage AWS Backup Delve into disaster recovery and learn about AWS Backup—a crucial service for safeguarding your data and ensuring business continuity. This guide covers the essentials of disaster recovery, examines its benefits, and explains how various AWS services work together to create robust and cost-effective disaster recovery strategies. Understanding Disaster Recovery Disaster recovery is the systematic process of preparing for and responding to events that could result in data loss or system downtime. Disasters can be natural events like earthquakes or floods, or man-made events such as hardware failures and cyber attacks. The impact of downtime and data loss can be severe, often causing financial loss, reputational damage, and legal complications. A well-designed disaster recovery plan is vital for maintaining data integrity and ensuring continuous business operations. Backup vs. Disaster Recovery Although ""backup"" and ""disaster recovery"" are sometimes used interchangeably, they address different aspects of data protection. Backups involve creating copies of data to enable restoration after data loss. Disaster recovery encompasses backups along with the broader strategies, planning, and processes required for full system and application restoration. AWS and Disaster Recovery Amazon Web Services provides a suite of tools designed to build flexible, scalable, and cost-effective disaster recovery solutions. Two standout services are Amazon S3 and Amazon Elastic Block Store (EBS). Amazon S3 Amazon S3 offers scalable, durable, and highly available object storage, making it ideal for storing backup data. With an impressive 11 nines of data durability, S3 is engineered to handle the loss of multiple data centers. Its support for data replication between availability zones and cross-region replication makes it a reliable choice for disaster recovery. Amazon EBS and Snapshots Amazon EBS features snapshots—point-in-time copies of your EBS volumes—to safeguard your EC2 instances and data. Snapshots can be executed manually or scheduled automatically (e.g., every five minutes) to ensure continuous data protection. As these snapshots are incremental, they only capture changes since the previous snapshot, which optimizes storage usage and reduces costs. Additionally, snapshots can be used to create new EBS volumes during the recovery process. Introducing AWS Backup AWS Backup centralizes and automates data protection across multiple AWS services via a unified console. The service streamlines backup processes by automating scheduling, retention policies, and cross-region backups—significantly reducing manual effort and enhancing reliability. Key Components of AWS Backup AWS Backup revolves around three core components: Backup Vault: A secure container that stores all your backups. You can classify and organize backups by creating multiple vaults across different regions and accounts, tailoring your disaster recovery strategy to your application's specific needs. Backup Plan: This component defines the backup process, including scheduling frequency, retention policies, and the designated backup vault where the backups will be stored. Recovery Point: A recovery point marks the specific point in time from which your data can be restored. It serves as a snapshot of your data, enabling efficient and reliable recovery in the event of an incident. Example Scenario: Using AWS Backup Consider an application deployed in the US East (N. Virginia) region that consists of multiple EC2 instances, along with associated EFS and EBS volumes and a RDS instance. To implement AWS Backup effectively: Create a backup vault in the same region as your application to securely store the backups. Define a backup plan that specifies which resources (for example, resources for App One) will be backed up, along with the scheduling and retention policies. Optionally, set up another vault in a different region or AWS account and configure a copy job to transfer backups from US East 1 to US West 1. This configuration ensures that resources can be recovered in either region in case of an incident. AWS Backup Integrations and Monitoring AWS Backup integrates seamlessly with a wide range of AWS services, including EC2, EBS, EFS, and RDS, among others. To keep track of your backups and monitor events, AWS provides several useful services: Amazon EventBridge: Monitors backup events and triggers automated responses. Amazon CloudWatch: Tracks metrics, creates alarms, and provides dashboards for real-time monitoring. Amazon SNS and AWS CloudTrail: SNS delivers notifications, while CloudTrail logs and monitors AWS Backup API calls for comprehensive audit trails. Summary Disaster recovery is about preparing for and promptly responding to events that could lead to data loss or system failures. A well-crafted disaster recovery plan minimizes downtime, maintains data integrity, and supports continuous business operations. Unlike simple backups, disaster recovery encompasses a broader strategy that includes the entire system and application restoration process. AWS delivers a suite of advanced services—such as Amazon S3, EBS snapshots, and AWS Backup—to help you build a resilient disaster recovery strategy. Specifically, AWS Backup centralizes and automates the backup process using three fundamental components: the backup vault, the backup plan, and the recovery point. Together, these components provide a scalable, flexible, and reliable solution for data management and recovery in any disaster scenario. Note For more detailed information on AWS Backup and disaster recovery planning, visit the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,S3 Storage Classes Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/S3-Storage-Classes-Demo,"AWS Solutions Architect Associate Certification Services Storage S3 Storage Classes Demo This guide demonstrates how to set and modify the storage class for files in Amazon S3. You will learn how to configure the storage class during file upload and change it later if needed, ensuring your data is stored cost-effectively and in accordance with your access requirements. Uploading Files with a Custom Storage Class Create a new S3 bucket (for example, ""kk-sc-demo"") using the default settings. Once the bucket is created, click the Upload button and select a file from your computer. During the upload process, in the file properties section, notice that the default storage class is set to Standard . Tip For optimal cost management, consider selecting the One Zone Infrequent Access storage class if your data is accessed less frequently but requires rapid retrieval. Change the storage class from Standard to One Zone Infrequent Access , then upload the file. Once the upload completes, verify that the file's storage class is set to One Zone Infrequent Access . Modifying the Storage Class of an Existing Object If you need to change the storage class after the upload: Navigate to the object's properties in the S3 console. Edit the storage class setting and choose the desired option. For example, you can switch it back to Standard . Note Changing storage classes can help optimize costs and performance. Review your storage needs and access patterns to select the most appropriate option. Conclusion This tutorial has shown that setting the storage class is straightforward during the initial upload and can be easily adjusted later through the object properties in S3. Managing your storage class effectively can lead to significant cost savings and ensure that your data is stored appropriately based on its usage. For more detailed information, please visit the Amazon S3 Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,S3 Access Points,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/S3-Access-Points,"AWS Solutions Architect Associate Certification Services Storage S3 Access Points In this lesson, we dive into Amazon S3 Access Points and explore how they simplify the management of complex access policies. Access points provide dedicated “windows” into an S3 bucket, streamlining the creation and application of tailored policies for various groups, users, and roles. Consider an S3 bucket serving multiple stakeholders, such as developers, infrastructure teams, legal, and administrators. Each group may require distinct access levels; for example, developers might access specific folders, the infrastructure team may need full access, while legal may only require read-only permissions. When all these requirements are consolidated into a single bucket policy, the configuration can quickly become overly complex: To manage this complexity, you can create separate access points for each group. Each access point acts as a dedicated gateway into the S3 bucket, with its own Amazon Resource Name (ARN) and a set of finely tuned policies. For instance, when developers need access to objects in the S3 bucket, they reference the ARN of their dedicated access point rather than the bucket’s ARN. This segregation ensures that customized policies can be applied based on the specific needs of each group. The following diagram illustrates how different user roles access a central resource through dedicated access points: Note Access points not only simplify policy management but also enhance security by isolating permissions. This approach reduces the risk associated with misconfigurations typically found in complex bucket policies. Another significant benefit of access points is their capability to enforce network-level restrictions. For example, you can restrict access to the S3 bucket to specific Virtual Private Clouds (VPCs) by managing VPC endpoints via access points. This ensures that only authorized Amazon Elastic Compute Cloud (EC2) instances within the designated VPCs can access the bucket’s objects. This concept is demonstrated in the diagram below: When defining access point policies, it is important to note that the initial configuration requires the access point policy to be mirrored in the bucket policy. However, to streamline management and reduce redundancy, you can modify the bucket policy to delegate access control directly to the access points. This delegation centralizes policy management at the access point level and minimizes the need for frequent updates to the bucket policy. This approach is illustrated below: Benefits of Using S3 Access Points Benefit Description Simplified Management Provides each group or user with a dedicated access point acting as a unique tunnel to the bucket. Unique ARN for Each Access Point Users reference the access point URL instead of the S3 bucket URL, ensuring tailored access. Streamlined Policy Application Policies can be directly applied to each access point, reducing overall policy complexity. Enhanced Network Security Supports network restrictions by allowing access only from specific VPCs. The summary diagram below encapsulates these key points: Summary S3 Access Points offer an effective method to segregate and manage access rights for various roles, enhance security through network control, and simplify policy management. Leveraging dedicated access points can significantly reduce the complexity of your S3 bucket policies. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,S3 ACL and Resource Policies,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/S3-ACL-and-Resource-Policies,"AWS Solutions Architect Associate Certification Services Storage S3 ACL and Resource Policies In this guide, we explore S3 ACLs and resource policies, explaining how they control access to S3 buckets. You will learn how buckets are secured by default, discover the components of JSON-based bucket policies, and understand the differences between IAM policies, resource policies, and legacy ACLs. When you create an S3 bucket, it starts in a locked-down state. By default, only the bucket creator—and the root user, who has full account access—can access it. Other AWS users within your account, public users, and users from other AWS accounts do not get access automatically. Within AWS, resource policies define who can access a specific S3 resource. In the case of buckets, these are known as bucket policies. An S3 bucket policy determines which users have permission to access the bucket and specifies the operations they are authorized to perform. Bucket Policy Structure Bucket policies are written in JSON and generally include the following components: Version: Specifies the policy language syntax (e.g., ""2012-10-17""). Statement: Contains one or more policy statements. Each statement includes: Sid: An optional statement identifier for reference. Principal: The AWS user(s) or account(s) the policy applies to. Effect: Indicates whether the policy allows or denies access. Action: Specifies the allowed or denied actions. Resource: Defines the ARN (Amazon Resource Name) of the bucket or its objects that the statement affects. Example Bucket Policy Below is a sample bucket policy that grants the user JohnDoe (from account 111122223333) permission to perform the s3:GetObject action on any object within the bucket: {
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Sid"": ""AllowRule"",
      ""Principal"": {
        ""AWS"": [
          ""arn:aws:iam::111122223333:user/JohnDoe""
        ]
      },
      ""Effect"": ""Allow"",
      ""Action"": [""s3:GetObject""],
      ""Resource"": [""arn:aws:s3:::DOC-EXAMPLE-BUCKET/*""]
    }
  ]
} Multiple Statements in a Bucket Policy A bucket policy can contain multiple statements to enforce different rules. For instance, you might grant access to everyone and then specifically deny access to one user: {
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Sid"": ""AllowAll"",
      ""Principal"": ""*"",
      ""Effect"": ""Allow"",
      ""Action"": [""s3:GetObject""],
      ""Resource"": [""arn:aws:s3:::DOC-EXAMPLE-BUCKET/*""]
    },
    {
      ""Sid"": ""DenyDaisy"",
      ""Principal"": {
        ""AWS"": [
          ""arn:aws:iam::666438:user/DaisyM""
        ]
      },
      ""Effect"": ""Deny"",
      ""Action"": [""s3:GetObject""],
      ""Resource"": [""arn:aws:s3:::DOC-EXAMPLE-BUCKET/*""]
    }
  ]
} In this example, the ""Principal"": ""*"" means the first rule applies to everyone—including anonymous users. The second statement explicitly denies the user DaisyM from accessing the objects. Defining Access for Specific Prefixes Bucket policies also support restrictions based on object prefixes. For instance, you can allow a user to only access objects located under the /media prefix: {
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Sid"": ""AllowDaisy"",
      ""Principal"": {
        ""AWS"": [
          ""arn:aws:iam::666438:user/DaisyM""
        ]
      },
      ""Effect"": ""Allow"",
      ""Action"": [""s3:GetObject""],
      ""Resource"": [""arn:aws:s3:::DOC-EXAMPLE-BUCKET/media/*""]
    }
  ]
} In the policy above, the ARN includes the bucket name, the prefix ( media ), and a wildcard ( * ), which grants access to all objects within that folder. Using Conditions in Bucket Policies Conditions in policies can further refine access rules. For example, you might restrict operations based on the user's source IP address. The following policy only allows actions from the 192.0.2.0/24 subnet: {
  ""Id"": ""PolicyId2"",
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Sid"": ""AllowIP"",
      ""Effect"": ""Allow"",
      ""Principal"": ""*"",
      ""Action"": ""s3:*"",
      ""Resource"": [
        ""arn:aws:s3:::DOC-EXAMPLE-BUCKET"",
        ""arn:aws:s3:::DOC-EXAMPLE-BUCKET1/*""
      ],
      ""Condition"": {
        ""IpAddress"": {
          ""aws:SourceIp"": [
            ""192.0.2.0/24""
          ]
        }
      }
    }
  ]
} Additionally, you can combine conditions to allow access to multiple prefixes. Here’s an example that authorizes access to objects under both /audio and /video prefixes: {
  ""Id"": ""PolicyId2"",
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Sid"": ""AllowIP"",
      ""Effect"": ""Allow"",
      ""Principal"": ""*"",
      ""Action"": ""s3:*"",
      ""Resource"": [
        ""arn:aws:s3:::DOC-EXAMPLE-BUCKET"",
        ""arn:aws:s3:::DOC-EXAMPLE-BUCKET1/*""
      ],
      ""Condition"": {
        ""StringEquals"": {
          ""s3:prefix"": [""audio/"", ""video/""],
          ""s3:delimiter"": [""/""]
        }
      }
    }
  ]
} Public Access and Extra Security Measures AWS provides a ""Block Public Access"" setting as an extra security measure to prevent accidental exposure of your buckets. Even if you create a bucket policy that appears to grant public access, AWS will block it until you disable the ""Block all public access"" option. For example, consider this policy intended to allow all users within your AWS account access: {
  ""Id"": ""PolicyId2"",
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Sid"": ""AllowAll"",
      ""Effect"": ""Allow"",
      ""Principal"": ""*"",
      ""Action"": ""s3:*"",
      ""Resource"": [
        ""arn:aws:s3:::DOC-EXAMPLE-BUCKET1/*""
      ]
    }
  ]
} Warning Using ""Principal"": ""*"" as shown above unintentionally opens access to the public, including all users from other AWS accounts and anonymous users. This can lead to excessive permissions when using ""s3:*"" , which allows all S3 operations. Always review your bucket policies carefully to avoid unintended public access. IAM Policies vs. Resource Policies Understanding the differences between IAM policies and resource (bucket) policies is critical: IAM Policies: These are attached to users or roles and determine the actions they can perform. They apply only to authenticated AWS users. Resource Policies: These policies are directly attached to AWS resources such as S3 buckets. They can grant permissions to both authenticated AWS users and anonymous/public users. It is essential that both policies permit an operation for it to succeed. If either the IAM policy or the resource policy denies access, the request will be blocked. Legacy ACLs Access Control Lists (ACLs) are a legacy mechanism predating IAM that provide basic access control through a limited set of rules. ACLs offer five permissions such as reading objects, writing objects, reading ACLs, writing ACLs, and full control. Due to their limited flexibility and granularity, ACLs are not recommended for routine bucket management. Summary Bucket policies define who can access your S3 bucket and what operations they can perform by specifying the following components: Principal: The user, role, or group the policy affects. Resource: The AWS resource (e.g., bucket or objects) the policy governs. Action: The S3 operations that are allowed or denied. Effect: Whether the defined actions are permitted or denied. Bucket policies are used in conjunction with IAM policies. For instance, if an IAM policy denies access to a specific action, that denial takes precedence—even if a resource policy allows it. Similarly, granting public access requires the use of a bucket policy, as IAM policies only apply to AWS users. By understanding the distinct roles and interactions of IAM policies, resource policies, and legacy ACLs, you can configure your S3 bucket access securely and effectively. For additional insights on managing AWS permissions, check out the following resources: Kubernetes Basics AWS Documentation Docker Hub Terraform Registry Watch Video Watch video content"
AWS Solutions Architect Associate Certification,EFS Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/EFS-Demo,"AWS Solutions Architect Associate Certification Services Storage EFS Demo In this lesson, we explore Amazon's Elastic File System (EFS) and demonstrate how to set up and share it between multiple EC2 instances. This guide covers the creation of an EFS file system, configuring it for your network, and mounting it on EC2 servers to enable simultaneous read/write access. EC2 Setup Overview Our current setup consists of a simple VPC with two subnets across two availability zones (US East 1A and US East 1B). Each zone runs one t2.micro instance, ensuring redundancy and high availability. The goal is to mount a single EFS file system on both servers so that they can share data seamlessly. Creating the EFS File System Begin by navigating to the EFS page in the AWS console and selecting Create File System . Customizing Your EFS Settings Click Customize to adjust your EFS configuration: Enter a descriptive name (e.g., ""EFS demo""). Select a storage class. Choose a multi-zone configuration for enhanced durability, or a single zone option if your workload allows data redundancy limited to one availability zone. For instance, a single availability zone configuration keeps data redundant within that zone, which protects against individual instance failures but may risk data loss if the entire zone fails. Options for automatic backups and lifecycle management (such as moving files older than 30 days to cheaper storage tiers like S3 Standard-IA) are also available. Configuring Network Settings Next, set up the network access: Choose the VPC used by your EC2 instances (in our demo, “demo VPC”). AWS automatically creates mount targets in each availability zone within the selected VPC. Ensure every zone has a mount target to maintain availability. When specifying security groups, you can either create a custom group tailored for EFS traffic or use the security group attached to your EC2 instances if it already permits all necessary communication. After reviewing the configurations, click Next , review your selections, and then Create . Wait a few minutes for AWS to provision the file system. Configuring EC2 Instances for EFS Once the file system is active, log into your EC2 instances to complete the setup. The following steps must be performed on each server. Step 1: Create a Mount Directory Create a directory that will serve as the mount point for the EFS file system. In this demo, we use /efsdemo : [ec2-user@ip-10-0-11-189 ~]$ sudo mkdir /efsdemo Step 2: Install Amazon EFS Utils Install the Amazon EFS Utils package to simplify mounting the file system: [ec2-user@ip-10-0-11-189 ~]$ sudo dnf -y install amazon-efs-utils Repeat this installation on every EC2 instance you plan to mount the EFS. Step 3: Mount the EFS File System Mount the file system using the provided file system ID from the AWS console. Replace fs-08de7b8e04f984697 with your file system ID: [ec2-user@server ~]$ sudo mount.efs fs-08de7b8e04f984697 /efsdemo Verify the mount status with: [ec2-user@server ~]$ df -k
Filesystem                                                      1K-blocks       Used  Available Use% Mounted on
...
fs-08de7b8e04f984697.efs.us-east-1.amazonaws.com:/ 9007199254739968    0 9007199254739968   0% /efsdemo Persistence Notice This manual mount is not persistent across reboots. To ensure the EFS mounts automatically after a reboot, add the appropriate entry to your /etc/fstab file following the AWS documentation. Step 4: Test the Shared File System To verify the shared access, perform the following tests: On the first server, navigate to the mount directory and create a file: [ec2-user@ip-10-0-11-189 ~]$ cd /efsdemo
[ec2-user@ip-10-0-11-189 efsdemo]$ sudo vi file1 Insert the text: I made this on server1 On the second server, check the shared directory: [ec2-user@ip-10-0-22-199 ~]$ cd /efsdemo/
[ec2-user@ip-10-0-22-199 efsdemo]$ ls
file1
[ec2-user@ip-10-0-22-199 efsdemo]$ cat file1
I made this on server1 The appearance of the file on both servers confirms that the EFS file system is shared effectively. You can also create another file from the second server (e.g., ""file2"" with the content ""I made this on server2"") and verify its presence on the first server. Below is a consolidated view of the commands executed on one server: [ec2-user@ip-10-0-11-189 ~]$ sudo mkdir /efsdemo
[ec2-user@ip-10-0-11-189 ~]$ sudo mount.efs fs-08de7b8e04f984697 /efsdemo
[ec2-user@ip-10-0-11-189 ~]$ df -k
Filesystem                                                      1K-blocks       Used  Available Use% Mounted on
...
fs-08de7b8e04f984697.efs.us-east-1.amazonaws.com:/ 9007199254739968    0 9007199254739968   0% /efsdemo
[ec2-user@ip-10-0-11-189 ~]$ cd /efsdemo
[ec2-user@ip-10-0-11-189 efsdemo]$ sudo vi file1
[ec2-user@ip-10-0-11-189 efsdemo]$ ls
file1  file2 Summary In this lesson, you learned how to: Create an EFS file system with customized settings in the AWS console. Configure network settings and security groups to support mount targets in each availability zone. Install Amazon EFS Utils on EC2 instances. Mount the EFS file system and verify shared file access between multiple servers. Additional Resources For further details on persistent mounting and additional configuration options, please refer to the Amazon EFS documentation . Thank you for following this guide on setting up Amazon EFS. Stay tuned for more tutorials on AWS services in our upcoming lessons. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,S3 demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/S3-demo,"AWS Solutions Architect Associate Certification Services Storage S3 demo In this lesson, you will learn how to create your first Amazon S3 bucket and explore its key functionalities. This guide covers steps from accessing the AWS console to creating buckets, uploading files, organizing content, moving objects, and finally deleting buckets—all while ensuring your S3 operations remain secure and efficient. Start by opening the AWS console and searching for the S3 service. This search directs you to the S3 page. If no buckets have been created, you will see a prompt to create one. Otherwise, your existing buckets will be listed with an option to create a new bucket. Before proceeding, notice the ""global"" indicator on the interface. Although S3 employs a global namespace, you still specify a bucket's region during creation. Creating a Bucket To begin, click on ""Create bucket"" and follow these steps: Bucket Naming: Specify a globally unique bucket name. Avoid common names like ""demo"" that may already exist; instead, use unique identifiers such as ""KodeKloud-demo-123."" To understand the naming conventions, click the link provided next to the bucket name field. Region Selection: Choose the region where your bucket will reside (e.g., US East 1). Note If you have existing buckets and wish to mirror their settings, you can copy the configuration. Default Settings: You can configure options including object ownership, block public access, versioning, encryption, and advanced settings like object lock. For this demonstration, use the default settings and keep block public access enabled to restrict access solely to you. After configuring your options, click ""Create bucket."" Your newly created bucket (for example, ""kk-demo-123"") will now appear in your bucket list with details such as its region and creation date. Select the bucket to view its contents and properties. The Objects page displays all uploaded files (initially empty), while the Properties page outlines configuration details such as region, ARN, versioning status, tags, encryption, server access logging, CloudTrail data events, event notifications, transfer acceleration, object lock, requester pays, and static website hosting. In the Permissions tab, manage access controls. By default, only you have access to the bucket and its content. You can update bucket policies and permissions to grant access to other users if necessary. The Metrics section shows CloudWatch metrics (data size, object count) and the Management section enables you to set up lifecycle rules, replication configurations, inventory setups, and access points. These topics are explored in more detail in advanced lessons. Uploading Files Follow these steps to upload your first file: Navigate back to the Objects page and click ""Upload."" You can either add files, upload entire folders, or drag and drop files directly into the interface. In this example, drag and drop a photo file. Review the file details, which typically include file type, size, and destination bucket. You can adjust object details or enable bucket versioning if needed. For this demonstration, retain the default bucket permissions and storage class settings. S3 offers various storage classes—Standard, Intelligent-Tiering, and Glacier—that help balance data access, resiliency, and cost. For now, use the default settings. Click ""Upload"" to start the file transfer. When the upload completes, a green checkmark indicates success. After closing the confirmation, the file appears in the Objects section of your bucket. Click the file to view details such as region, file size, last modified date, unique URI, ARN, entity tag, and access URL. Scrolling further reveals settings like object lock, storage class, server-side encryption, checksum tags, and more. Note that these configurations can be applied on an individual object basis. If you attempt to view the object using its public URL, you might get an ""Access Denied"" error because the file is not accessible to unauthenticated users. <Error>
  <Code>AccessDenied</Code>
  <Message>Access Denied</Message>
  <RequestId>PDW0A5Z2R3E2K3J6J0G6</RequestId>
  <HostId>bsEm1k1oTn3u1kND7MbjamVADURSB1Vw6d2q2LKwRkMExUjBfoxTIgLkS</HostId>
</Error> When you click the ""Open"" button while authenticated, your session credentials allow you access to view the file. Security Reminder By default, your S3 bucket and its objects are secured so that only the creator has access. Adjust these settings only if you intend to share the file publicly. Creating Folders and Organizing Files Although S3 does not support a traditional folder structure, it simulates folders by using object prefixes. Follow these steps to organize your files: In your bucket, click ""Create folder"" and provide a name (e.g., ""food""). Open the folder and upload files, such as food-related images, using the same upload process and designating the folder as the destination. If you inspect the URI or ARN of a file within the folder (for example, burger.jpg), you will see that the file name is prefixed with the folder name. This prefix system simulates folders in a flat file system. To view an object, simply select it and click ""Open."" Remember, accessing it publicly may be blocked if proper permissions are not set. Deleting Files and Moving Objects Deleting Files Removing objects from S3 is straightforward. To delete a file, follow these steps: Select the file and click ""Delete."" In the confirmation pop-up, type ""permanently delete"" to confirm the action. If bucket versioning is disabled, the file will be permanently removed. Moving Objects You can simulate moving files between folders within S3. For example, to move ""steak.jpg"" into a new folder named ""test"": Select the file and choose ""Actions"" > ""Move."" Specify the full destination path (e.g., ""s3://kk-demo-123/test"") either manually or by browsing. Confirm the move operation. After the move is initiated, the status screen will display the results of the operation. Finally, navigate to the ""test"" folder to confirm that ""steak.jpg"" has been successfully moved. Deleting the Bucket Before deleting a bucket, ensure it is completely empty. Deleting a non-empty bucket will result in an error. Click the ""Empty"" button, type ""permanently delete"" in the confirmation box, and empty the bucket. Once empty, proceed to delete the bucket by typing its name to confirm the deletion. This completes the process of managing your S3 bucket, from creation and file management to deletion. Conclusion This lesson provided a comprehensive overview of working with Amazon S3 buckets. You learned how to: Create and configure buckets Upload and review files Organize files using simulated folders Move objects between folders Delete files and entire buckets For more information on AWS S3 and its features, consider exploring the AWS Documentation for further details and advanced configurations. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,S3 Static Website Hosting Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Storage/S3-Static-Website-Hosting-Demo,"AWS Solutions Architect Associate Certification Services Storage S3 Static Website Hosting Demo In this tutorial, you'll learn how to host a static website using Amazon S3. We'll cover everything from the website file structure to uploading your files and configuring public access. This guide assumes you have a basic static site that consists of an HTML file, a CSS file for styling, a custom 404 error page, and an images folder containing pictures of food. Website File Structure Before uploading your site to S3, it’s important to familiarize yourself with the file structure. Below is an illustration of the website components: The main components include: index.html : The landing page that loads the CSS file and displays images. index.css : Contains the styling and visual presentation for the website. 404.html : A custom error page shown when a user navigates to a non-existent page. images folder : Stores various food images referenced in the HTML file. Below is an example of the main index file: <!DOCTYPE html>
<html lang=""en"">
<head>
    <meta charset=""UTF-8"" />
    <meta http-equiv=""X-UA-Compatible"" content=""IE=edge"" />
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"" />
    <link rel=""stylesheet"" href=""index.css"" />
    <title>Document</title>
</head>
<body>
    <div class=""container"">
        <div class=""images"">
            <div class=""image""><img src=""images/food1.jpg"" alt="""" /></div>
            <div class=""image""><img src=""images/food2.jpg"" alt="""" /></div>
            <div class=""image""><img src=""images/food3.jpg"" alt="""" /></div>
            <div class=""image""><img src=""images/food4.jpg"" alt="""" /></div>
            <div class=""image""><img src=""images/food5.jpg"" alt="""" /></div>
            <div class=""image""><img src=""images/food6.jpg"" alt="""" /></div>
            <div class=""image""><img src=""images/food7.jpg"" alt="""" /></div>
            <div class=""image""><img src=""images/food8.jpg"" alt="""" /></div>
            <div class=""image""><img src=""images/food9.jpg"" alt="""" /></div>
            <div class=""image""><img src=""images/food10.jpg"" alt="""" /></div>
        </div>
    </div>
</body>
</html> Similarly, the 404 error page is structured as follows: <!DOCTYPE html>
<html lang=""en"">
<head>
    <meta charset=""UTF-8"" />
    <meta http-equiv=""X-UA-Compatible"" content=""IE=edge"" />
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"" />
    <link rel=""stylesheet"" href=""index.css"" />
    <title>Document</title>
</head>
<body>
    <div class=""container"">
        <h1 class=""head-404"">404</h1>
        <h2 class=""text-404"">Page not found</h2>
    </div>
</body>
</html> When users visit your website, the S3 bucket serves the index.html file by default, which in turn loads the necessary CSS and images. If a non-existent file is requested, the custom 404 page is returned. Uploading Files to S3 To get started, log in to the S3 console and create a new bucket for your static website. Create a new bucket (e.g., ""static-demo"") using the default settings. After creating the bucket, you will receive a confirmation notification. Open the bucket and proceed to upload all website files, including HTML, CSS, the 404 error page, and the images folder. Note that the upload duration may vary based on file size and quantity. Enabling Static Website Hosting Once your files are uploaded, enable the static website hosting feature: Navigate to the Properties tab in your bucket. Scroll down to the Static website hosting section and click Edit . Select the ""Host a static website"" option. Configure the following settings: Index document : Set this to index.html to serve as the homepage. Error document : Set this to 404.html to handle error messages. Click Save to update your settings. After saving, you will see a bucket website endpoint URL in the static website hosting section. This URL is how users will access your website. Note Clicking the website endpoint URL might initially result in an ""Access Denied"" error. This is expected until you configure public access. Configuring Public Access To allow public access to your website, you'll need to adjust the bucket's permissions and include a bucket policy. Click on the Permissions tab and disable the ""Block all public access"" setting. Confirm your changes to allow public access. Even after these changes, your bucket is not entirely public. You must add a proper bucket policy to permit public read (GetObject) access. Open the bucket policy editor and add the following policy: {
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Sid"": ""AllowPublic"",
            ""Effect"": ""Allow"",
            ""Principal"": ""*"",
            ""Action"": ""s3:GetObject"",
            ""Resource"": ""arn:aws:s3:::your-bucket-name/*""
        }
    ]
} Replace ""your-bucket-name"" with the actual bucket name. This policy enables any user to read your files while preventing modifications or deletions. After saving the policy, the bucket becomes publicly accessible. Verifying Your Website Return to the Properties tab and check the Static website hosting section again. The provided URL should now load your website for public access. Navigating to the bucket URL without specifying a file returns the index.html automatically. You can access specific assets by appending their paths (for example, /images/food1.jpg ) to the URL. If a user navigates to an invalid path, the custom 404.html page will be shown. This concludes the lesson on hosting a static website with Amazon S3. By following these steps, you can create a secure and publicly accessible static website with ease. For more detailed information about AWS and static website hosting, refer to the Amazon S3 Documentation . Happy hosting! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,EC2,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/EC2,"AWS Solutions Architect Associate Certification Services Compute EC2 In this lesson, we explore Amazon EC2 and how it enables you to run your application code on virtual servers in the cloud. EC2 simplifies the process of deploying applications by allowing you to focus on your code rather than managing the underlying physical hardware. When you create an application, you eventually need to deploy it to a server—a computer that handles client requests, processes your application code, and returns responses. The term ""server"" originates from the client-server architecture, where a computer provides resources (such as a web application) to the client. For example, when you access a website, your browser sends a request to the server's IP address or domain name, and the server responds by serving the corresponding HTML file. Before the rise of cloud computing, deploying an application involved renting space in a data center, purchasing physical servers (e.g., Dell or HP), installing and securing an operating system like Ubuntu, and manually managing hardware failures. AWS eliminates these complexities by offering EC2 instances—virtual servers for deploying and running your applications without worry about the infrastructure. An EC2 instance is a virtual server that offers computing power equivalent to a physical host. AWS provides various instance types to match your application's CPU, memory, and storage needs. These include: General Purpose: Offers a balance of compute, memory, and networking resources for diverse workloads, such as web servers. Compute Optimized: Designed for compute-bound applications requiring high-performance processors. Memory Optimized: Ideal for workloads that need to process large data sets in memory. Storage Optimized: Suited for applications with high I/O demands that frequently access disk storage. GPU Instances: Perfect for high-performance tasks like machine learning and deep learning that require powerful GPUs. Amazon Machine Images (AMIs) When launching an EC2 instance, you must select an Amazon Machine Image (AMI), which serves as the blueprint for your instance. The AMI includes the operating system and may come pre-configured with additional software or settings you require. Available for various operating systems—Linux distributions (like Ubuntu, Red Hat, or CentOS), macOS, or Windows—AMIs enable rapid deployment of identical servers at scale. Think of an AMI as a recipe for creating your server. If you modify an instance (for example, by installing new software or configuring security settings), you can create a new AMI from that instance and use it for future deployments. There are three types of AMIs: Public AMIs: Shared within the AWS community and available to anyone at no cost. Private AMIs: Customized AMIs that remain accessible only to the owner or specified AWS accounts. Shared AMIs: Private AMIs explicitly shared with select AWS accounts for controlled collaboration. Once an AMI is defined, you can create multiple, identically configured EC2 instances. Securely Connecting to an EC2 Instance After your EC2 instance is deployed, connecting via SSH is the typical method to manage and configure it. For a secure connection, you use a key pair consisting of a public key and a private key. The public key is embedded into the instance during launch, while you use the corresponding private key to connect via SSH. AWS allows you to manage multiple key pairs, enabling different access configurations per instance. EC2 Instance Lifecycle Understanding the lifecycle of an EC2 instance is crucial for effective management. The lifecycle includes several states: Pending: The instance is launching and transitioning to the running state. Running: The instance is active and available for connection. Stopping: The instance is in the process of shutting down. Stopped: The instance is powered off but can be re-started later. Shutting Down: The instance is preparing to be terminated. Terminated: The instance has been permanently deleted and cannot be recovered. Bootstrapping with User Data When launching an EC2 instance, you can provide user data—usually a shell script or cloud-init directives—which executes during startup. This automation allows you to install software, configure settings, or download necessary files immediately as the instance launches. Keep in mind that user data scripts have a size limit of 16 kilobytes. Security Groups Security groups in AWS act as virtual firewalls for your EC2 instances, controlling inbound and outbound traffic. For example, if you deploy a web server, you can configure your security group to allow inbound HTTP (port 80) and HTTPS (port 443) traffic. This ensures that only the necessary traffic reaches your instance while maintaining robust security. Persistent Storage with Elastic Block Store (EBS) EC2 instances provide computing power, but many applications require persistent data storage. AWS Elastic Block Store (EBS) is a scalable block storage service that can be attached to your instances. EBS is commonly used for boot volumes, databases, file storage, and backup solutions. It also supports snapshots—point-in-time copies of your volumes stored in S3. These snapshots are incremental, saving only changes since the previous snapshot, which helps reduce both storage costs and backup time. Integration with Elastic Load Balancing and Auto Scaling EC2 instances can be integrated with other AWS services to enhance performance and scalability: Elastic Load Balancer (ELB): Automatically distributes incoming traffic across multiple targets such as EC2 instances, containers, or IP addresses, ensuring seamless user experiences. Auto Scaling Groups: Automatically adjusts the number of EC2 instances based on traffic demand, scaling out during high-demand periods and scaling in to reduce costs during lower demand. Elastic IP Addresses By default, public IP addresses assigned to EC2 instances may change when the instance stops and restarts. To maintain a consistent IP, you can use an Elastic IP—a static, reserved IP address associated with your account. Elastic IPs remain constant even if you move the underlying instance between physical hosts, and can be re-assigned to different instances as needed. Launch Templates Launch templates let you define a set of configuration parameters (such as the AMI, security groups, subnet, and instance type) that can be reused when launching EC2 instances. They are particularly useful with Auto Scaling groups, as AWS uses these templates to automatically launch new instances during scale-out events. EC2 Instance Placement AWS automatically selects a suitable physical host in an availability zone when you deploy an EC2 instance. However, you can influence instance placement to best meet your application’s requirements: Cluster Placement Group: Places instances as close together as possible to achieve low-latency, high-throughput networking. Ideal for high-performance computing and big data analytics. Partition Placement Group: Distributes instances across logical partitions to reduce the risk of simultaneous hardware failures. This is particularly useful for distributed workloads such as Hadoop. Spread Placement Group: Distributes instances across distinct physical hardware to minimize the risk of correlated failures, perfect for a few critical instances that must remain isolated. EC2 Instance Pricing Options AWS offers a range of pricing models designed to suit various workloads and budgets: On-Demand: Pay per hour or second with no long-term commitment, ideal for short-term or unpredictable workloads. Spot Instances: Bid on unused AWS capacity at discounts of up to 90% compared to on-demand prices. Because spot instances can be interrupted, they are best suited for flexible applications like batch processing. Savings Plans: Commit to a consistent hourly cost over a one- or three-year term in exchange for lower prices, offering predictability and cost savings for steady-state usage. Reserved Instances: Reserve a specific amount of compute capacity for one or three years, providing cost savings for predictable workloads. Dedicated Hosts: Rent an entire physical server exclusively for your use—ideal for licensing requirements and compliance, ensuring your instances run on the same physical server. Dedicated Instances: Similar to Dedicated Hosts, these instances run on hardware isolated for your use, although the specific physical host may change over time. Pricing Analogy To better understand these pricing models, consider this analogy involving coffee purchases: On-Demand: Like buying a cup of coffee at full price whenever you want without any commitment. Spot Instances: Similar to catching a flash sale at your favorite coffee shop where leftover coffee is sold at a significant discount, albeit with the risk of unavailability. Savings Plans: Comparable to committing to spend a specific amount on coffee each month in exchange for a lower price. Reserved Instances: Like subscribing to a daily coffee plan at a discounted rate because you know you will be drinking coffee every day. Dedicated Hosts: Resembles renting an entire coffee machine exclusively for your use. Dedicated Instances: Similar to reserving your own seat in a coffee shop where the machine may be shared, but your seat remains exclusively yours. Tip By understanding these pricing options, you can choose the most cost-effective strategy to deploy and manage your applications on AWS EC2. This comprehensive overview of AWS EC2 covers the key concepts—from virtual server provisioning with AMIs and secure connections using SSH to integrating with other AWS services like Elastic Load Balancing and Auto Scaling. Whether you’re launching your first instance or managing a complex architecture, knowing these fundamentals helps ensure your cloud deployment is efficient, secure, and scalable. For further details, consider exploring: AWS EC2 Documentation AWS Pricing Models AWS Auto Scaling Happy cloud computing! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,EC2 Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/EC2-Demo,"AWS Solutions Architect Associate Certification Services Compute EC2 Demo In this detailed tutorial, you will learn how to deploy, connect to, and manage an Amazon EC2 instance securely using AWS. We will walk through launching an EC2 instance using a specific Amazon Machine Image (AMI), configuring networking and security, and connecting via SSH with a certificate. Launching an EC2 Instance Begin by accessing the AWS console and using the search bar to navigate to the EC2 service. In the EC2 dashboard, click on the ""Instances"" section. You can either click the direct ""Launch instances"" button on the dashboard or select the option within the instances page. On the instances page, click ""Launch instances"" to start the configuration process. The first step is to assign a name to your instance – for example, ""EC2 demo instance"" – and select a specific AMI. The selected AMI acts as a blueprint for your instance by defining the operating system and optionally including pre-installed software or services. You can choose AMIs from the Quick Start section, which include popular options like Amazon Linux, Ubuntu, and macOS. Custom AMIs will appear under ""My AMIs"", or you can use the search bar to find a specific image, such as typing ""Ubuntu"" to explore different versions and architectures. If preferred, explore the Amazon Marketplace for pre-configured AMIs provided by software vendors. This can help you deploy applications like a Cisco router with the appropriate license without manual setup of a base OS. For this demo, we will use the Amazon Linux 2023 AMI from the Quick Start tab. Keep in mind that each AMI includes a unique ID associated with a specific region, so verify the AMI ID for your desired region before proceeding. Next, choose your instance type. For example, the T2 micro is free tier eligible. Then, configure the key pair to securely connect to your instance. You have the option to select an existing key pair or create a new one by following these steps: Select ""Create new key pair"". Enter a name (e.g., ""EC2 demo""). Choose the default RSA type and the .pem format. Download the PEM file and store it securely. Configuring Networking and Security After setting up your key pair, proceed to configure the networking settings including the Virtual Private Cloud (VPC), subnet, and public IP assignment. The default VPC and subnet are usually sufficient, but ensure that you enable a public IP if you need to connect to your instance over the internet. Next, assign an existing security group or create a new one. By default, a new security group includes an inbound rule allowing SSH traffic and outbound rules permitting all traffic. Configure your storage options, such as an 18 GB root volume, and leave advanced options like spot instances or auto recovery settings as default unless your scenario demands a custom setup. Once all settings are applied, click ""Launch instance"". Your ""EC2 demo instance"" will now be deployed. Viewing Instance Details After launching the instance, return to the Instances page to review its details. Here you will find important information such as: Instance ID Current state (running) Instance type Availability zone Public and private IP addresses Public DNS name Additional details include the security group settings, key pair name, launch time, and monitoring metrics like CPU utilization. You can also review network settings such as inbound/outbound rules, subnet, and network interfaces. Connecting to Your EC2 Instance To establish an SSH connection with your instance, copy its public IP or public DNS address from the instance details page. Then, verify that your PEM file (e.g., ""EC2.pem"") is available in your terminal directory: ls
# Expected output:
# ec2.pem
# kubeseal-windows-installer/
# main.pem
# main2.pem
# test.code-workspace
# test.yaml
# wacom.pem Use the SSH command with the following format to connect to your instance: ssh -i ec2.pem username@<PUBLIC_IP_OR_DNS> Refer to the AMI documentation for the correct default username. For an Amazon Linux AMI, the username is typically ""ec2-user"". AWS also provides connection instructions when you click the ""Connect"" button. An example command is: chmod 400 ec2-demo.pem
ssh -i ""ec2-demo.pem"" [email protected] After executing the command, type ""yes"" when prompted to confirm the connection. Once connected, your prompt should look similar to the following: [ec2-user@ip-172-31-81-100 ~]$ ls
[ec2-user@ip-172-31-81-100 ~]$ ls -la
total 12
drwx------.  3 ec2-user ec2-user  74 Sep 28 23:43 .
drwxr-xr-x.  3 root     root       22 Sep 28 23:43 ..
-rw-r--r--.  1 ec2-user ec2-user 141 Jan 28  2023 .bash_profile
-rw-r--r--.  1 ec2-user ec2-user 492 Jan 28  2023 .bashrc
drwx------.  2 ec2-user ec2-user   29 Sep 28 23:43 .ssh
[ec2-user@ip-172-31-81-100 ~]$ Tip Always ensure your PEM file permissions are set correctly using the chmod command (e.g., chmod 400 ec2-demo.pem) to avoid connection issues. Stopping and Terminating the Instance To avoid unnecessary costs, it is important to stop or terminate your instance after use. To stop the instance, navigate back to the AWS console, select your instance, and choose ""Stop instance"" from the ""Instance State"" menu. The state will transition from ""running"" to ""stopping"" and finally to ""stopped"". You can restart, reboot, or hibernate the instance if needed. For complete cleanup and to prevent incurring additional charges, terminate the instance by selecting ""Terminate"" from the instance state options and confirming the action. After termination, the instance state will be updated to ""terminated"", and you will no longer be billed. Caution Always terminate instances that are no longer in use to prevent unexpected billing charges. This concludes our comprehensive guide on deploying, connecting to, and managing an Amazon EC2 instance. Enjoy your journey in cloud computing and explore more advanced AWS features to scale your infrastructure! Watch Video Watch video content Practice Lab Practice lab"
AWS Solutions Architect Associate Certification,ECS Demo Part 1,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/ECS-Demo-Part-1,"AWS Solutions Architect Associate Certification Services Compute ECS Demo Part 1 In this lesson, you will deploy a simple Node.js application to AWS ECS . Before working with ECS using the AWS Console, review the two public demo projects available on Docker Hub: • https://kodekloud.com/ecs-project1 • https://kodekloud.com/ecs-project2 Pull these images to follow along with the lesson. Project 1 Overview Project 1 features a basic Node.js application using the Express framework to serve a simple HTML file. A GET request to the root path returns the HTML document. index.html <!DOCTYPE html>
<html lang=""en"">
  <head>
    <meta charset=""UTF-8"" />
    <meta http-equiv=""X-UA-Compatible"" content=""IE=edge"" />
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"" />
    <link rel=""stylesheet"" href=""css/style.css"" />
    <title>Document</title>
  </head>
  <body>
    <h1>ECS Project 1</h1>
  </body>
</html> user1 on user1 in ecs-project1 is v1.0.0 via ⎊ Express Server Code The Express server is configured to render the HTML page. The application listens on port 3000. const express = require(""express"");
const path = require(""path"");

const app = express();

app.set(""view engine"", ""ejs"");
app.set(""views"", path.join(__dirname, ""views""));
app.use(express.static(path.join(__dirname, ""public"")));

app.get(""/"", (req, res) => {
  res.render(""index"");
});

app.listen(3000, () => {
  console.log(""Server is running on port 3000"");
}); user1 on user1 in ecs-project1 is 🧱 v1.0.0 via [] Dockerfile The Dockerfile below packages the application and exposes port 3000. When configuring your ECS container, ensure that port 3000 is specified. FROM node:16
WORKDIR /usr/src/app
COPY package*.json ./
RUN npm install
RUN npm ci --only=production
COPY . .
EXPOSE 3000
CMD [ ""node"", ""index.js"" ] Deploying with the ECS Console Navigate to the AWS Console and search for Elastic Container Service (ECS) . When you first use ECS, you will encounter a quick-start wizard. Although the wizard includes sample apps, choose a custom configuration to understand each underlying component. Step 1 – Using the Quick Start Wizard When prompted by the wizard: Click the Get started button. Instead of selecting one of the example applications, choose Custom to provide your own configurations. Configure the container using the following settings: Container Name: ECS-Project1 Image: kodekloud/ECS-Project1 (public repository; no credentials required) Memory Limits: Adjust as needed. Port Mapping: Specify port 3000 (TCP). As the Express app listens on port 3000, set the container port to 3000. Important Unlike a typical Docker run command (e.g., -p 80:3000 ), in ECS the external port must match the container port (e.g., 3000 mapped to 3000). For example, your Docker run command would look like: docker run -p 3000:3000 Advanced container settings like health checks can also be configured. For example, add the following health check command: CMD-SHELL, curl -f http://localhost/ || exit 1 These settings are similar to options found in a Docker Compose file, allowing you to specify environment variables, volumes, resource limits, and labels. Click Update to complete the container configuration. Service and Cluster Setup After the container configuration, click Next to proceed to the service definition. ECS will generate a service named ECS-project1-service . You may choose to attach a load balancer (for this demo, select None ). The wizard will automatically provision a new cluster and VPC along with the required subnets. Review the configuration details for your container, task definition, service, and cluster, then click Create . After a few minutes, select View Service to verify that your application has been deployed. Before verifying, let’s review some ECS components that were created in the background. Exploring ECS Components Task Definitions Task definitions serve as blueprints for your container configurations (including port mappings, environment variables, and volumes). Under Task Definitions in the ECS Console, locate the task definition for this project, noting that revisions indicate configuration changes. Select the relevant task definition (for example, “first-run-task-definition”). The highest revision number indicates the latest configuration. Upon reviewing the configuration, you will notice settings such as Fargate usage, memory/CPU allocations, and the host-to-container mapping (port 3000 to port 3000). Clusters and Services Within Clusters , the wizard-created cluster displays the active service ECS-project1-service . This service is associated with one desired task and, later, one running task. Click on the service to review the network details (VPC, subnets, and security groups). Under Tasks , you will find the running task. The details page displays a public IP address that can be used to access the application. When you access the public IP on port 3000, the browser shows the simple HTML page served by your application. IP Address Changes If multiple tasks are running, each task will have a unique IP address. This dynamic nature is why a load balancer is crucial—it provides a stable endpoint for clients. Tearing Down the Quick Start Deployment After confirming a successful deployment, delete the resources created via the quick start wizard. Delete the ECS service by navigating to the cluster, selecting the service, and confirming deletion (type “delete me” when prompted). Then, delete the cluster. Now that the ECS configuration is removed, you can deploy the application from scratch. Deploying from Scratch Using Fargate Creating a New Cluster In the ECS Console, click Create Cluster . Choose the ""Networking only"" option (Fargate). Provide a cluster name (e.g., ""cluster1""). Leave the default VPC CIDR block and subnets intact. Click Create . Select View Cluster to inspect the new cluster. Creating a Task Definition In the ECS Console, navigate to Task Definitions and click Create new Task Definition . Select Fargate as the launch type. Name the task definition (e.g., ""ECS-Project1-taskdef"") and choose the appropriate IAM role (this may have been auto-created if you previously ran the quick start wizard). Choose Linux as the operating system and keep the default execution role. Specify a minimal task size for this demo. Add a container with these settings: Container Name: node app Image: kodekloud/ECS-Project1 Health Check Command: CMD-SHELL, curl -f http://localhost/ || exit 1 Port Mapping: 3000 Click Add and then Create to finalize the task definition. Creating a Service from the Task Definition Within your cluster (""cluster1""), go to the Services tab and click Create . Set the Launch type to Fargate and the Operating system to Linux. Select the latest revision of your task definition (""ECS-Project1-taskdef""). Name your service (e.g., ""project1-service"") and choose the desired number of tasks (for instance, 2 to demonstrate scaling). Click Next . For the network configuration: Select the VPC created for ECS. Choose the two subnets. Edit the default security group to permit incoming traffic on port 3000 (instead of port 80). Skip the load balancer configuration for this demo. Optionally disable autoscaling and click Next . Finally, click Create Service . ECS will begin provisioning the two tasks. They may initially show a ""PENDING"" status until transitioning to ""RUNNING."" Once the tasks are running, each will receive a unique public IP. Note that manually tracking these changing IPs is not ideal; hence a load balancer is recommended, as it establishes a consistent endpoint. Click on a task to view its details and copy its public IP. Visiting this IP on port 3000 will display the simple HTML page of your application. Updating the Application Suppose you decide to update the application by modifying the HTML. For example, you can add extra exclamation points to the H1 tag. Updated index.html <!DOCTYPE html>
<html lang=""en"">
  <head>
    <meta charset=""UTF-8"" />
    <meta http-equiv=""X-UA-Compatible"" content=""IE=edge"" />
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"" />
    <link rel=""stylesheet"" href=""css/style.css"" />
    <title>Document</title>
  </head>
  <body>
    <h1>ECS Project 1!!!!!</h1>
  </body>
</html> Rebuild your Docker image and tag it: docker build . -t kodekloud/ecs-project1 Push the updated image to Docker Hub. The output may look like: 8cba2b7a5a3d: Layer already exists
2713d962e94: Layer already exists
72140ff0de3: Layer already exists
93a6676ff620: Layer already exists
c77311ff52e: Layer already exists
ba9804f7abed: Layer already exists
a5186a09280: Layer already exists
1e69438976e4: Layer already exists
47b6660a2b9b: Layer already exists
5cbc21d19fb: Layer already exists
07b905e91599: Layer already exists
20833a96725e: Layer already exists
latest: digest: sha256:98216dd964fd5bb910fb23a527ed9d804b5cedacaa47fb45264cebe664006b size: 3261
user1 on user1 in ecs-project1 [!] is v1.0.0 via ⬢ took 4s The ECS service must be notified of this update. In the ECS Console, update the service by selecting Update and then opting for Force new deployment . This command instructs ECS to pull the latest image and deploy new tasks. Alternatively, if you modify the task definition (even if only the image tag changes), create a new revision and update your service to use this latest revision. If you create a new revision of the task definition, update the service appropriately. After the update, the ECS Console will display both the old and new tasks running side by side until the health checks pass and the old tasks are terminated. The new tasks will receive different public IP addresses: Refreshing the service confirms that the new tasks are active with the updated application. This dynamic change of IPs further reinforces the necessity of a load balancer to provide a stable endpoint. Cleanup and Next Steps After verifying the updated deployment, you can delete the application resources if desired. In the ECS Console, delete the service (confirm by entering “delete me”) and ensure all tasks are terminated. Next, we will demonstrate how to deploy a more complex application that includes a database, persistent storage (volumes), and a load balancer to ensure a single, stable endpoint for your front-end applications. Happy deploying! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,EKS,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/EKS,"AWS Solutions Architect Associate Certification Services Compute EKS In this lesson, we dive into AWS Elastic Kubernetes Service (EKS), a fully managed Kubernetes service by AWS that simplifies the deployment, management, and scaling of containerized applications. EKS takes care of the provisioning, scaling, and management of the Kubernetes control plane components—including the API server, scheduler, controller manager, and etcd. These critical components are complex to configure manually, especially when it comes to scaling and backups. With AWS managing these tasks, you can focus on developing your applications and managing worker nodes. While AWS maintains the control plane, the responsibility for managing the worker nodes still lies with you. The advantages of using EKS include simplified cluster operations, enhanced security through AWS best practices, and seamless integrations with AWS services such as IAM, Secrets Manager, and Load Balancer. Worker Node Options in EKS When configuring your EKS cluster, you need to decide how to manage the worker nodes. AWS provides three main options: Self-Managed Nodes With self-managed nodes, you manually provision EC2 instances and install all necessary components such as kubelet, kube-proxy, and the container runtime. You are responsible for handling routine updates, security patches, and ensuring that each node properly registers with the Kubernetes control plane. This approach gives you full control over your nodes, similar to managing instances in ECS. Managed Node Groups Managed node groups automate the lifecycle management of EC2 instances for worker nodes. With managed node groups, AWS deploys EKS-optimized images and simplifies operations such as node creation, updates, and termination via API calls. Nodes are configured within an auto-scaling group, reducing manual overhead while still giving you the flexibility to manage the underlying instances. Fargate Fargate offers a serverless computing model for Kubernetes pods, eliminating the need to manage EC2 instances altogether. When you deploy an application, Fargate automatically provisions the necessary compute resources based on your container specifications. This model ensures you pay only for the resources you use, while AWS handles all aspects of resource management. Tip Consider your operational requirements carefully when selecting a worker node option. Self-managed nodes offer complete control, managed node groups balance automation with flexibility, and Fargate provides a hassle-free, serverless experience. Creating an EKS Cluster Setting up an EKS cluster involves several key steps: Cluster Creation: Specify the desired Kubernetes version and create the cluster. IAM Role Configuration: Provide an IAM role necessary for cluster operations. Networking Setup: Define the VPC, subnets, and security groups to guarantee appropriate network and security configurations. Worker Node Provisioning: Choose one of the aforementioned options to deploy worker nodes. For example, when opting for managed node groups, you define the instance type, specify the minimum and maximum node count, and associate the node group with your EKS cluster. After creating the cluster, connect to it using kubectl by updating your kubeconfig file with the cluster's endpoint and authentication details. There are several methods to manage an EKS cluster: AWS Management Console: A user-friendly interface to create and configure your EKS cluster and worker nodes, as well as auto-configure kubectl. eksctl CLI: A dedicated command-line tool that simplifies cluster creation. For instance: $ eksctl create cluster Infrastructure as Code: Use tools like Terraform or Pulumi to define and deploy your EKS cluster and associated resources programmatically. Pro Tip Leveraging Infrastructure as Code not only streamlines the deployment process but also ensures consistency and version control for your EKS configurations. By choosing the right combination of worker node management and cluster creation methods, you can efficiently manage your Kubernetes environment while focusing on application development and innovation. For further reading, check out: AWS EKS Documentation Kubernetes Official Site Watch Video Watch video content"
AWS Solutions Architect Associate Certification,ECR Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/ECR-Demo,"AWS Solutions Architect Associate Certification Services Compute ECR Demo In this lesson, you'll learn how to work with AWS Elastic Container Registry (ECR) by creating a repository, pushing a Docker image, and deploying a container from that image. This step-by-step guide is optimized for clarity and SEO, ensuring you quickly find the information you need. 1. Creating an ECR Repository Begin by searching for ""ECR"" in the AWS console. Select Create a repository or navigate to the Repositories section and click Create repository . On the Create repository page, review the General Settings where you'll find the visibility options: Public Repository: Anyone can pull the image without authentication (push rights remain restricted). Private Repository: Authentication is required to pull the image. Warning Once you create a repository, its visibility setting cannot be changed. For this demonstration, keep the repository visibility set to Private . Provide a repository name, for example, ""ECR-demo"". This name will be part of your full repository URI (e.g., …/ECR-demo). Optionally, you can configure additional settings like tag immutability, scan on push, or encryption. For this demo, these options are left disabled. Click Create repository to complete the process. Once created, you'll notice that there are no images in the repository yet. 2. Authenticating, Building, Tagging, and Pushing an Image AWS provides commands to push an image to your new ECR repository. Start by authenticating Docker to your AWS account using the AWS CLI command below. This command retrieves an authentication token for Docker: aws ecr get-login-password --region us-west-1 | docker login --username AWS --password-stdin 841860927337.dkr.ecr.us-west-1.amazonaws.com Note Ensure that the AWS CLI is installed and configured with your access key and secret key. If you haven't set it up yet, run: aws configure After successful authentication, build your Docker image, tag it with your repository URI, and push it to ECR: docker build -t ecrdemo .
docker tag ecrdemo:latest 841860927337.dkr.ecr.us-west-1.amazonaws.com/ecrdemo:latest
docker push 841860927337.dkr.ecr.us-west-1.amazonaws.com/ecrdemo:latest Once the push is complete, refresh the AWS console. Your repository will now display details about the latest image, including its tags, URI, digest, and push date. 3. Deploying a Container from Your ECR Image To confirm that the image in your ECR can be pulled and run as a container, start by removing the local image. This step ensures Docker pulls the latest image from the repository during deployment. List your images and remove the local one: docker image ls
docker image rm 841860927337.dkr.ecr.us-west-1.amazonaws.com/ecrdemo Now, run a Docker container using the remote image. In this example, the container is named ""app"" and port 3000 is mapped: docker run --name app -p 3000:3000 841860927337.dkr.ecr.us-west-1.amazonaws.com/ecrdemo The output should indicate that Docker did not find the image locally, pulled it from ECR, and started the server on port 3000. An example output is shown below: Unable to find image '841860927337.dkr.ecr.us-west-1.amazonaws.com/ecrdemo:latest' locally
latest: Pulling from ecrdemo
Digest: sha256:d5708e91c8580819a91fba467c25662a4f6ff55e7929341baaf0c9ab84cd822
Status: Downloaded newer image for 841860927337.dkr.ecr.us-west-1.amazonaws.com/ecrdemo:latest
Server is running on port 3000 To verify that the container is running, open another terminal and execute: curl localhost:3000 This should return HTML output similar to the example below, confirming that your application is up and running: <html lang=""en"">
<head>
  <meta charset=""UTF-8"" />
  <meta http-equiv=""X-UA-Compatible"" content=""IE=edge"" />
  <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"" />
  <link rel=""stylesheet"" href=""css/style.css"" />
  <title>Document</title>
</head>
<body>
  <h1>ECS Project 2</h1>
</body>
</html> 4. Important Naming Conventions and Permissions When pushing and pulling images from ECR, always use the full repository URI in the following format: ACCOUNT_ID.dkr.ecr.REGION.amazonaws.com/REPOSITORY_NAME For example, if your account ID is 841860927337, your region is us-west-1, and your repository is named ""ecrdemo"", then the full image URI is: 841860927337.dkr.ecr.us-west-1.amazonaws.com/ecrdemo:latest Make sure that any platform (EC2, ECS, Kubernetes, etc.) that uses this image has the proper IAM permissions or roles to pull images from ECR, ensuring a seamless deployment experience. That concludes the lesson on using AWS ECR. You've successfully learned how to create a repository, push a Docker image, and deploy a container. Continue exploring AWS container services for advanced deployment strategies and best practices. Happy coding! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,ECS,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/ECS,"AWS Solutions Architect Associate Certification Services Compute ECS In this lesson, you'll learn how to containerize applications, the benefits and challenges of using containers, and how AWS Elastic Container Service (ECS) addresses these challenges. For more detailed AWS ECS training, visit AWS Elastic Container Service (ECS) . What Are Containers? Containers package an application with all its necessary files, libraries, and dependencies into a portable and lightweight environment. This makes it easy to deploy your application anywhere—whether on your local machine or in production—without any additional installation steps. Think of containers as lightweight virtual machines that include only what is needed to run your application. Challenges with Containers When working with containers, be aware of the following challenges: Deploying a single container on one server creates a single point of failure. If that server goes down, your application is affected. For redundancy and high availability, you need to deploy multiple container instances across multiple hosts. Load balancing is essential to ensure even distribution of incoming requests. Containers often need to communicate across different networks or subnets. Automated monitoring is crucial to automatically redeploy containers if one fails. Scaling out as traffic increases (and scaling in when it decreases) is necessary for efficient resource usage. To address these challenges, container orchestrators are employed. Container Orchestrators Container orchestrators provide a management layer for containerized environments. They handle tasks such as: Deploying containers across multiple servers Load balancing requests Facilitating container-to-container communication Restarting failed containers automatically Relocating containers when hosts fail These orchestrators act like managers ensuring that your containers (think of them as employees) perform efficiently. Popular orchestrators include Kubernetes, Apache Mesos, and AWS ECS. ECS is AWS's proprietary solution for container orchestration challenges. Introducing Elastic Container Service (ECS) ECS is a fully managed container orchestration service designed to help you manage and scale containerized applications. With ECS, AWS manages the control plane—the ""brains"" of the operation—while you provide the compute resources. There are two primary launch options with ECS: EC2 Launch Type : You manage your own EC2 instances as a cluster, giving you complete control over the underlying servers. Fargate Launch Type: AWS handles the compute infrastructure using a serverless model. You only need to specify the required configurations, and AWS provisions the compute resources automatically. Choosing Between EC2 and Fargate EC2 Launch Type The EC2 Launch Type requires you to manage the underlying EC2 instances. This includes tasks like: Provisioning and configuring EC2 instances Installing Docker and the ECS agent for communication with ECS Managing firewall and overall security configurations Applying patches and updates to maintain secure infrastructure Once your instances register with the ECS control plane, containers (or tasks) are deployed across them. This method provides granular control over your infrastructure. Fargate Launch Type Fargate, on the other hand, offers a serverless approach where all infrastructure management is handled by AWS. You do not provision or maintain EC2 instances. Simply define your container configuration parameters (such as compute and memory requirements), and Fargate provisions the necessary compute resources on demand. This option follows a pay-as-you-go pricing model, making it cost-efficient. Once you set your configuration, ECS deploys your container tasks, and you pay only for the compute resources you actively use. Summary Comparison EC2: Requires upfront resource management, runs continuously, and offers enhanced control. Fargate: Minimizes management overhead, operates on a pay-as-you-go model, and streamlines the deployment process. Understanding ECS Tasks and Task Definitions A key concept in ECS is the task , which is an instantiation of a task definition. Task Definitions Before running containers on ECS, you first create a Dockerfile for your application, build an image, and upload it to a repository like Docker Hub. To launch this image on ECS, you define a task definition—a blueprint that includes details such as: The container image to use Memory and CPU allocations Port mappings Volume configurations Dependencies and other container settings Below is an example YAML snippet that resembles a Docker Compose configuration, outlining what you might include in a task definition: web:
  image: kodekloud-web
  ports:
    - ""8000:5000""
  volumes:
    - .:/code
  depends_on:
    - redis
  deploy:
    resources:
      limits:
        cpus: '0.50'
        memory: 50M In this example, the task definition serves as a blueprint for launching one or more container tasks. ECS Services An ECS service ensures that a specified number of tasks (containers) are always running. When you create an ECS service, you associate a task definition with the desired number of instances. For instance, if you're deploying a Python application, you might define its task and then create a service to maintain two or more instances. If a task fails or if an instance goes down, the service will automatically redeploy it. Typically, each component of your application (such as the front end, back end, and database) would have its own service to ensure the required number of containers are consistently available. Load Balancers in ECS Implementing load balancing is essential when deploying multiple container tasks across several hosts. AWS Elastic Load Balancers (ELB) are commonly used to distribute incoming traffic evenly across all available container instances. The load balancer performs the following functions: Receives incoming requests Forwards the requests to the appropriate container tasks Dynamically adjusts as new containers are added or removed Although adding a load balancer is optional, it is highly recommended for production environments to ensure high availability and reliability. This lesson has provided a comprehensive overview of container basics, the challenges associated with containers, and how AWS ECS simplifies container orchestration using both EC2 and Fargate launch types. Additionally, you learned about ECS tasks, task definitions, services, and the critical role of load balancers in managing your containerized applications. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Lambda,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/Lambda,"AWS Solutions Architect Associate Certification Services Compute Lambda AWS Lambda is a serverless, event-driven compute service that lets you run your code for any type of application or backend service without managing servers. For a comprehensive introduction, check out the AWS Lambda Course . Simply upload your code, define a trigger or event rule, and let AWS handle all the infrastructure, scaling, and management. Key Advantage Lambda removes the operational overhead of server management, allowing you to focus on writing code while AWS takes care of provisioning, scaling, and maintenance. Dynamic Scalability for E-Commerce Imagine an e-commerce website experiencing a surge in traffic during the holiday season. Instead of manually provisioning and managing EC2 instances, you can upload your application code to Lambda. With properly set triggers in place, AWS scales your application automatically, handling increased traffic effortlessly without any manual configuration of compute resources. File Processing with Amazon S3 A typical use case for Lambda is processing files in an Amazon S3 bucket. For example, a video-sharing platform might trigger a Lambda function automatically when a user uploads a video file. The function can process the file by converting it into the correct format or generating multiple resolutions (480p, 720p, 1080p, 2K, and 4K) suitable for various devices. This design is illustrated in the diagram below: Building APIs with Amazon API Gateway and Lambda AWS Lambda can be integrated with Amazon API Gateway to build robust, scalable APIs. In this architecture, API Gateway handles incoming HTTP requests and routes them to designated Lambda functions that process the requests based on the URL and HTTP method. This serverless setup can also connect with databases like Amazon DynamoDB, creating a complete, efficient backend system. The following diagram demonstrates this architecture: Key Benefits and Features AWS Lambda offers several powerful features that make it a preferred solution for serverless applications: Serverless Management: Upload your code while AWS automatically takes charge of provisioning, scaling, operating system updates, monitoring, and logging. Event-Driven Processing: Lambda functions are triggered by events (e.g., file uploads to S3, updates in DynamoDB), ensuring that your code executes in response to changes in your environment. Multi-Language Support: Develop your functions in a variety of languages including Java, Go, Node.js, C#, Python, and Ruby. Lambda's runtime API also enables support for other languages and third-party libraries. High Availability and Scalability: With compute capacity maintained across multiple availability zones, Lambda offers built-in high availability and automatically scales with the load, ensuring reliable performance without manual intervention. The diagram below encapsulates these features: Watch Video Watch video content"
AWS Solutions Architect Associate Certification,EC2 Image Builder,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/EC2-Image-Builder,"AWS Solutions Architect Associate Certification Services Compute EC2 Image Builder EC2 Image Builder is an AWS managed service designed to automate the creation, management, and deployment of customized AMI images. Whether you prefer to use the AWS Management Console, CLI, or APIs, EC2 Image Builder streamlines the process of producing tailored images for your AWS account. Organizations often start with a ""golden image"" when creating an AMI. A golden image is a pre-configured, fully provisioned master image that includes essential software applications, configurations, and settings. It acts as the definitive standard from which all subsequent instances are derived. How to Create a Golden Image Follow these steps to create a golden image with EC2 Image Builder: Select a Base Image: Begin with a clean operating system installation like Ubuntu or macOS. This base image serves as your starting point for further customization. Build Phase: Install necessary applications and tools. Remove any unwanted software packages to maintain a lean image. Customization: Adjust system and network settings and run any custom scripts to further tailor the image to your requirements. Testing Phase: Execute functional tests or security checks to confirm that the image meets your standards. While AWS offers built-in tests, you can also define custom tests suited to your environment. Distribution Phase: Specify the AWS regions where the image should be available and share it with other AWS accounts if needed. Let's visualize this golden image creation process using EC2 Image Builder: During the build phase, a pre-existing AMI serves as the source image. Customizations such as adding or removing software packages, adjusting settings, and running scripts transform the image. After extensive testing to ensure both functionality and security, the image is distributed to your specified AWS regions or shared accounts. Deploying the Custom Image Once your new image is ready, the process advances to the run stage. In this phase, deploy one or more EC2 instances using the custom image. You can manage this deployment seamlessly via the CLI, console, or SDK. Automating Your Image Creation Pipeline EC2 Image Builder allows you to automate the entire image creation pipeline. The process starts with specifying a source image and a build component (or ""recipe""). Then, you configure the necessary infrastructure settings—such as VPCs, subnets, and security groups—for the build and testing phases. Finally, you set up the distribution configuration, which dictates where and how the image should be shared. Key Benefits Automated Image Creation: Eliminates manual steps and minimizes human error. Enhanced Security: Automates patch management and applies AWS or custom security policies. Consistent Workflows: Ensures standardization across all deployments. Simplified Version Management: Easily roll back to previous images if needed. By automating these processes, EC2 Image Builder significantly reduces manual efforts, minimizes security vulnerabilities, and guarantees consistent deployments across various environments. For more detailed guidance and best practices, consider exploring the AWS Documentation and other related resources. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Elastic Network Interfaces Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/Elastic-Network-Interfaces-Demo,"AWS Solutions Architect Associate Certification Services Compute Elastic Network Interfaces Demo In this lesson, you'll learn how to work with AWS network interfaces to improve the flexibility and management of your EC2 instances. Instead of configuring network settings directly on your EC2 instance, you can create a standalone network interface that encapsulates key network configurations—such as subnet placement, IP address, and security groups. This modular approach allows you to attach network interfaces to one or more EC2 instances as needed. Creating a Network Interface Follow these steps to create a network interface in AWS: Open the EC2 page in the AWS Management Console. Scroll down and select ""Network Interfaces."" Click on the option to create a network interface. Provide a clear description (for example, ""my EC2 interface""). Choose the appropriate subnet for the interface. For the private IP address, decide whether to auto-assign it or specify a custom IP (in this example, auto-assign is used). Select the desired security group, such as ""Web SG."" After configuring these details, create the network interface. You should see the new interface in your list (e.g., ""my EC2 interface""). At this point, you have the option to attach the network interface to an existing EC2 instance or assign it during the launch of a new instance. AWS also allows you to associate an Elastic IP with the network interface so that a reserved public IP remains consistently linked to the interface. When associating an Elastic IP, simply select the Elastic IP from the dropdown menu: Launching an EC2 Instance with an Existing Network Interface Next, you'll learn how to launch an EC2 instance using an existing network interface: Start by launching a new instance and assign a descriptive name (for example, ""EC2 Interface Demo""). Select the Amazon Linux AMI, choose the T2 micro instance type, and pick the appropriate key pair. In the network settings section, select ""Edit"" to review the VPC, subnet, and security group configurations. Open the ""Advanced network configuration"" section to view the default network interface (device index 0) that will be used. Instead of keeping the default configuration, select the existing network interface you created earlier (look for an ID like ""eni-3E3E...""). You might need to search for the specific interface ID. Leave the other settings unchanged and launch the instance. Once the instance is up and running, inspect its network configuration to verify the assigned private IP address. Even though no public IP is directly assigned to the instance, AWS maps a public Elastic IP to the instance via network address translation (NAT). Attaching Additional Network Interfaces An EC2 instance can have multiple network interfaces. To attach an additional interface, proceed as follows: Create a new network interface (e.g., named ""NIC 2"") in the same availability zone as your EC2 instance. This interface can reside in a different subnet, provided it's within the same zone. You may use the same security group if desired. Optionally, associate an Elastic IP with ""NIC 2"" by selecting one from your Elastic IP addresses. Return to the EC2 instances page, select your demo instance, and choose the attach option for network interfaces. Select ""NIC 2"" from the list and attach it. This action can be performed while the instance is running. After the attachment, your instance will have two network interfaces: the original one and ""NIC 2"" (with its associated Elastic IP if configured). SSH Access Tip Using the public Elastic IP associated with ""NIC 2,"" you can SSH into the server without directly exposing the instance’s primary network configuration. Here is an example of logging in via SSH from a Windows command prompt: # Example SSH command from a Windows command prompt
C:\Users\sanje\Documents\scratch\aws-demo> ssh -i main.pem [email protected] ,#
    ,_#
     |########
   ~~~  ____
      V~->  
    ~~~  /m/'
Last login: Thu Oct 12 04:43:18 2023 from 173.73.184.248
[ec2-user@ip-10-0-5-93 ~]$ Once logged in, view the network interfaces on your instance with the ip add command: [ec2-user@ip-10-0-5-93 ~]$ ip add
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet6 ::1/128 scope host noprefixroute
       valid_lft forever preferred_lft forever
2: enX0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc fq_codel state UP group default qlen 1000
    link/ether 0e:c1:c6:4e:41:39 brd ff:ff:ff:ff:ff:ff
    altname eni-0f8642cda59e33e
    altname device-number-0
    inet 10.0.5.93/20 metric 512 brd 10.0.15.255 scope global dynamic enX0
       valid_lft 3291sec preferred_lft 3291sec
    inet6 fe80::cc1:c6ff:fe4e:4139/64 scope link
       valid_lft forever preferred_lft forever
3: enX1: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc fq_codel state UP group default qlen 1000
    link/ether 0e:c4:fc:6e:60:3c brd ff:ff:ff:ff:ff:ff
    altname eni-014935bd661362d75
    altname device-number-1
    inet 10.0.0.17/20 metric 522 brd 10.0.15.255 scope global dynamic enX1
       valid_lft 3548sec preferred_lft 3548sec
    inet6 fe80::fcff:fe6e:603c/64 scope link
       valid_lft forever preferred_lft forever
[ec2-user@ip-10-0-5-93 ~]$ The output lists the loopback interface and the two attached network interfaces. Notice that only private IP addresses are present because AWS uses network address translation (NAT) to map the public Elastic IPs—thus keeping the public IPs hidden from the instance itself. Detaching Interfaces Network interfaces can be detached from a running instance via the instance's actions menu. Detached interfaces retain their IP addresses and security configurations, allowing you to attach them to another instance without reconfiguration. This is especially useful for maintenance and scaling. This separation between EC2 instances and their network configurations offers a robust and scalable method for managing your AWS cloud networking resources. That concludes our demonstration. We hope this lesson on AWS network interfaces has been both informative and practical for managing your EC2 networking setup. Additional Resources AWS EC2 Documentation AWS Networking Concepts Happy networking! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Elastic Beanstalk Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/Elastic-Beanstalk-Demo,"AWS Solutions Architect Associate Certification Services Compute Elastic Beanstalk Demo In this guide, we demonstrate how to deploy a simple Node.js application using AWS Elastic Beanstalk. The application is a basic web app that displays the message ""Okay, this is Elastic Beanstalk demo."" Elastic Beanstalk automates the provisioning and management of your infrastructure, including EC2 instances, load balancers, security groups, auto scaling, and other AWS resources. Let's walk through the process step-by-step. 1. Creating an Application in Elastic Beanstalk First, log in to the AWS Management Console and search for ""Elastic Beanstalk."" Under Applications, click Create an Application . Assign a name (e.g., ""Node.js app""), optionally add a description and tags, then click Create . This will take you to the application dashboard, where you can manage multiple environments, such as development, staging, or production. 2. Creating a New Environment Click to create a new environment. Since you are deploying a web application rather than a background task, select the Web Server Environment option. The application name will be pre-populated. Name your environment (e.g., ""environment-prod"" for production). Optionally, assign a custom domain or use the auto-generated one. Provide a description if needed. Select Node.js as your platform and choose the appropriate version. 3. Uploading Application Code Under the Application Code section, choose to upload your code. You can either store your code in Amazon S3 or upload it directly from your computer. For this demonstration, click Choose file . Note that Elastic Beanstalk requires a ZIP archive containing your application’s source code. To prepare your deployment package: Copy all necessary files. Create a ZIP archive (e.g., ""EB demo-v1.zip""). Once the ZIP file is ready, select it for upload. 4. Configuration and Presets Proceed through the configuration wizard. Elastic Beanstalk automatically populates many settings from presets. For this demo, choose a high availability deployment using multiple EC2 instances behind a load balancer. Even when using presets, you can adjust the settings later. Before moving to the next step, assign a version label for your application code (e.g., ""1.0.0"") and click Next . 5. Setting Up Service Roles and EC2 Key Pairs Elastic Beanstalk requires an IAM service role to perform AWS operations. If you do not have an existing role: Select Create and use new service role to have one generated automatically. Alternatively, choose an existing role if available. Similarly, assign an EC2 key pair or create a new one if needed. 6. Configuring VPC, Subnets, and Database Choose the Virtual Private Cloud (VPC) and subnets for your deployment: Use two private subnets for EC2 instances (which don’t require direct public access). Deploy the load balancer on two public subnets. Optionally, you can enable an associated database, though this demo leaves that option disabled. Additional tags can be added if required, and default settings for attached storage volumes are retained. 7. Configuring Security Groups and Auto Scaling Next, configure the security measures and scaling preferences: Select the security group provided by your VPC. Configure the auto scaling group by setting: The minimum number of instances (e.g., one instance). The maximum number of instances (e.g., four instances) based on anticipated load. The demo uses on-demand EC2 instances with default architectural settings, instance types, and the Amazon Linux AMI. 8. Configuring the Load Balancer Configure your load balancer to ensure proper distribution of incoming traffic: Deploy it on public subnets. Choose the Application Load Balancer type with a dedicated load balancer. Update the listener settings: The load balancer listens on port 80 by default. Since the Node.js application listens on port 3000, modify the listener configuration to forward HTTP traffic accordingly. 9. Monitoring and Deployment Settings Head to the monitoring section and adjust settings based on your requirements: Customize options for health reporting, CloudWatch custom metrics, and managed updates if needed. For this demo, the default configuration is retained, and managed updates are disabled. Configure email notifications and set deployment policies, such as choosing a rolling update to minimize user impact during code deployment. Review your configurations, and once you’re satisfied, submit the settings. Elastic Beanstalk will provision the environment and deploy your application, a process that may take a few minutes. 10. Verifying the Deployment After the environment shows a successful deployment, click the provided domain name to access your application. You should see that version 1.0.0 of your application is running. For further verification, navigate to the EC2 console: Under Instances, locate the running Node.js app environment instance (e.g., ""Nodejs-app-env-prod""). The load balancer and its forwarding rules, along with target groups, are also visible in the console. 11. Updating the Application Once your application is running, updates and new releases are straightforward. Suppose you add new features or bug fixes that require redeployment. Follow these steps to update your application: Navigate back to the Elastic Beanstalk environment dashboard. Click Upload and Deploy to load the new version of your code. Create a new version label (e.g., ""1.0.1"") for the updated code. Below is an excerpt from the updated HTML code and corresponding console output: <!DOCTYPE html>
<html lang=""en"">
<head>
    <meta charset=""UTF-8"" />
    <meta http-equiv=""X-UA-Compatible"" content=""IE=edge"" />
    <meta name=""viewport"" content=""width=device-width, initial-scale=1.0"" />
    <link rel=""stylesheet"" href=""css/style.css"" />
    <title>Document</title>
</head>
<body>
    <h1>Elastic BeanStalk demo v2</h1>
</body>
</html> Run `npm audit` for details.
C:\Users\sanje\Documents\scratch\eb-demo
> node index.js
Server is running on port 3000
^C
C:\Users\sanje\Documents\scratch\eb-demo
> node index.js
Server is running on port 3000 After confirming the changes locally, create a new ZIP archive (e.g., ""EB demo-v2.zip"") that includes all updated source files. Upload this new package, assign it the version label ""1.0.1,"" and choose your deployment policy. The default rolling update method minimizes downtime by gradually updating instances. Click Deploy and wait a few minutes for the update to complete. Once finished, verify the running version on the dashboard to ensure that version ""1.0.1"" of your application is active. Conclusion In this tutorial, we demonstrated how to deploy and update a simple Node.js application using AWS Elastic Beanstalk. We covered essential steps, including: Creating an application and environment. Uploading and packaging application code. Configuring key resources such as EC2 instances, load balancers, security groups, and auto scaling. Updating the application with new code revisions. This process automates infrastructure management and provides a scalable and reliable environment for deploying web applications. Note Elastic Beanstalk simplifies the deployment process, making it ideal for developers who want to focus on coding rather than managing servers. We hope you find this guide informative and helpful. For further details, read the official AWS Elastic Beanstalk Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,ECR,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/ECR,"AWS Solutions Architect Associate Certification Services Compute ECR In this lesson, we explore AWS Elastic Container Registry (ECR), a fully managed container registry service that simplifies storing, managing, and deploying Docker container images. ECR seamlessly integrates with AWS container orchestration tools such as ECS, EKS, and Fargate. When working with containerized applications, the typical workflow involves building the application, creating a Dockerfile with the necessary instructions, and generating a Docker image. Once the image is ready, you upload it to a container registry. While you can host your own registry on-premises or use third-party services, ECR offers a managed solution within the AWS ecosystem. For example, if you have images stored in ECR, a container orchestrator like ECS will pull an image from the registry to deploy containers. This streamlined process ensures efficient management and deployment of your containerized applications. No matter your deployment platform—ECS, EKS, or an on-premises environment—the authentication mechanism in ECR ensures that authenticated users can easily pull the container image when required. Repository Types in ECR ECR supports two types of repositories, each designed for different use cases: Public ECR Repository: Public repositories allow anyone on the internet to access the container images, making them ideal for open-source projects. While anyone can pull images from a public repository, pushing images requires authentication. AWS bills for storage in public repositories, but data transfer out to the internet is free. Private ECR Repository: Private repositories restrict access to specific AWS accounts or IAM users. These repositories are not accessible to the public, providing a secure option for proprietary software. Authentication is required for both pulling and pushing images, and AWS charges for both storage and data transfers. Integrating ECR with a CI/CD Pipeline Integrating ECR with your CI/CD pipeline can enhance your deployment workflow by automating the build, test, and deployment processes within AWS. A typical CI/CD workflow includes the following steps: Update your source code and push it to AWS CodeCommit. Trigger a pipeline run in CodePipeline upon code commit. AWS CodeBuild picks up the changes, builds the Docker image after running tests, and uploads the final image to ECR. Your preferred orchestration service (ECS, EKS, or an on-premises solution) deploys the updated image. Note Automating your CI/CD pipeline not only saves time but also ensures consistency across deployments by integrating source control, continuous integration, and deployment into one smooth process. Key Features of ECR ECR provides a robust and secure platform for managing container images, with features designed to support modern DevOps practices. Here are some of the key features: Managed Service: Fully managed by AWS, reducing operational overhead. Seamless AWS Integration: Integrates easily with ECS, EKS, Fargate, and Lambda. Repository Flexibility: Choose between public or private repositories based on your security and accessibility needs. Lifecycle Policies: Automate image retention policies to remove outdated or unused images, optimizing storage costs. Image Scanning: Automatically scan images for vulnerabilities using Amazon Inspector during the security assessment process. Summary Amazon ECR offers a scalable, secure, and fully managed solution for storing and deploying Docker container images. Its seamless integration with other AWS services makes it an excellent choice for developers looking to streamline the deployment of containerized applications. Whether you're building open-source projects with public repositories or managing proprietary software in private repositories, ECR has you covered. For more detailed information, refer to the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Serverless Application Model,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/Serverless-Application-Model,"AWS Solutions Architect Associate Certification Services Compute Serverless Application Model In this lesson, we explore the Serverless Application Model (SAM) and its pivotal role in simplifying the deployment and management of serverless applications on AWS. By leveraging SAM, developers can streamline infrastructure management and focus solely on building efficient, scalable applications. What is the Serverless Application Model? SAM is a toolkit designed to enhance the developer experience when building and running serverless applications on AWS. Similar to an architect creating a detailed blueprint for a house, SAM provides a comprehensive plan for your serverless infrastructure. This approach means you don't need to focus on every individual resource—instead, you work from a well-defined template that outlines how everything fits together. In this analogy, the finished house represents your serverless application. Constructing a house requires the coordinated effort of various components, and similarly, building a serverless application involves integrating multiple AWS services such as Lambda functions, databases, and API Gateways. SAM acts as the blueprint for your application, detailing how to deploy both the infrastructure and your application code seamlessly. The Role of SAM in Application Development As a developer or architect, you define a SAM template—typically written in YAML (or JSON)—which specifies all the serverless resources required for your application. This includes: AWS Lambda functions API Gateways Databases And other essential services The SAM CLI is a powerful command-line tool that assists in building, testing, and deploying serverless applications. It automates many of the traditionally complex processes in application deployment. Note The SAM CLI workflow begins with a configuration file similar to an AWS CloudFormation template. Once your template is defined, it is packaged along with your application code and uploaded to an S3 bucket. AWS CloudFormation then utilizes this template to swiftly deploy the defined resources. Key SAM CLI Commands The SAM CLI offers a suite of commands to enhance the serverless development cycle. Below is an example workflow: sam init          # Initialize a new serverless application
sam build         # Build the deployment package for your application
sam local invoke  # Invoke a local Lambda function for testing (invokes once and quits)
sam package       # Package the application for deployment
sam deploy        # Deploy the application to AWS Additional useful commands include: sam logs: Retrieve logs for your Lambda function. sam validate: Check your SAM template for errors. sam publish: Share your application via the AWS Serverless Application Repository. SAM and AWS CloudFormation SAM is an extension of AWS CloudFormation and follows a similar template-based deployment process. When you create a SAM template, you describe the entirety of your serverless application, including functions, APIs, and other resources. Once packaged and uploaded to an S3 bucket, AWS CloudFormation deploys the resources as defined in your template. This integration not only sets up your infrastructure but also generates a change set for managing updates efficiently. Tip Leveraging SAM and its CLI can significantly streamline the creation and management of complex serverless architectures, resulting in an organized and efficient deployment process. Happy coding! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Amplify,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/Amplify,"AWS Solutions Architect Associate Certification Services Compute Amplify In this lesson, we will explore AWS Amplify, a powerful platform designed to streamline web and mobile application development. What is Amplify? AWS Amplify is a comprehensive framework that simplifies the development process for both web and mobile applications. By automating the configuration of underlying services, Amplify enables developers to quickly set up scalable, secure, and well-architected solutions. With Amplify, you can easily add features such as authentication, storage (using services like Amazon S3 and DynamoDB), APIs, continuous deployment, analytics integration, and AI-powered chatbots. Amplify abstracts the complexities of individual AWS services, allowing you to focus on coding rather than managing infrastructure. For instance, when you integrate authentication into your application, Amplify automatically configures Amazon Cognito in the background. Similarly, services such as AppSync for APIs and DynamoDB for data storage are seamlessly managed on your behalf. Key Benefit By automating service configurations, AWS Amplify empowers developers to launch applications swiftly without the overhead of manual setup. Integrating AWS Services When users interact with your application, they engage with a network of AWS services configured via Amplify. This eliminates the need to manually set up components like DynamoDB tables or Cognito user pools. Instead, Amplify automates these tasks through simple commands, ensuring a smooth integration process. Amplify Studio One of the standout features of AWS Amplify is Amplify Studio—a visual development environment that integrates both frontend and backend development. Amplify Studio not only helps you build and manage backend components like APIs and databases, but also accelerates frontend development through pre-built UI components. These components, designed for frameworks such as React, allow developers to set up authentication pages and other common functionalities with minimal coding. This full-stack approach ensures that when a user interacts with your React web application, both the frontend and the backend—created and managed by Amplify—work in unison. This not only accelerates development but also simplifies the maintenance of your application's configuration. Key Features of Amplify AWS Amplify significantly speeds up the development lifecycle by offering a suite of tools for both front-end and back-end development. Here are some of its core features: Rapid Development: Build and deploy applications quickly with pre-configured services. Full-Stack Integration: Seamlessly combine front-end and back-end services to create comprehensive applications. Amplify Studio: Leverage a visual development environment that simplifies backend setup alongside an integrated CLI. Pre-built UI Components: Utilize readily available components to implement essential features like authentication and login pages effortlessly. Multi-Platform Support: Enjoy compatibility with popular web frameworks such as React, Angular, and Vue, as well as mobile platforms including iOS, Android, and React Native. By leveraging AWS Amplify, developers can concentrate on building feature-rich, innovative applications without being encumbered by the complexities of service configuration and integration. Get Started Explore more about Amplify and its integration with AWS services by visiting the AWS Amplify Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,AWS RDS,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Database/AWS-RDS,"AWS Solutions Architect Associate Certification Services Database AWS RDS In this article, we explore AWS RDS, a managed and scalable solution for running relational databases in AWS. We'll discuss why this service is essential and how it simplifies database administration by handling complex tasks such as backups, patching, high availability, and scalability. When deploying a database, you must consider several critical tasks: Ensuring proper configuration and hardening. Deploying in a highly available, fault-tolerant manner. Scaling resources dynamically to meet fluctuating workloads. Monitoring health and performance continuously. Enforcing strict security to prevent unauthorized access. Security Warning Neglecting regular backups or software patches can expose your database to security vulnerabilities and potential data loss. AWS RDS addresses these challenges by automating routine database operations. With just a few clicks, you can deploy a production-ready relational database with robust security and high availability. This managed service minimizes administrative overhead, allowing you to focus on writing application code rather than managing infrastructure. RDS is tailored for relational (SQL-based) databases and supports multiple engines, including MySQL, PostgreSQL, MariaDB, Oracle, and SQL Server. The primary benefits include: Offloading administrative tasks to AWS. Ensuring high availability via multi-AZ deployments. Enhancing disaster recovery with automated backups and read replicas. Adhering to industry-standard security practices. Instance Types Amazon RDS offers two main types of instances: General Purpose (M Family): Combines a balanced mix of computing power, memory, and network resources. This option is cost-effective and suitable for a variety of workloads. Memory Optimized: Provides increased memory capacity for workloads that require handling large, in-memory datasets. Deployment Models Amazon RDS offers several deployment models to cater to different use cases: Single Availability Zone (AZ) Deployment In a single-AZ deployment, your RDS instance is launched within one availability zone. This option is more cost-effective, making it suitable for development or staging environments; however, it does not offer high availability since all data is stored in one location. A failure in that AZ can lead to data loss. Multi-AZ Deployment In a Multi-AZ deployment, AWS automatically replicates your primary database instance to a standby instance in a different availability zone. If the primary instance fails, an automatic failover ensures continuity, offering enhanced redundancy and high availability. Read Replicas Read replicas provide additional copies of your primary database instance to distribute and balance read traffic. These replicas are read-only, ensuring that write operations are processed solely by the primary instance. They also serve as a disaster recovery option by enabling promotion to a standalone instance if necessary. Selecting the appropriate deployment model depends on your workload's scalability needs, cost considerations, and the level of data redundancy required. Keep in mind that Multi-AZ clusters might incur additional costs due to the extra instances involved. For example, if your database is hosted in North America but serves users in Africa, creating a read replica in Africa can significantly reduce latency by bringing data access closer to the user. Multi-AZ Cluster A Multi-AZ cluster combines multiple strategies to enhance resilience: Primary nodes handle read and write operations. Data is replicated across read replicas to distribute read traffic. A standby server in another availability zone provides backup. In some configurations, data is also replicated to an additional region for rapid disaster recovery. Blue-Green Deployments Blue-green deployments involve maintaining two distinct database environments: The blue environment is live and handles production traffic. The green environment is used to test changes and new deployments. Once testing is complete and verified, a switch-over is performed, transferring production traffic to the green environment with minimal downtime and no data loss. Note that both environments must reside within the same AWS account, which could lead to increased costs. Storage Types Amazon RDS supports different storage options to meet varying performance requirements: General Purpose SSD: Cost-effective storage ideal for a wide range of workloads on medium-sized database instances. Best suited for development and testing environments, this option offers three IOPS per gigabyte with the ability to burst up to 3000 IOPS. Provisioned IOPS SSD: Designed for I/O-intensive workloads that demand low latency and consistent throughput. This storage type is ideal for production environments requiring high performance. Magnetic Storage: An older option based on traditional hard disk drives (HDDs). This storage type offers slower performance compared to SSDs and is being phased out for newer database engine versions. RDS Configuration Options When configuring RDS instances, several parameters and groups help you tailor the behavior of your database: Database Parameter Groups: Collections of parameters that control the behavior of your database engine, including performance, security, and resource allocation. Database Option Groups: Manage extra features such as encryption and performance enhancements. These options can be attached to database instances as needed. Subnet Groups: Specify the subnets within your Amazon VPC to deploy your database instances, ensuring proper network configuration. Security Groups: Control inbound and outbound network traffic to ensure that only authorized IP addresses and ports have access to your database. Database Snapshots: Backup copies of your database instances. You can create manual snapshots or configure automated daily backups, allowing restoration to a specific point in time. Additional configuration features include: Parameter Store: Securely stores configuration data and sensitive information. Performance Insights: Provides visual representations of database load and query execution patterns. Enhanced Monitoring: Collects detailed performance metrics for troubleshooting and optimization. Audit and Log Data: Tracks database activities and security events. Encryption: Supports encryption at rest and in transit, safeguarding sensitive data. Key Features and Benefits AWS RDS provides several essential advantages: Quick Deployment: Launch a production-ready relational database within minutes using the AWS Console, SDK, or API. Pre-configured optimal parameters allow immediate connection to your application. Managed Administration: AWS handles patching, backups, provisioning, and maintenance, significantly reducing the administrative burden. Built-in Monitoring: Integrates with CloudWatch and the RDS Console to provide real-time insights into compute, memory, storage capacity, I/O activity, and connections. Blue-Green Deployment: Enables safer and faster updates with minimal downtime and zero data loss. High Availability and Durability: Automated backups and Multi-AZ deployments allow restoration to any specific point in time (up to the last five minutes) during your retention period. Versatile Use Cases: Ideal for web and mobile applications, AWS RDS lets you focus on innovation by shifting database management to AWS. It also simplifies migration from legacy databases by delivering scalability, performance, and reliability cost-effectively. In summary, AWS RDS is a fully managed relational database service that supports multiple engines such as Aurora, PostgreSQL, MySQL, MariaDB, Oracle, and SQL Server. It can be deployed in single AZs for cost savings or across multiple AZs for high availability. With features such as blue-green deployments, multiple storage types, read replicas, and robust backup capabilities, RDS greatly simplifies database management while ensuring scalability, performance, and compliance with best practices. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,RDS Aurora and Aurora Serverless,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Database/RDS-Aurora-and-Aurora-Serverless,"AWS Solutions Architect Associate Certification Services Database RDS Aurora and Aurora Serverless In this lesson, we explore AWS Aurora, a cloud-based relational database service built to deliver next-generation commercial database features with cost efficiency. Engineered for high performance, scalability, and durability, Aurora offers significant advantages over traditional database solutions—all while reducing overall costs. Aurora is fully compatible with both MySQL and PostgreSQL, making it a straightforward drop-in replacement. Let’s dive into the unique benefits and architectural innovations that distinguish Aurora from conventional databases. The Challenge with Traditional Database Architectures Traditional databases typically combine compute and storage within a single instance, which introduces several challenges: Longer snapshot durations since the entire dataset must be copied. Difficulty scaling storage independently of compute resources, potentially leading to downtime or complex migrations. Extended backup and restore times, especially when dealing with large storage volumes. The necessity to overprovision storage for peak capacity, resulting in unnecessary expenses. These limitations underscore the importance of decoupling compute and storage to boost scalability, availability, and durability. Aurora Storage Architecture Aurora overcomes these challenges with a unique, log-structured distributed storage system designed for modern cloud environments. Key characteristics include: The storage volume is striped across hundreds of nodes with locally attached SSDs, ensuring high performance and durability. Continuous backup capability to Amazon S3 for seamless data protection and recovery. Data distribution across multiple Availability Zones (AZs), providing built-in high availability and resilience to data center failures. Automatic scaling of storage capacity, eliminating the need for manual intervention or downtime. Aurora further optimizes read-write I/O efficiency using its distributed design and a quorum model for operation approvals, ensuring data integrity and fault tolerance. In the event of an AZ failure, Aurora’s multi-AZ deployment supports continued write operations through effective data replication across zones. Additionally, Aurora employs a gossip protocol to rapidly identify and repair node discrepancies, significantly reducing recovery time in distributed systems. Components of an Aurora Database Cluster An Aurora database cluster is comprised of several key components: Database Instances: Primary Instance: Supports both read and write operations while updating the cluster volume. Each Aurora cluster has exactly one primary instance. Replicas: Support read-only operations and share the same storage volume as the primary. Up to 15 replicas can be deployed across different AZs, providing enhanced availability. If the primary fails, one of these replicas can be promoted to become the new primary. Cluster Volume: Acts as a virtualized storage layer that spans multiple AZs, with each zone maintaining a copy of the entire database cluster's data. Aurora organizes its database volume into 10-gigabyte segments. Each segment is recorded as a protection group, consisting of six copies spread across three AZs (one full segment with data pages and log records, and one tail segment with log records only per AZ). This design facilitates automatic scaling, maximizes I/O efficiency, and maintains data integrity. Deployment Options: Provisioned vs. Serverless When deploying an Aurora database cluster, you have two configuration options: Provisioned Allocate a database instance with pre-configured CPU, memory, storage, and IOPS. Scale resources manually as needed. Benefit from Aurora Global Databases, which provide low latency global reads and regional failover capabilities. Serverless Ideal for workloads with unpredictable capacity requirements. Automatically scales capacity on demand based on application load. Payment is based solely on the actual resources consumed. Aurora Serverless v2 offers enhanced scalability and integration with Aurora Global features. Choosing the Right Deployment Option Consider serverless for applications with variable load and provisioned for stable, predictable workloads. Aurora Global Database Aurora Global Database clusters enable deployment of a consistent database system across multiple AWS regions: A primary database cluster in one region accepts write operations. Up to five secondary clusters in different regions serve as read-only replicas. Data is asynchronously replicated from the primary to each secondary cluster via a low latency, high throughput system. For Aurora Serverless, capacity is measured using Aurora Capacity Units (ACUs). Each ACU approximates 2 GB of memory along with the corresponding CPU and networking capacity. When configuring an Aurora cluster, you specify a range—from a minimum of 0.5 ACU up to a maximum of 128 ACUs. Aurora Serverless continuously monitors and adjusts database capacity, ensuring cost effectiveness by charging only for the compute resources used. While Aurora Serverless v1 still exists, AWS primarily recommends Aurora Serverless v2 for modern deployments, as v1 is being phased out. Integrations with Other AWS Services Aurora integrates seamlessly with various AWS services to enhance its functionality: Amazon S3: Used for scheduled backups and data recovery. CloudWatch: Provides robust monitoring of performance metrics. AWS Lambda, EKS, ECS, etc.: Facilitates connectivity between your applications and the database. Secrets Manager: Securely stores database credentials. Aurora's capabilities make it well-suited for: Modernizing enterprise applications (CRMs, ERPs, and supply chain or billing systems). Building SaaS applications that require flexible compute and storage scaling. Implementing efficient serverless architectures where you pay only for actual usage. Developing globally distributed applications, including mobile games and social media platforms. Summary Aurora is a fully managed relational database service that automates hardware provisioning, database setup, patching, and backups. Its advanced, fault-tolerant, and self-healing storage system can scale up to 128 terabytes per instance, ensuring both high performance and reliability. Key features include: Up to five times the throughput of MySQL and three times that of PostgreSQL. Support for up to 15 replicas across three Availability Zones, enhancing read scalability and availability. Fast cloning capabilities for rapid database replication in development and testing environments. Seamless integration with multi-AZ and global deployments for improved local read-write performance and disaster recovery. Aurora Serverless, which automatically adjusts capacity based on demand, offering significant cost savings. Aurora’s powerful features and flexible deployment options make it an excellent choice for enterprises and developers seeking a robust, scalable, and highly available cloud-based database solution. Learn More Explore additional details on AWS Aurora Documentation to further enhance your understanding and deployment strategies. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Database Agenda and Introduction,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Database/Database-Agenda-and-Introduction,"AWS Solutions Architect Associate Certification Services Database Database Agenda and Introduction In this lesson, we dive into the world of databases by drawing an analogy between various road types and specialized database systems. Just as cities design highways for rapid transit, city streets for moderate traffic, jogging paths for pedestrians (and sometimes cyclists), and dedicated cycling lanes, databases are engineered to handle different types of data and workloads. Imagine a city’s road network: highways are built for high-speed travel, urban streets handle medium-speed traffic, while pedestrian paths and cycling lanes cater to specific transit needs. Similarly, when you design an application, it’s crucial to choose a database that aligns with your specific performance, scalability, and data structure requirements. Key Insight Structured data typically benefits from SQL databases, which enforce fixed schemas and relationships. Conversely, NoSQL databases offer flexibility for handling unstructured or semi-structured data. For example, a streaming platform like Netflix may require a different database architecture compared to a banking system. The right choice of database can mean the difference between smooth, efficient operations and potential performance bottlenecks. In this lesson, we will explore the variety of databases offered by AWS. We will discuss their key features and use cases—from high-speed transactional systems to hierarchical data storage solutions. By understanding the unique characteristics of each database type, you will be better equipped to select the ideal solution for your application workload. By the end of this lesson, you will have a clearer understanding of how to match your application’s specific data requirements with the appropriate database technology. Happy learning! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Serverless Application Respository,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/Serverless-Application-Respository,"AWS Solutions Architect Associate Certification Services Compute Serverless Application Respository In this lesson, we explore the Serverless Application Repository , a managed repository tailored for serverless applications. This platform enables developers to share, deploy, and manage serverless applications and components across teams, organizations, or the broader AWS community. A typical workflow starts with a developer creating a serverless application using a SAM (Serverless Application Model) template. Once the application and its corresponding template are packaged, they are published to the AWS Serverless Application Repository. Once published, the application is made available to other users and the global AWS community. This resembles hosting an open-source project on GitHub , with the key distinction of being specifically designed for serverless applications. Users can easily search for and discover serverless applications within the repository. After selecting an application, they have the flexibility to configure it for their environment using customizable parameters. This process streamlines deployment and ongoing management within AWS infrastructure. Deployment Process When an application is selected from the repository, its SAM template is converted into AWS CloudFormation templates. These templates detail all the necessary AWS resources—such as Lambda functions, API Gateway configurations, and other services—required by the application. CloudFormation then deploys these resources, allowing for seamless application interaction. Overview The conversion of SAM templates into CloudFormation templates automates the deployment process, ensuring that applications have the necessary infrastructure components configured correctly. Key Features and Benefits Centralized Discovery: Access a single repository to search and discover numerous serverless applications. SAM Integration: Leverages deep integration with the Serverless Application Model, simplifying resource definitions. Efficient Collaboration: Facilitates the sharing, development, and deployment of serverless applications across teams and organizations. Reusable Components: Reduces redundancy by providing pre-built applications that can be customized and redeployed. Enhanced Security: Integrates with AWS IAM for detailed resource-level access control. Final Thoughts The Serverless Application Repository is an essential resource for developers and organizations looking to leverage serverless architectures. Its streamlined process and integration with AWS tools simplify the development and deployment of innovative applications. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Outposts,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/Outposts,"AWS Solutions Architect Associate Certification Services Compute Outposts AWS Outposts is an innovative solution designed to bridge the gap between cloud and on-premise environments. As businesses increasingly adopt cloud technologies for scalability, cost savings, and rapid delivery, many organizations still retain on-site data processing to meet compliance and security mandates. AWS Outposts enables you to leverage the same AWS services, APIs, and tools on-premise that you use in the cloud, delivering a seamless hybrid experience. Key Benefit With AWS Outposts, you no longer need to manage separate environments for on-premise and cloud-based workloads. Instead, you can run applications and workloads locally while interfacing directly with additional AWS services in a nearby AWS Region. How AWS Outposts Works AWS Outposts is a family of fully managed solutions that delivers AWS infrastructure and services directly to almost any data center or edge location. AWS experts deliver and install the Outposts rack in your data center, while you ensure the setup has the necessary power and network connectivity. Once deployed, the Outposts system connects to the nearest AWS Region using AWS Direct Connect or a VPN, providing a direct link between your on-premise resources and the cloud environment. Extending Your Virtual Private Cloud (VPC) By extending your VPC to include an Outposts subnet, instances running locally on Outposts can securely communicate with other instances in your VPC via private IP addresses. This setup essentially creates two parallel AWS environments—one in the cloud and one on-premise—that operate identically. Supported Services and Key Features AWS Outposts supports a comprehensive range of AWS services, including: EC2 Instances S3 Storage Lambda Functions DynamoDB RDS And many other familiar services This ensures that whether your workloads are in the cloud or on-premise, they are managed consistently. Key Features and Benefits Feature Benefit Example/Detail Fully Managed Service AWS delivers, installs, maintains, and updates your Outposts hardware. No need for on-site hardware lifecycle management. Hardware Rack at Your Site AWS Outposts is a physical product installed at your location, complete with servers, switches, and network components. Delivery and installation by AWS professionals. Consistent Hybrid Experience On-premise operations with Outposts mirror AWS cloud operations, simplifying management and operational tasks. Uniform API, tools, and services across environments. Support for On-Premises Workloads Tailored for workloads requiring low latency or adherence to strict data residency and regulatory requirements. Ideal for industries with compliance mandates. Important Ensure that your data center meets the power and connectivity requirements before deploying AWS Outposts. Also, consider redundancy and backup planning to guarantee uninterrupted service. Summary AWS Outposts offers a robust solution for organizations wishing to combine the flexibility and scalability of cloud computing with the security and compliance advantages of on-premise infrastructures. By using AWS Outposts, you can modernize your IT environment and empower your teams to operate consistently across both environments without the complexity of managing disparate systems. For more information, visit the AWS Outposts product page and explore additional AWS documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,ECSEKS Anywhere,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/ECSEKS-Anywhere,"AWS Solutions Architect Associate Certification Services Compute ECSEKS Anywhere In this article, we explore ECS and EKS Anywhere—solutions that empower you to deploy, manage, and scale containerized applications seamlessly across both AWS and your on-premises data centers. Overview Amazon EKS and ECS are well-known for deploying containerized applications in AWS environments. With Amazon EKS Anywhere, you can take advantage of the robust capabilities of EKS on your own infrastructure without the reliance on AWS exclusively. This flexibility allows businesses to benefit from a managed control plane across hybrid environments. Key Benefit One significant advantage of using an EKS Anywhere cluster is its integration with the AWS-provided EKS dashboard. This dashboard centralizes monitoring, management, and real-time performance metrics, ensuring streamlined operations regardless of where your cluster is hosted. Features of EKS Anywhere EKS Anywhere extends the EKS experience beyond AWS by delivering a consistent Kubernetes environment anywhere—be it on your virtual machines, bare metal servers, or other infrastructures. This makes it an ideal choice for hybrid deployments that require both cloud and on-premises clusters. Key features include: Consistent Kubernetes experience irrespective of the hosting environment Compatibility with on-premises infrastructure to meet legal and compliance mandates Ability to leverage the same EKS distribution across various platforms ECS Anywhere Overview ECS Anywhere mirrors the principles of EKS Anywhere for Amazon's Elastic Container Service (ECS). It enables you to run ECS on different infrastructures—including virtual machines, bare metal servers, and other cloud environments—while ensuring a unified management experience with the AWS cloud. Dashboard Integration ECS Anywhere uses the ECS agent to register external instances, providing a centralized dashboard for oversight of containerized applications regardless of their deployment location. The following table summarizes the differences and similarities between EKS Anywhere and ECS Anywhere: Feature EKS Anywhere ECS Anywhere Primary Service Kubernetes Elastic Container Service (ECS) Infrastructure Flexibility Virtual machines, bare metal, hybrid deployments Virtual machines, bare metal, other cloud environments Dashboard AWS-provided EKS dashboard Centralized ECS dashboard Use Case Hybrid cloud and on-premises Kubernetes clusters Consistent AWS container management across diverse infrastructures Final Thoughts Both EKS Anywhere and ECS Anywhere offer powerful solutions for organizations aiming to extend their containerized application deployments beyond the public cloud. They provide consistent experiences across different environments, making them excellent choices for hybrid cloud strategies and ensuring compliance with local data regulations. For more detailed information, consider exploring the following resources: Kubernetes Basics AWS Documentation ECS Developer Guide Watch Video Watch video content"
AWS Solutions Architect Associate Certification,VMware Cloud on AWS,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/VMware-Cloud-on-AWS,"AWS Solutions Architect Associate Certification Services Compute VMware Cloud on AWS In this article, we delve into VMware Cloud on AWS and explore its numerous benefits in a hybrid cloud environment. This innovative solution combines the robust capabilities of VMware's on-premises tools with the scalability and flexibility of AWS cloud services. Understanding VMware Technologies VMware is well-known for its virtualization platform, primarily used in on-premises data centers. Key VMware products include: vSphere: The virtualization layer (hypervisor) that creates and manages virtual machines (VMs) on physical servers. vSAN: A software-defined storage solution for efficient storage management. NSX-T: A network virtualization and security platform that enhances network operations. While many organizations continue to rely on VMware for on-premises operations, adopting AWS services (such as EC2 and VPCs) can create management challenges due to distinct operational tools and procedures. Note VMware Cloud on AWS addresses these challenges by offering a unified, hybrid environment where the on-premises experience seamlessly extends into the cloud. With this integration, you continue to leverage familiar VMware tools like vSphere, vCenter, vSAN, and NSX-T while AWS takes care of the infrastructure. Integrating VMware with AWS VMware Cloud on AWS enables you to deploy VMware VMs in AWS using the familiar vCenter interface. AWS manages the underlying infrastructure, thus allowing you to focus on your applications without directly engaging with native AWS services. Key Benefits Using a consistent VMware infrastructure across both on-premises and cloud environments provides several significant advantages: Consistent Management: IT teams work with a single set of tools, eliminating the need for extensive retraining. Effortless Workload Balancing: Seamlessly migrate VMs between environments. When on-premises servers experience capacity challenges, workloads can be shifted to the cloud with just a few clicks. Simplified Disaster Recovery: Effortlessly replicate and recover workloads as both environments are built on the same technology. Extended Data Center Capabilities: Use AWS as an extension of your on-premises data center, enabling rapid production workload migration without the need for re-architecting. Note This unified approach not only streamlines operations but also opens opportunities for integrating advanced AWS services. This enables the development of next-generation applications that combine the strengths of both VMware's virtualization and AWS’s cutting-edge cloud offerings. Conclusion VMware Cloud on AWS delivers a consolidated infrastructure that simplifies hybrid cloud operations. With the consistent management of on-premises and cloud environments, organizations can efficiently balance workloads, ensure robust disaster recovery, and drive innovation by harnessing both VMware and AWS technologies. For additional resources on cloud integration and hybrid architectures, please check out: Kubernetes Basics AWS Documentation Embrace the future of IT infrastructure by leveraging the seamless connectivity between VMware and AWS, ensuring your operations are agile, resilient, and ready for modern application development. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,The Snow Family compute mainly,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/The-Snow-Family-compute-mainly,"AWS Solutions Architect Associate Certification Services Compute The Snow Family compute mainly In this article, we explore AWS Snowcone as part of the broader AWS Snow Family—a suite of devices designed primarily for data storage and transfer, with select models that also offer compute capabilities. These features make them ideal for scenarios where local processing is required before transferring data to AWS. AWS Snow Family devices are physical storage solutions that enable the efficient movement of large volumes of data in and out of AWS. Transferring terabytes or petabytes over the internet can be impractical due to speed and security concerns, even with encryption. Instead, AWS ships a physical device to you. Once you load your data onto the device, you send it back to AWS, where the data is securely uploaded to your account. Process Overview The typical workflow is as follows: Order a Snow Family device. Configure the device locally. Copy your data onto the device. Ship it back to AWS. AWS uploads your data and securely wipes the device after processing. Among the Snow Family products, this article focuses on two models with compute capabilities: Snowball Edge and Snowcone. Both devices are equipped with processors that allow you to run services such as AWS Lambda functions directly on the device, which is particularly valuable in rugged environments where deploying a full server is not feasible. These ruggedized devices are built to endure extreme conditions, making them suitable for remote locations such as construction sites, ships, airplanes, or other challenging environments. They allow you to perform local computations, securely store data, and later transfer the data to AWS when connectivity becomes available. AWS Snowcone Device Overview The AWS Snowcone is designed for portability and resilience. Key specifications include: Storage Capacity: 8 terabytes of usable storage Portability: Weighs about 4.5 pounds Dimensions: Approximately 9 inches x 6 inches x 3 inches Durability: Dust-tight and water-resistant construction Additional security features of the Snowcone include built-in encryption using AWS Key Management Service and support for a Trusted Platform Module (TPM) that establishes a hardware-based root of trust. Beyond data transfer, the Snowcone is capable of handling localized edge computing workloads. Services such as AWS IoT Greengrass or launching Amazon EC2 instances at the edge enable processing to occur right where your data is collected. Devices across the Snow Family offer similar functionalities with varying capacities and form factors, all providing rugged durability, secure storage, and onboard compute capabilities. Key Features at a Glance Feature Description Edge Computing Run compute workloads at the data source for faster processing and real-time analytics. Data Migration Collect data locally and securely ship the device back to AWS for smooth data integration. Rugged Design Built to operate in harsh environments such as remote construction sites or maritime areas. Built-in Encryption Protects data with AWS Key Management Service and a hardware-based Trusted Platform Module. Portability Compact and lightweight design ensures easy transport and deployment in the field. This article demonstrates how AWS Snowcone effectively bridges the gap between secure data storage, swift data migration, and on-site computational power, offering a versatile solution for environments where traditional data centers are inaccessible. For further information on AWS services, you can explore these resources: AWS Documentation AWS Lambda Amazon EC2 Watch Video Watch video content"
AWS Solutions Architect Associate Certification,App Runner,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/App-Runner,"AWS Solutions Architect Associate Certification Services Compute App Runner Discover AWS App Runner, a fully managed service that simplifies application deployment by abstracting the underlying infrastructure. What is AWS App Runner? AWS App Runner enables you to deploy applications effortlessly by simply pushing your code to a Git repository. The service automatically builds and deploys your application on a managed infrastructure—eliminating the need to configure EC2 instances, ECS clusters, or other infrastructure components. App Runner supports two primary deployment methods: Source Code Deployment: Upload your code, and App Runner builds and deploys your application. Container Image Deployment: If your application is containerized, push your container image to Amazon ECR, and App Runner deploys it directly. This streamlined process allows you to focus on writing code rather than managing servers or complex CI/CD pipelines. CI/CD Integration with App Runner One of the standout features of AWS App Runner is its built-in CI/CD pipeline, which integrates seamlessly with popular Git repositories like CodeCommit and GitHub. The workflow is outlined below: Push to Repository: A merge into the main branch triggers the CI/CD pipeline. Build Process: AWS CodeBuild retrieves the source code and executes tasks specified in the build configuration. This stage typically includes compiling, running unit tests, and assembling a Docker image. Deployment: Once built, the Docker image is pushed to Amazon ECR, and App Runner deploys the new image automatically. Below is a high-level diagram illustrating the CI/CD pipeline flow using various AWS services: After deployment, your application becomes accessible via a unique domain or URL provided by App Runner. Whether your application interacts with DynamoDB, uploads files to S3, or calls third-party APIs, App Runner handles it as a standard AWS-hosted application. Private VPC Resources When your application requires secure access to resources within a private VPC, such as a DynamoDB database or an RDS instance, AWS App Runner offers a VPC Connector. This feature securely bridges your application with private VPC resources while maintaining its public accessibility. Secure Access Reminder Ensure that the VPC Connector is correctly configured to maintain secure and reliable access to your private resources. Benefits and Features of App Runner AWS App Runner offers a wide range of benefits that simplify application deployment and management: Automatic Deployment: Automatically deploy new changes from your connected repository. Source Integration: Easily integrates with platforms such as CodeCommit, GitHub, and ECR. Automatic Scaling: Scales your application dynamically based on traffic, ensuring efficient resource usage. Integrated AWS Services: Works seamlessly with services like RDS and Lambda, enhancing your application's capabilities. Below is a visual representation of the key features provided by App Runner: With AWS App Runner, developers can concentrate on building and refining their applications while AWS manages deployments, scaling, and infrastructure maintenance. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Lightsail,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/Lightsail,"AWS Solutions Architect Associate Certification Services Compute Lightsail AWS Lightsail is an ideal solution for developers, students, and small business owners who wish to deploy applications quickly without the complexity of the full AWS ecosystem. By offering a streamlined virtual private server (VPS) experience, Lightsail provides essential services like compute instances, container deployments, managed databases, and networking configurations in an easy-to-use package. Lightsail can be seen as a simplified version of EC2, where many pre-configured software stacks are readily available. For instance, deploying a WordPress site requires just a single click—no need to manually configure VPCs, subnets, or public IP addresses. Lightsail takes care of these technical details behind the scenes. With Lightsail, you not only get virtual servers equipped with built-in firewalls for traffic management but also the ability to deploy containerized applications, use scalable load balancers, and leverage managed databases and content delivery networks (CDNs). This consolidation of essential AWS elements helps minimize complexity while maintaining the robust reliability of AWS infrastructure. How to Deploy a Lightsail Instance Deploying an instance on Lightsail involves a few straightforward steps: Select the Server Location: Choose a geographical region such as North America or Europe. Pick a Platform: Select from various operating systems like Ubuntu, Amazon Linux, CentOS, or Windows. Pre-install a Development Stack: Options include popular stacks such as WordPress, LAMP, or MEAN. Choose Your Instance Plan: Define the resources that fit your project requirements. Name Your Server and Launch: Once configured, click the ""Create Instance"" button. Lightsail automatically handles the underlying VPC configuration and provides public internet access to your instance. Note For a seamless setup experience, review your instance location and platform selection carefully to improve latency and resource availability for your target audience. Supported Operating Systems Lightsail is compatible with a broad range of operating systems to suit your deployment needs. Common choices include: Amazon Linux Ubuntu Debian openSUSE CentOS FreeBSD Windows Pre-configured Software Stacks For users requiring rapid deployment of content management systems and other applications, Lightsail offers a variety of blueprints. These pre-configured software stacks include popular solutions such as WordPress, Joomla, Magento, and many more. This feature eliminates the need for manual configuration post-deployment, saving time and effort. Deploying Containerized Applications If your application is containerized, Lightsail supports direct deployment of container images. The process is simple: Build your container image. Push the image to Lightsail. Deploy the containerized application so users can immediately interact with it. Key Benefits of AWS Lightsail Benefit Description Ease of Use Simplifies cloud deployment without requiring in-depth AWS expertise. Pre-configured Solutions One-click installations for common stacks like WordPress, LAMP, and MEAN streamline setup. Cost-effective Reliability Delivers robust performance on AWS infrastructure at a lower cost compared to traditional setups. Seamless Scalability Easily transition to advanced AWS services, supported by built-in integrations with over 90 AWS offerings. Note AWS Lightsail is perfect for small to mid-sized projects that require a balance of simplicity and reliability. As your project grows, transitioning to services like Amazon EC2 becomes straightforward. AWS Lightsail offers a powerful and hassle-free way to deploy cloud applications, combining simplicity with the strength of AWS infrastructure. Whether you're launching a blog, an e-commerce store, or a custom application, Lightsail provides the essential tools you need to get started quickly and efficiently. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Step Functions Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/Step-Functions-Demo,"AWS Solutions Architect Associate Certification Services Compute Step Functions Demo In this article, we demonstrate how to work with AWS Step Functions by creating a simple state machine that processes events from a camera service. Imagine a scenario where a smart doorbell captures video upon detecting motion. The recorded event is sent to our state machine, which then evaluates whether the motion indicates a threat or is merely a false alarm. Depending on the outcome, different Lambda functions are triggered to handle each case. Creating a New State Machine Begin by searching for ""Step Functions"" in the AWS Console. This will take you to the page where you can create your first state machine. Click on Create state machine . A state machine is the location where you build your workflow logic by combining individual steps that process your tasks. AWS offers a variety of templates to help you get started quickly. For this demo, we will select the blank template to build our workflow from scratch. After choosing the blank template, you are directed to the authoring window for your state machine. Overview of the State Machine Authoring Window At the core of the authoring window, you will see the graphical depiction of your state machine workflow. The flow begins from the Start node and progresses to the End node as all steps are executed. On the left side, you'll find various actions and integrations with other AWS services. Popular integrations include invoking AWS Lambda functions, publishing to SNS, or executing ECS tasks. Scrolling down reveals many additional options for customizing your workflow. The Flow section enables you to incorporate advanced logic into your state machine, such as if-then-else conditions with a choice state, parallel execution of steps, or wait states to introduce delays. The right panel displays specific configuration options for each state—allowing you to set parameters like function selection and error handling. Demo Application Scenario In our demo scenario, a camera records an event and sends the data (such as an image or video clip) to the state machine. The first step in the workflow is to invoke a Lambda function named detect_threat , which analyzes the input to determine if the event represents a threat. The detect_threat Lambda function uses simple logic to simulate threat detection by randomly returning either ""THREAT"" or ""NO_THREAT"": export const handler = async (event) => {
    // Determine if there's a threat
    const result = Math.random() < 0.5 ? ""THREAT"" : ""NO_THREAT"";
    return { result };
}; After dragging the Lambda function into the workflow, rename the state to Detect Threat . Select the optimized integration type and choose the appropriate Lambda function (in this case, detect_threat ) from the dropdown menu. Next, configure the payload so that the state input is forwarded to the Lambda function. This ensures that the event data triggering the state machine is available for analysis. You can also adjust error handling by defining retry intervals, maximum attempts, and backoff rates. For this demo, the wait-for-callback feature is disabled, and the state transitions to End after execution. Adding Decision Logic with a Choice State Based on the output from detect_threat , the state machine must decide which branch to follow. This is accomplished by adding a choice state to the workflow. Configure the choice state with two conditions: If Result Equals ""THREAT"": Set the condition by specifying the variable $.result , choosing ""is equal to"" as the comparator, and entering THREAT as the string value. Drag a new Lambda state into the workflow and name it Threat Found . Select the corresponding Lambda function (e.g., threatfound ) from the dropdown. Configure the branch to end the workflow after execution. If Result Equals ""NO_THREAT"": Add another rule where the result is equal to NO_THREAT . Drag in the Lambda state for handling false positives, name it No Threat , and select its associated Lambda function (e.g., falsepositive ). Set the transition for this branch to lead to the End state. Once the rules are configured, the diagram visually illustrates the conditional branches. You can also remove any default conditions if they are not required. The final configuration should clearly display two branches: one for THREAT and another for NO_THREAT . Lambda Function Logic for Post-Detection After the detect_threat function returns its result, the state machine executes one of two Lambda functions: Threat Found Function: This function runs when a threat is detected. It may handle actions such as sending alerts and notifications. False Positive Function: This function addresses situations where the detected event is a false alarm. Here is a simple implementation: export const handler = async (event) => {
    // Logic for a false positive detection
    return ""threat found"";
}; In a real-world application, these functions might include more advanced logic like sending emails via Amazon SES, logging events to a database, or performing additional verification steps. Note For production environments, ensure that each Lambda function is thoroughly tested and secured according to AWS best practices. Testing the State Machine After configuring all states and decision logic, click Create . AWS automatically provisions the necessary IAM role with permissions that allow the state machine to invoke the configured Lambda functions and access other services like AWS X-Ray. If you encounter errors related to error handling (such as an extraneous error catcher on the Detect Threat state), simply remove or adjust the configuration. Once the state machine is created, test it by initiating an execution. Provide sample input data simulating a camera event. For example, you may supply a simple JSON payload like: {
  ""comment"": ""Insert your JSON here""
} Or to mimic a realistic event: {
  ""camera_id"": ""ssldfjsikd-234-sdfs"",
  ""object_id"": ""video1.mp4""
} After you start the execution, the console displays a graphical representation of the execution path. Colors within the diagram indicate which branches were followed—for example, starting at Detect Threat , moving to the Choice state, then progressing along either the threat or false positive branch, and finally reaching the End state. Click on any state in the execution graph to inspect detailed information such as input, output, configuration parameters, and an event log. This is instrumental in troubleshooting and understanding your workflow. For instance, one execution might show that the detect_threat function returned ""NO_THREAT"" , triggering the false positive function. A different execution might reveal the threat branch being taken. After multiple executions, review the summary in the state machine main window. This summary provides a breakdown of successful, failed, timed out, or aborted runs. You can examine these details in either a graphical view or table format. Conclusion This demo has guided you through building a simple AWS Step Functions state machine that processes an event from a simulated camera service. We implemented conditional logic based on Lambda function outputs to handle distinct branches for threat and false positive scenarios. This example serves as a foundation for developing more complex workflows by integrating multiple AWS services and advanced custom logic. Explore Further Consider expanding this demo by integrating additional services like Amazon SES for notifications or more advanced error handling strategies. For more information, explore the AWS Step Functions Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Elastic Network Interfaces,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/Elastic-Network-Interfaces,"AWS Solutions Architect Associate Certification Services Compute Elastic Network Interfaces In this article, we delve into Elastic Network Interfaces (ENIs) in Amazon EC2 . An ENI is a virtual network interface that can be attached to EC2 instances within a VPC. It decouples network configurations from the compute instances, enabling you to seamlessly move an interface—with all its associated settings such as IP addresses, security groups, and more—across different instances. Key Properties of ENIs An ENI represents a logical version of a physical network card in a VPC. Its main properties include: A primary private IPv4 address automatically selected from the subnet's CIDR block. Optionally, a primary IPv6 address. Secondary private IPv4 addresses. The ability to associate an Elastic IP address. Public IP address allocation when enabled. A unique MAC address. Configurable flags like the source/destination check. Primary vs. Secondary ENIs When you launch an Amazon EC2 instance, it automatically receives a primary ENI, typically named Ethernet0. This primary ENI comes with a primary private IPv4 address sourced from the associated subnet. It remains attached to the instance for its lifetime and cannot be detached—even during instance stops or restarts—and is deleted when the instance is terminated. Additionally, if your subnet configuration auto-assigns public IP addresses, this ENI will obtain a public IP address as well. Important The primary ENI is permanently linked to its instance. Secondary ENIs, however, can be detached and reattached, making them ideal for scenarios like network appliances or management networks. Secondary ENIs offer enhanced flexibility: They can be attached to or detached from EC2 instances as needed. They can host a primary private IP along with multiple secondary private IP addresses. They may have different security groups compared to the primary ENI, allowing for distinct network and security configurations. Remember, while the primary ENI is immovable, secondary ENIs persist even when an instance is stopped or restarted, offering the ability to transfer them between instances when necessary. Elastic IP Addresses and Security Groups You can associate an Elastic IP address with an ENI, which guarantees that the IP remains constant even if the ENI is detached and later reattached to a different instance. Moreover, security groups can be directly linked to an ENI, allowing granular control over the traffic at the interface level. Benefits of Using ENIs ENIs offer several powerful features for designing flexible and secure network architectures on AWS: Feature Benefit Multiple IP Addresses Support for both primary and secondary private IPv4 addresses Elastic IP Association Maintain static IP configurations regardless of instance changes Enhanced Security Assign different security groups directly at the interface level Hot Attach/Detach Capability Attach or detach ENIs without stopping or restarting the instance Flow Logs Configuration Capture detailed IP traffic information for monitoring and troubleshooting Summary ENIs are robust and versatile components of Amazon EC2 that help you: Decouple network configurations from compute instances. Leverage multiple IP addressing schemes. Configure static IP addresses through Elastic IP associations. Implement specialized security configurations using dedicated security groups. Enhance operational flexibility by allowing dynamic attachment and detachment of network interfaces. For more detailed guidance and best practices on AWS networking, explore the Amazon EC2 documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Lambda Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/Lambda-Demo,"AWS Solutions Architect Associate Certification Services Compute Lambda Demo In this guide, you'll learn how to create and deploy an AWS Lambda function from scratch using the AWS Management Console. This tutorial covers configuring a Lambda function, testing it with custom events, adding triggers such as an API Gateway, incorporating third-party libraries using Lambda layers, and monitoring performance with CloudWatch. Creating a Lambda Function Begin by searching for the Lambda service in the AWS Management Console search bar. Once in the Lambda service, click Create a function . You will then see several options: Author from scratch: Write code entirely from scratch. Use a blueprint: Choose from AWS-provided blueprints for common tasks (e.g., processing an S3 upload, configuring an API Gateway trigger, etc.). Container image: Package your application as a container image if preferred. For this demo, select Author from scratch . Next, supply a name for your function (e.g., ""my-first-function"") and choose the runtime. AWS Lambda offers a variety of runtimes such as .NET, Go, Java, Node.js, Python, and Ruby. In this example, we will use Node.js. You may also select the architecture and configure the execution role. By default, Lambda creates a new role with CloudWatch Logs upload permissions. Alternatively, you can use an existing role or create a new one from an AWS policy template. Leave the optional settings (like VPC configuration) unchecked and click Create function . After creation, you'll see a graphical overview of your function. The interface includes a trigger section that indicates when your function will run. AWS Lambda can be initiated by various sources, including API Gateway, S3, SNS, SQS, Alexa, Apache Kafka, and more. To review available triggers, click Add trigger . You will see a list of potential trigger sources. The Default Lambda Function Code When your Lambda function is created, it comes with a default handler. The basic code is as follows: export const handler = async (event) => {
  // TODO implement
  const response = {
    statusCode: 200,
    body: JSON.stringify('Hello from Lambda!'),
  };
  return response;
}; This simple function always returns a status code of 200 along with a message. Such a response is particularly useful when integrating your function with an API Gateway that expects specific response formatting. Testing the Lambda Function You can test your Lambda function directly using the built-in test functionality without needing an external trigger. Click the Test button to configure a test event. Initially, you will be provided with a sample JSON event similar to this: {
  ""key1"": ""value1"",
  ""key2"": ""value2"",
  ""key3"": ""value3""
} You can customize this test event, for example by renaming it to ""test one"" and modifying its content to include a product identifier: {
  ""products"": ""TV""
} When you run the test, AWS passes this JSON object as the event to your Lambda function. Initially, your function returns a static message. To utilize the event data in your code, update your function as follows: export const handler = async (event) => {
  const products = event.products;
  const response = {
    statusCode: 200,
    body: JSON.stringify({ Product: products }),
  };
  return response;
}; After deploying the changes, re-run the test to see the function returning the product data (e.g., ""Product"": ""TV"" ). The log output will display details such as request IDs, duration, memory usage, and more, which are viewable in CloudWatch Logs. Adding an API Gateway Trigger To allow external HTTP requests to invoke your function, add an API Gateway trigger: In the Lambda console, click Add trigger . Select API Gateway . Choose Create a new API and configure its settings. For the demo, you can leave the security settings open. Click Add . An API Gateway is now attached to your function. You can view the API endpoint details in the configuration tab. The endpoint URL, including the specific path (e.g., ""first function""), will be displayed. Copy this URL and use your browser, curl, or Postman to send a GET request. If properly configured, you will receive a response from your Lambda function (e.g., ""lambda trigger example""). Incorporating Third-Party Libraries For more advanced functionality, you may need to use third-party libraries. In this example, we'll integrate the Node.js library bcryptjs to hash a user-supplied password. On your local development machine, install bcryptjs by running: npm install bcryptjs The console output should resemble: C:\Users\sanje\Documents\scratch\demo\lambda>npm install bcryptjs

added 1 package, and audited 2 packages in 1s
found 0 vulnerabilities
C:\Users\sanje\Documents\scratch\demo\lambda> Notice that a new folder named node_modules is created, which contains the bcryptjs code. Now, modify your Lambda function code to import and utilize bcryptjs: import bcrypt from ""bcryptjs"";

export const handler = async (event) => {
  const numSaltRounds = 8;
  const password = event.password;
  const hashedPassword = await bcrypt.hash(password, numSaltRounds);
  const response = {
    statusCode: 200,
    body: JSON.stringify(""Hashed Password: "" + hashedPassword),
  };
  return response;
}; Prepare a test event with a password property (for example, ""password"": ""123"" ) and run the test. If you encounter an error like: {
  ""errorType"": ""Error"",
  ""errorMessage"": ""Cannot find package 'bcryptjs' imported from /var/task/index.mjs"",
  ""trace"": [
    ""Error [ERR_MODULE_NOT_FOUND]: Cannot find package 'bcryptjs' imported from /var/task/index.mjs"",
    ""... stack trace ...""
  ]
} it indicates that although bcryptjs was installed locally, the node_modules folder was not included when deploying your function. Remember: To resolve this issue, ensure that you bundle your node_modules folder with your Lambda function. Bundling Dependencies with Your Code To prevent errors like the package not found error, bundle your code along with all required third-party dependencies. One way to do this is to zip your project directory—including your code file and the node_modules folder—and then upload the zip file directly to AWS Lambda. This approach ensures that AWS Lambda has access to bcryptjs and other dependencies. After uploading the zip file, deploy your changes and run your test again. Your function should now return a successful response with the hashed password. Using Lambda Layers An alternative solution for managing dependencies is to use Lambda layers. Layers allow you to package libraries separately and share them across multiple functions. To create a Lambda layer for bcryptjs: Prepare the folder structure required by Lambda. For Node.js, create a folder named Node.js (or nodejs as per the documentation) and move your node_modules folder inside it. Zip the Node.js folder. For example, name the zip file hash-layer.zip . In the Lambda console, navigate to Layers and click Create layer . Name your layer (e.g., ""hash"") and upload the zip file. Select the supported runtime (Node.js) and create the layer. Next, associate the newly created layer with your Lambda function: Open your function in the Lambda console. In the Layers section, click Add a layer . Choose Custom layers and select your ""hash"" layer. Add the layer and deploy your function. Your function now loads bcryptjs from the layer instead of bundling it with the function code. For further details on folder structure and additional instructions, refer to the AWS Lambda Developer Guide . After adding the layer, test your function to ensure that bcryptjs is correctly imported and the output meets expectations. Monitoring Your Lambda Function AWS Lambda integrates seamlessly with Amazon CloudWatch to monitor your function’s performance. In the Monitoring tab, check metrics such as invocation count, duration, errors, and throttling. For more detailed information, click View logs in CloudWatch . Example log snippet: @log  0404973171401:/aws/lambda/secondfunction
@logStream  2023/03/09/[LATEST]49f42dab34f9682a151629fdcbff
@maxMemoryUsed  76
@memorySize  128
@message  REPORT RequestId: 83cd19c9-e317-446c-8880-c266adeaeb9b Duration: 847.18 ms Billed Duration: 848 ms Memory Used: 128 MB Max Memory Used: 76 MB Init Duration: 244.71 ms
@requestId  83cd19c9-e317-446c-8880-c266adeaeb9b
@timestamp  1678350237422
@type  REPORT For a complete overview, access the full CloudWatch interface. Conclusion In this guide, we covered how to create a simple AWS Lambda function, test it using custom event data, and extend its functionality by adding an API Gateway trigger and third-party libraries. We discussed methods for bundling dependencies and using Lambda layers for efficient code management, alongside monitoring performance with CloudWatch logs. With AWS managing the underlying infrastructure, you can focus on designing, implementing, and deploying your serverless applications with ease. Happy coding with AWS Lambda! Watch Video Watch video content Practice Lab Practice lab"
AWS Solutions Architect Associate Certification,Elastic Beanstalk,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/Elastic-Beanstalk,"AWS Solutions Architect Associate Certification Services Compute Elastic Beanstalk AWS Elastic Beanstalk is a powerful service that simplifies the deployment and management of applications on AWS. By abstracting the complexity of configuring multiple AWS services—such as VPCs, subnets, EC2 instances, load balancers, databases (RDS), S3 buckets, and CDNs—Elastic Beanstalk lets you focus on your application code while it automatically provisions and manages the underlying infrastructure. How It Works When building a web application, you first upload your source code (HTML, CSS, JavaScript, etc.) to a storage location. Then, you configure Elastic Beanstalk with your deployment specifications, which include environment variables, instance types, runtime settings, and the desired platform. Once your code is uploaded, Elastic Beanstalk automatically deploys all the required resources, such as VPCs, subnets, EC2 instances, load balancers, and databases if needed. The result is a fully operational web application with managed infrastructure. Environment Isolation Elastic Beanstalk introduces the concept of environments—a collection of AWS resources required for your application to run. You can create separate environments for development, staging, and production, ensuring consistent service configurations across all stages of your development lifecycle. Supported Platforms Elastic Beanstalk supports a wide range of programming languages and platforms, including Docker, .NET Core, Python, PHP, Go, Java, Node.js, and Ruby. The process is straightforward: upload your code, select the appropriate runtime, and let Elastic Beanstalk handle the rest. Deployment Workflow The deployment process with Elastic Beanstalk involves three core steps: Create an Application: Start by creating an application in the Elastic Beanstalk console. Upload Your Code: Upload your application source code along with deployment configurations. Deploy to an Environment: Deploy the code to a designated environment, allowing you to test and manage the application. Once set up, you can monitor application health, conduct tests, and deploy new versions seamlessly. For instance, deploy updates in your staging environment to evaluate performance before releasing them in production. Key Benefits and Features Elastic Beanstalk offers several advantages that can accelerate your development process and ensure efficient management of your applications: Feature Description Easy Deployment Simplifies the deployment process by letting AWS handle the creation and management of services. Managed Platform Updates Automatically updates the underlying platform, including the OS, application server, and runtime. Autoscaling and Load Balancing Distributes incoming traffic across EC2 instances and scales resources based on demand. Monitoring and Health Checks Provides built-in monitoring through AWS CloudWatch for continuous application health checks. Pre-configured Stacks Offers ready-to-use stacks for popular programming languages such as Node.js, Python, Ruby, Go, and Docker. Focus on Code, Not Infrastructure With Elastic Beanstalk, you can focus on developing your application without worrying about the underlying infrastructure. AWS manages the heavy lifting, ensuring that your environment is secure, scalable, and optimized for performance. Conclusion AWS Elastic Beanstalk is the ideal solution for developers who want a hassle-free deployment process. Its powerful features, such as managed platform updates, autoscaling, and integrated monitoring, allow you to launch and maintain web applications with confidence. Explore more about AWS Elastic Beanstalk to boost your development workflow and reduce operational overhead. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Batch,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/Batch,"AWS Solutions Architect Associate Certification Services Compute Batch In this lesson, we explore AWS Batch—a fully managed service that efficiently handles batch computing jobs without any manual intervention. Imagine a professional kitchen where every dish requires precise preparation: vegetables must be diced and peeled, meat marinated, soup simmered, and frozen ingredients thawed. A manager assigns the right number of staff to each task, ensuring that critical processes are never delayed while preventing resource waste on simpler tasks. AWS Batch works in a similar way for computing jobs. For instance, if you need to convert a large collection of videos, you simply submit a job and AWS Batch executes the conversion script on the most appropriate compute resources, all without constant oversight. AWS Batch automates the entire process—when you specify your job requirements, the service intelligently selects the appropriate EC2 instance(s) or Fargate instances from a resource pool, based on CPU, memory, and other needs. It also queues jobs until the necessary resources are available, allowing you to assign different priority levels to ensure that critical tasks are executed first. Key Benefit AWS Batch eliminates the manual burden of managing compute resource allocation so you can focus on improving your application and workflows. Job Lifecycle The lifecycle of an AWS Batch job is comprised of several well-defined stages: Submitted: The job has been received by AWS Batch but has not yet been queued. Pending: The job waits in line, either for resource availability or to satisfy job dependencies. Runnable: The job becomes ready for assignment and waits for the necessary compute resources (vCPUs, memory, etc.) to become available. Starting: The job is assigned to a compute resource, and its container begins to initialize. Running: The job is actively executed, processing the prescribed commands. Succeeded/Failed: The job concludes by entering a ""succeeded"" state if it completed successfully or ""failed"" if any issues arose during execution. Key AWS Batch Components AWS Batch is built around several core components that work together to streamline batch job processing: Job Definition: Acts as a template that outlines how a job should run. It includes specifications such as the Docker image to use, vCPU count, memory requirements, commands to execute, and environment variables. Job Submission: With a job definition in place, submissions create an instance of that template. Parameter overrides can be applied at this stage as needed. Job Queue: Submitted jobs are placed into queues that are linked to one or more compute environments. Each queue features its own priority setting to help determine execution order. Job Scheduler: This component assesses job priorities and resource requirements to optimally place jobs onto available compute resources. When suitable resources are free, the scheduler transitions jobs to the runnable and then running state. Container Execution: Jobs are executed within Docker containers. AWS Batch pulls the specified Docker image, sets up the container environment, and runs your job command, ensuring consistency and isolation. Compute Environment: A set of compute resources (either EC2 instances or Fargate) where jobs run. AWS Batch supports three compute environment types: EC2 On-Demand EC2 Spot Fargate AWS Batch automatically scales these environments based on the job queue, ensuring optimal resource utilization. Features and Benefits Leveraging AWS Batch can significantly optimize your workflow and resource management. Here are some key advantages: Dynamic Resource Provisioning: Automatically provisions the optimal amount and type of compute resources according to your batch job’s requirements. Job Queues and Prioritization: Efficiently manages job queues, ensuring that critical jobs are prioritized. Cost Efficiency: Utilizes EC2 Spot instances and auto-scaling features to minimize over-provisioning, subsequently reducing operational costs. Seamless AWS Integration: Conveniently integrates with other AWS services such as EC2, ECS, ECR, CloudWatch, and IAM. Customizable Compute Environments: Allows you to define compute environments tailored to specific needs—including instance types, vCPU limits, and launch templates. Efficiency Tip By leveraging AWS Batch’s automated scaling and priority-based job scheduling, you can enhance both performance and resource utilization while keeping costs under control. Transcribed by https://otter.ai For more detailed insights, consider exploring additional AWS documentation or related AWS Batch tutorials . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,ECS Demo Part 2,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Compute/ECS-Demo-Part-2,"AWS Solutions Architect Associate Certification Services Compute ECS Demo Part 2 In this guide, we explore our multi-container application running on AWS ECS. The application comprises a Node.js/Express API and a MongoDB container, demonstrating a robust architecture for containerized deployments. Below is an in-depth overview, including configuration details, API endpoints, and AWS ECS setup. Docker Compose Configuration The following Docker Compose file defines two services: an API container and a MongoDB container. The API service uses our pre-built image ""kodekloud/ecs-project2"" and environment variables to connect to MongoDB, while the MongoDB service leverages persistent storage through volumes. version: ""3""
services:
  api:
    build: .
    image: kodekloud/ecs-project2
    environment:
      - MONGO_USER=mongo
      - MONGO_PASSWORD=password
      - MONGO_IP=mongo
      - MONGO_PORT=27017
    ports:
      - ""3000:3000""
  mongo:
    image: mongo
    environment:
      - MONGO_INITDB_ROOT_USERNAME=mongo
      - MONGO_INITDB_ROOT_PASSWORD=password
    volumes:
      - mongo-db:/data/db

volumes:
  mongo-db: The API container listens on port 3000 and connects to MongoDB using the provided environment variables. Meanwhile, the MongoDB container utilizes the official image and leverages volume mapping for durable data storage. Note Ensure that environment variables for MongoDB connectivity are properly set to avoid connection issues. API Endpoints Overview The API, built with Express and Mongoose, implements basic CRUD functionality for managing notes. The code snippets below illustrate each endpoint: GET /notes Retrieves all notes from the database. const app = express();
app.use(cors());
app.use(express.json());

const mongoURL = `mongodb://${process.env.MONGO_USER}:${process.env.MONGO_PASSWORD}@${process.env.MONGO_IP}:${process.env.MONGO_PORT}/?authSource=admin`;

app.get(""/notes"", async (req, res) => {
    try {
        const notes = await Note.find();
        res.status(200).json({ notes });
    } catch (e) {
        console.log(e);
        res.status(400).json({ error: e.message });
    }
}); GET /notes/:id Retrieves a specific note by its ID. app.get(""/notes/:id"", async (req, res) => {
    try {
        const note = await Note.findById(req.params.id);
        if (!note) {
            return res.status(404).json({ message: ""Note not found"" });
        }
        res.status(200).json({ note });
    } catch (e) {
        console.log(e);
        res.status(400).json({ status: ""fail"" });
    }
}); POST /notes Creates a new note. app.post(""/notes"", async (req, res) => {
    console.log(req.body);
    try {
        const note = await Note.create(req.body);
        return res.status(201).json({ note });
    } catch (e) {
        console.log(e);
        return res.status(400).json({ status: ""fail"" });
    }
}); PATCH /notes/:id Updates an existing note. app.patch(""/notes/:id"", async (req, res) => {
    try {
        const note = await Note.findByIdAndUpdate(req.params.id, req.body, {
            new: true,
            runValidators: true,
        });
        if (!note) {
            return res.status(404).json({ message: ""Note not found"" });
        }
        res.status(200).json({ note });
    } catch (e) {
        console.log(e);
        res.status(400).json({ status: ""fail"" });
    }
}); DELETE /notes/:id Deletes a note by its ID. app.delete(""/notes/:id"", async (req, res) => {
    try {
        const note = await Note.findByIdAndDelete(req.params.id);
        console.log(note);
        if (!note) {
            return res.status(404).json({ message: ""Note not found"" });
        }
        res.status(200).json({ status: ""success"" });
    } catch (e) {
        console.log(e);
        res.status(400).json({ status: ""fail"" });
    }
}); MongoDB Connection and Server Initialization The snippet below uses Mongoose to connect to MongoDB. Upon successful connection, the server starts listening on port 3000. mongoose
  .connect(mongoURL, {
    useNewUrlParser: true,
    useUnifiedTopology: true,
  })
  .then(() => {
    console.log(""Successfully connected to DB"");
    app.listen(3000, () => console.log(""Server is listening on PORT 3000""));
  })
  .catch((e) => {
    console.log(e);
    process.exit(1);
  }); Console Confirmation: user1 on user1 in ecs-project2 [!] is 📦 v1.0.0 via ⎇ AWS ECS Setup and Task Definition Before deploying the application on ECS, proper configuration of AWS components is crucial. This section will guide you through setting up the security group, task definition, container configurations, and volumes. ECS Task Definition and Security Group Step 1: Create a Security Group for ECS Navigate to EC2 → Security Groups. Create a new security group (e.g., ""ECS-SG"") and allow all traffic temporarily. Select the correct VPC matching your ECS cluster. Step 2: Create an ECS Task Definition Go to the ECS console and create a new task definition using Fargate. Assign the ECS task execution role accordingly. Name the task definition (e.g., ""ECS-Project2""). Container Configuration MongoDB Container Container Name: Mongo Image: Default Mongo image from Docker Hub Port Mapping: 27017 Environment Variables: MONGO_INITDB_ROOT_USERNAME: mongo MONGO_INITDB_ROOT_PASSWORD: password Volume Mapping: Mount the volume (e.g., ""mongo-db"") at /data/db . environment:
  - MONGO_INITDB_ROOT_USERNAME=mongo
  - MONGO_INITDB_ROOT_PASSWORD=password
volumes:
  - mongo-db:/data/db After setting the environment variables, proceed through the configuration. Express API Container Container Name: e.g., ""web API"" Image: kodekloud/ecs-project2 Port Mapping: 3000 Environment Variables: MONGO_USER: mongo MONGO_PASSWORD: password MONGO_IP: mongo (adjust to ""localhost"" if required in ECS) MONGO_PORT: 27017 version: ""3""
services:
  api:
    build: .
    image: kodekloud/ecs-project2
    environment:
      - MONGO_USER=mongo
      - MONGO_PASSWORD=password
      - MONGO_IP=mongo
      - MONGO_PORT=27017
    ports:
      - ""3000:3000"" Volume Setup for MongoDB To ensure persistent data storage, define a volume for MongoDB. In the task definition, add a volume (e.g., ""MongoDB"") and select AWS Elastic File System (EFS) as the volume type. Create an EFS File System: Select ""Create File System"" via the provided link. Name the file system (e.g., MongoDB) and select the correct VPC. Customize the setup to include desired subnets. Update the EFS security group to allow only NFS access (port 2049) from your ECS security group. Finally, update the MongoDB container's task definition to assign the EFS volume mount at /data/db . Complete the task definition creation. You might see multiple revisions if this is not the first deployment. Creating the ECS Service With the task definition ready, follow these steps to create an ECS service and attach a load balancer: Go to your ECS cluster and click ""Create Service."" Select Fargate as the launch type with the Linux platform. Choose the task definition (e.g., ""ECS-Project2"") and provide a service name (e.g., ""notes app service""). Set the desired number of tasks to 1, ensuring the correct VPC and subnets are selected. For the security group, select the one created earlier (""ECS-SG""). Add an Application Load Balancer (ALB): Open the load balancer setup in a new tab. Create a new ALB (e.g., ""notes LB"") using IPv4; ensure it is internet-facing and assigned to the correct VPC. Create a new security group for the load balancer (e.g., ""LB-SG"") that permits HTTP traffic on port 80. Configure Listener Rules and Target Group: Define listener rules on port 80 to forward traffic to the API container: Create a target group for the ALB: Choose ""IP addresses"" as the target type (suitable for ECS). Name the target group (e.g., ""notes-target-group1""). Set the health check path to /notes and the port to 3000. Assign the target group to your ALB. Return to the ECS service configuration. Specify that the load balancer forwards traffic on port 80 to port 3000 on the API container. Review and create the service. After the service is created, verify that the task transitions from provisioning to running. Both containers should reflect a running status. Selecting the task reveals detailed container statuses and networking information. Testing the Deployment Test the API by directly accessing the container's IP on port 3000 via tools like Postman: Send a GET request to /notes to retrieve notes (an empty array if no notes are present). For production, use the ALB DNS name; the ALB on port 80 forwards traffic to the API container on port 3000. Example: Testing a POST Request Send a JSON payload to create a note: {
  ""title"": ""second note"",
  ""body"": ""remember to do dishes!!!""
} A successful POST creates a note. A subsequent GET request returns: {
  ""notes"": [
    {
      ""_id"": ""63211a3c034fdd55dce212834"",
      ""title"": ""second note"",
      ""body"": ""remember to do dishes!!!"",
      ""__v"": 0
    }
  ]
} Securing the ECS Security Group It is important to restrict access to your ECS resources. Currently, the ECS security group permits all incoming traffic, which is not secure. Action Steps: Remove the rule that allows all traffic. Add a new rule to allow Custom TCP traffic on port 3000. Set the source to the load balancer security group (LB-SG) only. After updating the security group, test the API through the ALB DNS name to ensure that only traffic from the ALB is accepted. Warning Securing your ECS security group is critical. Ensure that only the necessary traffic is allowed to minimize potential exposure and vulnerabilities. By following these detailed steps, you have successfully deployed a containerized application on AWS ECS with a load balancer ensuring efficient traffic routing, along with security group modifications to secure your environment. For more information on container orchestration and best practices, visit Kubernetes Documentation and Docker Hub . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,RDS RDS proxy,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Database/RDS-RDS-proxy,"AWS Solutions Architect Associate Certification Services Database RDS RDS proxy Amazon RDS Proxy is a fully managed service designed to improve application performance and enhance database availability. Modern applications frequently open direct connections to their databases to execute queries, insert records, or delete data. As the number of application instances grows, each one establishes its own connection, which can lead to excessive connection overhead and strain the database's CPU and memory resources. Understanding Connection Overhead When numerous instances attempt to connect directly to the database, the overhead associated with constantly opening and closing connections can lead to resource exhaustion. RDS Proxy acts as an intermediary between your application and the RDS database, maintaining a pool of persistent connections. When your application requires a database connection, it routes the request through the proxy, which provides an available, re-established connection from its pool. This strategy keeps the number of active database connections within a manageable limit, thereby reducing the load on the database server. Preventing Resource Exhaustion By limiting the number of active connections to the database, RDS Proxy efficiently prevents resource exhaustion. Additional connection requests are either queued or throttled until a connection becomes available. Key benefits of using Amazon RDS Proxy include: Improved Application Performance: Reusing established connections minimizes the performance penalty associated with repeatedly opening and closing connections, leading to lower CPU and memory usage. Enhanced Availability: During a database failover, such as the transition to a standby instance, the proxy automatically reconnects to the new primary instance. This seamless failover handling can reduce downtime by up to 66% for Aurora and RDS databases. Strengthened Security: RDS Proxy integrates with AWS services like IAM and AWS Secrets Manager, eliminating the need to hardcode credentials and ensuring secure access to your database. Fully Managed and Scalable: Being a serverless solution, RDS Proxy automatically scales across multiple availability zones and requires no manual deployment, patching, or maintenance. In summary, Amazon RDS Proxy provides a robust solution for applications with high connection demands. By pooling persistent connections, it minimizes the overhead on your database, enhances availability during failovers, and integrates seamlessly with AWS security services. This makes it an ideal choice for applications facing sudden traffic spikes, ensuring that your database remains both resilient and efficient. For further reading on optimizing database connections and integrating AWS services, explore additional resources on AWS Documentation and Amazon RDS . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,RDS Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Database/RDS-Demo,"AWS Solutions Architect Associate Certification Services Database RDS Demo In this tutorial, you will learn how to deploy a PostgreSQL instance using Amazon RDS. By using RDS, AWS manages the underlying database infrastructure, letting you focus on your application rather than manual database management tasks. Step 1: Creating a Database Begin by accessing the AWS console and searching for RDS. In the RDS dashboard, click on the Create database button. Depending on your history with the service, the interface might present the options as a toolbar at the top or bottom, or as a pop-up if it's your first visit. You will be presented with two options: Standard Create – Offers full control over configuration. Easy Create – Automatically applies AWS best practices. For this demo, choose Standard Create to explore all available configuration options. Engine Selection Select the database engine you want to deploy. In this demonstration, we choose PostgreSQL , although other engines like Aurora, MySQL, MariaDB, Oracle, and Microsoft SQL Server are available. Once PostgreSQL is selected, you will have an option to choose a version; the default option is generally sufficient. Next, choose a template according to your deployment needs: Production: High availability and resilience. Dev/Test: Ideal for development and testing environments. Free Tier: Qualifies for free usage under AWS guidelines. For this walkthrough, select Dev/Test . Instance Configuration Provide a name for your database instance (e.g., ""my-first-db"") and set the master username (default is ""postgres""). You will then enter a password, though you can opt for AWS to generate a secure password automatically. In the Instance Configuration section, select the EC2 instance type on which your database will run. For this demonstration, sticking to the default option is recommended. Proceed to configure the storage settings where you can specify: The type and amount of allocated storage. Storage autoscaling (a good cost optimization feature, especially for free tier users). Connectivity and Security Under the Connectivity section, you can skip the compute settings if you're not testing connectivity with an external EC2 instance. Select the default VPC and subnet. For production deployments, it's recommended to set ""Public access"" to No and restrict connectivity to your backend or API. However, for this demo, enable ""Public access"" so you can connect to the database from your local machine. Select your VPC and create a new security group (for example, ""my DB security group"") to control traffic. The availability zone can be selected or left as default if no specific preference is required. Additional Configuration In the additional configuration section, you can modify settings such as: The default PostgreSQL port (modifiable if necessary). Authentication methods (default is password authentication). Monitoring and backup configurations. For the purposes of this demonstration, it is safe to leave these settings at their defaults. Once you have completed all configuration steps, click Create database . The creation process might take a few moments until the status displays as ""available"". Step 2: Connecting to Your Database After your database instance is running, select it from the AWS console to view connectivity details. The key elements you need are the endpoint (IP/domain name) and the port. The connection string typically includes the following information: Host: The database endpoint. Port: Default PostgreSQL port, usually 5432. User: The master username (e.g., ""postgres""). Password: The password you configured. Database: The default database, typically named “postgres.” Below are two common methods to connect to your PostgreSQL instance. Method 1: Connecting via Code The following JavaScript snippet uses the Knex query builder to connect to the PostgreSQL instance: const knex = require(""knex"")({
  client: ""pg"",
  connection: {
    host: ""my-first-db.cidipbxuwdgl.us-east-1.rds.amazonaws.com"",
    port: 5432,
    user: ""postgres"",
    password: ""password"",
    database: ""postgres"",
  },
}); Method 2: Connecting with pgAdmin pgAdmin is a widely used GUI tool for managing PostgreSQL databases. Follow these steps to connect: Open pgAdmin and create a new server. In the Create - Server dialog: Enter a name (e.g., ""AWS - RDS""). Provide the server connection details including the host (the endpoint copied from AWS), port, username, and password. Save the configuration. After saving, you will see the default PostgreSQL database in pgAdmin. You can create additional databases as needed for your application. For example, you might create a new database called ""my_app"". In case of an error, it may appear similar to the following: (sqlite3.IntegrityError) UNIQUE constraint failed: database.id, database.server
[SQL: INSERT INTO ""database"" (id, schema_res, server) VALUES (?, ?, ?)]
[parameters: (16402, None, 1)]
(Background on this error at: http://sqlalche.me/e/13/gkpj) Resolve any errors accordingly and verify that your new database is visible in the pgAdmin interface. Step 3: Managing Your RDS Instance Once connected, you can manage your RDS instance through the AWS console. Modifying the Instance If you need to update any configurations (such as engine version, instance type, or security settings), select the instance and click Modify . This option allows you to adjust various settings for your running PostgreSQL instance. Deleting the Instance When you are ready to remove the database instance, select it and click Delete . AWS will prompt you to confirm the deletion and ask if you wish to retain snapshots and automated backups. Warning Deleting your database instance will result in data loss if snapshots or backups are not retained. Proceed with caution. After confirming, the database instance will be deleted, concluding the demonstration process. Conclusion You have now successfully deployed, connected to, and managed a PostgreSQL database using Amazon RDS. This hands-on demonstration showcased how to configure your instance and perform key operations such as connecting via code and pgAdmin, modifying settings, and safely deleting the instance. Happy coding! Additional Resources Resource Type Details Link AWS RDS Amazon Relational Database Service documentation for further configuration details AWS RDS Documentation PostgreSQL Official PostgreSQL documentation for advanced database features PostgreSQL Docs Knex.js Learn more about the Knex query builder for Node.js Knex.js Documentation Note For further insights into AWS and PostgreSQL best practices, refer to the resource links provided above. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Redshift Serverless,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Database/Redshift-Serverless,"AWS Solutions Architect Associate Certification Services Database Redshift Serverless In this article, we explore AWS Redshift Serverless, a modern and scalable data warehousing solution that addresses many of the challenges posed by traditional Redshift deployments. Overview Traditional Redshift setups involve provisioning a fixed combination of CPU, RAM, and storage resources. This model means you incur continuous charges—even when query activity is low—leading to overprovisioning and underutilization. As usage patterns fluctuate, you might end up paying for idle capacity. Redshift Serverless eliminates this inefficiency by automatically scaling the data warehouse capacity in response to real-time demand. With the ability to start for as low as $3 per hour, you only pay for the compute capacity during active query workloads—making cost management more efficient. How Redshift Serverless Works Redshift Serverless measures its data warehouse capacity in Redshift Processing Units (RPUs). Billing is executed on a per-second basis for RPU hours, covering all query workloads, including those accessing data in open file formats stored in Amazon S3. There are two key configurable options: Base RPU: Defines the minimum data warehouse capacity deployed to serve queries. This value is expressed in RPUs. Max Capacity: Sets usage limits to control costs and defines automatic actions when limits are reached. The maximum capacity is also measured in RPU hours and can be configured for daily, weekly, or monthly durations. Understanding RPUs Each RPU provides 16 gigabytes of memory. Adjusting the base capacity directly influences query performance, particularly for resource-intensive data processing tasks. Configuring RPUs The default base capacity for Amazon Redshift Serverless is 128 RPUs. You have the flexibility to adjust this setting between 8 and 512 RPUs in increments of 8 (for example, 8, 16, 24, etc.). These changes can be made using the AWS Management Console, the update workgroup API operation, or via the AWS CLI. For workloads operating with 8 to 24 RPUs, you can efficiently handle up to 128 terabytes of data. However, if your data storage requirements exceed this limit, a minimum capacity of 32 RPUs is recommended. For high concurrency workloads or tables with a larger number of columns, it is advisable to operate with 32 or more RPUs to ensure optimal performance. Features and Benefits Redshift Serverless offers several key advantages: Cost Savings: You pay only for the compute resources used, rather than incurring charges for pre-provisioned capacity that may remain idle. Scalability: The service automatically scales resources up or down based on workload demands, ensuring efficiency during peak usage and cost reduction during off-peak periods. Simplified Management: With automated scaling and resource allocation, you no longer need to manually balance the risks of overprovisioning and underprovisioning. Feature Benefit Pricing Detail Auto-Scaling Dynamically adjusts capacity in real time based on usage Cost calculated per second of RPU use Configurable Base Flexible configuration allows tuning for optimal performance Customizable RPU settings Cost Efficiency Only pay for what you use, reducing unnecessary cost on idle resources As low as $3 per hour Summary In summary, AWS Redshift Serverless is an on-demand data warehousing solution designed to automatically manage scaling and resource provisioning. RPUs serve as the measurement for both computational capacity and cost, with each RPU delivering 16 gigabytes of memory. You have the flexibility to adjust the capacity between 8 and 512 RPUs (in increments of 8) to meet your specific workload requirements. This dynamic adjustment ensures efficient query processing and predictable cost management while supporting modern data processing needs. Learn More For further details on AWS Redshift Serverless and how it can revolutionize your data warehousing strategy, refer to the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Redshift Main,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Database/Redshift-Main,"AWS Solutions Architect Associate Certification Services Database Redshift Main In this article, we explore AWS Redshift—a fully managed, petabyte-scale data warehouse service that is essential for modern analytical workloads. We will compare data warehouses to traditional databases, then dive into the architecture and core features of Redshift. Understanding Data Warehouses Data warehouses consolidate data from various sources such as relational databases, log systems, cloud storage, and more. Their primary goal is to provide a unified view of your data, which is critical for making informed business decisions. Unlike traditional databases that focus on transactional operations (inserts, updates, and deletes), data warehouses excel at analytical operations, such as complex queries and reporting. Key points about data warehouses include: They store historical data that is essential for tracking changes, identifying trends, and performing comparative analysis. They incorporate ETL (Extract, Transform, Load) processes to ensure data is cleaned, transformed, and aggregated for quality and consistency. They work on structured data at an aggregate level, simplifying large-scale analytics. They scale effectively by increasing compute and storage resources as data and query loads grow. They often prove more cost-effective for analytical workloads compared to traditional databases. Introduction to AWS Redshift AWS Redshift is designed for handling extremely large data sets with enterprise-class performance. Its ability to support a wide range of client connections makes it perfect for business intelligence (BI), reporting, and analytical applications. With Redshift, you benefit from efficient storage, optimized query performance, and seamless integration with various data sources and analytics tools. Redshift Architecture and Components Redshift’s architecture comprises several key components that collaborate to deliver high performance and scalability. Clusters and Nodes The core of a Redshift data warehouse is the cluster: A cluster consists of one or more compute nodes. Once a cluster has multiple compute nodes, a leader node is introduced to coordinate compute nodes and manage external communications with client applications. The leader node: Handles communications with client programs. Parses SQL queries, develops execution plans, and distributes compiled code to compute nodes. Delegates query operations by assigning portions of the data to each compute node. (Note: Simple queries run exclusively on the leader node, while more complex queries referencing tables stored on compute nodes are distributed accordingly.) Each compute node comes with its own CPU and memory. As your workload increases, you can scale your cluster’s compute capacity by adding more nodes or upgrading the node types. Data Storage Redshift decouples data storage from compute resources using Redshift Managed Storage (RMS): RMS leverages Amazon S3 to scale storage to petabytes. Compute and storage resources can be scaled independently, allowing clusters to be sized based solely on compute needs. It utilizes high-performance SSD-based local storage as a tier-one cache for frequently accessed data. Database Component Inside each cluster, one or more databases are maintained. AWS Redshift is a relational database management system (RDBMS) based on PostgreSQL. Despite the similarities, Redshift is optimized specifically for data warehousing applications. Core Features of Redshift AWS Redshift offers a rich set of features that empower efficient and secure data warehousing: Columnar Storage: Speeds up query performance by reading only the necessary columns from disk. Massively Parallel Processing (MPP): Distributes query processing across multiple nodes to accelerate execution. Automatic Data Compression: Minimizes storage requirements and enhances query performance. Elastic Scalability: Adjust compute resources dynamically by adding or removing nodes based on demand. Integration with Diverse Data Sources: Provides seamless connections with Amazon S3, Amazon RDS, EMR, and other data sources. Data Ingestion Methods: Simplifies data ingestion with COPY commands and direct loading from Amazon S3 . Standard SQL Support: Enables the use of familiar SQL queries and reporting tools. Enhanced Security: Offers encryption at rest and in transit, fine-grained access control, and AWS IAM integration. Automated Backups and High Availability: Features continuous automated backups, multi-node clusters, automated failovers, and read replicas. Materialized Views: Stores precomputed aggregated data for faster retrieval during complex queries. Workload Management: Efficiently allocates query resources through integrated workload management. BI Tool Integration: Easily integrates with popular business intelligence tools. Data Lake Integration: Combines structured data in Redshift with semi-structured and unstructured data in data lakes for comprehensive analytics. Tip For a quick comparison of resource types, consider the following table: Resource Type Use Case Example Pod Basic unit of deployment kubectl run nginx --image=nginx Deployment Managed pods with scaling kubectl create deployment nginx --image=nginx Service Network access to pods kubectl expose deployment nginx --port=80 Use Cases for Redshift Redshift is versatile and meets a wide range of analytical and data processing requirements, including: Enhancing financial and demand forecasting through advanced predictive analytics. Securely sharing data across accounts, organizations, and partner ecosystems. Building insightful reports and dashboards using industry-standard business intelligence tools. Simplifying data ingestion and access from diverse programming languages and platforms without complex configurations. Summary of Redshift AWS Redshift stands out as a robust, fully managed, petabyte-scale data warehouse service that delivers: Two Node Types: The leader node to manage query coordination. Compute nodes to execute queries and store data. Optimized Storage: Automatic data compression and efficient storage architecture to reduce I/O operations. Strong Security: Implements encryption at rest, fine-grained access control, and AWS IAM integration to safeguard your data. Elastic Scalability: Automatically adjusts capacity to handle fluctuating query loads. Backup and Recovery: Provides automated snapshots to Amazon S3 for reliable point-in-time recovery. Additional Resources For further information, check out these useful links: AWS Redshift Documentation Amazon S3 Overview Watch Video Watch video content"
AWS Solutions Architect Associate Certification,MemoryDB for Redis,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Database/MemoryDB-for-Redis,"AWS Solutions Architect Associate Certification Services Database MemoryDB for Redis In this lesson, we explore AWS MemoryDB for Redis—a fully managed, Redis-compatible, in-memory database service engineered for high performance and streamlined architecture. MemoryDB for Redis allows you to consolidate both caching and primary data storage into a single database instance. Unlike ElastiCache for Redis—which is typically deployed as a caching layer to offload frequently accessed data from primary databases (like SQL databases)—MemoryDB simplifies your architecture by using Redis as your main database. This consolidation offers microsecond read latencies, low single-digit millisecond writes, and enterprise-grade security with scalability. MemoryDB Components Let's dive into the key components of MemoryDB and see how they work together. Clusters and Nodes A MemoryDB cluster consists of one or more nodes serving a single dataset. Within each cluster: The primary node manages both read and write requests. Replica nodes handle read requests, offloading the primary node to improve read scalability. In the event of a primary node failure, a replica can be promoted to become the new primary, ensuring high availability. Each node runs on an EC2 instance using the selected version of Redis and belongs to a shard within the cluster. Shards MemoryDB partitions the dataset across shards. Each shard contains: One primary node dedicated to write operations. Up to five replicas that serve read requests. Every cluster features at least one shard, ensuring that workload distribution and scalability are maintained as your data grows. Other Key Components Parameter Groups: Named collections of engine-specific parameters and configuration settings applied to your cluster. Subnet Groups: Ensure that only designated traffic has access to your cluster. Access Control Lists: Define user permissions for executing Redis commands and accessing data. MemoryDB stores your entire dataset in memory while employing a distributed multi-AZ transactional log for durability, consistency, and recoverability. By replicating data across multiple availability zones, MemoryDB supports swift recovery and restarts. Its in-memory architecture ensures ultra-fast performance and high throughput compared to traditional disk-based databases. Scaling is versatile with MemoryDB: Horizontally: Add or remove nodes (replicas or shards) to scale as required. Vertically: Choose different node types to meet performance demands. Write throughput can be enhanced by adding additional shards, and read throughput can be improved by increasing the number of read replicas. Use Cases and Benefits MemoryDB for Redis caters to various workloads that demand high performance and low latency. Here are some common use cases: Use Case Description Web and Mobile Applications Use Redis data structures to deliver personalized experiences, manage user profiles, store preferences, and track inventories with extremely fast read and write operations. Online Gaming Build robust data stores for player data, session histories, and leaderboards that demand massive scale and high concurrency for real-time updates. Streaming Media and Entertainment Support high concurrency for streaming data feeds, ingest user activity, and process millions of requests per day seamlessly. Note MemoryDB’s ability to serve both as a primary database and cache simplifies your architecture by reducing the overhead of managing separate systems for caching and primary data storage. In summary, AWS MemoryDB for Redis is a managed service that combines high throughput, low latency, and simplified database management. It is an ideal solution for applications requiring fast, reliable, and scalable data operations—making it a strong choice for modern, high-demand workloads. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,DynamoDB Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Database/DynamoDB-Demo,"AWS Solutions Architect Associate Certification Services Database DynamoDB Demo In this demonstration, we explore how to work with DynamoDB by creating a table to store customer orders for an e-commerce website. The tutorial covers table creation, configuration, adding items (orders), performing queries, editing and deleting items, using PartiQL, and monitoring. The sequence below matches the original demonstration while enhancing clarity, technical accuracy, and SEO. Creating the Orders Table Begin by searching for the DynamoDB service within the AWS Console. Once you are in the DynamoDB interface, click Create table . For this demonstration, we will store orders placed by customers on an e-commerce site. You can name the table “orders” (or any name you prefer). When creating the table, choose a partition key (and optionally a sort key) that best suits your data model: Partition key: customer ID (String) – enables efficient queries. Sort key: order ID (String) – distinguishes multiple orders for the same customer. Below is a screenshot of the Orders table creation interface: Configuring Table Settings After specifying the keys, you can choose default settings or customize them: Table Class: Typically, DynamoDB Standard is used for general purposes. If your data is infrequently accessed, consider using DynamoDB Standard-IA. Read/Write Capacity Settings: Set fixed capacity (e.g., five read and five write capacity units) or enable auto scaling. With auto scaling, define minimum and maximum capacity units and a target utilization percentage to automatically adjust throughput based on demand. Review the screenshots below to understand the settings configuration: After setting the capacity and reviewing additional options like secondary indexes, encryption settings, and deletion protection, click the button to create the table. The creation process should take a few seconds until the table's status becomes “active.” Below is an image showing the active table: Exploring the Table Overview After creation, select your orders table from the console to view an overview of its configurations: Key Information: Displays the partition key (customerId) and sort key (orderId), along with the chosen capacity mode. Metrics: Initially, the table contains zero items. The overview provides details like table size, average item size, and capacity metrics. Indexes and Monitoring: Explore secondary indexes in the “Indexes” section and review CloudWatch metrics in the “Monitor” section, including read/write usage, latency, and alarms for performance tracking. Refer to the images below for an overview and monitoring dashboard: Adding Items to the Table To populate your table with data, navigate to Explore Table Items . Since the table is initially empty, you will see a message indicating no items and minimal read capacity usage: Click Create item and populate the required fields: Partition Key (customerId): For example, “cust1” for customer one. Sort Key (orderId): For example, “order10” as the order identifier. You can add additional attributes, such as: Price: Numeric (e.g., 100) Delivered: Boolean (true or false) The following image shows the item creation interface: Add further attributes as needed: For demonstration purposes, add the following orders: First Order for Customer One: Partition Key: cust1 Sort Key: order10 Price: 100 Delivered: true Second Order for Customer One: Partition Key: cust1 Sort Key: order20 Price: 50 Delivered: true Order for Customer Two: Partition Key: cust2 Sort Key: order30 Price: 35 Delivered: false The image below shows another item creation interface: And a final form view: After adding these items, you will have three orders stored in your table. Querying the Table Queries are more efficient than scans, so use them to retrieve specific data based on the partition key. For example, to retrieve orders for customer one: Navigate to the Query Editor in the DynamoDB Console. Enter the partition key value (“cust1”) and run the query to return both orders associated with that customer. See the example below: To retrieve orders for customer two, simply change the partition key to “cust2”. For more refined searches, you can apply filters. For example, to retrieve only orders for customer one with a price greater than 60: Set cust1 as the partition key. Add a filter condition where price > 60 . Run the query to obtain only those orders that match the condition. A filtered query example is displayed below: Editing and Deleting Items To update an existing item, select it from the table view and click Edit item . For instance, you might update the price from 100 to 120 if the order cost has changed: After making the changes, save your updates. To delete an item, select it and choose Actions > Delete item . This will remove the selected item from your table. Note Always ensure you have a backup or have exported your data if you need to retain it before deleting any items. Using PartiQL for Queries DynamoDB supports PartiQL, an SQL-compatible query language that allows you to interact with your table using familiar SQL-like syntax. PartiQL translates SQL commands into DynamoDB queries, making it a useful alternative if you are more comfortable with SQL. For further details on PartiQL, check out the DynamoDB Documentation . Monitoring and Cleanup To monitor your table's performance, navigate to the Monitor tab in the DynamoDB Console. Here you can review CloudWatch metrics such as read/write usage, latency, and throttled requests. When your demonstration is complete and you wish to clean up resources, simply select the table, click Delete , and confirm the deletion. Warning Deleting the table will permanently remove all stored data. Ensure you have backed up any necessary information before proceeding with the deletion. This demonstration has guided you through creating and configuring a DynamoDB table, adding and querying items, updating and deleting entries, leveraging PartiQL for SQL-style operations, and monitoring performance. We hope this tutorial offers a solid foundation for using DynamoDB in your projects. Watch Video Watch video content Practice Lab Practice lab"
AWS Solutions Architect Associate Certification,OpenSearch,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Database/OpenSearch,"AWS Solutions Architect Associate Certification Services Database OpenSearch In this lesson, we explore AWS OpenSearch, a powerful search and analytics suite designed to handle a wide range of data types. You'll learn about its purpose, key components, and common use cases, as well as how it compares to traditional relational databases. Modern applications often generate diverse data—including textual content, login events, geospatial information, time series data, JSON, and other semi-structured formats. While traditional relational databases excel at handling structured data with predefined schemas, they may struggle with the flexibility and scalability required for these varied data types. OpenSearch, built on the robust capabilities of Elasticsearch, efficiently manages this data in real time, making it an ideal solution for search engines, log management, and analytics. Before diving deeper, it is important to understand the relationship between Elasticsearch and OpenSearch. Originally, both Elasticsearch and Kibana were free and open source. However, when the managing company transitioned Elasticsearch to a proprietary license, Amazon forked the latest available open source version to maintain a truly open environment. While Elasticsearch now operates under a proprietary model, OpenSearch continues to be offered as a free and open source alternative under proper licensing. Components of the OpenSearch Solution OpenSearch can be deployed as a managed cluster on Amazon or used via a serverless configuration. OpenSearch Serverless is an on-demand, auto-scaling offering that removes the operational burden of provisioning, configuring, and tuning an OpenSearch cluster—Amazon manages these tasks for you. Additionally, OpenSearch Ingestion serves as a fully managed, serverless data collector that delivers real-time logs, metrics, and trace data to both OpenSearch Service domains and OpenSearch Serverless collections. For each OpenSearch cluster, this service creates a dedicated domain to simplify data delivery and management. With OpenSearch Ingestion, there is no longer a need for third-party tools like Logstash or Jaeger. You can configure your data producers to send information directly into OpenSearch Ingestion, which can also handle data transformation and cleaning before the data is delivered to your clusters. One notable feature of OpenSearch is its ability to aggregate logs, traces, and metrics into a unified view, enabling comprehensive application analytics. Its machine learning integration further supports anomaly detection and alerting. OpenSearch also allows replicating indexes, mappings, and metadata between clusters, ensuring cross-cluster redundancy or offloading reporting queries. The service offers a variety of CPU, memory, and storage configurations—supporting up to three petabytes of attached storage. Moreover, OpenSearch provides a familiar SQL query syntax, enabling users to perform aggregations with WHERE clauses and to read data as JSON documents or CSV tables. For those with a background in SQL, this makes transitioning to OpenSearch straightforward. The platform also supports trace analytics, allowing the ingestion and visualization of open telemetry data. Note For users transitioning from Elasticsearch, OpenSearch provides a familiar environment with additional serverless and ingestion features, allowing for streamlined operations and enhanced scalability. Integrations for OpenSearch OpenSearch service domains automatically send operational metrics to CloudWatch, which helps monitor domain health and performance. AWS CloudShell maintains a history of OpenSearch configuration API calls and related events for auditing purposes. Other integrations include: Amazon Kinesis: Load and stream data into OpenSearch. Amazon S3: Use S3 for index storage. IAM: Securely manage cluster access. Lambda: Preprocess data before ingestion. DynamoDB: Automatically transfer table data to an OpenSearch cluster. Amazon QuickSight: Create interactive dashboards for visualizing OpenSearch data. Common Use Cases OpenSearch is ideally suited for several real-world applications: Observability: Storing logs, metrics, and traces from applications and infrastructure in a centralized platform. Security: Managing security and event data in real time for threat detection and incident management. Search Functionality: Powering search capabilities within applications and websites to enhance user experiences. Summary OpenSearch is a community-driven search and analytics suite that originated as a fork of Elasticsearch to preserve its open source nature. With both traditional and serverless deployment options, OpenSearch simplifies the management and scaling of search clusters while integrating diverse data sources for comprehensive analytics. For more detailed information and best practices, consider visiting the following links: AWS OpenSearch Documentation Elasticsearch Overview AWS CloudWatch Monitoring Warning Ensure that you understand the licensing differences between OpenSearch and Elasticsearch to avoid unexpected limitations or costs when deploying in production environments. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,DocumentDB,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Database/DocumentDB,"AWS Solutions Architect Associate Certification Services Database DocumentDB In this lesson, we explore AWS DocumentDB—a fully managed, MongoDB-compatible database service that simplifies the deployment and management of scalable NoSQL databases in the cloud. Overview MongoDB is renowned for its flexibility and scalability as a NoSQL database. However, scaling MongoDB—like any database—can present operational challenges that require considerable expertise. AWS DocumentDB eliminates these obstacles by providing a managed service that mimics the functionality of MongoDB without the burden of manual scaling and infrastructure management. With AWS handling the complexities behind the scenes, you can focus on building and optimizing your application. DocumentDB Architecture Amazon DocumentDB architecture consists of a cluster that is made up of two main components: Cluster Volume The cluster volume is a unified storage unit that manages the data for all instances in the cluster. It employs cloud-native storage to replicate your data six different ways across three Availability Zones, ensuring high durability and availability. With support for up to 128 terabytes of data, the cluster volume is engineered to handle immense workloads with reliability. Instances Instances provide the computational resources necessary for database operations, reading from and writing to the cluster volume. A cluster can include between zero and 16 instances, grouped into two roles: Primary Instance: This instance handles both read and write operations and is solely responsible for data modifications. Every cluster has exactly one primary instance. Replica Instances: Replica instances are designated for read-only operations. With up to 15 replicas, you can distribute read workloads effectively, freeing up the primary instance for write operations. Note Cluster instances can be provisioned in different instance classes and scaled up or down as needed, allowing you to adjust compute capacity independently from storage. Global Clusters To support critical global workloads, DocumentDB offers global clusters. This feature allows for automatic data replication across multiple AWS regions with sub-second latency. Typically, you can maintain a primary cluster in one region and secondary clusters in up to five different AWS regions. These secondary clusters are independently scalable, ensuring optimal cost and performance based on regional demand. Fast, storage-based physical replication from the primary to secondary clusters ensures that compute resources remain dedicated to handling application requests rather than replication tasks. Key Benefits and Features AWS DocumentDB is designed to deliver a robust, scalable, and highly available data storage solution with these standout features: MongoDB Compatibility: Continue leveraging your existing MongoDB tools and libraries with minimal to no changes required. High Availability and Durability: Data is automatically replicated across three Availability Zones, which minimizes the risk of data loss and ensures continuous availability. Automatic storage repair techniques detect and resolve failures using redundant data copies. Efficient Crash Recovery: By managing the page cache separately from the database process, DocumentDB ensures rapid recovery. The asynchronous, parallel crash recovery process quickly re-warms the buffer pool to the most current state. Write Durability: Client acknowledgments are only issued after writes have been durably recorded on a majority of nodes. Read Scaling with Replicas: Dedicated read-only replicas efficiently handle query loads, allowing you to adjust the number of replicas based on your application's read capacity needs. To optimize client-side read scaling, DocumentDB supports various read preference options: Primary: Routes reads exclusively to the primary instance. If the primary instance is unavailable, the read operation will fail. db.example.find().readPref('primary') Primary Preferred: By default, reads are directed to the primary instance; if it becomes unavailable, a replica is used. db.example.find().readPref('primaryPreferred') Secondary: Directs all read operations to replica instances. If no replicas are available, the read operation fails. db.example.find().readPref('secondary') Secondary Preferred: Prefers replica instances for read operations but falls back to the primary instance if necessary. db.example.find().readPref('secondaryPreferred') Nearest: Allocates read queries to the instance with the lowest network latency, whether it is a primary or replica. db.example.find().readPref('nearest') Use Cases AWS DocumentDB is well-suited for applications that demand high scalability and low-latency global reads. Typical use cases include: Content management systems User profile management, including preferences and requests Applications designed to handle millions of user requests per second Summary AWS DocumentDB is a managed document database service that brings the power of MongoDB to the cloud with enhanced scalability and simplified operations. Its advanced architecture—with six-way data replication across three Availability Zones and support for global clusters—ensures high availability and durability. Additionally, flexible read preferences and independent scaling of secondary clusters provide optimized performance tailored to your needs. Global clusters deliver low-latency reads on a global scale, while flexible read preferences enable you to direct traffic between primary and replica instances to best meet your application's requirements. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Keyspaces,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Database/Keyspaces,"AWS Solutions Architect Associate Certification Services Database Keyspaces AWS Keyspaces is a fully managed, serverless Apache Cassandra database service designed to deliver fast, scalable, and highly available NoSQL solutions. Built on Apache Cassandra’s renowned performance for rapid read and write operations, AWS Keyspaces eliminates the operational complexity of managing a Cassandra cluster, such as scaling, node management, and specialized expertise. By offloading critical tasks like provisioning, patching, and scaling of the underlying infrastructure, AWS Keyspaces allows you to concentrate on developing your applications. Its serverless architecture means you pay only for the resources you consume, while the service automatically scales table capacity based on your traffic patterns. In a traditional Apache Cassandra setup, you must manage a cluster of nodes—often spanning hundreds of machines across multiple data centers. This setup demands constant oversight using Cassandra Query Language (CQL), a SQL-like interface. With AWS Keyspaces, this complexity is removed, offering a fully managed environment compatible with your existing CQL queries. Throughput Capacity Modes AWS Keyspaces supports two distinct throughput capacity modes for handling read and write operations: On-Demand: With on-demand capacity, you pay solely for the reads and writes your application executes. There is no need to pre-allocate throughput, making it ideal for workloads with unpredictable traffic fluctuations. Provisioned: For applications with predictable traffic, the provisioned mode allows you to specify the required throughput in advance. This approach not only optimizes cost but also provides the option for automatic scaling, with adjustments allowed once per day to align with changing usage patterns. Note For predictable workloads, provisioning capacity provides a straightforward way to manage costs while maintaining high performance. AWS Keyspaces Architecture The AWS Keyspaces architecture is designed for seamless operation and high availability. When a client—using Cassandra drivers, CQLSH, or the AWS Keyspaces console—submits a CQL statement, AWS Keyspaces processes the request and stores three copies of your data across separate Availability Zones. This design not only ensures high availability but also integrates features such as encryption at rest and robust security with AWS IAM and VPC endpoints. Additionally, AWS Keyspaces automatically handles data expiration with Time to Live (TTL) management. Multi-Region Capabilities AWS Keyspaces also supports multi-region replication, which is essential for global applications. When you create a multi-region keyspace, each AWS region hosts a replica of your tables with an identical schema. Data written to one region is asynchronously replicated to other regions—typically with less than one second delay—ensuring that both reads and writes are served locally with single-digit millisecond latencies. Key Benefit Regional replication enhances business continuity by allowing your application to seamlessly switch to healthy regions in the event of a regional outage. The storage-based replication mechanism ensures high throughput even during replication activities. Conflict resolution for simultaneous writes in different regions is automatically managed by AWS Keyspaces, ensuring data consistency across your deployments. Key Use Cases AWS Keyspaces is ideal for applications that require low latency and are built on open-source technologies. It is particularly well-suited for: Industrial equipment monitoring Trade monitoring Fleet management and route optimization By leveraging AWS Keyspaces, developers can focus more on application logic without the burden of manually managing complex Cassandra clusters. Summary AWS Keyspaces simplifies the deployment, management, and scaling of Apache Cassandra database tables by offering both on-demand and provisioned capacity models. Its architecture guarantees high availability, global replication, and seamless compatibility with Cassandra Query Language (CQL). Whether you are handling unpredictable or steady workloads, AWS Keyspaces provides the necessary scalability, security, and performance for modern applications. For users with predictable workloads, the provisioned capacity mode lets you define the precise number of reads and writes your application needs, enabling cost optimization while maintaining optimal performance. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,DynamoDB,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Database/DynamoDB,"AWS Solutions Architect Associate Certification Services Database DynamoDB In this article, we explore AWS DynamoDB—a fully managed NoSQL database service designed to simplify database management and scaling for web applications and services. DynamoDB addresses common database administration challenges by offering a serverless architecture that ensures high availability, automatic sharding, data consistency, and durability. Its seamless scalability makes it ideal for applications dealing with rapidly changing workloads and vast datasets. Since DynamoDB is a managed service, AWS takes care of the provisioning and maintenance of the underlying resources. Below, we dive into the core components of DynamoDB. Components of DynamoDB Tables, Items, and Attributes Tables in DynamoDB function similarly to tables in a relational database—they act as collections where data is stored. Each table contains multiple items, with each item representing a single entry (such as a user or a product). Items are composed of attributes that are analogous to columns in a relational database. For example, consider a table named ""people"" that stores individual records. Each item in the ""people"" table could represent a person with attributes like name, email, or phone number. Below is an example representation of a table containing two people: [
    {
        ""PersonID"": 101,
        ""LastName"": ""Smith"",
        ""FirstName"": ""Fred"",
        ""Phone"": ""555-4321""
    },
    {
        ""PersonID"": 102,
        ""LastName"": ""Jones"",
        ""FirstName"": ""Mary"",
        ""Address"": {
            ""Street"": ""123 Main"",
            ""City"": ""Anytown"",
            ""State"": ""OH"",
            ""ZIPCode"": 12345
        }
    }
] Each JSON object in the example above represents an item in the table, with attributes corresponding to properties about the person. Primary Keys Every item in a DynamoDB table is uniquely identified by a primary key. There are two types of primary keys in DynamoDB: Simple Primary Key (Partition Key): Consists of a single attribute. Composite Primary Key: Consists of both a partition key and a sort key, ensuring uniqueness across the table by combining these attributes. In the ""people"" table example, the PersonID attribute serves as the primary key, ensuring that each person is uniquely identified. It is important to avoid using non-unique attributes (e.g., first names) as primary keys. For tables with composite primary keys, DynamoDB uses the partition key to determine the physical storage location (via an internal hash function) and stores items with the same partition key in order, sorted by the sort key. This design allows multiple items to share the same partition key as long as their sort key values differ. Composite Primary Key Example Consider a music table that stores information about different songs. In this table, a composite primary key is defined by the combination of the artist and the song title. This uniqueness ensures that while an artist can have multiple songs—and different artists might have songs with the same title—the combination always remains unique. [
    {
        ""Artist"": ""No One You Know"",
        ""SongTitle"": ""My Dog Spot"",
        ""AlbumTitle"": ""Hey Now"",
        ""Price"": 1.98,
        ""Genre"": ""Country"",
        ""CriticRating"": 8.4
    },
    {
        ""Artist"": ""No One You Know"",
        ""SongTitle"": ""Somewhere Down The Road"",
        ""AlbumTitle"": ""Somewhat Famous"",
        ""Genre"": ""Country"",
        ""CriticRating"": 8.4,
        ""Year"": 1984
    },
    {
        ""Artist"": ""The Acme Band"",
        ""SongTitle"": ""Still in Love"",
        ""AlbumTitle"": ""The Buck Starts Here"",
        ""Price"": 2.47,
        ""Genre"": ""Rock"",
        ""PromotionInfo"": {
            ""RadioStationsPlaying"": [""KHCR"", ""KQBX"", ""WTNR"", ""WJJH""],
            ""TourDates"": {
                ""Seattle"": ""20150622"",
                ""Cleveland"": ""20150630""
            },
            ""Rotation"": ""Heavy""
        }
    }
] With a composite primary key, you can query the table efficiently—either by using just the partition key (to retrieve all songs by an artist) or by using both the partition and sort key (to retrieve a specific song by an artist). Secondary Indexes Secondary indexes enable alternative query paths using non-primary key attributes. DynamoDB supports two types of secondary indexes: Global Secondary Index (GSI): Features a partition key and an optional sort key that can differ from the table's primary key attributes. Local Secondary Index (LSI): Uses the same partition key as the base table but requires a different sort key. Each DynamoDB table can have up to 20 global secondary indexes and 5 local secondary indexes. For instance, if you need to query the music table by genre and album title, you can create a secondary index with genre as the partition key and album title as the sort key. When a secondary index is created, DynamoDB automatically maintains it as items in the base table are added, updated, or deleted. You can also specify which attributes should be included in the index projection; by default, key attributes are included. DynamoDB Streams DynamoDB Streams is an optional feature that captures data modification events in near-real time. Every modification—whether an insert, update, or delete—creates a stream record that includes details such as the table name, timestamp, and a snapshot of the item. For example, when a new item is added to the table, DynamoDB Streams captures the complete image of the new item. Similarly, for an update, the stream records both the before and after images. When an item is deleted, the stream captures its pre-deletion image. Each stream record is retained for 24 hours. You can integrate DynamoDB Streams with AWS Lambda to trigger code execution in response to these events. For instance, in a customer table, a Lambda function could be triggered to send a welcome email whenever a new customer is added. {
    ""CustomerID"": 1034,
    ""LastName"": ""Major"",
    ""FirstName"": ""Alice"",
    ""EmailAddress"": "" [email protected] ""
} Table Classes and Capacity Modes DynamoDB offers two table classes for optimizing cost and performance: Standard Table Class: The default option suitable for most workloads. Standard-Infrequent Access (Standard-IA): Designed for scenarios where storage cost is significant and data is accessed less frequently (e.g., application logs or historical records). All secondary indexes inherit the table class of the base table. DynamoDB also provides two capacity modes: Provisioned Mode: Requires capacity planning for predictable workloads. On-Demand Mode: Automatically scales capacity using a pay-per-read/write model, ideal for unpredictable workloads. Key Features and Integrations DynamoDB supports both key-value and document data models, offering a flexible schema that easily adapts to changing application requirements without the need for pre-defining a schema before data insertion. Serverless Benefits One of DynamoDB's most impressive features is its serverless nature. With no servers to provision, patch, or manage, DynamoDB offers on-demand pricing with auto-scaling and maintains high availability even during unexpected traffic surges. Another powerful feature is global tables, which enable active-active replication across multiple AWS regions. This ensures that globally distributed applications can achieve single-digit millisecond read and write performance by accessing data locally. Additional features include: Secondary Indexes: Provide additional query flexibility beyond the primary key. On-Demand Backup and Restore: Allow full backups of your tables at any time. Read/Write Capacity Modes: Offer both provisioned and on-demand throughput configurations. Integrations: Seamlessly integrate with AWS services such as Amazon S3, AWS Glue, Amazon Kinesis Data Streams, CloudWatch, and CloudTrail to support data streaming, monitoring, logging, and long-term data archiving. Use Cases DynamoDB is versatile and can be applied to a wide range of scenarios. It is particularly well-suited for: High-concurrency, internet-scale applications that handle millions of requests per second. Applications managing user-generated content, metadata, and caching. Real-time video streaming and interactive content platforms due to its low-latency and multi-region replication features. Ideal Use Cases DynamoDB's ability to handle unpredictable workloads and scale seamlessly makes it an excellent choice for applications with rapidly changing data access patterns and extensive read/write demands. Summary DynamoDB is a fully managed NoSQL database service by AWS that delivers fast, predictable performance and seamless scalability. Its key components include: Tables: The primary data storage units. Items: Individual entries within each table. Attributes: The properties or fields of an item. Primary Keys: Unique identifiers for items, which can be either simple or composite (combining a partition key and a sort key). Secondary Indexes: Global and local indexes that extend query flexibility. Table Classes: Options include Standard and Standard-IA, allowing cost and performance optimization based on access patterns. Capacity Modes: Provisioned for predictable workloads or on-demand for unpredictable workloads. With features such as serverless operations, on-demand scalability, global tables, and comprehensive integrations with other AWS services, DynamoDB is ideally positioned for building modern, high-performance, and highly responsive systems. This article provides an in-depth overview of DynamoDB's architecture, core components, and practical applications—guiding you on how to leverage it for building scalable and responsive systems. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,TimeStream,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Database/TimeStream,"AWS Solutions Architect Associate Certification Services Database TimeStream In this lesson, we explore AWS Timestream—a fast, scalable, and serverless time series database service—and explain why managing time series data is essential for modern IoT and operational applications. Why Use a Time Series Database? Time series databases are crucial when dealing with data from IoT devices and monitoring cameras. Traditional databases often struggle with these workloads because of: High Volume and Velocity IoT devices and surveillance cameras produce massive streams of real-time data. Conventional databases can suffer from performance bottlenecks and even data loss when processing such high-speed, high-volume streams. Variable Data Structure Data generated by sensors and monitoring systems can change significantly over time. Traditional databases rely on a fixed schema, making it challenging to adjust to evolving data formats. Data Lifecycle Management The value of time series data decreases with age. Without efficient data retention and deletion policies, traditional databases struggle with archiving or purging outdated information. Note AWS Timestream was designed to overcome these challenges by offering a flexible, serverless solution that scales dynamically with your data needs. How AWS Timestream Addresses These Challenges AWS Timestream offers a serverless architecture that decouples data ingestion, storage, and querying, allowing each component to scale independently. Here are some of its key features: Dynamic Schema Creation : Unlike fixed-schema databases, Timestream automatically adapts its table schema based on the incoming time series data, allowing for incremental schema evolution. Efficient Data Partitioning : Data is partitioned by time and various attributes using purpose-built indexes, which accelerates query performance. Automated Data Lifecycle Management : Timestream uses an in-memory store for recent data and a magnetic store for historical data. Configurable rules automatically transition data between these tiers as it ages. Key Features of AWS Timestream Serverless : No need to manage infrastructure or provision capacity. Storage Tiering : Separates recent data in memory from older data in a magnetic store with seamless movement between the two. Built-in Time Series Analytics : Supports advanced aggregates, window functions, and complex data types. Custom Query Engine : Enables querying across storage tiers using a single SQL statement. Data Protection : Integrates with AWS Backup to safeguard your time series data. Integrations and Use Cases Data can be sent to AWS Timestream using several AWS services, such as AWS IoT Core, Amazon Kinesis, and Amazon Managed Streaming for Apache Kafka (MSK). For visual analysis, you can use Amazon QuickSight or Grafana, while integration with Amazon SageMaker enables advanced machine learning scenarios. Common Use Cases Use Case Description IoT Applications Centralizes data from IoT devices like cameras for real-time monitoring and analysis. DevOps Collects and analyzes operational metrics to monitor system health and performance. Summary AWS Timestream is engineered to handle the scale and performance demands of IoT and operational applications. Its serverless, fully managed service automatically scales to match workload demands, while its flexible data model accommodates evolving data formats without requiring predefined schemas. With seamless integration into popular BI and ML tools, AWS Timestream makes it easy to visualize and analyze massive amounts of time series data. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Neptune,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Database/Neptune,"AWS Solutions Architect Associate Certification Services Database Neptune In this lesson, we will explore AWS Neptune and discuss why graph databases are essential in today's data-driven world. Graph databases offer an intuitive and efficient method for representing, querying, and traversing interconnected data—capabilities that traditional relational databases often struggle to provide. This makes them especially valuable in scenarios where understanding complex relationships is key. Imagine an investigation board at a police station where evidence, documents, suspect photographs, and clues are all linked by relationships. This visualization perfectly illustrates how graph databases can effectively store and manage intricate interconnections. What is Amazon Neptune? Amazon Neptune is a fully managed graph database service designed specifically for the cloud. It simplifies building and running graph applications by handling infrastructure management, ensuring built-in security, continuous backups, and serverless operations. With Neptune, developers can focus on building applications without the overhead of maintaining the underlying infrastructure. Additionally, Neptune integrates seamlessly with numerous AWS services. Neptune is engineered for global distribution. A single Neptune database can operate across multiple AWS regions, replicating data with minimal performance impact. The Neptune global database feature allows you to deploy a primary database in one AWS region while replicating data to up to five secondary read-only clusters in different regions. In addition, Neptune Serverless enables you to run graph workloads that automatically scale to meet demand without manual capacity management. With a serverless model, you pay only for the resources you use, ensuring cost efficiency for both high-demand and unpredictable applications. A standout feature of Neptune is Amazon Neptune ML. By leveraging graph neural networks (GNNs), Neptune ML offers faster and more accurate predictions—improving forecast accuracy by over 50% compared to traditional methods. This integration of machine learning directly into graph databases enables enhanced analytical capabilities. Key Features of AWS Neptune Serverless Operation: Neptune Serverless is an on-demand deployment model that automatically adjusts capacity based on your application's needs. It guarantees high throughput and low latency for graph queries. With a few clicks in the AWS Management Console, you can scale compute and memory resources for your production clusters. Storage scales accordingly, and the service delivers access to low latency read replicas. Additionally, it supports open graph APIs including Apache TinkerPop and Gremlin. Integration with AWS Services: AWS Neptune easily integrates with various AWS services such as AWS Glue, SageMaker, Lambda, Amazon Athena, AWS Database Migration Service (DMS), and AWS Backup. This broad integration ecosystem enhances its utility for diverse application scenarios. Diverse Use Cases: Neptune is commonly used for constructing identity graphs that provide a 360-degree view of customers, enabling targeted advertising, personalization, and comprehensive analytics. It is also effective in detecting fraud patterns. Furthermore, Neptune ML harnesses graph neural networks to enhance prediction accuracy and can detect and investigate IT infrastructure issues using layered security approaches. Tip Remember, designing graph databases enables your applications to naturally model real-world connections, offering a significant advantage over traditional databases when dealing with complex relationships. Summary of Neptune Amazon Neptune is a fast, reliable, and fully managed graph database service optimized for handling complex, highly connected datasets. It is built to manage billions of relationships while supporting millisecond query latencies. By offloading the operational responsibilities such as hardware provisioning, database setup, patching, and backups to AWS, Neptune allows you to concentrate on application development. Neptune supports multiple graph models, such as the Property Graph and W3C RDF, and offers powerful query languages like Apache TinkerPop, Gremlin, and SPARQL. Its capability to create global databases spanning multiple AWS regions ensures high performance and resilience against regional outages. Moreover, Neptune ML extends your analytical capabilities by integrating machine learning for advanced tasks such as node classification, link prediction, and entity resolution. This makes it an ideal solution for extracting deeper insights from interconnected data. Additional Resources For more information on AWS Neptune, explore the AWS Neptune Documentation . You may also find the Kubernetes Basics guide useful when considering how graph databases interact with containerized environments. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,DynamoDB Accelerator,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Database/DynamoDB-Accelerator,"AWS Solutions Architect Associate Certification Services Database DynamoDB Accelerator In this lesson, we explore AWS DynamoDB Accelerator (DAX), a fully managed, in-memory caching service designed to boost Amazon DynamoDB performance. DAX can improve read performance by up to 10 times, reducing latency from milliseconds to microseconds even when handling millions of requests per second. DAX Architecture and Workflow A DAX cluster is composed of one or more nodes. Within the cluster, one node operates as the primary node while any additional nodes act as read replicas. A DAX cluster can include up to 10 nodes (one primary and nine replicas). At runtime, the DAX client transparently redirects all DynamoDB API requests from your application to the DAX cluster, which intelligently manages load balancing and routing among the nodes. When a get item request is made: The DAX client first attempts to serve the request from its in-memory cache. If the item is cached, it is returned in microseconds. If the item is not in the cache, the request is forwarded to DynamoDB, and the returned data is subsequently cached for future requests. Key Benefit By caching frequently accessed data, DAX minimizes direct calls to DynamoDB, thereby reducing database load and operational costs. Scalability and Management DAX scales on demand. You can start with a modest three-node cluster and add more nodes as your traffic increases, up to the maximum of 10 nodes. Like DynamoDB, DAX is fully managed by AWS, which removes the need to manage the underlying infrastructure manually. Seamless Integration One of the major advantages of using DAX is its API compatibility with DynamoDB. To integrate DAX into an existing application, you only need to swap out the standard DynamoDB client with the DAX client—no major code rewrites are required. Additionally, DAX offers flexible deployment options: Provision a single DAX cluster to serve multiple DynamoDB tables. Create multiple clusters tailored to a single table, depending on your workload and performance needs. Integration with AWS Compute Services DAX integrates effortlessly with various AWS compute services such as AWS Lambda and Amazon EC2 . Simply replace your DynamoDB client with the DAX client and your application will benefit from enhanced performance without further modifications. Operational Efficiency In summary, DynamoDB Accelerator (DAX) enhances DynamoDB's read performance through an in-memory caching layer that: Increases throughput by handling millions of requests per second. Reduces latency by caching frequently accessed items. Lowers operational costs by offloading read requests from DynamoDB. Simplifies application architecture by eliminating the need for custom caching logic. DAX ensures data consistency and high availability by writing data concurrently to both the cache and the underlying DynamoDB table. This write-through and read-through caching mechanism guarantees that the cached data remains up-to-date. Performance Boost Leveraging DAX allows your application to achieve higher throughput and lower latency, ensuring that you can scale effectively while managing costs. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,QLDB,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Database/QLDB,"AWS Solutions Architect Associate Certification Services Database QLDB In this lesson, we'll explore AWS Quantum Ledger Database (QLDB) and explain why traditional databases fall short for ledger applications. QLDB offers a fully managed, immutable ledger database with transparency and cryptographic verification to meet the rigorous demands of ledger use cases. Why Traditional Databases Are Not Ideal for Ledger Use Cases Traditional databases have several limitations when used as a ledger: Mutability: Records can be updated or deleted, making data mutable rather than tamper-proof. Lack of Transparency: Data changes can be hard to trace, impeding auditability. Centralization: Managed by a single entity, they do not support decentralized, multi-party systems. Single Point of Failure: Centralized servers can become a single point of vulnerability. Absence of Consensus Mechanisms: They lack built-in methods for distributed trust. Limited Privacy Features: Advanced privacy options like selective disclosure or confidential transactions are often missing. Key Insight QLDB addresses these limitations with an immutable, append-only journal that ensures data integrity and provides complete auditability. Overcoming Limitations with the QLDB Journal In a conventional database structure, data is stored in tables, and every transaction is recorded in a mutable transaction log. While essential for disaster recovery, these logs can be directly modified, compromising data integrity. QLDB replaces this approach with an immutable, append-only journal that records every write operation—whether an insert, update, or delete. A Car Lifecycle Example Imagine using QLDB to track a car’s lifecycle: Initial Entry: The car’s first record is written to the journal as a block containing a unique identifier, metadata, and a hash value. This entry is updated in both the current data table (C table) and the history table (H table). First Resale: When the car is sold to Thomas, a new journal entry is added. The C table reflects the current owner, and the H table records the transaction. Subsequent Resale: When sold to Smith, the process repeats—appending a new block to the journal, ensuring every transaction is recorded chronologically and securely. Because QLDB's journal is append-only, every change is permanently recorded. This design guarantees that the full history of the ledger can be queried, as no data is ever altered once committed. Ensuring Data Integrity with Cryptographic Verification QLDB writes one block to its journal per transaction. Each block contains entry objects representing the processed document and the PartiQL statements applied. These blocks are sequenced and cryptographically linked using SHA-256, similar to blockchain technology, to maintain data integrity. Using digest and Merkle audit proofs, QLDB lets you verify that no unauthorized changes have occurred. Each block, containing data documents, metadata, and PartiQL statements, is hashed and chain-linked to ensure a verifiable record. Core Features of QLDB QLDB brings several powerful features to ledger management: Open-Source and SQL-Compatible Query Language: Utilizes PartiQL which extends SQL to work with Amazon Ion, its document-oriented data model. Fully Managed and Serverless: Eliminates the need for server management while ensuring high availability. ACID Compliant: Supports enterprise-level reliability for mission-critical applications. Use Cases for QLDB QLDB is ideal for applications that demand a verifiable, immutable ledger: Financial Transactions: Track credit and debit card purchases. Supply Chain Management: Document shipments and purchases with an indelible audit trail. Claims History: Maintain an immutable record of insurance or digital claims. Below is a comparison table summarizing the key differences between traditional databases and AWS QLDB: Feature Traditional Database AWS QLDB Data Mutability Mutable records can be updated or deleted Immutable, append-only journal records all changes Transparency Limited data traceability Full auditability with cryptographically verifiable logs Centralization Single-entity management Central trusted authority with complete history preservation Single Point of Failure Vulnerable if the central server fails Serverless, highly available architecture Consensus Mechanism Absent Built-in cryptographic hash chaining Privacy Features Basic Supports advanced privacy measures Learn More Discover further details in the AWS QLDB documentation . Dual Table Structure: Current vs. History QLDB organizes data using two table types: Current Table (C table): Reflects the latest data state. History Table (H table): Stores an immutable ledger of all historical changes. This structure allows for rapid access to current data while preserving a comprehensive audit trail. Conclusion Amazon QLDB transforms ledger management through its immutable, highly transparent, and cryptographically verifiable transaction log. With its append-only journal, dual table architecture, and robust verification through SHA-256 hash chaining, QLDB is the ideal solution for applications where data integrity, transparency, and auditability are paramount. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,ElastiCache,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Database/ElastiCache,"AWS Solutions Architect Associate Certification Services Database ElastiCache In this article, we explore AWS ElastiCache—a fully managed in-memory caching service that boosts application performance by reducing latency and offloading database operations. Before diving into ElastiCache specifics, it is important to understand the role of caching in modern application architectures. Understanding Application Architecture and Caching A typical application architecture includes three core components: Database: Stores all persistent data such as user profiles, order details, inventory records, etc. Application Server: Hosts business logic to process client requests and manages interactions with the database. Client: End-users access data via web browsers, mobile apps, or API consumers. Each time a user interacts with an application—whether signing up or making a purchase—the client sends a request to the server, which processes the request and communicates with the database. Given that these components may reside on different networks or physical hosts, each interaction can incur significant latency. As the user base grows, the database often becomes the performance bottleneck. To alleviate this, a caching layer is introduced between the application server and the database. An in-memory cache holds frequently accessed data—such as the top-selling items on an e-commerce site—for faster retrieval compared to disk-based storage. The cache significantly reduces the load on the database. When data is retrieved during the first request, subsequent requests can fetch it directly from the cache—leading to improved response times and enhanced performance under heavy loads. Introducing AWS ElastiCache AWS ElastiCache is a managed caching service that simplifies deploying, operating, and scaling an in-memory cache in the cloud. With enterprise-grade security and reliability, ElastiCache can easily scale to support hundreds of millions of operations per second with microsecond latency. ElastiCache supports two leading caching engines: Redis: Provides advanced features such as read replicas, data persistence, encryption at rest, and a built-in Pub/Sub messaging system. Memcached: Offers simplicity, multi-availability zone deployments for high availability, auto discovery for managing dynamic clusters, and efficient data partitioning. Key Benefit With ElastiCache, you can offload frequent data requests from your underlying database, which not only reduces latency but also ensures that your application scales seamlessly under increasing demand. Core Components of AWS ElastiCache Cache Cluster A cache cluster groups one or more cache nodes. It serves as the primary scalability unit in ElastiCache, and multiple clusters can operate within a single ElastiCache environment. Node Types Node types determine the CPU, memory, and performance characteristics of individual cache nodes. AWS offers various node types to match different application needs and workload requirements. Cache Parameter Group This group configures the engine-specific settings for your cache cluster, ensuring the optimal performance of your caching layer. Cache Security Group Cache security groups control network access to your cache clusters by defining inbound and outbound traffic rules. This helps safeguard your caches by ensuring that only authorized traffic has access. Subnet Groups Subnet groups are collections of subnets designated for your cache clusters. They allow you to control the network placement of your clusters effectively. Engine-Specific Features For Redis Read Replicas: Offload heavy read traffic to replicas, enhancing high availability. Data Persistence: Save in-memory data to disk for durability. Built-In Pub/Sub: Support real-time messaging applications. Encryption: Secure your data at rest with encryption features. For Memcached Multi-Availability Zone Deployments: Distribute cache nodes across zones for higher reliability. Auto Discovery: Automatically manage nodes during scaling events. Data Partitioning and Sharding: Efficiently distribute data across multiple nodes. Regardless of the caching engine chosen, AWS ElastiCache takes away the administrative overhead of managing your caching environment by ensuring that it is always properly configured, secured, and scaled. Use Cases and Integration Scenarios ElastiCache is widely adopted to reduce the load on relational databases (like AWS RDS) by caching frequently accessed data. Common use cases include: Web and Mobile Applications: Enhance performance by caching session data or user-specific information. Real-Time Applications: Support scenarios such as online leaderboards in gaming where rapid data retrieval is essential. Integration with AWS Services: Seamlessly combine with AWS EKS , AWS Lambda , or Amazon Elastic Container Service (AWS ECS) to boost performance. Monitoring and Security: Integrate with AWS CloudWatch , AWS CloudTrail, and AWS IAM for monitoring metrics, logging activities, and managing access controls. Use Case Description AWS Integration Performance Enhancement Caching frequently accessed web and mobile application data Integrates with AWS Lambda, AWS ECS, and AWS EKS Real-Time Data Applications Supporting dynamic applications such as leaderboards or messaging apps Enables rapid data retrieval and scalability Database Offloading Reducing direct database queries to alleviate load Works with relational databases like AWS RDS Additionally, ElastiCache’s global datastore feature allows for write operations in one AWS region with read operations from cross-region replica clusters. This capability supports low-latency access and strengthens disaster recovery across multiple AWS regions. Summary AWS ElastiCache is a cutting-edge, fully managed in-memory caching service designed to accelerate your applications by minimizing the need for repetitive database queries. With support for both Redis and Memcached, ElastiCache offers microsecond response times, automated scaling, and robust security features. Key highlights include: For Redis: Advanced data persistence, built-in Pub/Sub messaging, and encryption features. For Memcached: Simple yet powerful clustering with multi-threaded support and auto discovery. Overall Benefits: Reduced latency, minimized database load, and seamless integration with other AWS services for a holistic cloud application environment. Further Learning For more detailed information on AWS ElastiCache and related AWS services, explore the AWS Documentation and additional tutorials available online. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,OpenSearch Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Database/OpenSearch-Demo,"AWS Solutions Architect Associate Certification Services Database OpenSearch Demo In this lesson, we guide you through working with Amazon OpenSearch by creating and configuring an OpenSearch domain. A domain represents an OpenSearch cluster with specific settings, instance types, instance counts, and storage resources. In this demo, our domain is named ""demo."" Domain Creation: Easy Create vs. Standard Create When creating a domain, you have two options: Easy create: Quickly set up the domain with recommended default configurations. Standard create: Customize each setting in detail. For this lesson, we select Standard create to explore the available configuration options. Domain Configuration Options Template Selection and Availability Zones Begin by selecting an environment template: For production data, choose production . For demos or testing, select dev/test . You can deploy the domain with or without a dedicated standby node. In this demo, we proceed without a standby node. Next, choose the number of Availability Zones. We select one Availability Zone—although for production deployments, using multiple zones is recommended for high availability. OpenSearch Version and Instance Configuration Choose the OpenSearch version you require. You may also select an Elasticsearch version if needed. For this demo, we opt for version 2.9 (the latest). Next, select the EC2 instance type to run your cluster. Since this is a demo, we choose a cost-effective T3 small instance. Determine the number of nodes per Availability Zone; for example, one node per zone yields a single node. (If you configured three Availability Zones with one node each, you would have three total nodes.) Storage Options Configure your storage preferences: Choose EBS as the storage type. Select the appropriate volume type. Define the storage size per node. For this demo, we use a minimal configuration of 10 GB per node. Optionally, you can enable a dedicated master node; however, for this demonstration, that option remains disabled. Network and Access Control Settings Scroll down to configure network settings. For this demo, choose public access. In a production environment, you may wish to deploy within a specific VPC, subnet, and defined security groups. For access control, you have two options: use IAM or create a master user. In this example, we create a master user by entering a default master username and password that meet security requirements. Additional options include enabling SAM authentication or Amazon Cognito authentication. Under access policies, this demo uses fine-grained access control without further modifications. Leave the remaining settings (such as automatic software updates) as default and deploy your cluster. Deployment Time The deployment process may take 15 to 20 minutes. Accessing the OpenSearch Cluster After deployment, click the endpoint to connect to your cluster. You will see two URLs: one for sending requests to add or retrieve data and another for accessing the OpenSearch Dashboard. Click the dashboard URL and log in using your master username (for example, ""admin"") and the password you configured. During the initial login, a pop-up message may prompt you to choose between specific tenants or a global dashboard. For simplicity, select the global option. At this point, the cluster will be empty. OpenSearch provides a demo dataset to help you practice with sample data. Adding Sample Data When adding data, you will see three sample datasets. For this lesson, select the e-commerce orders dataset. After installation, click ""view data"" to explore the new configurations created automatically within OpenSearch. Within the OpenSearch Dashboard, navigate to Management > Index Management > Indices to view the indices created for the dataset. By selecting the specific e-commerce dataset, you can review the index mappings and see which fields are indexed. Switch to the Discover tab in the OpenSearch Dashboard to view all data points. By default, the date filter covers the last seven days, but you can adjust this range as needed. Filters allow you to refine the data further. For instance, selecting the ""manufacturer"" field displays the top five manufacturers. You can also add filters for fields like ""day of week"" (e.g., filtering for Saturday) to combine multiple criteria. Uploading Data to OpenSearch For those interested in uploading their own data and performing queries on an OpenSearch cluster, refer to the AWS Documentation for a demo with commands to upload data and run queries. Single Document Upload Use this curl command to upload a single document to the ""movies"" index. Remember to update the username, password, and domain endpoint as needed. curl -XPUT -u 'admin:Password123!' 'domain-endpoint/movies/_doc/1' -d '{ ""director"": ""Burton, Tim"", ""genre"": [""Comedy"", ""Sci-Fi""], ""year"": 1996, ""actor"": [""Jack Nicholson"", ""Pierce Brosnan"", ""Sarah Jessica Parker""], ""title"": ""Mars Attacks!"" }' -H 'Content-Type: application/json' Bulk Document Upload For bulk uploads, prepare a file (e.g., bulk_movies.json) with the following content: { ""index"": { ""_index"": ""movies"", ""_id"": ""2"" } }
{ ""director"": ""Frankenheimer, John"", ""genre"": [""Drama"", ""Mystery"", ""Thriller"", ""Crime""], ""year"": 1962, ""actor"": [""Lansbury, Angela"", ""Sinatra, Frank""] }
{ ""index"": { ""_index"": ""movies"", ""_id"": ""3"" } }
{ ""director"": ""Baird, Stuart"", ""genre"": [""Action"", ""Crime"", ""Thriller""], ""year"": 1998, ""actor"": [""Downey Jr., Robert"", ""Jones, Tommy Lee"", ""Snipes, Wesley""] }
{ ""index"": { ""_index"": ""movies"", ""_id"": ""4"" } }
{ ""director"": ""Ray, Nicholas"", ""genre"": [""Drama"", ""Romance""], ""year"": 1955, ""actor"": [""Hopper, Dennis"", ""Wood, Natalie"", ""Dean, James"", ""Mineo, Sal""] } Then execute the following command to perform the bulk upload: curl -XPOST -u 'admin:Password123!' 'https://search-demo-kt2xqr2r2yoeyqvf7hqjzna2m.us-east-1.es.amazonaws.com/_bulk' --data-binary @bulk_movies.json -H 'Content-Type: application/json' If successful, your response will indicate that documents have been added (status code 201). An example response might look like this: {
  ""took"": 61,
  ""errors"": false,
  ""items"": [
    {
      ""index"": {
        ""_index"": ""movies"",
        ""_id"": ""2"",
        ""_version"": 1,
        ""result"": ""created"",
        ""_shards"": {
          ""total"": 2,
          ""successful"": 1,
          ""failed"": 0
        },
        ""_seq_no"": 0,
        ""_primary_term"": 1,
        ""status"": 201
      }
    },
    {
      ""index"": {
        ""_index"": ""movies"",
        ""_id"": ""4"",
        ""_version"": 1,
        ""result"": ""created"",
        ""_shards"": {
          ""total"": 2,
          ""successful"": 1,
          ""failed"": 0
        },
        ""_seq_no"": 0,
        ""_primary_term"": 1,
        ""status"": 201
      }
    }
  ]
} After uploading, verify the data in the Index Management section of the OpenSearch Dashboard by reviewing the ""movies"" index mappings. This view will display fields such as actor, director, genre, title, and year. Searching Data via Dashboards To search for data in OpenSearch Dashboards: Navigate to Dashboards > Index Patterns and create an index pattern for ""movies."" Go to the Discover tab. Select the ""movies"" index and run queries. For example, search for movies featuring a specific actor like Robert Downey Jr. Deleting the OpenSearch Domain To conclude this lesson, delete your OpenSearch cluster by following these steps: In the AWS console, navigate to Domains . Select your ""demo"" domain. Click delete and confirm by typing the domain name (""demo"") into the confirmation dialog. Lesson Complete This concludes our lesson on Amazon OpenSearch. Enjoy exploring the capabilities and features that OpenSearch has to offer! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Autoscaling Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Application-Integration/Autoscaling-Demo,"AWS Solutions Architect Associate Certification Services Application Integration Autoscaling Demo In this lesson, you will learn how to configure and deploy an Auto Scaling group on AWS by launching a simple Nginx web server. The demo walks you through creating a launch template, setting up network and load balancer configurations, establishing scaling policies, and testing auto scaling behavior under high CPU load. Creating the Auto Scaling Group Start by logging into the AWS EC2 console and navigating to the EC2 service. Scroll down to find the ""Auto Scaling Groups"" section. Next, click on ""Create Auto Scaling Group"" and provide a descriptive name such as ""web-auto scale."" You will be prompted to choose between a launch template or a launch configuration. Launch templates offer enhanced customization, making them preferable. In the launch template, you will specify the following: AMI to use Instance type Key pair Security groups Since a launch template is not yet available, select the option to create one. A new tab will open for creating the launch template. Creating a Launch Template In the new tab, complete the settings for your EC2 instances. For example, set the template name to ""my web template"" and add a brief description like “prod web server.” While you can add tags or start with a source template, this demo builds the template from scratch. At the AMI selection section, choose your specific AMI. In this demonstration, we use a custom Linux AMI called ""web ASG demo"" that runs an Nginx server. Next, set the instance type. For this demo, select ""t2.micro"" as it qualifies for the free tier. Choose your key pair (e.g., ""main""). In the network settings, specify a security group (such as ""web SG"") that allows port 80. Leave the subnet settings blank to ensure flexibility when using the template in multiple auto scaling groups. Review the storage, resource tags, and advanced settings, retaining the defaults. Then click ""Create launch template."" Your new launch template (""my web template"") will appear with version 1. Remember, you can update the template settings (like the AMI) and generate new versions later. Return to the Auto Scaling Group tab, refresh the page, and select your newly created launch template. If multiple versions exist, pick the default (version one). Configuring Network Settings Select the appropriate VPC for your auto scaling group. In this demo, a demo VPC is used and instances are deployed in private subnets (with a load balancer later placed in public subnets). While you have the option to override launch template settings (e.g., different instance types), the default values are used here. Proceed to the next configuration step. Configuring the Load Balancer Decide if you want to associate a load balancer with your auto scaling group. For this demonstration, choose to create a new load balancer. An Application Load Balancer (ALB) is recommended because it is well-suited for web servers. Keep the default load balancer name (e.g., ""web autoscale one""), set the scheme to ""internet-facing,"" and assign it to public subnets. Ensure that the listener is configured for port 80. Now, create a target group to route requests from the load balancer to the EC2 instances. For example, name the target group ""web autoscale one tg."" You may add optional tags and specify VPC Lattice integration options. Enable the Elastic Load Balancing health checks with a 300-second grace period and retain the default CloudWatch metrics settings. Setting Capacity and Scaling Policies Define the capacity settings for your auto scaling group. For instance, you might use: Desired capacity: 1 Minimum capacity: 1 (ensuring at least one instance remains active) Maximum capacity: 3 (to handle increased load) Next, establish a target tracking scaling policy based on average CPU utilization. For the demo, set the target CPU utilization to 40%. In a production scenario, you might choose a different threshold (such as 70% or lower) depending on your traffic tolerance. Optionally, configure an instance warm-up period and enable instance scaling protection. Leave additional notification settings as default, review all configurations, and create the auto scaling group. Once the auto scaling group is created, AWS deploys the desired number of EC2 instances (in this case, one), sets up the load balancer, and configures the target group. Verifying the Setup After deployment, verify the auto scaling group's configuration. Confirm that: Desired capacity: 1 Minimum capacity: 1 Maximum capacity: 3 Ensure that the launch template, network settings, and load balancer details display correctly. For example, clicking on the load balancer should show the target group with the deployed instance. At the load balancer level, double-check the security settings. Then, inspect the EC2 instances list to confirm that the instance from the auto scaling group is running—it may initially be in the ""initialization"" or ""status check initialization"" phase. To test the setup, copy the load balancer's DNS name, paste it into a new browser tab, and verify that you see a welcome message (e.g., ""Welcome to KodeKloud""). This confirms that the EC2 instance was deployed correctly and the load balancer is properly forwarding requests. Testing Auto Scaling To confirm that the auto scaling group operates as expected, manually terminate the EC2 instance. The auto scaling group should detect the termination and automatically launch a new instance to maintain the desired capacity. Wait a few moments and then check the auto scaling group's activity log. You should see an entry for an instance being removed due to a failed health check, followed by an entry for the launch of a new instance. Return to the EC2 instances list to verify that a new instance is running. Simulating High CPU Load This section demonstrates how to trigger the scaling policy by simulating high CPU load. Connect to your EC2 instance via SSH. Run the monitoring tool top to observe the initial CPU utilization: top - 04:33:30 up 3 min,  2 users,  load average: 0.01, 0.04, 0.01
Tasks: 114 total,   1 running, 113 sleeping,   0 stopped,   0 zombie
%Cpu(s):  0.0 us,  6.2 sy,  0.0 ni, 93.8 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
MiB Mem :  949.4 total,  572.5 free,    1.8 used,  217.6 buff/cache
MiB Swap:    0.0 total,    0.0 free,    0.0 used.  650.5 avail Mem To simulate load, run the following command to stress CPU resources: stress -c 1 Run top again to observe the CPU usage, which should spike to 100%: top - 04:34:00 up 4 min,  2 users,  load average: 0.29, 0.10, 0.03
Tasks: 116 total,   2 running, 114 sleeping,   0 stopped,   0 zombie
%Cpu(s): 100.0 us,  0.0 sy,  0.0 ni,  0.0 id,  0.0 wa,  0.0 hi,  0.0 si,  0.0 st
MiB Mem :   949.4 total,   572.3 free,   159.4 used,   217.7 buff/cache
MiB Swap:     0.0 total,     0.0 free,     0.0 used.   650.3 avail Mem Allow the stress test to run until the average CPU utilization exceeds 40%. This triggers the auto scaling group's target tracking policy, which scales the instance count from one to three. Examine the auto scaling activity log in the AWS console. You should see entries detailing the triggered alarm and the launch of additional instances. Refresh the EC2 instances list; you should now see three instances running in the auto scaling group. Despite any further CPU load, the maximum capacity remains capped at three as configured. Cleaning Up Cleanup Reminder After completing this demo, ensure you delete the auto scaling group to terminate all associated resources (EC2 instances and load balancer configurations) and avoid extra charges. To clean up, select the auto scaling group and choose ""Delete."" This action terminates the auto scaling group along with its associated resources. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Autoscaling,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Application-Integration/Autoscaling,"AWS Solutions Architect Associate Certification Services Application Integration Autoscaling In this lesson, we will explore auto-scaling and how it dynamically adjusts compute resources to meet demand. Auto-scaling ensures that your applications have the right resources at all times without manual intervention. Imagine a bakery where cupcakes are baked on demand. As more customers arrive, the bakery activates additional ovens when an existing one hits 80% capacity to maintain smooth operations. Conversely, if an oven is only operating at 20% capacity, it is shut down after production to conserve energy. This analogy mirrors how AWS Auto Scaling groups adjust the number of EC2 instances based on current demand. Auto-Scaling Group Features The core element of an auto-scaling group is its scaling policy. A scaling policy modifies the desired capacity by launching or terminating EC2 instances within a defined minimum and maximum range. AWS supports three categories of scaling policies: Manual Scaling: Adjust instances by direct user input. Dynamic Scaling: Automatically adjusts instances based on real-time metrics. Scheduled Scaling: Predetermines scaling actions at specific times. Another critical feature is auto-healing. If an EC2 instance is flagged as unhealthy based on your health check configurations, the auto-scaling group terminates it and launches a replacement to maintain the ideal group configuration. When configuring an auto-scaling group, you need to specify three key properties: Minimum: The absolute lowest number of instances in the group. Desired: The ideal number of instances you want running. This value is often set equal to or higher than the minimum. Maximum: The upper limit on the number of instances the group can scale out to. For example, if you set the group with a minimum of 2, desired of 2, and maximum of 10, the group will begin with 2 instances. Later, if you change the desired count to 5, the group adjusts to 5 instances. Similarly, decreasing the desired count will scale the group down accordingly. For example, consider an auto recovery scenario: if your group is maintained at 5 instances and one or more become unhealthy, the auto-scaling group will terminate the affected instances and launch new ones, ensuring that the total remains at the desired count. Tip While manual adjustments to the desired capacity can handle planned scaling needs, it is best to automate scaling based on real-time metrics in production environments. Dynamic Scaling Policies Dynamic scaling policies enable AWS to monitor various metrics and adjust the number of instances automatically. Common metrics include average CPU utilization, network I/O, and Application Load Balancer request counts. When these metrics exceed or fall below designated thresholds, AWS scales the auto-scaling group accordingly. One popular dynamic approach is target tracking scaling . For instance, you might configure a group with a minimum of 1 instance, desired capacity of 2, and a maximum of 5 instances, while targeting an average CPU utilization of 50%. If the CPU usage stays above 50% for a defined period (such as 5 minutes), a new EC2 instance is added. The group continues to add or remove instances as needed to maintain the target. Another strategy is simple scaling . With simple scaling, CloudWatch alarms initiate scaling actions based on set conditions. For example, if a scale-up alarm triggers, the policy might add a fixed number of instances (e.g., 2) each time the alarm condition is met. A corresponding alarm can reduce the instance count when conditions improve. A more granular option is step scaling . This method uses CloudWatch alarms with tiered rules. For instance, you can define: Add 2 instances if CPU utilization is between 60% and 70%. Add 3 instances if utilization rises between 70% and 85%. Add 5 instances if it reaches between 85% and 100%. A similar approach applies when scaling down. Scheduled Scaling Scheduled scaling is ideal for predictable workloads. For example, a website might experience peak traffic between 8 AM and 10 AM. In such cases, you can schedule the auto-scaling group to increase the desired count during those hours and lower it during off-peak times. Scheduled scaling policies can trigger actions once or on a recurring basis using cron-like schedules. Launch Templates For EC2 auto-scaling groups, a launch template is required to define the specifications for deploying new instances. A launch template contains details such as: The Amazon Machine Image (AMI) Instance type and size Networking configurations (e.g., subnets, security groups) IAM roles and instance access keys Launch templates provide several advantages: Advantage Benefit Standardization Consistent configuration across multiple instance launches Versioning Easy updates without disrupting existing instances Parameterization Override specific parameters (e.g., instance type or purchase options) Tagging & Metadata Simplifies management, billing, and monitoring Integration Manageable via AWS Management Console, CLI, SDKs, or automation tools Termination Protection & User Data Enable instance protection and automation scripts on launch A launch template ensures you deploy instances consistently without manual setup each time. Integration with Other AWS Services AWS auto-scaling groups integrate seamlessly with a variety of AWS services: Elastic Load Balancer (ELB): Distributes incoming traffic across multiple EC2 instances. CloudWatch: Offers monitoring metrics and alarms that drive scaling decisions. Simple Notification Service (SNS): Sends notifications detailing auto-scaling events, such as scaling actions. By leveraging these integrations, AWS Auto Scaling ensures your application can gracefully handle fluctuations in traffic while optimizing resource usage and cost. Summary This guide has covered scaling policies, dynamic and scheduled scaling, launch templates, and AWS service integrations, empowering you to design robust auto-scaling groups that automatically adjust compute resources based on demand. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Elastic Loadbalancing,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Application-Integration/Elastic-Loadbalancing,"AWS Solutions Architect Associate Certification Services Application Integration Elastic Loadbalancing In this article, we explore AWS Elastic Load Balancers and why they are essential for building scalable and highly available cloud architectures. Introduction to Load Balancers Imagine hosting your website, mywebsite.com, on a single EC2 instance . Initially, as traffic remains low, a single instance may be sufficient. However, as your site gains popularity, a single server may struggle to handle increased load. There are two primary scaling strategies: Vertical Scaling: Increase the capacity (CPU, RAM) of your EC2 instance . Horizontal Scaling: Add more EC2 instances to distribute the traffic. However, even with multiple servers, the domain (mywebsite.com) still points to a single IP address unless a load balancer is implemented. This is where load balancers become indispensable. They provide a unified domain name for users while distributing incoming requests across multiple EC2 instances . By balancing the load evenly, these devices enhance both application performance and availability. AWS Elastic Load Balancers Overview AWS offers a robust, scalable, and fault-tolerant load balancing service that spans multiple availability zones. Instead of restricting traffic to a single EC2 instance or availability zone, the load balancer intelligently forwards incoming requests to all configured instances across zones. Before exploring the different types of load balancers, let's review two critical components of their configuration: Listeners and Target Groups . Listeners and Target Groups Listener: A listener defines the rules for processing incoming traffic. For instance, you might configure a listener to accept HTTP requests for http://www.mywebsite.com . Target Group: A target group is a collection of endpoints, such as EC2 instances , that receive the forwarded requests. You can specify the port on which these endpoints listen (e.g., port 80 for HTTP or port 443 for HTTPS). For example, to enable HTTPS traffic on port 443, you can configure a listener for port 443 and route requests to a target group operating on that port. Similarly, additional listeners (e.g., on port 8080) can support different types of requests. This setup allows a single entry point to distribute incoming requests to the appropriate group of servers. Additionally, target groups can include various endpoint types (such as EC2 instances , IP addresses, Lambda functions, or even other load balancers) and support health checks for monitoring endpoint availability. Key Characteristics of Target Groups A target group in AWS can include: EC2 Instances : Multiple instances that handle traffic. IP Addresses: Direct traffic to specific IP addresses, whether hosted on-premises or in another cloud. Lambda Functions: Use serverless functions as backend endpoints. Other Load Balancers Supported protocols include TCP, UDP, TLS, HTTP, HTTPS, and even gRPC. With regular health checks, the load balancer ensures that only healthy endpoints handle traffic. Public vs. Private Load Balancers AWS load balancers can be deployed as either: Public Load Balancers: Exposed to the internet, ideal for public APIs and web applications. Private Load Balancers: Restricted to a private network, perfect for backend services or databases where security is a priority. When deploying a load balancer, you select specific availability zones and subnets. With cross-zone load balancing enabled, traffic is evenly distributed across instances in different zones. Without it, each load balancer node only routes traffic within its zone, which can lead to an uneven load. Types of AWS Load Balancers AWS offers several types of load balancers, each designed for specific use cases: Application Load Balancer (ALB) Operates at Layer 7 (the application layer) and is aware of HTTP/S traffic. Supports advanced routing rules based on host headers, URL paths, HTTP methods, query strings, and custom headers. Ideal for web applications and microservices requiring flexible routing and redirection. Network Load Balancer (NLB) Operates at Layer 4 (the transport layer) and supports protocols such as TCP, UDP, and TLS. Designed for high-performance applications, capable of handling millions of requests per second. Provides static IP addresses per availability zone, offering greater routing control. When you configure an NLB, a network interface is created in each availability zone. For internet-facing NLBs, an optional Elastic IP can be assigned for each subnet. The NLB routes traffic based solely on port numbers. For example, traffic arriving on port 8080 can be directed to one target group, while traffic on ports 80 and 443 is sent to different groups. Advanced ALB Features and Routing Rules Because the Application Load Balancer operates at Layer 7, it provides advanced traffic management options: Host Header Matching: Route traffic based on domain names. Default rule: Applies when no specific host is matched. Custom rule: For instance, route traffic for blog.mywebsite.com to a dedicated target group. Path-Based Routing: Direct traffic based on URL paths (e.g., routing /blog to a specific group). HTTP Method Matching: Distribute traffic based on HTTP methods (e.g., GET vs POST). For example, you might have separate target groups for POST requests. An example using curl for a POST request: curl -X POST -H ""Content-Type: application/json"" -d '{""key1"":""value1""}' https://mywebsite.com/api For clarity, here is a similar command with escaped quotes: curl -X POST -H ""Content-Type: application/json"" -d ""{\""key1\"":\""value1\""}"" https://mywebsite.com/api Source IP and Custom Header Matching: Create rules to match specific source IP addresses or HTTP headers (e.g., x-environment ). For example, matching a header with curl: curl -X POST -H ""x-environment: staging"" https://mywebsite.com/api Query String Matching: Route traffic based on query parameters, such as ?category=books . Multiple routing rules can be combined to support complex microservices architectures. If none of the custom rules match, the load balancer reverts to the default rule. Integrations with AWS Services Elastic Load Balancers seamlessly integrate with numerous AWS services, creating a robust and scalable architecture: AWS Service Use Case Example Amazon EC2 Distributing traffic to virtual servers EC2 instances Amazon ECS Running containerized applications Amazon ECS AWS Lambda Serverless computing for backend endpoints AWS Lambda AWS WAF Adding web application firewall for security Learn more about AWS WAF Amazon Route 53 Managing custom domains and routing policies Learn more about Route 53 Auto Scaling Groups Automatically adjusting resource capacity Auto Scaling Documentation Integrating these services with Elastic Load Balancing not only provides scalability but also ensures that your applications remain resilient and highly available. Best Practice For optimal performance, always configure health checks on your target groups. This ensures that traffic is routed only to healthy endpoints. This article provided an in-depth overview of AWS Elastic Load Balancers, discussing basic concepts, setup with listeners and target groups, distinctions between ALBs and NLBs, advanced routing techniques, and key integrations with other AWS services. By leveraging these capabilities, you can build a robust mechanism for managing traffic and ensuring high availability in your AWS cloud environment. Watch Video Watch video content Practice Lab Practice lab"
AWS Solutions Architect Associate Certification,Demo of using Eventbridge to respond to changes in your environment with AWS config,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Application-Integration/Demo-of-using-Eventbridge-to-respond-to-changes-in-your-environment-with-AWS-config,"AWS Solutions Architect Associate Certification Services Application Integration Demo of using Eventbridge to respond to changes in your environment with AWS config In this guide, we’ll show you how to leverage Amazon EventBridge to respond dynamically to changes in your AWS environment. You'll learn how to trigger events based on EC2 instance state changes and generate custom events from your application code. When an EC2 instance changes state—whether it’s stopped, started, or restarted—a corresponding event is fired to invoke a subscribed Lambda function. We also illustrate how to generate a custom event using a simple Node.js API. Configuring an EventBridge Rule for EC2 Instance State Changes Begin by opening the Amazon EventBridge console and navigating to the Event buses section. Every AWS account has a default event bus. Although you can create custom event buses, this demo uses the default. Next, go to the Rules section. Here, you define the events of interest and specify the actions to take when those events occur. Click Create rule , provide a suitable name (e.g., ""InstanceStatusChange""), and optionally add a description. Ensure the rule is associated with the default event bus. There are two types of rules available: Event Pattern: The rule triggers when an incoming event matches a defined pattern. Schedule: The rule executes based on a cron-like schedule. For our scenario, select Event Pattern . Then specify the event source. In this demo, we focus on AWS service events for EC2. Defining the Event Pattern Select AWS services as the event source, choose EC2 as the service, and pick EC2 instance status, instance state change notification as the event type. By default, this pattern covers any state change: {
  ""source"": [""aws.ec2""],
  ""detail-type"": [""EC2 Instance State-change Notification""]
} If you need to target a specific state—such as when an instance is shutting down—you can refine the event pattern: {
  ""source"": [""aws.ec2""],
  ""detail-type"": [""EC2 Instance State-change Notification""],
  ""detail"": {
    ""state"": [""shutting-down""]
  }
} For our demo, we maintain the broader pattern that captures any state change. Tip Keep the event pattern broad during testing for ease of debugging. You can narrow it down once you’re confident the event rule works. Specifying the Target In the next step, define the target for the rule. For this demo, select a Lambda function (for example, one named ""test one""). Use the sample Lambda function code provided below to log event data and inspect details via CloudWatch logs: export const handler = async (event) => {
    // Log the event data for debugging purposes
    console.log(event);
    const response = {
        statusCode: 200,
        body: JSON.stringify('Hello from Lambda!'),
    };
    return response;
}; After reviewing your configuration and optionally adding tags, click Create rule . Your EventBridge rule is now active. Testing the EC2 Instance State Change To verify your setup, open the EC2 console and change the state of a running instance. For instance, select the instance, then choose Instance state > Stop instance . Allow a few seconds for the instance state to update. After the state change, EventBridge generates an event that triggers your Lambda function. Go to the Lambda console, navigate to the Monitor tab, and click the link to view CloudWatch logs. Look for an output similar to the following: 2023-10-12T03:47:58.870Z	5fdcc7b3-520b-42ce-b221-3f059a01a69e	INFO	{
    version: '0',
    id: 'a9ae6bf8-cbe8-cb37-ef59-e34c069f7268',
    'detail-type': 'EC2 Instance State-change Notification',
    source: 'aws.ec2',
    account: '123456789012',
    time: '2021-11-17T00:00:00Z',
    region: 'ca-central-1',
    resources: ['arn:aws:ec2:region:account-id:instance/instance-id'],
    detail: { state: 'stopped', instanceId: 'i-0123456789abcdef0' }
} This confirms that your event triggered properly and the Lambda function processed the event data. Creating and Testing a Custom Event Now, let’s demonstrate how to generate custom events from your application. We have set up a basic Node.js API using the AWS SDK to publish events to EventBridge. In this example, a POST request to the /signup endpoint triggers an event. Node.js API Setup Below is the code snippet for creating the API: require(""dotenv"").config();
const express = require(""express"");
const { EventBridgeClient, PutEventsCommand } = require(""@aws-sdk/client-eventbridge"");

const app = express();
app.use(express.json());

const client = new EventBridgeClient();

app.post(""/signup"", async (req, res) => {
    const { username } = req.body;

    const event = {
        Source: ""my-app"",
        DetailType: ""New User"",
        Detail: JSON.stringify({ user: username }),
    };

    try {
        const response = await client.send(new PutEventsCommand({ Entries: [event] }));
        console.log(response);
        res.status(201).json({ status: ""success"" });
    } catch (error) {
        console.error(error);
        res.status(500).json({ status: ""error"" });
    }
});

app.listen(3000, () => {
    console.log(""Server is running on port 3000"");
}); When you send a POST request with a JSON body (for example, { ""username"": ""user2"", ""password"": ""password123"" } ), the API sends an event to EventBridge with the source ""my-app"" and the detail type ""New User"" , along with the user details. Configuring the Custom Event Rule Open the EventBridge console and create a new rule (e.g., ""MyAPIEvent""). Use the following custom event pattern: {
    ""source"": [""my-app""],
    ""detail-type"": [""New User""]
} Set the target for the rule to another Lambda function (for example, ""test two""). Review the configuration and click Create rule . Manually Testing the Custom Event To test the custom event directly via the EventBridge console: Navigate to Event Bus and click Send event . Select the default bus, set the Source to ""my-app"" , and the Detail type to ""New User"" . In the Detail section, provide a JSON object such as: {
  ""username"": ""user1""
} Click Send event . After a few seconds, check the CloudWatch logs for the ""test two"" Lambda function. You should see log entries resembling: {
  ""version"": ""0"",
  ""id"": ""a9a06fb6-cb37-ef59-e34c069f7268"",
  ""detail-type"": ""New User"",
  ""source"": ""my-app"",
  ""account"": ""848169027373"",
  ""time"": ""2023-10-12T03:47:59Z"",
  ""region"": ""us-east-1"",
  ""resources"": [],
  ""detail"": {
    ""username"": ""user1""
  }
} This confirms that your custom event was processed successfully by EventBridge. Testing the API Integration Finally, start your Node.js API and send a POST request to the /signup endpoint containing the new user's information. For example, using a REST client, submit the following JSON: {
  ""username"": ""user2"",
  ""password"": ""password123""
} The API should respond with: {
  ""status"": ""success""
} After a few seconds, verify the CloudWatch logs for the ""test two"" Lambda function to confirm that it received the event with ""username"": ""user2"" . Final Check Ensure that each step has been tested thoroughly to confirm that both AWS service events and custom application events are processed properly by your Lambda functions. Conclusion By following these steps, you’ve successfully configured Amazon EventBridge to handle both AWS service events and custom events from your application. This event-driven approach enables you to integrate various AWS services and custom workflows seamlessly. Enjoy building and expanding your event-driven architectures! Happy coding! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Managed Apache Airflow,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Application-Integration/Managed-Apache-Airflow,"AWS Solutions Architect Associate Certification Services Application Integration Managed Apache Airflow In this article, we explore AWS Managed Workflows for Apache Airflow—a fully managed orchestration service that lets you programmatically author and schedule workflows using Apache Airflow without the hassle of managing underlying infrastructure. Amazon Managed Workflows for Apache Airflow (MWAA) takes care of provisioning, scaling, and managing the Apache Airflow instance. As your workflow execution demands increase, MWAA automatically allocates additional resources, simplifying the setup, configuration, deployment, and ongoing management of your workflows. This managed service not only reduces operational overhead but also ensures reliability with features such as automatic scaling, high availability, and fault tolerance. Note When you create an Apache Airflow instance in AWS, you are essentially receiving the same open source Apache Airflow—but managed in the cloud. Simply upload your directed acyclic graphs (DAGs) defining your data pipelines to an S3 bucket, and Apache Airflow orchestrates the extraction, transformation, and loading (ETL) of your data. Key Benefits and Features Amazon MWAA offers several advantages that allow you to focus on building robust data pipelines rather than managing infrastructure. Key features include: Managed Infrastructure: AWS provisions and maintains the Apache Airflow instance, eliminating the burden of server upkeep. Security: The environment is secured using Amazon VPCs and encrypted automatically with AWS KMS. Monitoring: System metrics and logs from Apache Airflow are sent directly to Amazon CloudWatch, facilitating efficient monitoring of task delays and workflow errors across multiple environments. AWS Integrations: MWAA integrates seamlessly with a variety of AWS services such as AWS Athena, Batch, DynamoDB, DataSync, EMR, Firehose, Glue, and Lambda, enabling you to build intricate workflows that harness the full ecosystem of AWS capabilities. For a comprehensive list of supported AWS service integrations within Apache Airflow, please refer to the official MWAA documentation . Use Cases Apache Airflow is well-suited for constructing and managing complex data pipelines. Common use cases include: Scheduling and executing workflows to process and prepare intricate datasets. Orchestrating multiple ETL processes that involve diverse technology stacks. Automating pipelines for data ingestion, transformation, and loading to power machine learning models and analytics platforms. Summary Amazon Managed Workflows for Apache Airflow significantly simplifies the deployment, management, and scaling of Apache Airflow workflows in the cloud. By leveraging AWS's robust, integrated infrastructure, teams can effectively automate and manage complex data pipelines without the overhead of traditional infrastructure management. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Glue,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Glue,"AWS Solutions Architect Associate Certification Services Data and ML Glue Welcome back, Solutions Architects! In this presentation by Michael Forrester, we explore AWS Glue—a powerful service designed to simplify data ingestion, extraction, transformation, and loading (ETL). Although Glue is part of AWS’s machine learning suite, its primary purpose is to seamlessly move and transform data from various source systems into target repositories like data catalogs or Amazon S3. Key Components of AWS Glue AWS Glue is built around three main components that work together to streamline your data workflows: Crawler The crawler automatically connects to data sources (such as S3 buckets or RDBMS systems) to scan for data. It then populates the Glue Data Catalog with table definitions and associated metadata. This centralized catalog maintains both raw data references and the critical structural metadata required for ETL operations. ETL Jobs with Apache Spark and PySpark AWS Glue supports ETL jobs that can be authored manually in Spark or PySpark, or you can use prebuilt scripts provided by Amazon. These jobs extract data from cataloged sources, apply transformations—including renaming fields, filtering records, joining datasets, or aggregating information—and load the transformed data into target destinations like S3, Redshift, Athena, or QuickSight. Visual Interface with Glue Studio Glue Studio provides a user-friendly Integrated Development Environment (IDE) for creating, testing, and monitoring Apache Spark jobs visually, eliminating the need for managing local environments or physical infrastructure. How AWS Glue Works The diagram below illustrates a typical data processing workflow using AWS Glue. It shows the flow from a data source through extraction, transformation, and loading stages into a data target, with the Data Catalog maintaining vital metadata: In practice, you start by defining a data source for the crawler, which then automatically discovers the schema and structure of your data. If the inferred schema doesn’t perfectly align with your expectations, you can easily adjust the configuration. Once the data is cataloged, you can trigger an ETL job by scheduling it on-demand, via a time-based trigger, or driven by specific events. Since AWS Glue is a serverless service, you only pay for the underlying resources used during the job execution—thus, eliminating the need to manage dedicated instances. The following flowchart further clarifies how AWS Glue operates. It shows data moving from an input S3 bucket through a crawler to the Glue Data Catalog, then through the ETL process, and finally into an output S3 bucket: Quick Tip AWS Glue’s serverless nature means you can focus on developing your ETL processes without worrying about the underlying infrastructure. Built-in Transformation Libraries and Glue Studio AWS Glue also includes built-in transformation libraries—simple, reusable functionalities for common data operations such as field renaming, record filtering, and data aggregation. This means you can quickly set up data cleaning and normalization routines without coding these functions from scratch. Moreover, the Glue Data Catalog acts as a persistent metadata repository, allowing other AWS services like Athena, EMR, or Redshift to utilize the stored data effectively. Glue Studio enhances this process by offering a visual interface for designing, executing, and monitoring Spark jobs. This simplifies development, debugging, and management of ETL tasks, making it easier to maintain robust data pipelines. The image below summarizes the key features of AWS Glue, including its serverless ETL capability, centralized Data Catalog, automatic schema discovery, visual job authoring, and built-in transformation libraries: Conclusion AWS Glue simplifies data transformation and migration across multiple platforms, supporting targets like S3, Redshift, QuickSight, and more. Its serverless model eliminates infrastructure management overhead while providing a centralized catalog that offers a consolidated view of your data assets. This makes AWS Glue an indispensable tool for modern data workflows. If you have any questions about AWS Glue, please join us on the KodeKloud Slack under AWS Courses. We look forward to sharing more insights in our next lesson. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Glue Databrew,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Glue-Databrew,"AWS Solutions Architect Associate Certification Services Data and ML Glue Databrew Welcome back, AWS Solutions Architects. In this article, presented by Michael Forrester, we explore the power of data transformation with Glue DataBrew—a service that markedly differs from Glue ETL. Glue DataBrew is a visual data preparation tool that enables you to clean and normalize data without writing a single line of code. Unlike Glue ETL, which allows you to apply Python or PySpark code for data transformations, DataBrew relies purely on a graphical user interface. How Glue DataBrew Works The workflow of Glue DataBrew is straightforward: Create a Project: Establish a workspace to interact, analyze, explore, and perform data preparation tasks. Select Datasets and/or Data Sources: Import data from various sources such as S3, Redshift, or other services—similar to the process in Glue ETL. Choose Recipes: Recipes are sets of visual data transformation steps, including operations like filtering rows and converting data types (e.g., string to number). All operations are applied from an intuitive menu without the need for coding. Run the Recipe: When executed, DataBrew applies all specified transformations to the complete dataset. The processed data is then stored in Amazon S3 for consumption by other services. Serverless Advantage One significant advantage of Glue DataBrew is its serverless nature. This means you do not need to manage, secure, or scale servers manually. Instead, operational aspects like monitoring are seamlessly handled via services such as CloudWatch. Data sources for Glue DataBrew include the Glue Catalog, various database services, and S3, all of which can be directly integrated into your workflows. Example Workflow Consider a workflow where data is sourced from S3 and ingested into Glue DataBrew. It leverages pre-built transformations, and the output is subsequently loaded into Athena. The processed data then becomes accessible to QuickSight for analysis by data and business analysts. Under the hood, Glue DataBrew utilizes AWS Glue to perform data transformations and supports machine learning workflows. For example, you can source data via DataBrew and export the processed output to services such as SageMaker, Rekognition, or Polly. Key Features of Glue DataBrew Feature Description Visual Data Preparation Clean and transform data through an intuitive graphical interface—no coding required. Data Profiling Automatically generate metadata statistics to identify outliers, anomalies, missing values, and inconsistencies. Scalability Automatically scales with your data preparation workload without manual intervention. Integration with AWS Data Stores Seamlessly integrates with services such as Aurora, Redshift, and RDS. Job Scheduling and Reusability Schedule data tasks based on triggers or time, and create reusable project templates. In Summary Glue DataBrew offers a streamlined, visual, and code-free solution for data transformation that harnesses the power of AWS Glue behind the scenes. By simplifying the data preparation process, DataBrew makes it accessible for users who prefer not to write code and accelerates the journey from raw data to actionable insights. Thank you for reading—see you in the next article. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,API Gateway Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Application-Integration/API-Gateway-Demo,"AWS Solutions Architect Associate Certification Services Application Integration API Gateway Demo In this guide, you'll learn how to set up an API using AWS API Gateway integrated with AWS Lambda functions. This demo covers configuring a REST API to route incoming HTTP requests to dedicated Lambda functions that handle back-end logic for various operations on a car inventory. Creating the REST API Start by searching for API Gateway in the AWS Management Console to access the API Gateway service page. Here, you can choose from HTTP APIs, WebSocket APIs, and REST APIs (public or private). For this demo, we will create a REST API. Click Build to create a new API rather than importing or cloning an existing one. Provide a name for your API (for example, ""cars"") and an optional description. Select Regional as the endpoint type and then click Create API . Defining a Resource Next, define a resource representing a specific path in your API. In this example, all requests target the /cars endpoint. This configuration helps route HTTP methods to their corresponding Lambda functions. Select your API and create a resource with the path /cars . For this demonstration, you can leave CORS settings unchecked. Click Create Resource to proceed. Configuring the GET Method Under the /cars resource, create a GET method that links to a preconfigured Lambda function. Open the AWS Lambda console in another browser tab and verify that you have a function (named ""GetCars"" in this demo) returning a fixed list of cars. The simplified Lambda function example is shown below: // Lambda function: GetCars
export const handler = async (event) => {
  const cars = [
    { model: ""accord"", make: ""honda"", color: ""blue"" },
    { model: ""camry"", make: ""toyota"", color: ""red"" },
    { model: ""civic"", make: ""honda"", color: ""black"" }
  ];
  const response = {
    statusCode: 200,
    body: JSON.stringify(cars),
  };
  return response;
}; Returning to API Gateway, create the GET method on the /cars resource by clicking Create Method . Choose GET as the HTTP method. Set the integration type to Lambda Function and enable Lambda Proxy Integration to pass the complete event to Lambda. Finally, select your ""GetCars"" Lambda function and create the method. After configuration, any GET request to /cars will trigger the ""GetCars"" Lambda function. Deploying the API Before testing, deploy your API. Click the Deploy API button, choose a stage (e.g., development ), and deploy it. Once deployed, you will receive an invoke URL for sending requests. To test the GET endpoint, copy the invoke URL and append /cars (since testing the root URL may result in a ""Missing Authentication Token"" error): Testing the Endpoint Ensure you append /cars to your invoke URL, as leaving it out may trigger an incorrect response due to missing endpoint context. For example, if you receive the following response: {""message"":""Missing Authentication Token""} It means the request was sent to the base URL instead of the complete URL with /cars . Creating the POST Method to Add Cars To add a new car entry, create a POST method under the /cars resource. Select the /cars resource (which currently has the GET method) and click Create Method , then choose POST . Configure it with Lambda proxy integration and select your ""CreateCars"" Lambda function. Since POST requests cannot be tested directly in a browser, use tools such as cURL or Postman. Remember to deploy the API after configuring the POST method. To test, configure your tool to make a POST request using the deployment URL appended with /cars . Include a JSON payload in the body, for example: {
  ""make"": ""toyota"",
  ""model"": ""camry"",
  ""color"": ""red""
} If you receive an error like: {""message"": ""Missing Authentication Token""} double-check that you are sending the request to the correct endpoint ( /cars ). Your ""CreateCars"" Lambda function should parse and process the request body. Here's a sample implementation: // Lambda function: CreateCars
export const handler = async (event) => {
  const response = {
    statusCode: 201,
    body: JSON.stringify({ newCar: JSON.parse(event.body) }),
  };
  return response;
}; In API Gateway, ensure that the complete event (including the request body) is passed to Lambda by checking the Integration Request settings, enabling proxy integration, and redeploying the API. After redeploying, send a POST request. You should receive a JSON response confirming the creation of the new car: Request payload: {
  ""make"": ""toyota"",
  ""model"": ""camry"",
  ""color"": ""red""
} Response: {
  ""newCar"": {
    ""make"": ""toyota"",
    ""model"": ""camry"",
    ""color"": ""red""
  }
} Implementing the PATCH Method for Updating Cars To update a car entry, add a PATCH method under a resource that includes a path parameter. First, under the /cars resource, create a sub-resource with the path /{id} where {id} serves as a placeholder for the car's unique identifier. Under the newly created /{id} resource, create a PATCH method. Select PATCH as the HTTP method and integrate it with your ""UpdateCars"" Lambda function using Lambda proxy integration. Your ""UpdateCars"" Lambda function should parse the request body and return a JSON response with the updated data. Below is an example implementation: // Lambda function: UpdateCars
export const handler = async (event) => {
  const body = JSON.parse(event.body);
  const response = {
    statusCode: 200,
    body: JSON.stringify({ updatedCar: body }),
  };
  return response;
}; For additional debugging, you might temporarily log or return the full event (including path parameters and headers) as follows: // Debug version of UpdateCars
export const handler = async (event) => {
  const body = JSON.parse(event.body);
  const response = {
    statusCode: 200,
    body: JSON.stringify(event),
  };
  return response;
}; After validating the event structure, revert to the streamlined version. Redeploy your API once changes are complete. Testing the PATCH Method To test the PATCH endpoint, replace {id} in your invoke URL (e.g., /cars/5 ) with an actual car ID and send a PATCH request with the updated JSON data. Here is the final version of your update handler: // Final version of UpdateCars
export const handler = async (event) => {
  const body = JSON.parse(event.body);
  const response = {
    statusCode: 200,
    body: JSON.stringify({ updatedCar: body }),
  };
  return response;
}; Setting Up the DELETE Method Following REST conventions, you can delete a car by specifying its ID. Under the /{id} resource (or directly under /cars if using query parameters), add a DELETE method and integrate it with the corresponding Lambda function responsible for deletion. After creating the DELETE method, you can test it by sending a DELETE request with the car's ID. API Gateway will handle the routing and the Lambda function will execute the deletion logic. If you later decide to remove the API, select the API (e.g., ""cars""), click the DELETE button, confirm the action, and the API will be deleted. Summary This demo has illustrated how to configure AWS API Gateway to route GET, POST, PATCH, and DELETE requests to various AWS Lambda functions. Each endpoint demonstrates how Lambda functions can simulate back-end operations—such as retrieving car listings, creating a new car entry, updating car details, or deleting a car—without the need for an actual database integration. For further reading and additional details, consider checking out: AWS API Gateway Documentation AWS Lambda Documentation Happy coding! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Amazon MQ,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Application-Integration/Amazon-MQ,"AWS Solutions Architect Associate Certification Services Application Integration Amazon MQ In this lesson, we explore Amazon MQ—a managed message broker service designed to simplify communication between disparate software systems. Message brokers enable systems built in different programming languages and hosted on varied platforms to exchange information reliably. Amazon MQ supports popular message brokers such as Apache ActiveMQ and RabbitMQ. By adopting Amazon MQ, you no longer need to manually provision and manage ActiveMQ or RabbitMQ on an Amazon Elastic Compute Cloud (EC2) instance. Instead, Amazon manages provisioning, maintenance, and monitoring, similar to how AWS RDS handles databases. Key Benefits of Amazon MQ Amazon MQ facilitates message queuing to promote a decoupled architecture, meaning that message producers and consumers operate independently without needing direct knowledge of each other. This decoupling enhances system scalability and flexibility. The service guarantees reliable message delivery by persisting messages to disk, ensuring that critical data is not lost even if consumers are temporarily unavailable or in the event of broker failures. Key features include: Decoupled architecture for improved scalability Reliable message delivery with disk persistence Support for multiple messaging protocols, including MQTT, STOMP, AMQP, and OpenWire Note When deciding between Amazon MQ and AWS Simple Queue Service (SQS), consider your application's requirements. SQS is ideal for new applications due to its simplicity, while Amazon MQ serves as a drop-in replacement for existing applications that already use RabbitMQ or ActiveMQ—eliminating the need for code changes. Administrative and Security Features Amazon MQ alleviates the burden of many administrative tasks such as hardware provisioning, broker setup, software upgrades, and failure detection and recovery. The service also incorporates robust security features: Encryption of messages at rest and in transit Secure connections established via SSL Option to restrict broker access to private endpoints within an Amazon VPC Additionally, Amazon MQ offers various broker instance types, providing different combinations of CPU, memory, and network performance to meet diverse application needs. Use Cases and Integration Amazon MQ is designed as a drop-in replacement for RabbitMQ or ActiveMQ and supports applications that require low latency for processing thousands of events. It is an excellent choice for both migrating applications to the cloud and integrating with a wide range of AWS services. Its robust configuration options make it adaptable to varied messaging needs. Another powerful integration is with AWS Lambda . By triggering Lambda functions based on queue messages, you can build serverless, event-driven architectures that respond instantly to new events. Note Leveraging Amazon MQ can streamline your messaging architecture, especially when transitioning from on-premises solutions or when your application demands high throughput and reliable message delivery. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,SNS,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Application-Integration/SNS,"AWS Solutions Architect Associate Certification Services Application Integration SNS In this lesson, we explore AWS SNS (Amazon Simple Notification Service) and its role in enabling real-time communication for applications built with distributed architectures. When working in environments with multiple services, backend endpoints, and clients that need to interact, ensuring efficient real-time communication may be challenging. SNS simplifies this process by acting as a managed messaging service that delivers notifications from publishers to subscribers reliably and at scale. Key Components of SNS Understanding the components of SNS helps in designing a solution that is scalable, resilient, and easy to maintain. Topics A topic is a communication channel where publishers send messages and subscribers receive them. Topics serve as the central hub for transmitting notifications across your application. For example, you could create a ""New User"" topic where different services subscribe to receive user registration events. When a new user registers, details like the username and email are published to the topic, triggering processes such as sending a confirmation email, updating logs, or initiating other workflows. Subscribers Subscribers receive the messages published to topics. They can be configured as: SQS queues Lambda functions HTTP endpoints Email endpoints Once a subscriber is configured, every message published to the related topic is delivered immediately, allowing each component to process the information accordingly. Publishers Publishers are the entities responsible for sending messages to a topic. They typically utilize various data formats (e.g., JSON or plain text) and can be any application using the AWS SDK, running on EC2, Lambda, or other environments. For instance, during user registration, the application handling the HTTP POST request publishes the new user's details to the ""New User"" topic. Messages Messages are the units of data transmitted via SNS that include notifications, alerts, or updates. Their structure can be adjusted based on the subscribers' needs, ensuring each component receives information in the appropriate format. Types of Topics SNS supports two primary types of topics: Standard and FIFO (First-In-First-Out). Standard Topics Standard topics deliver messages on a best-effort basis. This means that while messages are not guaranteed to be received in the exact order sent, the throughput is nearly unlimited. Standard topics are ideal for high-volume use cases where perfect ordering is not critical and occasional duplicate messages are tolerable. FIFO Topics FIFO topics ensure strict message ordering and eliminate duplicate message delivery. These topics are essential in scenarios where the sequence of operations is critical, such as transaction logging, stock monitoring, flight tracking, inventory management, or price updates. However, FIFO topics support up to 300 messages per second or 10 megabytes per second per topic, which is lower compared to standard topics. Additional Considerations When integrating SNS into your application, keep these best practices in mind: You can publish one or two messages per API request. If many events occur in a short time span, consider batching messages together (for example, waiting until 10 users have registered) to optimize costs. Each message supports up to 256 KB of data. For larger payloads, utilize the extended client library to store the payload in an S3 bucket and send a reference to the data via SNS. Tip Consider batching events when possible to reduce operational costs and improve system performance. Common Use Cases for SNS SNS is versatile and applies to a variety of use cases, including: Secure Notifications: Leverage AWS KMS to encrypt messages, ensuring secure communication. Use resource policies and tags in conjunction with AWS PrivateLink for fine-grained access control. Event Fan-Out: Seamlessly distribute events from over 60 AWS services to multiple endpoints, allowing coordinated automated responses across your system. SMS Notifications: Send text messages globally, reaching customers in over 240 countries for time-sensitive alerts. Additional Resource For more in-depth details on AWS SNS, refer to the AWS SNS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,API Gateway,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Application-Integration/API-Gateway,"AWS Solutions Architect Associate Certification Services Application Integration API Gateway In this article, we delve into AWS API Gateway, explaining its purpose and the challenges it overcomes when building and managing APIs. API Gateway streamlines the integration of various backend services while providing robust tools for API management, security, transformation, and monitoring. Streamlining Backend Integration A common challenge in modern application architectures is unifying multiple backend services, data sources, and functions under a single API interface. API Gateway simplifies this integration by connecting with services such as AWS Lambda, EC2 instances, ECS, and other HTTP endpoints. This enables you to aggregate data from disparate systems into a centralized API and reduce development complexity. Another key benefit is the efficient management of APIs at scale. Deploying, versioning, and maintaining APIs manually can be error-prone. API Gateway addresses these issues by centralizing API creation, lifecycle management, and deployment processes. Its built-in tools support seamless versioning and deployment, ensuring that your API evolves with your business needs. Moreover, API Gateway helps transform requests and responses between differing data formats. It validates and maps data from backend services to client-required formats, ensuring consistency and reliability across end-to-end communication. Note API Gateway is particularly beneficial for organizations looking to streamline complex integrations while reducing operational overhead. Ensuring Robust Security and Traffic Management Security is paramount for API development. Protecting your endpoints with proper authentication and authorization mechanisms is essential. API Gateway integrates with AWS IAM, OAuth, and Amazon Cognito, allowing you to enforce strict access controls and maintain a secure API environment. In addition, API Gateway is equipped to handle excess traffic and mitigate abuse. Instead of writing custom logic, you can configure rate limits and throttling policies that safeguard your backend from traffic surges while ensuring optimal system performance. For insights into your API’s performance, API Gateway uses comprehensive monitoring, logging, and analytics features. These capabilities allow you to track usage, diagnose issues, and measure latency, facilitating rapid optimization and troubleshooting. Documentation and Developer Experience High-quality documentation is crucial for API adoption and developer success. API Gateway supports auto-generated documentation and the creation of developer portals that include testing capabilities. This helps minimize configuration drift and ensures that the API behavior is always in sync with its documentation. API Types Supported by API Gateway API Gateway supports a variety of API models designed to meet different application needs: HTTP API HTTP APIs enable you to route requests to AWS Lambda functions or any routable HTTP endpoint. For example, when a client calls an HTTP API, API Gateway forwards the request to a Lambda function and returns its corresponding response. WebSocket API WebSocket APIs support bi-directional, real-time communication, making them ideal for applications such as chat services, collaboration platforms, and real-time financial trading systems. With WebSocket APIs, both clients and servers can send messages independently for instant communication and notifications. REST API REST APIs consist of resources and methods integrated with backend HTTP endpoints, Lambda functions, or other AWS services. They operate on a synchronous request-response model, making them suitable for applications requiring immediate responses. Key Features and Benefits Unified API Creation: Whether you need RESTful, WebSocket, or HTTP APIs, API Gateway simplifies their creation and management. Private Resource Routing: Enable secure routing to resources within your VPC. API Gateway can expose private load balancers (application or network) and IP-based services registered in AWS CloudMap. Resiliency: Protect backend services by setting throttling rules and caching configurations (for REST APIs) with customizable keys and time-to-live settings during traffic surges. Monitoring and Analytics: With integration into Amazon CloudWatch, you get detailed dashboards and metrics (e.g., API call counts, latency, error rates) that can trigger custom alarms for proactive monitoring. Authorization and Access Control: Use Signature Version 4 for secure authentication, along with AWS IAM, Lambda-based authorizers for JWT or SAML tokens, and API key management for controlled third-party access. SDK Generation: Automatically generate client SDKs for platforms such as Java, JavaScript, Android, Objective-C, Swift, and Ruby, complete with integrated API key handling and AWS request signing. Lifecycle Management: Manage multiple versions and stages (e.g., alpha, beta, production) concurrently, with the ability to associate custom domain names to streamline transitions between environments. Integrations API Gateway seamlessly integrates with a wide range of AWS services, including: AWS Lambda: Invoke Lambda functions to process API requests. Elastic Beanstalk and EC2: Integrate APIs with your application hosted on EC2 instances handling backend logic. Amazon S3: Serve static website assets like HTML, CSS, and JavaScript. DynamoDB: Facilitate efficient data storage and retrieval. AWS Step Functions: Orchestrate workflows and complex business processes. Kinesis Data Streams: Process and analyze real-time data streams. RDS: Connect with relational databases to execute SQL queries. Cognito: Manage user authentication and authorization. Secrets Manager: Securely store and manage sensitive credentials and API keys. By leveraging these integrations, API Gateway offers a flexible and robust platform for building, deploying, and securing modern APIs on AWS. Further Reading For additional insights on API design and AWS services, visit AWS Documentation and API Gateway Developer Guide . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,AppFlow,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Application-Integration/AppFlow,"AWS Solutions Architect Associate Certification Services Application Integration AppFlow In this lesson, we explore Amazon AppFlow, a fully managed integration service that enables organizations to efficiently share data across departments such as finance, HR, sales, and engineering. In many organizations, data resides in distinct applications and services, resulting in data silos that hinder effective data aggregation and utilization. Integrating data from SaaS applications with internal systems—and even with AWS services—can be challenging. Manual data transfer and transformation are not only time-consuming but also error-prone, which can lead to inconsistencies and poor data quality. Amazon AppFlow overcomes these challenges by securely transferring data between SaaS applications (such as Salesforce) and AWS services (like Amazon S3 and Amazon Redshift). Acting as a bridge between various tools and platforms, AppFlow automates data transformation, mapping, and filtering, ensuring seamless connectivity between disparate systems. Components of Amazon AppFlow The key components of AppFlow include: Connectors Connectors establish secure connections between source and destination applications. They securely store configuration parameters, credentials, API keys, and other sensitive information required for communication. Flows A flow defines the data transfer process between a source and a destination. You can map source fields to destination fields and apply filtering criteria to extract only the relevant records. Triggers Triggers specify when a flow should run. AppFlow supports three trigger types: Run on demand: Initiated manually by the user. Run on an event: Automatically triggered by an event from a SaaS application. Run on a schedule: Configured to execute on a recurring basis (for example, monthly, weekly, or yearly). When a flow is executed, AppFlow checks for available data in the source, processes it based on the defined configuration (including field mappings and filtering), and then transfers the processed data to the destination. Features and Benefits Amazon AppFlow offers numerous advantages, ensuring a robust and efficient data integration solution: Speed and Automation: AppFlow enables rapid integration within minutes—eliminating the need for custom code development. With built-in support for data pagination, error logging, network management, and auto-retries, it minimizes manual intervention and accelerates deployment. Data Quality Management: Data integrity is maintained through built-in mechanisms for data masking, mapping, merging, filtering, and validation, ensuring high-quality data flow between systems. Security and Privacy: All data transferred via AppFlow is encrypted both in transit and at rest. You can choose between AWS-managed keys or bring your own encryption keys. Additionally, AppFlow enforces fine-grained permissions using existing identity and access management policies, and when integrated with AWS PrivateLink, it ensures data security by avoiding public internet exposure. Scalability: Designed to automatically scale, AppFlow handles large volumes of data effortlessly. It can process millions of records within a single flow while AWS manages the underlying infrastructure seamlessly. Note For more information on AWS security best practices, refer to the AWS Security Documentation . AWS Service Integration Amazon AppFlow integrates with a wide range of AWS services, including: Amazon S3 Amazon Redshift Amazon RDS Amazon DynamoDB Amazon EventBridge AWS Glue DataBrew AWS Lambda AWS KMS This extensive integration allows AppFlow to serve as a central hub for data transfer and transformation, linking key AWS services with various third-party applications. Real-World Use Cases Amazon AppFlow is a powerful solution for a variety of data integration scenarios: Salesforce to Amazon Redshift: Configure a flow that triggers on new Salesforce records, processes the data—for example, calculating sales potential—and transfers the updated records to an Amazon Redshift table. Analyzing Slack Conversations: Set up a scheduled flow to extract conversation data from a Slack channel and transfer it to destinations like Amazon Redshift, Snowflake, or Amazon S3 for in-depth analysis. Zendesk Ticket Integration: Create a manually triggered flow to transfer Zendesk ticket data, filtered by common case numbers, to storage and analysis destinations such as Amazon Redshift, Snowflake, or Amazon S3. Aggregating Data Across Multiple Applications: Develop a scheduled flow that aggregates data from multiple sources such as Salesforce, Marketo, ServiceNow, and Zendesk, enabling the transfer of up to 100 gigabytes per flow to Amazon S3 with minimal latency. Note For additional integration patterns and best practices, check out the Amazon AppFlow Documentation . In summary, Amazon AppFlow streamlines and automates the integration and exchange of data between diverse applications and AWS services. It enhances data quality, security, and scalability while reducing the need for manual intervention in data transformation processes. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Eventbridge,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Application-Integration/Eventbridge,"AWS Solutions Architect Associate Certification Services Application Integration Eventbridge In this lesson, we explore AWS EventBridge, a fully managed, serverless event bus that simplifies building distributed, event-driven architectures. We begin by discussing the limitations of tightly coupled applications and then demonstrate how distributed systems and EventBridge can solve these challenges. Understanding Tightly Coupled vs. Distributed Applications Tightly coupled applications require extensive coordination between teams. When one service changes, every interdependent component might also need to be updated. This interconnectedness significantly increases operational complexity as the application scales. In contrast, distributed applications consist of independent components that scale separately. Each team manages only their specific service, thereby reducing cross-dependencies. An event-driven architecture further decouples these components, allowing them to interact solely through events. However, managing an event system brings its own challenges such as ensuring event delivery during server crashes, handling high event volumes, and maintaining overall resilience. This is where AWS EventBridge becomes an essential service. By providing a scalable and fully managed event bus, EventBridge makes it easier to integrate AWS services, third-party applications, and your custom applications—all without building an on-premises event infrastructure. Key Insight AWS EventBridge enables loosely coupled communication between services. This reduces complexity, improves scalability, and allows your teams to work more efficiently. Key Benefits of AWS EventBridge AWS EventBridge supports loosely coupled, event-driven architectures, offering benefits such as: Decoupling of Components: Services communicate through events, reducing direct dependencies. Scalability: Components scale based purely on event loads. Real-Time Processing: Efficient handling of high volumes of events. Advanced Routing & Filtering: Define rules to trigger specific actions based on event properties. For example, you can configure a rule that instructs EventBridge to trigger two Lambda functions and one ECS task when an event with designated properties is received—while ignoring irrelevant events. ![The image illustrates the need for AWS EventBridge, highlighting features like decoupling and scalability, event processing at scale, and event routing and filtering, with colorful cubes and event icons.](/images/AWS-Solutions- Architect-Associate-Certification-Eventbridge/aws-eventbridge-features-illustration.jpg) Components of AWS EventBridge AWS EventBridge is composed of three primary components: Event Bus, Pipes, and Scheduler. Below is an overview of each component. 1. Event Bus An Event Bus serves as a centralized hub where events from various sources—such as AWS services, custom applications, or SaaS applications—are collected and routed to the appropriate targets based on defined rules. ![The image shows the components of AWS EventBridge: Event Bus, Pipes, and Scheduler, each represented with an icon.](/images/AWS-Solutions- Architect-Associate-Certification-Eventbridge/aws-eventbridge-components-icons.jpg) Every AWS account includes a default event bus, allowing you to start sending events immediately. Additionally, you can create custom event buses to organize events by application or functionality. Once events are on the bus, rules determine which services (e.g., Lambda functions, SNS topics, API destinations) are triggered. ![The image is a diagram illustrating AWS EventBridge Buses, showing the flow from event sources like AWS services and custom apps to event buses, schema registry, and targets such as AWS Lambda and Amazon SNS. It includes components like schema discovery, rules, and API destinations.](/images/AWS-Solutions- Architect-Associate-Certification-Eventbridge/aws-eventbridge-buses-diagram.jpg) 2. Pipes EventBridge Pipes offer a streamlined integration mechanism by directly routing events from a source to a target. Pipes provide additional functionalities such as: Filtering: Forward only the events that meet defined criteria. Transformation: Convert event data into the required format before reaching the target. Enrichment: Enhance event data by adding supplementary information (e.g., using a Lambda function to retrieve additional details based on a transaction ID). This targeted approach reduces overhead by processing only essential events. ![The image is a diagram illustrating AWS EventBridge Pipes, showing how events are pulled from various AWS services, filtered, enriched, and then sent to target services.](/images/AWS-Solutions- Architect-Associate-Certification-Eventbridge/aws-eventbridge-pipes-diagram.jpg) 3. Scheduler The EventBridge Scheduler is a serverless service that allows you to effectively schedule tasks and events. With configurable scheduling patterns, delivery windows, and retry policies, the Scheduler ensures that critical tasks are executed exactly when needed. ![The image is a diagram explaining the AWS EventBridge Scheduler process, including steps to create a schedule, set a schedule pattern, select a target, and set a payload.](/images/AWS-Solutions- Architect-Associate-Certification-Eventbridge/aws-eventbridge-scheduler-diagram.jpg) Advanced Features of AWS EventBridge AWS EventBridge offers several advanced features designed to further enhance your event-driven architecture: API Integrations EventBridge easily connects to various API endpoints, whether hosted on-premises or provided by third-party SaaS providers. This low-code integration option helps manage throughput and authentication, ensuring secure communication. Event Replay Event replay allows you to reprocess historical events by sending them back onto an event bus or a specific rule. This feature is invaluable for debugging, as it lets you analyze past events in real time to pinpoint issues in your architecture. Schema Registry To maintain consistency in event structures across teams, EventBridge provides a schema registry. This registry stores event schema definitions so that developers can easily search, understand, and adhere to agreed-upon event formats, ensuring smooth interoperability. ![The image lists features of AWS EventBridge, including Low Code Integrations, Event Replay, and Schema Registry, each represented with an icon.](/images/AWS-Solutions- Architect-Associate-Certification-Eventbridge/aws-eventbridge-features-icons.jpg) Reliable Event Delivery EventBridge ensures at-least-once event delivery with automatic retries incorporating exponential backoff for up to 24 hours. Events are replicated across multiple availability zones, guaranteeing high availability (with a 99.99% SLA) and continuous application operation. Reliability Assurance AWS EventBridge's robust event delivery mechanism minimizes data loss and ensures that your applications remain responsive and resilient, even under heavy load. Integrations with AWS Services AWS EventBridge integrates seamlessly with over 200 built-in event sources and targets. This extensive integration includes popular AWS services such as Lambda functions, SQS, SNS, Step Functions, Kinesis Data Streams, and Kinesis Data Firehose. AWS Service Use Case Example Command/Reference AWS Lambda Serverless compute for event handling AWS Lambda Documentation Amazon SQS Message queuing between services Amazon SQS Documentation Amazon SNS Pub/sub messaging between distributed systems Amazon SNS Documentation AWS Step Functions Orchestrating workflows AWS Step Functions Amazon Kinesis Streams Real-time data streaming Amazon Kinesis For a complete list of supported integrations and detailed usage scenarios, refer to the AWS EventBridge documentation . ![The image shows a diagram of AWS EventBridge integration with various AWS services, including AWS Lambda, Amazon SQS, Amazon SNS, AWS Step Functions, Amazon Kinesis Data Streams, and Amazon Kinesis Data Firehose.](/images/AWS-Solutions- Architect-Associate-Certification-Eventbridge/aws-eventbridge-integration-diagram.jpg) Watch Video Watch video content"
AWS Solutions Architect Associate Certification,SQS,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Application-Integration/SQS,"AWS Solutions Architect Associate Certification Services Application Integration SQS In this article, we explore how AWS Simple Queue Service (SQS) helps build resilient and scalable applications by decoupling components in distributed systems. Why Use SQS? Imagine an e-commerce application consisting of multiple services such as cart management, payment processing, invoicing, inventory management, labeling, dispatch, and tracking. In a tightly coupled, sequential processing model, the placement of an order requires each service to wait for the previous one to finish, leading to slow performance, backlogs, and increased operational costs. To address these challenges, services can be decoupled using queues. Instead of directly invoking the payment service, the cart service sends a message to an SQS queue. The payment service later retrieves and processes the message at its own pace. This decoupling enhances responsiveness and prevents delays in one service from impacting the entire system. Advantages of Using SQS SQS offers several key benefits: Message Decoupling: Producers send messages to a queue without needing to know the details of the consumers. Load Leveling: SQS buffers messages during traffic bursts, preventing service overload during peak demand. Asynchronous Processing: Consumers process messages as they become available, ensuring overall system responsiveness. Scalable Serverless Architecture: SQS integrates seamlessly with serverless platforms, enabling efficient event-driven workflows. SQS Components Understanding the core components of SQS is essential for leveraging its full potential: Queue: A buffer where producers send messages and from which consumers retrieve them. Message: The unit of communication in SQS, with a size limit of up to 256 kilobytes. Producers: Applications or AWS services that send messages to the SQS queue, such as an S3 event triggering a message upon a file upload. Consumers: Services that retrieve and process messages from the queue. Message Attributes: Metadata provided as name-value pairs that add context to each message. Dead Letter Queue: An optional queue where messages are sent after a specified number of processing failures, enabling isolation and further analysis of problematic messages. Visibility Timeout: The period during which a retrieved message remains hidden from other consumers to ensure it is processed by only one consumer at a time. Message Locking: Prevents multiple consumers from processing the same message simultaneously; should processing fail, the lock expires and the message becomes available again. Types of Queues SQS supports two types of queues: Standard and FIFO (First-In-First-Out). Each type is designed for different application requirements. Standard Queues Best-Effort Ordering: Ensures that messages are delivered in a best-effort order, though the exact sequence may not be preserved. At-Least-Once Delivery: Guarantees that every message is delivered at least once, with occasional duplicates. High Throughput: Supports nearly unlimited transactions per second, ideal for applications with high throughput needs. FIFO Queues Exactly-Once Processing: Each message is processed only once and remains in the queue until successfully processed and explicitly removed. Preserved Order: Delivers messages in the exact order they were sent. Throughput Limits: Supports up to 3,000 messages per second with batching (300 messages per second without batching) by default; high throughput mode can raise these limits. SQS Features and Benefits SQS comes packed with advanced features: Message Retention and Re-delivery: Configurable retention periods ensure that messages not processed successfully can be re-delivered according to a retry policy. Message Prioritization: Use message attributes or multiple queues to prioritize critical messages. Dead Letter Queue: Automatically routes messages that fail processing to a separate queue for additional evaluation. Integration Highlights AWS SQS seamlessly integrates with several AWS services including Lambda, EC2, S3, and AWS Step Functions, enabling powerful event-driven architectures. Integrations and Use Cases SQS integrates with a wide range of AWS services. For example, a Lambda function can send and process messages, while S3 events can trigger message creation for further processing. By incorporating SQS, organizations can: Improve application reliability by decoupling frontend interactions from lengthy backend processes. Decouple microservices in distributed architectures, ensuring immediate user acknowledgment while background processes handle the heavy lifting. Scale processing capacity dynamically with Auto Scaling groups, adjusting the number of consumers based on real-time workload. Ensure message order and deduplication in systems where transaction order is critical. Conclusion AWS Simple Queue Service (SQS) is a vital tool for enhancing the resilience, scalability, and decoupling of modern applications. Its robust integration with AWS services makes it indispensable for designing distributed and microservices-based architectures. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Simple Workflow Service,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Application-Integration/Simple-Workflow-Service,"AWS Solutions Architect Associate Certification Services Application Integration Simple Workflow Service In this article, we explore AWS Simple Workflow Service (SWF), a fully managed state tracker and task coordinator designed to help developers build, run, and scale background jobs with both parallel and sequential steps. SWF efficiently orchestrates distributed tasks by managing the flow of work among multiple services and components, ensuring each task is assigned to the appropriate worker based on defined policies and priorities. Based on its role as a task coordinator and its ability to construct flexible workflows, SWF shares similarities with AWS Step Functions . While both services offer workflow orchestration, AWS generally recommends using AWS Step Functions for most scenarios. However, there are cases where SWF provides distinct advantages. Why Choose SWF? One key scenario for preferring SWF over AWS Step Functions is when your application requires external signals to intervene in the process. For example, if you need to integrate external inputs or trigger child processes from a parent process before returning a result, SWF is a more flexible option. It supports common programming languages—such as Python, JavaScript, Java, or C#—giving you the freedom to implement custom orchestration logic. From a solutions architecture perspective, both services serve similar functions. A significant benefit of using SWF is its ability to abstract the complexities of state management, task dispatch, and flow control. This abstraction allows developers to revise the application logic without worrying about workflow intricacies. By replacing custom-coded workflow solutions or third-party process automation tools, SWF simplifies development while enhancing scalability, resilience, and availability. Moreover, SWF provides the flexibility to implement workflow logic in the language of your choice, making it suitable for a variety of use cases such as: Use Case Example Backend Media Processing Video processing pipelines Complex Web Backend Operations Multi-step task management in web applications Business Process Workflows Automating tasks and analytics pipelines Key Takeaway AWS Simple Workflow Service offers a programmable approach to workflow orchestration, abstracting the complexities of state management and task coordination. Whether you need to manage media processing tasks, build sophisticated web backends, or streamline business processes, SWF is a robust solution tailored to handle complex workflows. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Kinesis,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Kinesis,"AWS Solutions Architect Associate Certification Services Data and ML Kinesis Welcome, Future Solutions Architects! I'm Michael Forrester, and in this article we'll explore the power of data ingestion using Amazon Kinesis, with a special focus on its machine learning applications. Amazon Kinesis is a real-time streaming service—think of it as a flowing river of data ready to be consumed downstream. It provides multiple products under its umbrella, each designed for specific data ingestion and processing needs. Amazon Kinesis Product Overview The core products of Amazon Kinesis include: Product Use Case Latency Target Services Data Streams Custom streaming data ingestion for applications like website behavior tracking. Milliseconds EC2, containerized apps, Lambda, EMR, Data Analytics Data Firehose Automatically ingesting data and performing ETL before streaming to data lakes or warehouses. ~60 seconds (batching) S3, Redshift, OpenSearch Service, Splunk Video Streams Streaming and processing video plus time-encoded data from devices like cameras and sensors. Varies (designed for video) Lambda, SageMaker, third-party applications Data Streams Data Streams is tailored for high-throughput streaming data. It enables the building of custom data inputs using the Kinesis library, capturing terabytes of data per hour with millisecond-level latency. Common use cases include website behavior metrics and video footage analysis. Once ingested, the data can be distributed to various AWS services such as EC2, containerized applications, Lambda, Data Analytics, or even EMR for further batch processing. Data Firehose Unlike Data Streams, Data Firehose automatically ingests data and performs ETL (Extract, Transform, and Load) operations before sending it to target destinations. It is ideal for streaming data directly into data lakes, warehouses, and analytical services. The service supports near real-time processing with a minimum latency of around 60 seconds, allowing it to batch, compress, and encrypt data prior to loading into destinations such as Amazon S3, Redshift, OpenSearch Service, and Splunk. Video Streams Video Streams is designed specifically for streaming video along with time-encoded data from diverse devices, including surveillance cameras, mobile devices, and other sensors. This service is well-suited for applications that process fragmented time-encoded data—whether it's video, audio, or sensor outputs. Consumers, such as AWS Lambda, SageMaker, or third-party applications, can process this data to perform tasks like sentiment analysis and machine learning predictions. Note For both Data Streams and Video Streams, a producer library is used to stream data into Kinesis while consumer libraries allow you to pull data for further processing. These libraries contribute to the scalability and reliability of your applications by partitioning data into shards. Each shard acts as a virtual container handling a defined volume of data, and you can adjust your stream’s capacity by adding or combining shards. Data records across these services are replicated across multiple Availability Zones, ensuring high durability. Data Streams and Video Streams offer a high availability rate (four nines), whereas Data Firehose provides three nines. Integration with AWS Services Amazon Kinesis seamlessly integrates with AWS, making it easier to build custom applications using battle-tested libraries refined over the past decade. Its real-time and near real-time processing capabilities make it an excellent fit for a range of applications, from processing social media streams and financial transactions to managing online gaming activities. Summary In summary, Amazon Kinesis is a powerful solution for real-time data streaming and processing. Whether you are using Data Streams, Data Firehose, or Video Streams, Kinesis provides flexible, scalable, and resilient ingestion that is integral to modern data architectures. If you have any questions or need further clarification, please feel free to reach out. Thank you for reading! Additional Resources Amazon Kinesis Documentation AWS Machine Learning Services AWS Solutions Architect Certification Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Simple Email Service,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Application-Integration/Simple-Email-Service,"AWS Solutions Architect Associate Certification Services Application Integration Simple Email Service In this lesson, we explore AWS Simple Email Service (SES), a cloud-based email solution designed to meet the needs of businesses and developers. AWS SES leverages AWS's highly available infrastructure to ensure reliable email delivery while minimizing common issues such as bounced emails and delivery failures. Its scalability makes it a perfect choice for businesses of any size. Key Components of AWS SES Below are the main components that make up the SES solution: Verified Identities A verified identity can be a domain, subdomain, or email address used to send emails through SES. Before sending emails on behalf of a particular domain, you must verify your ownership. For instance, to send emails as [email protected] , you must confirm that you own the domain kodekloud.com. This verification protects against spoofing and ensures the legitimacy of your emails. Configuration Sets Configuration sets help organize, track, and manage different segments of your email sending operations. They allow you to specify which sending events to record and where to send these events, providing valuable insights into your email campaigns. Dedicated IPs SES offers the option to use dedicated IP addresses which are exclusively reserved for your account. This ensures full control over your sender reputation, as your dedicated IP directly reflects the quality and volume of your email programs. Receipt Rules and IP Filters SES supports industry-standard email authentication protocols such as DomainKeys Identified Mail (DKIM), Sender Policy Framework (SPF), and Domain-based Message Authentication, Reporting, and Conformance (DMARC). With receipt rules and IP filters, you can determine how SES handles emails received on your behalf, enhancing your email security and deliverability. Note The Reputation Dashboard of SES is an essential feature that provides real-time metrics such as bounce rates, complaint rates, and other deliverability indicators. Additionally, SES features a Reputation Dashboard that provides a high-level overview of your email delivery performance by tracking key metrics and alerting you when deliverability-impacting events occur—such as spam trap hits or references to blocked domains. The Reputation Dashboard also provides detailed insights into your email usage, including data on the total number of emails sent and your current sending quota, ensuring you can effectively monitor your sender health. Additional Features of AWS SES AWS SES comes with several other powerful features that enhance its flexibility and usability: Email Templates: Create and send personalized messages to your customers by using customizable email templates. Email Receiving Capabilities: Manage incoming emails with rules that allow you to accept or reject messages based on criteria such as IP addresses and domains. Mailbox Simulator: Simulate various scenarios (e.g., email bounces or complaints) without risking your sender reputation, enabling safer testing environments. Suppression List Management: Control email delivery by configuring suppression lists at either the account level or within individual configuration sets. This ensures that specific email addresses do not receive emails from your account. Seamless Integration with AWS Services AWS SES integrates effortlessly with other AWS services to provide a comprehensive email ecosystem. For example, you can configure SES to trigger AWS Lambda functions to process incoming emails in real time. Additionally, you can store incoming emails in Amazon S3 and monitor events like bounces by publishing notifications to Amazon SNS. Primary Use Cases for AWS SES There are two main use cases for deploying AWS SES: Automated Transactional Messages: Automatically send notifications such as order confirmations or shipping alerts immediately after a transaction occurs. Bulk Email Communication: Efficiently manage and send newsletters or mass communications to large groups and communities. Conclusion This lesson provided an in-depth overview of AWS SES, covering its key components, robust features, seamless integrations, and practical use cases. By leveraging AWS SES, you can implement reliable, scalable, and secure email communications within your applications on the AWS platform. For more details on AWS SES and other AWS services, consider visiting the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,SNSSQS Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Application-Integration/SNSSQS-Demo,"AWS Solutions Architect Associate Certification Services Application Integration SNSSQS Demo In this lesson, you will learn how to integrate Amazon SNS and Amazon SQS to build a video processing application. The goal is to trigger workflows when a user uploads a video file (e.g., MP4) to an S3 bucket. This upload starts an SNS notification that fans out to two SQS queues. One queue invokes a Lambda function for video conversion (e.g., converting the video into HLS format), while the other triggers a Lambda function to generate video thumbnails. Below is an overview of the architecture along with detailed configuration steps. Architecture Overview S3 Bucket for Raw Videos: When a user uploads a video, the file is stored in an S3 bucket in its original format. SNS Topic Notification: The S3 bucket invokes an SNS notification (e.g., topic name video-uploaded ) each time a video is uploaded. Fan-out to SQS Queues: The SNS topic sends out notifications to two SQS queues: Video Processing Queue: A subscribed Lambda function retrieves the file name from the message, downloads the video from S3, processes it (e.g., converts the video), and then uploads the converted video to a designated S3 bucket. Thumbnail Processing Queue: A second Lambda function automatically creates video thumbnails and stores them in a specified thumbnails S3 bucket. Below is the architecture diagram illustrating the entire flow: Step 1: Configuring the SNS Topic Open the SNS service console and navigate to ""Topics."" Click on ""Create topic"". Enter the topic name (for example, video-uploaded ) and select the Standard type (message ordering is not required for this application). Optionally, set a display name and retain default settings for encryption and access policies. Click ""Create topic"" to finalize the configuration. Step 2: Creating SQS Queues Video Processing Queue Go to the SQS console and click ""Create queue."" Select the Standard queue type and name the queue video-processing . Accept the default settings for visibility timeout, delivery delay, and message retention. Under the access policy, leave it at its default setting (only the queue owner can send/receive messages). After queue creation, subscribe the queue to the SNS topic. Click ""Subscribe to Amazon SNS topic,"" select the video-uploaded topic, and hit ""Save."" Thumbnail Processing Queue Create another Standard queue and name it thumbnail-processing . Use the default settings and subscribe this queue to the video-uploaded SNS topic. At this point, any message published to the SNS topic is delivered to both queues. Each queue holds the incoming messages until the respective Lambda functions process them. Step 3: Testing SNS and SQS Integration Navigate to the SNS console and select the video-uploaded topic. Click ""Publish message"" and enter the message body. The message can be structured in plain text or JSON. For example, a JSON message might look like this: {
  ""bucket"": ""raw-videos-kodekloud"",
  ""key"": ""b415c94e-de85-4f6a-949c-2eb2e293bf30""
} After publishing the message, refresh the SQS console to verify that the message appears in both queues. If no consumer (Lambda or EC2) is configured, the messages remain in the queues. Publish another message with different details to see the message count increment. Step 4: Configuring Lambda Functions Video Processing Lambda Function Sign in to the Lambda console and click ""Create function."" Select ""Author from scratch."" Set the function name as video-processing , choose the appropriate runtime (e.g., Node.js 18.x), and assign a role with SQS and S3 permissions. You may create a new role (e.g., Lambda_SQS_S3 ) and attach additional policies (such as S3 Full Access) via the IAM console. Once the Lambda function is created, add an SQS trigger by selecting the video-processing queue. Configure the batch size (for example, 1 for individual processing or a higher number for batching) and set the appropriate batch window. Update the function code to process the event. The sample code below logs the event and extracts the message from the first record: export const handler = async (event) => {
    console.log(""Received event:"", event);
    // Extract and parse the message from the SQS event
    const body = JSON.parse(event.Records[0].body);
    const message = JSON.parse(body.Message);
    console.log(""Message details:"", message);

    // Processing logic (e.g., retrieving the video from S3, converting it, and uploading) goes here.
    const response = {
        statusCode: 200,
        body: JSON.stringify(""Hello from Lambda!""),
    };
    return response;
}; After deploying the function, check the SQS queue’s message count. A reduction indicates that the Lambda function is processing messages. You can also review CloudWatch logs for detailed execution information. Thumbnail Processing Lambda Function Create another Lambda function named thumbnail-processing using the same steps as above. Use the same role (e.g., Lambda_SQS_S3 ) because this function also requires SQS and S3 access. Add an SQS trigger for the thumbnail-processing queue and configure the batch settings. Implement the code to handle thumbnail generation, then deploy the function. Step 5: Configuring S3 Event Notifications Automate the workflow by configuring your S3 bucket (storing raw videos) to trigger an event notification to the SNS topic each time a video is uploaded: Open the S3 console and select the raw videos bucket (e.g., raw-videos-kodekloud ). Under the ""Properties"" tab, scroll down to ""Event notifications"" and create a new notification. Set an event name (e.g., video-uploaded ). Optionally, specify a prefix (files are assumed to be uploaded to the root for this demonstration). Under ""Event types,"" select the object creation events (PUT, POST, COPY, etc.). For the destination, select SNS and choose the video-uploaded topic. Important: Before S3 can publish notifications to SNS, update the SNS topic's access policy to allow the Amazon S3 service to publish messages. For example, add the following policy statement to your SNS topic (replace placeholders with actual values): {
  ""Sid"": ""ExampleSNSPublishPolicy"",
  ""Effect"": ""Allow"",
  ""Principal"": {
    ""Service"": ""s3.amazonaws.com""
  },
  ""Action"": ""sns:Publish"",
  ""Resource"": ""arn:aws:sns:us-east-1:841860927337:video-uploaded"",
  ""Condition"": {
    ""ArnLike"": {
      ""aws:SourceArn"": ""arn:aws:s3:::raw-videos-kodekloud""
    },
    ""StringEquals"": {
      ""aws:SourceAccount"": ""841860927337""
    }
  }
} To update the SNS topic policy: Open the SNS topic configuration page. Click ""Edit"" next to Access Policy. Add the above JSON statement to the existing policy and save your changes. Finally, save the event notification within the S3 console. Step 6: Testing and Verification Send Test Messages: Publish a test message via the SNS console and verify that both SQS queues receive it. Upload a Video File: Upload a short video file to the raw videos bucket. This triggers an SNS notification which, in turn, invokes the linked Lambda functions. Verify Outputs: Check the processed videos bucket for the converted video files (e.g., an .m3u8 file along with corresponding .ts chunk files). Review the thumbnails bucket to ensure the thumbnail images have been generated appropriately. Cleanup After verifying the configuration, it is important to clean up resources to avoid incurring unnecessary charges: Delete SQS Queues: In the SQS console, delete both the video-processing and thumbnail-processing queues. Delete the SNS Topic: Open the SNS console and delete the video-uploaded topic. Remove Lambda Functions: Delete both Lambda functions ( video-processing and thumbnail-processing ). Delete S3 Buckets: Empty the raw videos, processed videos, and thumbnails buckets and then delete them. This demonstration has shown how to integrate SNS with SQS and Lambda for automated video processing and thumbnail generation. Leveraging AWS services enables efficient and scalable event-driven processing for video applications. Happy Learning! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Managed Service for Kafka,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Managed-Service-for-Kafka,"AWS Solutions Architect Associate Certification Services Data and ML Managed Service for Kafka Welcome back, Future Solutions Architects! In this article presented by Michael Forrester, we explore the Managed Service for Kafka—a critical component of the Machine Learning Services suite focused on the power of data ingestion. Overview Managed Service for Apache Kafka (MSK) simplifies running Kafka on AWS. Built on the popular open-source streaming data platform Apache Kafka (originally developed by LinkedIn), MSK streamlines deployment, scaling, and integration in a cloud-based environment. Whether you choose a server-based or serverless option, MSK allows you to build robust applications that ingest, process, and interact with high-volume real-time data. Key Use Cases MSK is ideal for real-time behavioral analytics, clickstream data, website interactions, event sourcing, log aggregation, IoT data, and e-commerce transactions. Below is the official AWS marketing diagram for Managed Service for Kafka, which outlines AWS’s presentation of the service: How It Works Imagine MSK as your personal data radio station—broadcasting real-time data by creating topics, sending data streams to these topics, and allowing consumers to process the data downstream. MSK essentially serves as a ""programming guide"" for orchestrating data streams, enabling applications to handle massive volumes of real-time data with ease. Architecture Insight The managed service architecture for Kafka leverages AWS-managed brokers, ensuring that the underlying infrastructure remains largely invisible to end users. This architecture comprises Kafka brokers, ZooKeeper nodes that route requests, and multiple subnets across different availability zones. For users opting for a serverless configuration, simply setting your capacity is enough—producers and consumers directly interact with the managed brokers. Important When configuring MSK in non-serverless mode, ensure you specify the appropriate broker sizes and configurations to meet your workload requirements. Key Features of MSK MSK is designed to handle data ingestion at scale, delivering high performance and reliability. Here are some of its standout features: Automated Cluster Management: Reduces manual overhead by automating routine cluster tasks. Scalability and Performance: Provides excellent scalability to manage high volumes of data. Security and Compliance: Ensures network isolation, encryption at rest and in transit, integration with AWS IAM, and compliance with GDPR, HIPAA, and other standards. Reliability and High Availability: Offers automated backups, multi-zone replication, and automatic recovery. Monitoring and Alerts: Seamlessly integrates with AWS monitoring and alerting services like CloudTrail and CloudWatch. Conclusion MSK is engineered for robust data ingestion, making it possible to ingest, process, and deliver vast quantities of data effortlessly. Its managed architecture allows organizations to focus on building applications and extracting insights, rather than managing infrastructure. For more information or if you have any questions, please visit KodeKloud Slack . Thank you for reading, and we look forward to sharing more insights in our upcoming articles. Links and References AWS Managed Streaming for Kafka (MSK) Apache Kafka AWS Identity and Access Management (IAM) Machine Learning Services Watch Video Watch video content"
AWS Solutions Architect Associate Certification,EMR,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/EMR,"AWS Solutions Architect Associate Certification Services Data and ML EMR Hello future certified architects! In this lesson, we dive into Amazon Elastic MapReduce (EMR), a cornerstone service in AWS for big data processing. EMR provides a managed environment for running big data frameworks like Hadoop, Apache Spark, Hive, and Pig, enabling you to analyze massive datasets, drive business intelligence, and power machine learning workloads. EMR commonly ingests data from repositories such as Amazon S3 into a managed cluster that processes data using open-source frameworks. This cluster, acting as a central processing unit, transforms and transfers data across AWS services like DynamoDB, RDS, and S3. The diagram below illustrates a typical data flow in an EMR environment, where various data sources feed information into Apache Spark running on EMR, which then processes and outputs data to services such as Redshift, S3, or Kinesis. EMR Cluster Architecture At the heart of EMR lies its cluster—a set of Amazon EC2 instances known as nodes. Understanding the roles these nodes play is essential: Primary Node: Manages the overall cluster and orchestrates the distribution of data and tasks. Core Node: Hosts components that store data in the Hadoop Distributed File System (HDFS) and actively participate in data processing. In multi-node clusters, at least one core node is essential. Task Node: Exclusively handles data processing tasks without storing data. These nodes are optional and help scale the processing workload. The diagram below provides a clear visualization of how these nodes interact within an EMR cluster. How EMR Works When launching an EMR cluster, you determine its size and specify node roles. Data is imported into the cluster from supported sources like S3 or DynamoDB. The primary node leverages frameworks such as Hadoop, Apache Spark, HBase, Presto, or Hive to distribute and process the data concurrently. AWS provides tools like the CLI and EMR API, which allow you to monitor cluster performance and dynamically adjust the number of instances or manage the cluster lifecycle. You can submit multiple processing steps to a running EMR cluster. For instance, a workflow might include running a Pig script on an input dataset, followed by a Hive program on a subsequent dataset, finally producing results. The step execution process works as follows: Initially, all steps appear in a “pending” state. The first step transitions to a “running” state while later steps remain pending. Completed steps update to “completed.” If a step fails (e.g., due to a Pig script error), its status changes to “failed,” and any pending steps are automatically canceled. Optionally, you may opt to ignore a failure to allow subsequent steps to run, or terminate the cluster immediately. The flowchart below outlines this process along with step status indicators. Key Features of Amazon EMR Amazon EMR offers several standout features that make it a powerful solution for big data processing: Managed Hadoop Framework: Leverage native support for Hadoop alongside Spark, HBase, Presto, Hive, and more. Scalability and Flexibility: Easily scale clusters from a single instance to thousands, taking full advantage of AWS’s elastic infrastructure. Cost-Effective Processing: Optimize costs with EC2 spot pricing for task nodes, ideal for interruptible workloads. Seamless AWS Integration: Integrates effortlessly with services such as S3, RDS, DynamoDB, CloudWatch, and CloudFormation. Robust Security: Multiple security layers include IAM integration, customer-managed key support, encryption (at rest and in transit), and network isolation. EMR also complies with standards like GDPR and HIPAA. The diagram below visually summarizes these key features. Note For detailed integration guidelines and best practices, refer to the Amazon EMR Documentation . Conclusion Amazon EMR simplifies and accelerates your big data processing needs, allowing you to efficiently transform and analyze vast datasets through a managed and scalable cluster. By integrating seamlessly with other AWS services and offering reliable performance, EMR is an invaluable tool for approaching data transformation and analytics at any scale. Happy architecting, and may your journey with EMR be both insightful and productive! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Step Functions,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Application-Integration/Step-Functions,"AWS Solutions Architect Associate Certification Services Application Integration Step Functions In this lesson, we will explore AWS Step Functions and understand how they simplify complex workflows in modern, distributed applications. Why Use Step Functions? Imagine you are developing an e-commerce application responsible for processing orders. When a user places an order, a workflow is initiated that drives the process from order placement to fulfillment, shipment, and delivery. The overall workflow might involve the following steps: Order Placement: Triggered when a customer places an order on the platform. Payment Processing: Validates and processes the payment. Inventory Check: Verifies product availability. Shipping Label Generation: Creates a shipping label after confirming inventory. Customer Notification: Sends order status updates and tracking information. Shipping: Prepares and dispatches the order. Order Completion: Marks the order as fulfilled in the system. Managing such a complex and distributed workflow manually is error-prone. Often, each component is implemented as a Lambda function that performs a distinct task without maintaining state. This introduces challenges, particularly because Lambda functions have a maximum runtime of 15 minutes and require explicit data passing. How AWS Step Functions Help AWS Step Functions address these challenges by offering a visual and programmatic way to orchestrate workflows. They allow you to: Define Each Step: Specify whether the step involves invoking a Lambda function, running an ECS task, or sending notifications via SNS. Visualize and Modify: Use a drag-and-drop GUI to easily visualize and adjust the workflow. Execute in Parallel: Run independent tasks simultaneously, such as payment processing and fraud detection. Key Benefits Workflow Orchestration: Visually define, manage, and monitor workflows without embedding orchestration logic in your applications. Parallel Execution: Speed up processing with concurrent task execution. Error Handling: Automate exception management, retries, and escalation mechanisms. Visual Interface: Gain insights using the AWS console, which makes it easier for teams to understand workflow behavior. State Management: Seamlessly pass data between tasks, overcoming the stateless nature of Lambda functions. E-Commerce Order Processing with Step Functions Let’s revisit the e-commerce order processing workflow and see how Step Functions can enhance it: Order Placement: An event triggers the order placement Lambda function when a customer places an order. Payment Processing (Parallel Tasks): A task validates the payment using an external service running on an EC2 instance. Simultaneously, another task runs a fraud and risk detection Lambda function. Inventory Check: Once payment and fraud detection clear successfully, a Lambda function verifies product availability. Post-Inventory Actions (Parallel Tasks): One task generates the shipping label. Another task notifies the customer via SES (email) and SNS (SMS) with tracking details. Shipping: Initiates the shipment process. Order Completion: Updates the system to mark the order as complete. Consider the complexity if you had to manually coordinate these tasks without Step Functions. Tracking the state transitions and managing parallel executions across multiple Lambda functions, EC2 instances, and notification channels would require significant custom logic. Integrations and Use Cases Step Functions integrate seamlessly with over 220 AWS services and more than 10,000 APIs. They work with services across various categories: Category Examples Compute Lambda, ECS, EKS, AWS Fargate Databases DynamoDB and other data stores Messaging SNS, SQS Analytics Athena, AWS Batch, AWS Glue, EMR, Glue DataBrew Machine Learning SageMaker API Management API Gateway Step Functions support a wide range of use cases, including: Orchestrating multiple long-running ETL jobs. Automating security and IT service workflows with manual approval steps. Coordinating microservices to build responsive serverless applications. Conclusion By leveraging AWS Step Functions, you can break down complex workflows into manageable, modular tasks, simplify error handling, and benefit from a visual representation of your entire process. This approach leads to a more resilient and maintainable application architecture. Additional Resources For more information, check out the AWS Step Functions documentation and our serverless application guides . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Demo Kinesis in real time consumption and production,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Demo-Kinesis-in-real-time-consumption-and-production,"AWS Solutions Architect Associate Certification Services Data and ML Demo Kinesis in real time consumption and production In this lesson, you'll learn how to work with Amazon Kinesis by sending dummy data—simulating crypto or stock trading prices—to a Kinesis data stream. This data is then automatically forwarded to a Kinesis Data Firehose and delivered to an S3 bucket. Follow the steps and diagrams below to set up your streaming data pipeline. Creating a Kinesis Data Stream Start by navigating to the Kinesis service page in the AWS console and creating a new data stream. Enter a stream name (for example, ""crypto stock price""). Choose the capacity mode: Select Provisioned if you know your overall throughput requirements. Select On-Demand for dynamic scaling. Pricing Consideration Note that the On-Demand option may be pricier compared to Provisioned capacity. Configure additional stream settings as required. After configuring your settings, click Create data stream to complete this step. Configuring the Kinesis Data Firehose Next, configure a Kinesis Data Firehose to channel data from the Kinesis data stream to an S3 bucket. Choose the data stream you just created as your source. For the destination, select or create an S3 bucket. In this example, a new bucket named ""Kinesis Code Cloud Demo"" is created. Optionally, enable data transformation by activating a Lambda function. For this demo, leave the transformation settings as default. Review your settings and create the delivery stream. Setting Up an S3 Bucket If you do not already have an S3 bucket, follow these steps to create one: Enter the bucket name (""Kinesis Code Cloud Demo"" in this example). Select your AWS region and configure additional settings as needed. Click Create bucket to finalize the setup. After creation, verify the bucket's availability by browsing your bucket list. Sending Test Data to the Data Stream With the Kinesis data stream and Firehose set up, send test data using the AWS SDK with JavaScript. The sample code provided below sends a test record every 50 milliseconds. Code Example: Sending a Single Record // Sample code to send a record using Kinesis PutRecordCommand
const command = new PutRecordCommand(input);
const response = await client.send(command);
console.log(response); Complete Snippet: Generating Dummy Data This complete snippet generates dummy data with timestamps and simulated prices: setInterval(async () => {
    const input = {
        // PutRecordInput
        StreamName: ""crypto-stock-price"",
        Data: Buffer.from(
            JSON.stringify({
                date: Date.now(),
                price: ""$"" + (Math.floor(Math.random() * 40000) + 1)
            })
        )
    };
    // Create and send the PutRecordCommand using your AWS SDK client
    const command = new PutRecordCommand(input);
    const response = await client.send(command);
    console.log(response);
}, 50); Run this code from your working directory (for example, C:\Users\sanje\Documents\scratch\kinesis). It will continuously generate and send data into the stream over several minutes. Verifying Data Delivery in S3 Once the test data has been sent, verify delivery by checking your S3 bucket. The Kinesis Data Firehose typically organizes files into folders based on the current date (e.g., ""2023""). Opening one of these files should reveal JSON objects similar to the examples below: Example 1 {""date"":169810779318,""price"":""$46941""}
{""date"":169810793259,""price"":""$342561""}
{""date"":169810793368,""price"":""$301171""}
{""date"":169810793418,""price"":""$274001""}
{""date"":169810793469,""price"":""$6611""}
{""date"":169810793518,""price"":""$94176""} Example 2 {""date"":1689188257344,""price"":""115.48""}
{""date"":1689188257345,""price"":""348.47""}
{""date"":1689188257346,""price"":""519.32""} These records confirm that data is successfully transmitted from your Kinesis data stream through the Firehose and stored in the S3 bucket. Conclusion This demonstration has shown you how to set up a real-time data ingestion and delivery pipeline using Amazon Kinesis. The steps covered include: Creating a Kinesis data stream Configuring a Kinesis Data Firehose Setting up an S3 bucket Sending test data using the AWS SDK Verifying data delivery in S3 This robust setup not only supports real-time data processing but also offers capabilities such as data transformation via Lambda functions, providing a scalable solution for various applications. Happy streaming and exploring further real-time data processing techniques! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Lake Formation,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Lake-Formation,"AWS Solutions Architect Associate Certification Services Data and ML Lake Formation Welcome back, Solutions Architects. In this article, we explore AWS Lake Formation—a robust service engineered to aggregate and manage your organization’s diverse data sets. Discover how Lake Formation streamlines data ingestion, storage, and processing, and learn how it integrates seamlessly with AWS services such as Athena and QuickSight. Data Ingestion and Storage AWS Lake Formation aggregates data from a variety of sources including DynamoDB, Redshift, S3, RDS, Aurora, and even the AWS Glue Data Catalog. The ingestion process operates much like AWS Glue—a serverless data integration service that crawls your data sources to populate the Glue Data Catalog with metadata. This process can be scheduled at regular intervals (for example, every two hours or every 24 hours) to ensure your data lake remains current. After ingestion, data is centrally stored in S3 in its native format (CSV, TSV, etc.) or in analytics-optimized formats such as Apache Parquet or ORC. These optimized formats greatly enhance query performance, especially when using services like Athena. The data is subsequently cataloged as tables in the Glue Data Catalog, simplifying data management and enforcing granular access control. The diagram below illustrates the key components of AWS Lake Formation, highlighting data ingestion, storage, and processing: Data Processing Once data is ingested and stored, Lake Formation leverages AWS Glue jobs to process the data. These ETL (Extract, Transform, Load) jobs enrich and transform data, preparing it for downstream services such as Athena, Redshift, EMR, or various machine learning platforms. To summarize the process: Data is ingested from multiple sources and registered in the Glue Data Catalog. Data is stored in S3 and optionally converted into analytics-optimized formats (e.g., Parquet or ORC). AWS Glue ETL jobs process the data, making it accessible for querying and further analysis. The diagram below outlines the complete architecture, demonstrating how data flows from source systems to processing services: Integration with Other AWS Services AWS Lake Formation integrates effortlessly with various AWS services, enabling comprehensive data consumption and analysis: Athena: Executes queries on data stored in optimized formats, resulting in improved performance and cost reduction. QuickSight: Offers advanced data visualization capabilities by querying Athena, which facilitates dynamic dashboards and in-depth analytics. Additional Services: AWS Glue can perform further data transformations, while CloudTrail logs API calls made against Lake Formation for complete monitoring and auditing. The following flowchart demonstrates how diverse data sources converge in Lake Formation before being analyzed by services such as Athena and Amazon QuickSight: Key Features of AWS Lake Formation AWS Lake Formation simplifies the creation of a modern data lake by centralizing data access and employing advanced techniques such as data deduplication through built-in machine learning algorithms. Additionally, the service supports cross-region data replication, enhancing data durability, disaster recovery, and compliance with data residency requirements. Key Benefits Centralized data management and control Optimized storage formats for enhanced query performance Automated metadata extraction and cataloging via AWS Glue Comprehensive monitoring and auditing with CloudTrail integration The diagram below summarizes the key features of Lake Formation: Conclusion AWS Lake Formation serves as a powerful solution for unifying heterogeneous data sources into a standardized and easily manageable data lake. With integrated support for data transformation via AWS Glue, robust access control mechanisms, and seamless connectivity to analytics tools like Athena and QuickSight, Lake Formation significantly simplifies the process of constructing, securing, and managing your data ecosystem. By harnessing these capabilities, organizations can ensure their data is always ready for deep analytics and processing, unlocking valuable insights that drive informed business decisions. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Athena,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Athena,"AWS Solutions Architect Associate Certification Services Data and ML Athena Welcome back, Future Solutions Architect Associates. I'm Michael Forrester, and in this lesson we'll dive into AWS Athena—an essential tool in the AWS ecosystem for data storage and analysis. Athena empowers you to query datasets directly from Amazon S3, eliminating the need to manage traditional database infrastructure. Overview AWS Athena is designed to efficiently analyze both structured and unstructured data. It leverages the AWS Glue Data Catalog as a metadata repository, enabling you to perform SQL queries on data that has been extracted and transformed from various sources. Typical data sources include sales transactions, operational metrics, and more. The data processing workflow generally follows these key steps: Ingest raw data from diverse sources. Use AWS Glue to extract, clean, and convert data into efficient formats (e.g., Parquet, CSV, ORC). Store the processed data in an S3 bucket. Catalog the dataset with the AWS Glue Data Catalog. Query the data using AWS Athena with SQL. The following diagram illustrates a typical data processing workflow using AWS services: Once the data is cataloged by AWS Glue, Athena enables business analysts to run SQL queries directly on the S3-stored datasets. For instance, if you want to analyze customer purchase data to compute total sales by product within a specific date range, your SQL query might look like this: SELECT product_id, SUM(purchase_amount) AS total_sales 
FROM sales_data 
WHERE purchase_date >= '2023-01-01' 
  AND purchase_date <= '2023-03-31' 
GROUP BY product_id 
ORDER BY total_sales DESC 
LIMIT 5; Note Athena allows you to execute complex, ad hoc queries without managing any underlying infrastructure, making it both simple and cost-effective. Integration with Visualization Tools After running queries in Athena, you can seamlessly integrate the results with Amazon QuickSight for data visualization. QuickSight automatically updates dashboards based on the latest Athena query results, streamlining your data analysis and reporting workflow. Athena's flexibility extends to many data sources, including Keyspaces, DynamoDB, DocumentDB, and RDS. These sources can be ingested into the Glue Data Catalog, queried with Athena, and then visualized using QuickSight. Key Features Serverless and Cost-Efficient: Athena is fully serverless, meaning you only pay for your query execution time. Performance Optimization: Benefit from built-in functions and data partitioning to significantly boost query performance. Standard SQL and UDF Support: Use standard SQL along with user-defined functions (UDFs) written in Java for customized operations. Multiple Data Format Support: Query datasets stored in a variety of formats such as Parquet, ORC, JSON, CSV, and TSV. Broad Integration: Compatible with JDBC and ODBC drivers, Athena easily serves as a data source for popular BI tools like Power BI, and integrates seamlessly with numerous AWS services. Conclusion AWS Athena provides a fast, scalable, and cost-effective way to perform ad hoc SQL queries on large datasets stored in Amazon S3. With effective integration with AWS Glue for data cataloging and Amazon QuickSight for visualization, you can quickly derive insights without the complexity of traditional database management. Whether analyzing customer purchase trends or monitoring operational metrics, Athena is a robust solution for your data analytics needs. Further Resources For more details on how to optimize your queries and integrate Athena with other AWS services, visit the AWS Athena Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Polly,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Polly,"AWS Solutions Architect Associate Certification Services Data and ML Polly Welcome back. In this article, we explore AWS Polly—a powerful machine learning service that converts text into lifelike speech. AWS Polly is designed to provide natural-sounding audio output for applications such as voice assistants, automated announcements, and more. It seamlessly integrates with other AWS services to deliver streamlined, end-to-end voice applications. How AWS Polly Works At its core, Polly takes text input, synthesizes it, and returns voice output. For example, if you ask, ""Hey, tell me about the weather,"" Polly will generate a spoken response like ""Today's forecast is mostly sunny with a high of 25°C."" Polly supports both real-time speech synthesis streaming and audio file generation. This flexibility allows you to either stream the audio directly during interactions or store audio files for later playback. Moreover, Polly supports the Speech Synthesis Markup Language (SSML), which provides granular control over pronunciation, volume, pitch, and speed—making the interactions sound more natural and expressive. Custom Lexicons Polly allows the use of custom lexicons, essentially custom dictionaries, to accommodate specialized pronunciations or terminology unique to your business or specific use case. Key Features of AWS Polly Polly's feature set makes it an essential tool for voice-enabled applications: Lifelike Speech: Generate natural-sounding audio from text. Real-Time Streaming and File Generation: Choose between streaming audio directly or saving it for later use. SSML Support: Control pronunciation, volume, pitch, and speed. Custom Lexicons: Use dictionaries tailored to your specific needs. AWS Integration: Easily integrate Polly with other AWS services. Example Implementation Workflow A typical implementation of AWS Polly involves the following steps: A transcription file (e.g., a video script or document) is uploaded to an Amazon S3 bucket. This event triggers an AWS Lambda function that calls Polly to perform text-to-speech conversion. The resulting audio file is then saved to an output S3 bucket, making it immediately available for playback or further dissemination. Practical Use Case: Smart Thermostat Integration Consider a practical example where a smart thermostat, equipped with sensors and connected via AWS IoT Core, responds to voice commands. Here's how it works: The thermostat sends a voice command using the MQTT protocol. Amazon Transcribe converts the spoken command into text. An AWS Lambda function processes the text and interacts with AWS IoT Core. AWS Polly converts the resulting text (e.g., ""Temperature set to 22°C"") back into spoken audio for the user. Service Separation Note that AWS Transcribe handles the conversion of speech to text, while AWS Polly solely focuses on transforming text back into speech. This clear separation ensures optimal performance and smoother interactions across your applications. Get Involved I'm Michael Forrester. If you have any questions or need further clarification, please join our discussion on kodekloud.slack.com under the AWS courses channel. Thank you for reading this article, and I look forward to connecting with you in the next lesson. For additional insights, explore our documentation and related resources on AWS Documentation and AWS Blogs . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,MigrationTransfer Agenda and Introduction,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Migration-and-Transfer/MigrationTransfer-Agenda-and-Introduction,"AWS Solutions Architect Associate Certification Services Migration and Transfer MigrationTransfer Agenda and Introduction In this lesson, we explore the range of AWS services specifically designed to facilitate the migration of applications and data from on-premises environments or other clouds to AWS. By drawing an analogy between moving to a new house and cloud migration, we can better understand the steps required for a successful transition. Insight Migrating to AWS requires careful planning and execution just like moving to a new home. Both processes involve an assessment phase, organization of items (or applications/data), and a well-orchestrated execution plan. Home Migration Process Moving to a new house typically involves the following steps: Assessment and Inventory: Begin by assessing all your belongings and creating an inventory. This ensures you have a complete list of your items before making any decisions. Sorting and Decluttering: Determine which items to keep, donate, sell, or discard. This step helps reduce the volume of belongings, ensuring you only move what is necessary. Categorization: Group items by type, fragility, size, and weight. For example, group furniture, appliances, electronics, clothing, kitchenware, and books separately. This simplifies both packing and subsequent organization at your new home. Determining Transportation Needs: Evaluate the volume and dimensions of your items to decide whether you need a large moving truck, a small van, or a combination of transportation modes. Packing: Securely pack items using appropriate materials, taking extra precautions with fragile objects by labeling them clearly. Loading and Transportation: Load your items carefully, ensuring heavy items are at the bottom and fragile items remain on top, while securing everything to prevent damage during transit. Unloading and Organizing: At your new home, unload and arrange your belongings room by room to streamline the unpacking process. AWS Cloud Migration Process Cloud migration parallels the home moving process, with key steps tailored to digital environments: Assessment and Inventory: Catalogue your digital assets, including applications, databases, storage, and dependencies present in your current environment. Categorization: Group applications based on complexity, interdependencies, and business criticality to streamline migration planning. Determining Cloud Services: Choose the most suitable AWS services based on your assessment to ensure compatibility and optimal performance in the cloud. Migration Planning: Develop detailed migration strategies for each application. Decide whether to re-host, refactor, re-architect, or rebuild your applications. Migration Execution: Execute the migration plan while monitoring progress and ensuring a smooth transition to AWS. Overview of AWS Migration Services AWS offers a comprehensive suite of services that ease the transition to the cloud. In this section, we introduce key AWS migration tools: AWS Migration Hub: A centralized platform that streamlines the migration of applications and workloads to AWS. It integrates with various AWS migration tools to provide a unified view for managing and monitoring migrations. Application Discovery Service: Aids in the assessment phase by inventorying applications and identifying their dependencies, helping you understand the full scope of your migration. Application Migration Service: Simplifies the transfer of applications with an automated process, ensuring a smooth and efficient migration. Database Migration Service (DMS): Dedicated to moving databases to AWS, DMS includes schema conversion tools to seamlessly transition between different database systems. Elastic Disaster Recovery: While primarily designed for disaster recovery, this service enables the rapid establishment of backup cloud environments to ensure business continuity during migration. Mainframe Modernization: Facilitates the migration of mainframe workloads to the cloud, modernizing legacy systems for enhanced performance and scalability. AWS Transfer Family and AWS DataSync: These services address various data transfer needs, allowing efficient and reliable movement of data from on-premises environments to AWS. AWS Snow Family: Provides physical devices for transferring large volumes of data into AWS, ideal for situations where network-based transfers are not feasible. AWS delivers a comprehensive set of solutions that cover every aspect of cloud migration—from application transfer and database migration to large-scale data transfer. In this lesson, we will delve into each service, providing you with the insights needed to plan and execute a successful migration to the AWS Cloud. For additional details and best practices, visit the AWS Migration Hub documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Sagemaker,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Sagemaker,"AWS Solutions Architect Associate Certification Services Data and ML Sagemaker Welcome, Solutions Architects and future Solutions Architects! In this article, you'll discover Amazon SageMaker—AWS’s premier machine learning service that streamlines the process of hosting, training, and deploying ML models. SageMaker empowers you to detect patterns in data and make reliable predictions, making it an essential tool for any AI-driven project. Machine learning and artificial intelligence rely on mathematical algorithms to identify trends and patterns. These algorithms analyze input data to make predictions or decisions. For example, an algorithm might analyze demographic data to predict a person's likelihood of purchasing an item, such as a hat. SageMaker supports the entire ML workflow with a comprehensive suite of tools and services. Below is an overview of the key steps in the machine learning lifecycle with SageMaker. Key Steps in the Machine Learning Workflow 1. Data Ingestion and Preparation Begin by gathering relevant data that addresses your business or technical problem. For example, you might store demographic data and historical purchase information in an Amazon S3 bucket. SageMaker integrates with services like AWS Glue DataBrew, Data Wrangler, and SageMaker Notebooks (Jupyter Notebooks) to help you explore, clean, and pre-process your data. Tools such as SageMaker Data Wrangler offer an intuitive interface to remove outliers, transform data, and ensure high-quality input for model training. 2. Model Training Once your data is prepared, you can train your machine learning model. During training, the model learns relationships within the data by processing both known outcomes and new cases. The objective is to develop a model capable of making accurate predictions—for instance, determining whether a person is likely to purchase a hat with high certainty. 3. Model Evaluation and Tuning After training, SageMaker provides tools to validate and fine-tune your model. This phase involves adjusting parameters to improve accuracy and reliability. Evaluating the model before deployment ensures that it performs effectively in production environments. 4. Model Deployment When your model is tuned and validated, you can deploy it to production using SageMaker Endpoints. This enables near real-time predictions and supports advanced deployment options such as A/B testing, auto scaling, and concurrent serving of multiple models. Deep Dive into SageMaker Components Notebook Instances SageMaker offers interactive Jupyter Notebooks that allow you to write, run, and visualize Python code. This environment is ideal for exploring data and developing code in a web-based setting. Training Jobs You can launch training jobs that leverage various machine learning algorithms suitable for diverse data scenarios. SageMaker also supports distributed training across multiple instances, which can significantly reduce the time required to train your models. Endpoints for Deployment Deploy your trained and tuned models to SageMaker Endpoints to integrate machine learning predictions into your live applications. This feature includes support for auto scaling to handle varying workloads. Note SageMaker integrates seamlessly with other AWS services. For example, AWS Glue can be used for ETL tasks, while S3 serves as secure and scalable data storage. Advanced Features of SageMaker SageMaker offers several advanced capabilities that make it a complete ML platform: Integrated Jupyter Notebooks: Use a familiar environment for interactive development. Distributed Training: Scale training across multiple instances to speed up the process. Automatic Model Tuning: Optimize model parameters to enhance prediction accuracy. SageMaker Studio: Access a unified IDE covering every stage of the ML lifecycle, from data preparation to deployment. Built-in Algorithms and Bring Your Own Algorithm (BYOA): Flexibility to choose from pre-built algorithms or integrate your custom algorithms within SageMaker’s containerized environment. Summary If you're aiming to train machine learning models using production-ready notebooks, distributed training environments, automatic model tuning, and flexible algorithm choices, Amazon SageMaker is the ultimate solution. Its end-to-end support for every phase of the ML workflow makes it an indispensable tool for deploying scalable, efficient AI applications. For more detailed information, check out the AWS Documentation and Amazon SageMaker Developer Guide . I'm Michael Forrester. If you have any questions, please reach out on Slack. Otherwise, see you in the next article! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Forecast,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Forecast,"AWS Solutions Architect Associate Certification Services Data and ML Forecast Welcome back, future solutions architects! In this article, we explore the power of inference with Amazon Forecast—a fully managed machine learning service from AWS designed to generate highly accurate time series predictions without requiring deep expertise in machine learning. Amazon Forecast processes historical data such as sales figures, inventory levels, or website traffic to identify underlying patterns like seasonality and trends. By simply uploading your historical data, the service automatically inspects and detects key attributes, including seasonal buying patterns. For example, when analyzing sales data, Forecast may identify recurring trends on an annual, quarterly, or custom schedule, then select the best-suited built-in algorithm based on these characteristics. After the algorithm is selected, Amazon Forecast trains the model using your historical data, fine-tuning parameters to minimize forecasting errors. The result is an optimized forecasting model capable of predicting future data points by leveraging the trends and patterns learned during training. While you could build a forecasting solution using Amazon SageMaker, doing so typically requires a deeper understanding of machine learning. Amazon Forecast simplifies this process by automating many complex tasks involved in time series forecasting, making it an ideal choice for users without specialized ML expertise. Amazon Forecast provides multiple methods for accessing your forecasts: Visualize predictions directly in the AWS Console. Export the forecasts as a batch CSV file. Retrieve predictions programmatically via the Forecast API. The service ensures the accuracy of its forecasts by accounting for significant factors such as seasonality and trends. Importantly, Forecast outputs include quantiles (or percentiles) which help you gauge a range of possible outcomes. For example, a forecast might indicate a 70% probability for one scenario and a 25% probability for an alternate outcome, empowering you to plan for both best-case and worst-case scenarios. Integration with AWS Ecosystem Amazon Forecast seamlessly integrates with several AWS services. Forecast outputs can be exported to Amazon S3 and then further processed or utilized by other services like AWS Lambda. Additionally, integration with AWS CloudTrail and IAM enhances the overall security and management of your forecasting workflows. In summary, Amazon Forecast empowers you to generate time series predictions without needing deep machine learning knowledge. Its automated approach makes it a top choice for scenarios where rapid, accurate forecasting is essential—especially when the project team lacks advanced machine learning skills. This is a crucial consideration for AWS certification exam questions that require selecting a service based on ease of use and minimal setup. We hope this article clarifies how Amazon Forecast works and inspires you to incorporate its capabilities into your architectural solutions. See you in the next lesson! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Quicksight,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Quicksight,"AWS Solutions Architect Associate Certification Services Data and ML Quicksight Good day, Cloud Practitioners and future Solutions Architects! In this article, we take an in-depth look at AWS QuickSight—a powerful data visualization service designed to transform raw data into actionable insights through highly interactive dashboards. QuickSight supports a wide range of visualization formats including bar graphs, tables, pie charts, trend charts, anomaly detection visuals, and geographic dispersion maps. This versatility enables you to present analytics data in the most effective way for decision-making. Key Insight AWS QuickSight seamlessly integrates with various AWS data services, enabling you to easily connect to and visualize data from multiple sources. Robust Data Source Integration QuickSight easily integrates with core AWS services: Amazon S3 & Athena: Directly connect to data stored in S3 buckets, use Athena for querying, and then visualize the output in your dashboards. Relational Databases: QuickSight connects with RDS, Aurora (the cloud-native variant of MySQL and PostgreSQL), and third-party databases like SQL Server and MySQL. Data Warehouses & Data Catalogs: Integrate with Amazon Redshift and data catalogs provided by Glue, including both Glue ETL and Glue DataBrew. The SPICE Engine At the core of QuickSight is the SPICE engine (Super-fast, Parallel, In-memory Calculation Engine). SPICE is responsible for: Storing large datasets in memory for real-time queries Reducing database load and query costs Automatically updating visualizations as data changes This high-speed performance means you always see the most current information without manual refreshes. Additionally, SPICE scales automatically to handle large volumes of complex data, ensuring efficient and reliable data processing. Enhanced Security and Machine Learning Security remains a top priority: SPICE encrypts data at rest and in transit, maintaining confidentiality. SPICE powers QuickSight Q, which supports natural language queries and leverages machine learning for anomaly detection and forecasting. Fully Managed and Serverless QuickSight is a fully managed, serverless service. This means you don’t have to worry about managing underlying infrastructure. It offers: Interactive Dashboards: Drill down into various levels of data granularity. Data Preparation Tools: Prepare your data easily for analysis. Machine Learning Insights: Utilize features like forecasting, anomaly detection, and natural language narratives. Broad Data Integration: Besides seamless integration with AWS services (RDS, Aurora, Redshift, Athena), QuickSight also supports external data sources. Remember QuickSight's serverless architecture means less operational overhead, allowing you to focus on gaining insights from your data rather than managing infrastructure. Conclusion This article provided a brief overview of AWS QuickSight and showcased its deep integration within the AWS ecosystem. With its robust integration capabilities, high-speed SPICE engine, and rich feature set for data visualization and machine learning insights, QuickSight is a key tool for those looking to extract meaningful insights from their data. Stay tuned as we explore more AWS services and their innovative features in upcoming lessons. For more details and related topics, check out: AWS QuickSight Documentation AWS Solutions Architect Resources Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Translate,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Translate,"AWS Solutions Architect Associate Certification Services Data and ML Translate Welcome future solutions architects. I'm Michael Forrester, and in this lesson we will explore one of AWS's advanced machine learning services—Amazon Translate. Amazon Translate is a robust language translation service that delivers fast, high-quality, and affordable translations. With support for over 5,000 language combinations, it is an excellent solution for global applications. For example, consider a chatbot originally designed for an English-speaking audience. By integrating Amazon Translate, the chatbot can seamlessly respond in the user’s native language—such as Japanese—without requiring multiple chatbot deployments or a large multilingual support team. Workflow Overview The translation process is straightforward: Upload the UTF-8 formatted text to Amazon S3. The upload action triggers an AWS Lambda function. Lambda invokes Amazon Translate to process the text. Amazon Translate returns the translated text, which is then stored back in Amazon S3. After translation, the text can be used by other services like Amazon Polly for converting text-to-speech or directly displayed in a chatbot interface. How Translation Works At the core of Amazon Translate is a neural machine translation (NMT) algorithm, which consists of two main components: Encoder: Reads the source text and constructs a semantic representation of the content. Decoder: Generates the translated text word-by-word in the target language using the encoded representation. This NMT approach ensures translations are accurate and natural-sounding, significantly improving upon traditional rule-based or statistical methods. With near real-time processing, translations are completed within milliseconds. Quick Fact Amazon Translate leverages state-of-the-art NMT algorithms to deliver translations that sound natural and contextually accurate. Integration with Other AWS Services Amazon Translate integrates seamlessly with several AWS services, enabling a broad range of use cases: Chatbots & Customer Support: Develop a chatbot in one language (e.g., English) and use Amazon Translate to convert messages into the customer’s native language (e.g., Japanese or Swahili) in real time. Voice Communication: Use Amazon Transcribe to convert spoken language into text, translate the text with Amazon Translate, and finally synthesize speech with Amazon Polly. This integration reduces the need for multiple language-specific implementations. Additionally, Amazon Translate supports custom terminology dictionaries, allowing you to handle domain-specific vocabulary (such as AWS, ECR, or EKS) for consistent and accurate translations. Another exciting application is the translation of HTML pages. Just like browser translation features in Chrome, you can submit an HTML page to Amazon Translate, which converts the content into the user's native language. Moreover, when used in conjunction with Amazon Comprehend, you can analyze translated text to derive sentiment, intent, and other insights. This combined approach provides a comprehensive solution for natural language processing and understanding. Conclusion In summary, Amazon Translate is a powerful AWS service that uses neural machine translation to deliver fast, accurate, and natural language translations. Its seamless integration with other AWS services like Lambda, S3, Polly, Transcribe, and Comprehend makes it a cornerstone for building globalized applications. Next Steps Thank you for following along in this lesson on Amazon Translate. If you have any questions, feel free to reach out on kodekloud.slack.com under AWS Courses. I look forward to seeing you in the next lesson. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Transcribe,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Transcribe,"AWS Solutions Architect Associate Certification Services Data and ML Transcribe Welcome back, AWS Solutions Architects! In this lesson, we dive into Amazon Transcribe—a cutting-edge service that leverages advanced automatic speech recognition (ASR) to convert audio into text. As part of AWS's comprehensive suite of language services alongside Amazon Translate and Amazon Textract , Transcribe seamlessly integrates with other AWS offerings, making it ideal for processing customer calls, meetings, video subtitles, and much more. Amazon Transcribe is designed to handle audio inputs from a variety of sources, such as meetings, presentations, and customer service calls. It automatically converts these recordings into text files, which are then stored directly in an Amazon S3 bucket. The output transcript is enriched with metadata, including speaker identification when applicable, providing enhanced value for interviews, live shows, and meetings with multiple participants. Key Feature: Speaker Diarization One of the standout features of Amazon Transcribe is speaker diarization. This capability distinguishes between different speakers in an audio clip by labeling each participant from ""SPK0"" to ""SPK9."" This clear designation is especially useful in multi-participant environments like conference calls and interviews. How Speaker Diarization Works Speaker diarization helps in accurately mapping conversations by identifying individual speakers, which makes it easier to analyze dialogue patterns and context in multi-speaker recordings. Extending the Workflow with AWS Integration After a transcription is completed and stored in Amazon S3 , the transcription process can be further automated and enhanced using other AWS services. For example, an AWS Lambda function can be triggered to initiate additional processing tasks, such as: Forwarding the transcription results to Amazon Comprehend for sentiment analysis. Translating the text using Amazon Translate . Storing metadata and transcripts in Amazon DynamoDB for quick retrieval and further analysis. Use Case: Transcribe Call Analytics A compelling use case of Amazon Transcribe is in call analytics. By enabling Transcribe Call Analytics, businesses can extract actionable insights from customer interactions. This service allows organizations to identify key topics, follow-up actions, and areas for improvement in agent productivity and customer engagement, making it a game changer for call center operations. Deployment Consideration Ensure that your AWS Lambda functions and other integrated services are correctly configured to handle the data flow between Amazon S3 and Transcribe to avoid disruptions in the automated workflow. Conclusion Amazon Transcribe offers a robust, fully managed solution for audio-to-text conversion, enhanced by detailed analytics through its seamless AWS integrations. Whether you’re processing customer interactions, meetings, or media files, Transcribe provides a scalable solution that enhances the way you interact with and analyze audio content. Start exploring the power of speech recognition in your projects and unlock new insights from your audio data. For additional information on related AWS services, check out: Amazon S3 Documentation AWS Lambda Documentation Amazon Comprehend Documentation Amazon Translate Documentation Amazon DynamoDB Documentation Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Demo of Rekognition recognizing an image,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Demo-of-Rekognition-recognizing-an-image,"AWS Solutions Architect Associate Certification Services Data and ML Demo of Rekognition recognizing an image In this guide, we explore Amazon Rekognition's powerful image analysis capabilities. Learn how to leverage label detection, image property analysis, image moderation, facial analysis, and face comparison to enrich your applications with advanced image recognition. Label Detection Amazon Rekognition's label detection feature automatically identifies objects, scenes, and activities in an image. By uploading an image, the service detects and labels various objects such as cars, people, skateboards, and shoes. Below is an illustration of a real-world scenario where the service accurately tags objects in an image: For instance, by testing with an image of a crosswalk, Rekognition identifies multiple details including road tarmac, zebra crossing, bag, handbag, person, car, wheels, and traffic lights. Every detected object is carefully mapped with its respective location, providing detailed information useful for real-time applications. Image Properties Another impressive feature of Amazon Rekognition is its ability to analyze image properties. When an image is uploaded, the service returns essential details such as dominant colors, foreground and background distinctions, and overall quality metrics. By understanding these properties, developers can better manage and optimize images for various use cases like automated image enhancements or content organization. Image Moderation Managing user-generated content is simplified with image moderation. Amazon Rekognition evaluates images to detect any potentially inappropriate or explicit content. This functionality helps ensure that only appropriate images appear on your website or application. Note Image moderation is crucial for platforms that rely on user-uploaded content to maintain community standards and prevent inappropriate material. Facial Analysis Facial analysis is another robust capability of Amazon Rekognition. By uploading a face-containing image, the service detects facial features and attributes such as gender, age range, facial expressions, and additional details. For example, an analysis of an image of a woman might confirm the presence of a face with 99.9% confidence, classify the subject as female with an age range of 18 to 24, and note attributes like a calm expression with a closed mouth and open eyes. This detailed facial analysis enables more intelligent applications, from personalized marketing to advanced security systems. Face Comparison Face comparison allows you to verify whether the same individual appears in different images by comparing a reference image with one or more target images. Rekognition calculates a similarity score for each comparison, providing a precise match—such as a 99.9% similarity score confirming the identity in one of the target images. This feature is especially useful in security systems, user authentication, and systems that require identity verification across different datasets. Additional Capabilities In addition to the features described above, Amazon Rekognition can identify celebrities and detect text within images. For example, if an image contains a mug with text saying ""Monday, but keep smiling,"" Rekognition can extract and recognize that text automatically. Integrate the Amazon Rekognition SDK into your application to analyze images in real time, whether they are user uploads or other media. This empowers your application to adapt instantly based on the content of the images. Final Note This overview highlights some of the core features of Amazon Rekognition. As you integrate these capabilities, consider exploring more advanced features to fully leverage the potential of visual recognition in your applications. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Augmented AI,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Augmented-AI,"AWS Solutions Architect Associate Certification Services Data and ML Augmented AI Welcome back, future solutions architects. I'm Michael Forrester, and in this article, we delve into the exciting world of augmented AI. Learn how to integrate human judgment seamlessly into machine learning workflows to boost prediction accuracy and continuously improve your models. Amazon Augmented AI (A2I) is an AWS service that integrates human reviewers into your machine learning process. When an ML model processes input data, it outputs predictions with varying confidence levels. For predictions with low confidence—such as uncertain labels—the system automatically triggers a human review, ensuring that even borderline cases receive expert validation. Below is a flowchart illustrating the augmented AI process. The diagram shows how input data is analyzed by an AI/ML model and, based on the prediction's confidence level, either accepted automatically as high-confidence output or forwarded for human review when needed: Note Augmented AI allows you to designate specific low-confidence data for human review, ensuring that each critical decision—like verifying if an image indeed contains a dog—is confirmed by a subject matter expert. A2I is designed for flexibility. You can choose to have your internal teams or third-party vendors, such as Amazon Mechanical Turk , handle these human reviews. This service integrates with several AWS labeling services including Textract , SageMaker , Rekognition , and Comprehend . This integration simplifies the creation of built-in human review workflows for applications like content moderation and text extraction without the hassle of writing extra code. One of the primary benefits of augmented AI is its continuous learning capability. Feedback from human reviewers is fed back into the machine learning model, allowing it to learn from previous mistakes and improve its accuracy over time. Furthermore, the system scales automatically with the volume of predictions requiring human review: If you rely on Mechanical Turk, scaling is managed seamlessly. If you use your own workforce, ensure you have enough available reviewers to manage the workload within your target timeframe. The image below highlights five key features of augmented AI: Easy integration with ML services Built-in human review workflows Access to human reviewers Continuous learning and improvement Seamless scaling In use cases like language translation where nuance is paramount, a translation service may sometimes generate low-confidence outputs. Human review in these instances ensures the translations are both nuanced and correct. Both reviewed outputs and automatically accepted high-confidence results can be stored in Amazon S3 for later use. The following flowchart demonstrates this process: input data is translated using Amazon Translate ; low-confidence translations are sent for human review, while high-confidence outputs are directly stored in Amazon S3. Additionally, when augmented AI identifies data that requires human validation, it can automatically dispatch review jobs to Mechanical Turk. Once the human experts verify the data, the results are reintegrated into the augmented AI process, maintaining high standards of accuracy throughout your workflow. Key Takeaway Amazon Augmented AI is an indispensable solution when you need a reliable human second opinion to validate machine learning predictions and continuously refine your models with expert feedback. For more detailed information on integrating human review into machine learning workflows, check out these resources: Amazon Augmented AI (A2I) Amazon Mechanical Turk Amazon S3 Thank you for reading! If you have any questions or need further assistance, feel free to reach out to me on Slack. Michael Forrester Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Comprehend,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Comprehend,"AWS Solutions Architect Associate Certification Services Data and ML Comprehend Welcome back, future Solutions Architects. In this lesson, Michael Forrester introduces Comprehend—a robust text analysis service by AWS that extracts valuable insights from textual data, including emails, documents, and newsletters. Comprehend analyzes text by: Identifying speakers. Extracting key phrases (e.g., ""Apple"", ""stocks"", dates). Recognizing languages. Performing sentiment analysis (informative, positive, neutral, or mixed). Determining discussion topics (such as politics, sports, or technology). Additionally, Comprehend offers syntax analysis and, crucial for exam preparation, detects personally identifiable information (PII). If your text contains sensitive data like a credit card number or address, Comprehend pinpoints this information so you can take steps to mask or remove it, ensuring customer data protection. Managed Service Benefits Being a fully managed AWS service, Comprehend supports multiple languages and integrates seamlessly with other AWS services in near real time. Sentiment Analysis Use Case for Product Reviews Consider a scenario where you need to analyze sentiment from customer product reviews. Here’s a high-level workflow: Data Upload: Multiple product reviews are uploaded to an Amazon S3 bucket. Reviews can be stored as individual files or aggregated in a single file. Lambda Trigger: An AWS Lambda function is triggered upon upload, which then calls Comprehend to analyze the sentiment of each review. NLP Processing: Comprehend processes the reviews using natural language processing (NLP) techniques and returns detailed sentiment scores, storing the results back in the S3 bucket. Once Comprehend has processed the data, you can use Amazon Athena for further analysis. Athena enables you to: Aggregate review data. Filter results based on specific criteria. Prepare data for visualization. To visualize insights, Amazon QuickSight can be used to display trends, such as comparing positive versus negative review distributions over time or analyzing sentiment differences across product categories. Summary Comprehend is a powerful natural language processing service that handles sentiment analysis, entity recognition, keyword extraction, topic modeling, and PII detection. Its capability to integrate with a suite of AWS services makes it particularly effective for real-world applications like product review analysis. If you have any questions, feel free to reach out on Slack. Otherwise, we'll see you in the next lesson. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Demo AWS Translate,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Demo-AWS-Translate,"AWS Solutions Architect Associate Certification Services Data and ML Demo AWS Translate In this article, we demonstrate how to use AWS Translate to convert text from one language to another. Whether you need real-time translation for quick text or batch translation for files, AWS Translate offers flexible workflows to suit your needs. Real-Time Translation To begin, navigate to the AWS Translate console. Within Amazon Translate, you will find two main options: real-time translation and batch translation. When you select real-time translation, you will notice options for specifying the source and target languages. Simply copy the text you want to translate, paste it into the text input box, and let AWS Translate handle the rest. If the auto-detect option is enabled, the service automatically identifies the input language. Currently, the service is set to auto-detect the language, correctly identifying English for the provided sentence. You also have the option to manually specify the source language if necessary. For example, to translate the text into Spanish, simply choose ""Spanish"" as the target language. The same process applies if you select another target language such as French or any other supported language. In addition to text input, AWS Translate also supports document translation. For document translation: Specify the source and target languages. Upload your file. Indicate the document type (e.g., a text file, HTML, or a .docx file) to ensure proper handling by the service. Quick Tip For best results, always verify that the correct source and target languages are selected before initiating the translation. Batch Translation Batch translation is ideal for processing multiple files or large documents. To create a batch translation job, follow these steps: Provide a unique job name. Specify the source language. Indicate the S3 bucket where the files to be translated are stored. Define the file format. Choose the location where the translated files will be saved (either in a different S3 bucket or in a specific folder within the same bucket). The batch translation process mirrors that of real-time translation—determine the source and destination languages, and select the appropriate files for translation. For illustration, here is another example related to real-time translation: Suppose you aim to translate the following sentence into Spanish: ""I graduated college in 2013. And I have over 10 years of experience working with Amazon Web Services. I mainly configure the networking side of things, like configuring virtual private clouds."" When translating, each word is processed individually. For example, ""virtual private clouds"" might be translated directly as ""Nubes Privadas Virtuales."" However, direct translations may not always capture the industry-specific meaning accurately. Important If you are not fluent in the target language, be cautious with direct translations as they might alter the intended meaning of technical terms. Custom Terminology AWS Translate offers custom terminology to ensure that industry-specific terms are preserved during the translation process. This feature allows you to manually define translations for specific phrases, preventing them from being altered by the auto-translation process. To set up custom terminology: Navigate to the ""Custom Terminology"" section. Click on ""Create Terminology."" Name the terminology file (for example, ""AWS term""). Provide the terminology file in one of the supported formats: CSV, TMX, or TSV. Specify the translation directionality. Below is an example CSV file format ensuring that ""virtual private clouds"" remains unchanged across multiple languages: en,fr,es
virtual private clouds,virtual private clouds,virtual private clouds This CSV file instructs AWS Translate to keep the term ""virtual private clouds"" intact in English, French, and Spanish. Once your file (e.g., ""custom-terminology.csv"") is ready, upload it to create your custom terminology entry. Afterward, return to the real-time translation interface, paste your sentence again, and select your desired target language. By enabling custom terminology in the additional settings and choosing your custom terminology (e.g., ""AWS term""), ""virtual private clouds"" remains unchanged in the translated output. Leveraging custom terminology ensures your translations are technically accurate and contextually appropriate, especially when handling specialized content. Summary AWS Translate is a versatile tool that supports both real-time and batch translation workflows. By incorporating custom terminology, you can maintain consistency in industry-specific language across different translations, ensuring clarity and precision in your communication. For further reading and additional resources, check out the following links: AWS Translate Documentation AWS Solutions Architect Certification Guides With these techniques, you can enhance your translation workflows, whether you're translating individual texts or managing complex document translations with AWS Translate. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Fraud Detector,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Fraud-Detector,"AWS Solutions Architect Associate Certification Services Data and ML Fraud Detector Welcome back, Future Solutions Architects. In this article, we explore one of AWS's cutting-edge machine learning services: Fraud Detector. Similar to AWS Forecast , Fraud Detector is a fully managed service that empowers users to build, deploy, and manage fraud detection models—even without prior experience in machine learning. Fraud Detector significantly reduces online payment fraud by identifying suspicious transactions in real time. Leveraging over 20 years of Amazon's expertise, it analyzes transaction data using advanced machine learning techniques to generate a risk score. This allows users to create customized models that determine whether a transaction should proceed, be flagged for review, or require further scrutiny, all without the need for deep machine learning knowledge. Managed Service Advantage Since Fraud Detector is managed by AWS, it takes care of the complex aspects of the process—from identifying relevant data to model creation, training, and deployment. This results in near real-time fraud detection responses, typically within milliseconds. How Fraud Detector Works The service offers a range of pre-built detectors and customizable fraud templates, making it ideal for common scenarios such as online payment fraud and fraudulent account registrations. It also allows businesses to merge their own data and rules, tailoring the detection process to unique business needs. Key aspects include: Real-Time Analysis: Processes transactions instantly to generate a confidence score that indicates potential fraud risk. Custom Outcomes: Enables setting outcomes like automatic approval, manual review, or flagging for additional investigation. Scalability & Security: Designed with built-in scalability, encryption for data in transit and at rest, and robust access management. Security Reminder Always integrate Fraud Detector with strict access controls using AWS Identity and Access Management (IAM) to ensure that only authorized users can access sensitive data. Setting Up Fraud Detector The process to set up Fraud Detector is streamlined into several clear steps: Define the Business Use Case: Identify the specific fraud type you wish to detect. Data Provisioning: Provide historical or current transaction data relevant to your case. Model Recommendation: Fraud Detector analyzes the input data and suggests the best model type. Training the Model: The system trains the model with your data, producing risk confidence scores. Performance Evaluation: Assess the model’s effectiveness using the generated confidence scores. Adjust thresholds—for instance, flagging transactions with risk scores above 90% for review—and retrain if necessary. Deployment: Deploy the detector for real-time monitoring or batch review, continuously comparing new data with historical trends. Integrating with Client Applications Once deployed, client applications can send requests to the Fraud Detector model endpoints. The model evaluates the transaction risk and assigns a score. For high-risk transactions, the system can be configured to: Route the transaction for human review. Store the output in Amazon S3 for future model training and enhanced accuracy. Practical Example: Fake or Abusive Reviews Consider a scenario where Fraud Detector is employed to identify fake or abusive product reviews. When a customer submits a review, the service automatically scans the content for misleading or harmful behavior. This automated screening is crucial for handling the high volume of reviews, thereby preventing overwhelming manual investigation and ensuring operational efficiency. Conclusion AWS Fraud Detector is a simplified yet powerful tool for combating fraud. It enables organizations to integrate sophisticated fraud detection capabilities into their applications rapidly, without needing extensive machine learning expertise. This service not only streamlines operations and minimizes false positives but also adapts to the ever-evolving tactics employed by fraudsters. I'm Michael Forrester. Thank you for reading this article—I look forward to our next discussion. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Textract,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Textract,"AWS Solutions Architect Associate Certification Services Data and ML Textract Welcome back, Solutions Architects. In this article, presented by Michael Forrester, we explore the power of language processing with AWS Textract—a fully managed machine learning service designed to automatically extract text and data from scanned documents. Textract efficiently processes various document types, such as handwritten notes, PDFs, and more, converting them into standardized digital formats that include tables, formatted text, and key data points. Once the extraction is complete, you can either download the data or store it in a database to meet your business requirements. For example, imagine medical reports being ingested into a HIPAA-compliant database to enhance patient-doctor communication, improve accuracy, and provide deeper insight through detailed analyses. Key Capabilities Textract enables you to: Extract raw text from documents. Identify key-value pairs by correlating form data with extracted text. Extract structured table data. Recognize signatures within documents. Return results in multiple formats (JSON, CSV, TXT). Execute queries against specific document information. In addition to text extraction, AWS Textract also supports form and table extraction, as well as signature recognition. Its seamless integration with other AWS services, such as Lambda, makes it possible to build robust document processing pipelines. For example, you can send the extracted text to AWS Comprehend for sentiment analysis, thereby chain-processing your documents without any hassle. Its scalability is exceptional—even when working with challenging handwritten texts, Textract often requires minimal post-processing corrections. In my own experience, I have tested Textract with some of the most challenging handwritten notes in English. Despite encountering a few minor errors with technical jargon, the service consistently produced meaningful, usable text with very little editing required. Textract's scalability is practically limitless. I have processed millions of documents in just a few hours—all without the need for specialized machine learning expertise. The typical architecture of Textract follows this process: Documents are sourced and stored in Amazon S3. An AWS Lambda function is triggered to call Textract. Textract processes the document. The extracted data is output back to S3 or can be directly downloaded via the AWS Management Console. For those new to AWS Textract, I recommend uploading a sample document with handwritten notes to your AWS account and observing how Textract transforms it into digital text. In another example workflow, after Textract processes a document uploaded to S3 through a Lambda function, the extracted text is stored in a DynamoDB table. This structured data can then be retrieved for further processing or analysis. Next Steps This article has covered the core functionality of AWS Textract: extracting text from scanned documents and converting it into a digital format for further use. If you have any questions or need further assistance, please join us on Slack. We look forward to connecting with you in our next lesson. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Demo of Athena in Action,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Demo-of-Athena-in-Action,"AWS Solutions Architect Associate Certification Services Data and ML Demo of Athena in Action In this guide, we demonstrate how to leverage Amazon Athena to query CSV data containing sample user information. The CSV file, which holds roughly 10,000 entries, includes dummy records for users (or contacts) with attributes such as ID, first name, last name, primary and secondary email addresses, profession, join date, and country. Below is a snippet from the CSV file: id,firstname,lastname,email,email2,profession,date joined,Country
100,Calla,Judy, [email protected] , [email protected] ,developer,1943-10-17,UA
101,Letizia,Lanita, [email protected] , [email protected] ,firefighter,1995-02-08,AD
102,Jorry,Zuzana, [email protected] , [email protected] ,developer,2005-01-01,GY
103,Dorothy,Kannry, [email protected] , [email protected] ,doctor,1921-02-15,LR
104,Annora,Burkle, [email protected] , [email protected] ,firefighter,1912-08-26,PN
105,Alyssa,August, [email protected] , [email protected] ,worker,2014-06-20,ET
106,Fanchon,Grobe, [email protected] , [email protected] ,doctor,1902-09-17,UY
107,Atlanta,Daveta, [email protected] , [email protected] ,firefighter,2015-04-04,KE
108,Kalina,Durware, [email protected] , [email protected] ,firefighter,1944-12-13,IL
109,Meg,Henegry, [email protected] , [email protected] ,developer,1909-01-13,GE
110,Raina,Earlie, [email protected] , [email protected] ,police officer,1991-08-23,KN
111,Marsiella,Noman, [email protected] , [email protected] ,police officer,1993-12-24,GU
112,Dode,Colbert, [email protected] , [email protected] ,police officer,2008-09-30,SG
113,Leona,Aida, [email protected] , [email protected] ,developer,1994-02-25,CL
114,Bertine,Stanwood, [email protected] , [email protected] ,doctor,2008-01-10,US
115,Paulita,Gahl, [email protected] , [email protected] ,doctor,1932-12-14,HN Overview This demonstration involves uploading the CSV file to an S3 bucket and then querying the data using Amazon Athena. All steps—from uploading data to executing SQL queries—are covered in this guide. Uploading the CSV to S3 and Configuring Athena Once you have uploaded your CSV file to an S3 bucket, the next step is to set up Amazon Athena to query the data. Follow these steps: Navigate to the Amazon Athena Console . Clean up any default databases or test queries by removing unnecessary test databases. For example, you can drop a pre-existing test database using: DROP DATABASE test; Click on ""Create Table"" and follow the on-screen prompts to define a new table linked to your CSV data. During the set-up, you need to provide: Table name and database name (either an existing one or a new one). S3 location of your CSV file. File format , which in this case is CSV. Specify the details of the CSV such as the field delimiter (a comma) and configure each column's data type. For example, map the ID to an integer type, text fields such as first name, last name, and emails to string types, and the join date to a date type. The following image shows the AWS Athena console, where you configure the table details and database settings: Upon completing the form, Athena will generate a SQL statement similar to the one below to create an external table: CREATE EXTERNAL TABLE IF NOT EXISTS `athena_demo`.`user` (
  id int,
  firstname string,
  lastname string,
  email string,
  email2 string,
  profession string,
  date date,
  country string
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
WITH SERDEPROPERTIES ('field.delim' = ',')
STORED AS INPUTFORMAT 'org.apache.hadoop.mapred.TextInputFormat' 
OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION 's3://athena-demo-kodekloud/'
TBLPROPERTIES ('classification' = 'csv'); This query creates an external table that maps directly to your CSV data on S3, making it possible to run SQL queries on the data. Querying Data with Amazon Athena To execute queries on your newly created table, follow these guidelines: Open a new query tab in the Athena console. To retrieve all the records from the table, use the following SQL statement: SELECT * FROM user; This query will return over 10,000 results corresponding to all the CSV entries. To limit output for testing purposes, use the SQL LIMIT clause: SELECT * FROM user
LIMIT 10; This query returns only 10 rows, providing a quick preview of your data. To filter data (for example, to display only users from the United States), run: SELECT * FROM user
WHERE country = 'US'; The image below shows the AWS Athena query editor with the results displayed in a tabular format, featuring columns such as ID, first name, last name, email, profession, and country: Cleaning Up Resources After you've completed your queries, it's a good practice to clean up your Athena environment by removing any tables or databases you no longer need. To delete the table, execute: DROP TABLE user; Then, to drop the database, use: DROP DATABASE athena_demo; Cleanup Reminder Cleaning up your resources helps prevent unnecessary charges and keeps your AWS environment organized. Conclusion This guide has demonstrated how to upload a CSV file to an S3 bucket and query it using Amazon Athena with standard SQL commands. By mapping an external table to your S3-based CSV file, Athena enables you to easily analyze large datasets. Enjoy using Amazon Athena for your data querying needs, and happy querying! For additional resources, consider exploring: Amazon Athena Documentation Amazon S3 Documentation Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Application Discovery Service,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Migration-and-Transfer/Application-Discovery-Service,"AWS Solutions Architect Associate Certification Services Migration and Transfer Application Discovery Service In this article, we explore the Application Discovery Service and its key role in the migration process. By gathering essential data about your on-premises applications and infrastructure, this service lays the foundation for effective migration planning. AWS Migration Hub then leverages this information to track and manage your migration journey. The Application Discovery Service collects comprehensive details about your environment, including: Infrastructure components Application dependencies Network traffic Performance metrics This data empowers businesses to make informed decisions and execute a seamless migration to the cloud. Understanding relationships and dependencies between applications, servers, and processes is crucial for planning migrations without disrupting critical services. The service supports two primary methods for data collection: Agent-Based Discovery and Agentless Discovery. Agent-Based Discovery Agent-based discovery involves deploying lightweight software agents directly on your on-premises servers. These agents continuously scan your systems to collect detailed information, including: Network traffic Running processes Performance metrics Configuration settings This approach provides an in-depth view of your current environment and a clear picture of interdependencies between applications and services. Agentless Discovery The agentless method simplifies deployment by using an agentless collector deployed as a virtual machine within your VMware vCenter server environment. This VM, which incorporates necessary agent functionalities, gathers essential information such as: Server profiles (CPU, RAM) Database metadata Utilization metrics This approach eliminates the need for installing individual agents on each server, making it a streamlined solution for data collection. A visual comparison of the key differences between the agent-based and agentless methods is provided below. This comparison can assist you in determining the best approach for your environment: Data collected by either method is securely transmitted to the Application Migration Service every 15 minutes over a TLS tunnel and stored in an Amazon S3 bucket for further analysis. Amazon Athena can be utilized to visualize and analyze this data, providing deeper insights into your migration readiness. Key AWS Integrations The Application Discovery Service integrates seamlessly with several AWS migration tools, such as AWS Database Migration Service (DMS), Migration Hub, and Migration Evaluator, enabling a coordinated migration strategy. Use Cases The Application Discovery Service supports several critical use cases, including: Discovering on-premises server and database inventories to accelerate migration planning. Mapping network communication patterns to uncover hidden dependencies and form migration groups. Collecting utilization data to right-size Amazon EC2 instances via insights provided by Migration Hub or AWS DMS. In summary, the Application Discovery Service delivers vital insights into your on-premises environment, laying the groundwork for an informed and smooth migration to AWS. For further reading on AWS migration tools, consider visiting the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Lex,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Lex,"AWS Solutions Architect Associate Certification Services Data and ML Lex Welcome, future architects! In this lesson, Michael Forrester introduces AWS Lex—our premier chatbot service that leverages the power of machine learning for building intuitive conversational applications. Imagine needing to create a hotel booking system where users interact with a chatbot to answer questions and provide reservation details interactively. For instance, a user might say, ""I want to book a hotel in London,"" and the chatbot will intelligently follow up with questions such as, ""When will you be coming and how long will your stay be?"" AWS Lex manages these nuanced user interactions seamlessly. Key Benefits of AWS Lex Fully managed service that scales automatically with demand. Seamless integration with AWS services such as Lambda, Cognito, and DocumentDB. Simplifies chatbot creation with built-in Automatic Speech Recognition (ASR) and Natural Language Understanding (NLU). How It Works AWS Lex is a highly abstracted machine learning service that allows developers to design and deploy chatbot-style workflows with minimal overhead. It integrates natively with various AWS services, enabling you to create dynamic and scalable applications with ease. In the diagram above, notice the inclusion of Amazon Cognito, which handles application authentication rather than AWS infrastructure security. For example, when a user logs into your website via Cognito, they gain access to Lex for interactive engagement. When Lex processes a user's request for hotel booking information, it can trigger an AWS Lambda function. This function then interacts with Amazon DocumentDB (Amazon's MongoDB-compatible database) to verify booking details or fetch available dates. The result is a responsive chatbot that guides the user with either an existing booking status or available booking dates. Core Features of AWS Lex AWS Lex brings two major features that make it a powerful tool for developing conversational interfaces: Automatic Speech Recognition (ASR): Converts spoken language into text, enabling voice-driven interactions. Natural Language Understanding (NLU): Interprets user input to determine intent, ensuring that conversations flow naturally. These core features empower you to create highly engaging applications that offer lifelike conversational experiences. Lex’s design simplicity and tight integration with other AWS services like Lambda, Cognito, and Polly amplify its functionality, allowing for rapid development across multiple platforms including mobile devices, chat apps, and IoT devices. Integration and Deployment AWS Lex’s flexibility facilitates the support of multiple messaging platforms, ensuring a consistent user experience—from edge devices to core backend systems. Its ease of deployment across platforms means that you can rapidly iterate and enhance your chatbot's functionality as your user base grows. Real-World Use Case Consider a scenario where a hospitality business uses Lex to automate hotel bookings. Integrated with Lambda and DocumentDB, the chatbot can quickly verify room availability, process reservations, and provide recommendations—all while delivering a seamless conversational experience. I'm Michael Forrester. If you have any questions, feel free to connect on Slack. Otherwise, I'll see you in the next lesson. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Rekognition,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Data-and-ML/Rekognition,"AWS Solutions Architect Associate Certification Services Data and ML Rekognition Welcome back! I’m Michael Forrester, and in this article, we dive into Amazon Rekognition—a robust machine learning service designed for comprehensive image and video analysis. Learn how Rekognition simplifies automatic tagging and categorization of images, making multimedia management faster and more efficient. Imagine you have an extensive image database on your website. Instead of manually assigning tags to every image, Rekognition leverages sophisticated machine learning algorithms to analyze image content automatically. For instance, when an image shows a dog riding a skateboard, Rekognition can intelligently tag it with labels such as ""dog,"" ""skateboard,"" or even ""dog riding skateboard."" This automated approach is invaluable for streamlining content management and significantly reducing manual effort. Amazon Rekognition employs deep learning techniques to detect objects, people, text, scenes, and activities in both images and videos. It can even identify inappropriate content, which makes it a crucial tool for content moderation on platforms where user-generated content must adhere to strict guidelines. Note Integrating Rekognition can help maintain community standards on platforms like Reddit, Pinterest, or Instagram by quickly filtering unsafe content. Rekognition is a fully managed service that seamlessly integrates with other AWS services. For example, you can set up an S3 bucket to trigger Rekognition—either directly or via AWS Lambda—each time a new image is uploaded. After processing, Rekognition generates detailed metadata and tags that can be stored in DynamoDB for further analysis. Additionally, for images with lower confidence scores, Amazon Augmented AI (A2I) can be incorporated to allow human reviewers to verify the tags before finalizing them. For example, if Rekognition returns a confidence score of around 70% for some images, you might opt for human review. In contrast, images with confidence scores above 85–90% can be automatically approved. This hybrid approach ensures a reliable, scalable content moderation workflow through tight AWS integration. Key Features of Amazon Rekognition Rekognition provides a variety of powerful features designed to enhance image and video analysis: Object and Scene Detection: Recognize diverse objects and settings from furniture to sunsets. Facial Analysis: Evaluate facial expressions to determine emotions such as anger, happiness, or sadness. Face Matching: Compare and match faces against a stored database to confirm identities. Text Detection: Extract text from images, including street signs, product labels, or license plates. Activity Detection: Identify actions like writing, eating, or jumping. Unsafe Content Detection: Automatically flag content that does not adhere to community guidelines. Celebrity Recognition: Identify celebrities and public figures across various industries. Real-Time Analysis: Process and analyze data in real time for immediate insights. Custom Labels: Train models to detect custom, domain-specific content. Advanced Facial Analysis: Determine details such as age range, gender, and nuanced emotions. Integrating Rekognition with AWS Services Amazon Rekognition integrates tightly with other AWS products, enabling you to build sophisticated, automated workflows for image and video analysis. Common integrations include: AWS Service Use Case Example Amazon S3 Storage and event triggering upon new image uploads Trigger Rekognition using S3 event rules AWS Lambda Serverless function execution Process images as they are uploaded Amazon DynamoDB Storage of processed metadata and tags Store image analysis results for querying Amazon Augmented AI Human review for unreliable results Improve accuracy for low-confidence images Warning Ensure that the confidence score thresholds are carefully calibrated to balance automation with quality control, particularly for sensitive content moderation scenarios. Conclusion Amazon Rekognition automates image and video analysis to facilitate efficient content moderation, image organization, and metadata generation. By integrating seamlessly with key AWS components such as S3, Lambda, and DynamoDB—and optionally leveraging Amazon Augmented AI for human review—Rekognition offers a scalable, reliable solution for managing multimedia content. We hope this comprehensive overview of Amazon Rekognition provides valuable insights into its capabilities and potential applications. Stay tuned for more technical deep dives in our upcoming articles! For more detailed information on AWS services and machine learning, visit the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Migration Hub,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Migration-and-Transfer/Migration-Hub,"AWS Solutions Architect Associate Certification Services Migration and Transfer Migration Hub In this lesson, we explore AWS Migration Hub—a powerful service from AWS designed to assist enterprises and organizations in planning and managing application and workload migrations to the AWS cloud. With a centralized view, Migration Hub enables you to monitor and track migration progress across multiple AWS and partner solutions efficiently. Discovery Phase Before initiating your cloud migration, gathering detailed information about your on-premises environment is crucial. AWS Migration Hub facilitates this through a discovery process that collects extensive data about your servers and services. You can choose from two discovery methods: Agent-based discovery: Install an agent on your servers to retrieve detailed metrics and insights. Agentless discovery: Collect essential data without installing any software on your servers. This process compiles information such as CPU utilization, memory usage, and network traffic, providing you with the necessary details to plan your migration accurately. Note For optimal accuracy, ensure that the chosen discovery method aligns with your current infrastructure and privacy requirements. Assessment Phase After completing the discovery phase, AWS Migration Hub transitions into the assessment phase. Here, the service analyzes the gathered data to recommend ideal target EC2 instance sizes and provide cost estimates. This phase addresses important questions such as: Am I selecting the optimal EC2 instance size for my workloads? Will this migration be cost-effective based on projected resource usage? These assessments simplify the planning process by offering actionable insights tailored to your specific environment. Migration Process Upon finalizing instance sizing and evaluating costs, you can move forward with the migration process. AWS Migration Hub serves as the central management console, integrating seamlessly with several AWS migration services, including: AWS Application Migration Service AWS Database Migration Service ...as well as various partner migration tools. During this stage, Migration Hub continuously provides updates on your migration progress until the entire process is successfully completed. Tip Integrate with multiple migration tools to leverage specialized functionalities and streamline the overall migration process. Summary AWS Migration Hub offers a streamlined and centralized approach for managing your migration projects. Its key features include: Centralized View: Monitor the entire migration process from a single dashboard. Application Cataloging: Discover and catalog on-premises applications, along with their dependencies, to prepare an effective migration plan. Organized Migration Groups: Group related applications and workloads to enhance management and tracking. Integration with Migration Tools: Seamlessly work with various AWS and partner migration services. Reporting and Analytics: Generate detailed reports and analytics to optimize migration strategy and effectiveness. Conclusion AWS Migration Hub simplifies your migration journey by offering a comprehensive set of tools to handle discovery, assessment, and management. This centralized console helps ensure your migration is efficient, well-planned, and cost-effective. By leveraging AWS Migration Hub, you can confidently transition your on-premises workload to the cloud while minimizing complexity and optimizing performance. For more detailed guidance, consider exploring additional resources on AWS Migration Hub Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Application Migration Service,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Migration-and-Transfer/Application-Migration-Service,"AWS Solutions Architect Associate Certification Services Migration and Transfer Application Migration Service In this lesson, we dive into AWS Application Migration Service, a powerful tool designed to streamline your migration to AWS. Building on the foundation provided by Migration Hub and the Application Discovery Service—which together help you plan, track, and analyze your applications and on-premises infrastructure—this service takes your migration process to the next level. Once the Application Discovery Service gathers initial data and insights for migration planning, AWS Application Migration Service performs the actual lift-and-shift operations. It automates converting your physical servers, virtual machines (VMs), and databases to run natively on AWS, thereby simplifying the migration process, reducing overall costs, and accelerating timelines. Notably, you can take advantage of 90 days free of charge. Key Benefits for Solutions Architects AWS Application Migration Service minimizes time-intensive, error-prone manual processes by: Automating the conversion of source servers to operate natively on AWS. Simplifying application modernization with both built-in and custom optimization options. Supporting a wide range of platforms including physical servers, VMware, Microsoft Hyper-V, and even other cloud providers. Enabling migrations for commonly used applications such as SAP, Oracle, and Microsoft SQL Server. Key Features of the Application Migration Service Assessment and Planning AWS Application Migration Service performs a comprehensive analysis of your on-premises environment, mapping dependencies and creating tailored migration plans for your applications. Replication and Continuous Data Sync With continuous replication and synchronization, the service ensures that your AWS environment always holds the most up-to-date data from your on-premises sources. Automated Cutover and Testing The service automates the cutover process to minimize downtime during migration. Built-in testing functionality verifies that your applications perform as expected in AWS before executing a full cutover. Migration Flow Overview The following scenario illustrates how AWS Application Migration Service orchestrates migration: Imagine your on-premises data center hosts two servers: the top server with two attached disks and the bottom server with three attached disks. On the right, your AWS environment is set up with specific subnets configured for migration: Staging Area: Hosts EC2 instances responsible for managing data replication from your on-premises environment to AWS. Migrated Resources Subnet: Serves as the deployment area for your servers after data replication is completed. The migration process begins by installing a replication agent on your on-premises servers. Once active, these agents communicate with the Migration (MGM) API endpoint to register with the service. Based on the received data, the Application Migration Service creates corresponding resources in the staging area. For example, if a server has a 10-gigabyte disk on-premises, the service creates a 10-gigabyte EBS volume in the staging area. This setup allows continuous replication of data from on-premises disks to the respective EBS volumes, ensuring an up-to-date copy of your data is always available. When executing a failover, cutover, or migration test, the service deploys the required servers within the migrated resources subnet as EC2 instances, sized appropriately and equipped with the latest snapshots from the replicated EBS volumes. Even after cutover, continuous replication persists from on-premises to the staging area. This approach enables you to remigrate with the latest data in case of issues or for additional testing. Integration with Other AWS Services AWS Application Migration Service seamlessly integrates with several AWS services, enhancing its capabilities: AWS Systems Manager : Facilitates automated run commands. Amazon S3 : Serves as a repository for fetching the latest configurations. Elastic Disaster Recovery: Enables automated disaster recovery mechanisms to protect your environment. Use Cases AWS Application Migration Service caters to a variety of migration scenarios, including: Use Case Description Migrating On-Premises Applications Supports applications such as SAP, Oracle, SQL Server, VMware vSphere, and Hyper-V. Cloud-to-AWS Migrations Assists in migrating cloud-based applications from providers like Azure or GCP to AWS. Inter-Region Migrations Enables migration of applications between different AWS regions. By embracing a step-by-step, incremental approach, AWS Application Migration Service ensures that you continually have access to up-to-date data, significantly reducing risks associated with data loss or downtime during the migration process. Further Reading For additional information on AWS Application Migration Service and best practices for moving to AWS, visit the AWS Documentation and Migration Hub . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,AWS Health Dashboard,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/AWS-Health-Dashboard,"AWS Solutions Architect Associate Certification Services Management and Governance AWS Health Dashboard In this lesson, you will learn about the AWS Health Dashboard—a powerful tool that provides alerts, guidance, and detailed insights into events that might impact your AWS environment. Whether you're managing a single service or an entire region, AWS Health ensures you stay informed about maintenance events, service interruptions, and other critical issues. Event Classification AWS Health Dashboard categorizes events into two main types: Public Events These events affect multiple accounts. For instance, if there is a disruption in the Amazon EC2 service that impacts all customers, AWS Health logs it as a public event. Private Events These events are specific to your AWS account. They may involve issues with a particular resource, such as an EC2 or RDS instance in your designated region. AWS Health provides detailed information about the affected resources, allowing for targeted remediation. Integration with Other AWS Services AWS Health Dashboard is designed to work seamlessly with services like Amazon CloudWatch, SNS, and EventBridge. This integration enables you to automate notifications and remediation processes, ensuring that your team receives near-instant alerts regarding any issues that could affect your applications. Tip For streamlined operations, consider configuring automated alerts through EventBridge or SNS. This helps minimize downtime and simplifies incident management. AWS Health Dashboard Infographic Below is an infographic summarizing five key features of AWS Health Dashboard: Key Benefits Implementing the AWS Health Dashboard within your AWS environment offers several advantages: Enhanced visibility into the performance and availability of AWS resources. Better understanding of how AWS service changes can affect your applications. Timely, relevant information that supports proactive event management. Advanced planning capabilities for AWS maintenance activities. A centralized dashboard that aggregates all event-related data, reducing the need to check multiple sources. Additionally, the AWS Personal Health Dashboard provides a customized experience for your account by offering detailed insights tailored to the performance and availability of the underlying AWS services that support your resources. Core Features Diagram The following diagram illustrates five core features of the AWS Health Dashboard ecosystem, emphasizing its role in reducing downtime and bolstering reliability: Summary The AWS Health Dashboard is an essential tool for maintaining the health and performance of your AWS environment. By providing centralized visibility into critical events and integrating with automation services, it helps reduce downtime and enhance reliability. The Personal Health Dashboard further enriches this experience with account-specific insights, ensuring that your team is well-prepared to handle any service-related events efficiently. For additional information on AWS services and monitoring best practices, consider exploring the following resources: AWS Health Documentation Amazon CloudWatch AWS EventBridge AWS SNS Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Prometheus,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/Prometheus,"AWS Solutions Architect Associate Certification Services Management and Governance Prometheus In this lesson, we explore Prometheus—a powerful open-source tool engineered to collect, store, and analyze metrics. With its dedicated time series database and flexible query language, PromQL, Prometheus offers deep insights into the health and performance of your applications and infrastructure. Monitoring an application's health is akin to keeping track of a person's well-being. Just as healthcare professionals monitor tests like blood sugar, cholesterol, and vitamin levels, infrastructure monitoring captures metrics like CPU utilization, memory usage, and application latency. These metrics are essential in ensuring that your systems operate within optimal ranges. Note Prometheus not only gathers current metrics but also stores historical data, enabling you to investigate trends and diagnose issues retrospectively. After collecting these valuable metrics, it is crucial to store them efficiently. For instance, if an incident occurs on a Monday, historical metrics from the previous weekend can help pinpoint the cause. Prometheus addresses this need by periodically pulling metrics from defined targets—such as servers, applications, or containers—and storing them in its built-in time series database. Users can then query this data through an HTTP server interface using PromQL. While similar in function to services like AWS CloudWatch , Prometheus offers the flexibility of an open-source system that adapts to virtually any platform. Prometheus Architecture Understanding the core components of Prometheus is key to leveraging its full potential. The primary elements include: Targets: These are the services or applications from which metrics are collected. Receiver: This component gathers metrics from targets by sending HTTP requests at defined intervals. Time Series Database: Metrics are stored here, making historical analysis efficient and responsive. HTTP Server: Provides an interface for querying metrics using PromQL, facilitating data analysis and visualization. Managed Prometheus on AWS Managing a Prometheus server can be challenging due to the necessity of ensuring its health, handling high traffic, and scaling resources as needed. Amazon Managed Prometheus simplifies these challenges by handling the underlying infrastructure, allowing you to focus on monitoring and analysis. Metrics from various services—such as those running on an Amazon Elastic Container Service (AWS ECS) cluster—can be seamlessly directed to the managed Prometheus instance. Once the data is in the system, you can query these metrics with PromQL or visualize them using dashboarding tools like Grafana. Key features of Amazon Managed Prometheus include: Automatic scaling to manage increased workloads. Seamless integration with AWS services such as Amazon Elastic Compute Cloud (EC2) , AWS EKS , AWS Lambda , and Amazon ECS . Reduced operational overhead, thanks to AWS managing the infrastructure. Robust support for PromQL, empowering you to filter, aggregate, and analyze your metrics efficiently. Cost-effectiveness with a pay-as-you-go pricing model. Integrating Prometheus with CloudWatch A common and powerful use case is integrating Prometheus with CloudWatch. Imagine a containerized application running not only in an EKS cluster but also in an on-premises environment. Prometheus employs service discovery to identify and monitor all relevant targets, and it allows you to configure alerting rules within its setup. In a standalone Prometheus setup, you might use Alert Manager to send notifications directly via email or Slack. When integrated with AWS, these alerts can be relayed to AWS CloudWatch , which then routes the notifications to your on-call team through channels like SNS. This ensures a prompt and systematic response to any incidents. Note For a deeper dive into how Prometheus compares with other monitoring solutions, explore our detailed articles on modern monitoring practices. In summary, Prometheus delivers a robust, scalable, and flexible monitoring framework that ensures your systems remain healthy and issues are resolved quickly. Its extensive ecosystem, integration with managed services, and comprehensive query capabilities make it an essential tool for proactive system management. Additional Resources For further reading and related topics, explore the following resources: Kubernetes Basics AWS CloudWatch Amazon Elastic Compute Cloud (EC2) PromQL Documentation Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Grafana,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/Grafana,"AWS Solutions Architect Associate Certification Services Management and Governance Grafana Grafana is a robust tool for visualizing metrics and creating interactive dashboards. In this guide, we explore how Grafana enhances data visualization by integrating with services like CloudWatch and Prometheus, providing more customization and analytics than the default UIs available in these services. Previously, we discussed monitoring solutions such as CloudWatch and Prometheus that collect metrics from various AWS services and applications. While CloudWatch offers basic log and metric viewing, it falls short for advanced dashboard creation. Grafana fills this gap by querying metrics from CloudWatch (or Prometheus) and rendering them into human-friendly visualizations. Grafana empowers you to build custom dashboards that consolidate critical metrics on a single page. Unlike the basic UI provided by CloudWatch , Grafana is engineered for advanced analytics and visualization. This benefit applies equally when using Prometheus as your metrics backend—Grafana’s intuitive interface turns complex data into accessible insights. Key Benefit Grafana transforms raw metrics into interactive and visually appealing dashboards, enabling you to monitor system performance effectively. Below is an overview of the top benefits of using AWS Managed Grafana: Feature Benefit Example Use Case Fully Managed Service AWS handles the underlying infrastructure and scaling. Focus on metrics analysis without server hassles. Interactive Data Visualization Create dynamic dashboards tailored to your operational needs. Monitor multiple systems on a single page. Unified Observability Integrates metrics, logs, and traces from various sources. Correlate data for effective troubleshooting. Integrated AWS Data Sources Supports AWS services like TimeStream, X-Ray, and CloudWatch . Centralized data source management. Single Sign-On (SSO) Integration Seamless authentication using corporate credentials. Enhance security and user experience. Grafana seamlessly integrates with a wide range of AWS services including TimeStream, X-Ray, and most notably, CloudWatch . This flexibility makes Grafana an ideal choice for visualizing operational data and monitoring system performance. Important For the Solutions Architect exam, remember that Grafana is not just about data visualization—its power lies in integrating data from diverse metrics sources like CloudWatch and Prometheus, providing a comprehensive view of system health. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Elastic Disaster Recovery,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Migration-and-Transfer/Elastic-Disaster-Recovery,"AWS Solutions Architect Associate Certification Services Migration and Transfer Elastic Disaster Recovery In this lesson, we explore AWS Elastic Disaster Recovery, a service designed to simplify and streamline disaster recovery by leveraging cloud technology. This guide explains why the service was developed, its key components, and the benefits it offers. Introduction Traditional disaster recovery infrastructure requires maintaining both a primary site and a backup site. This backup environment remains idle until failure occurs and demands constant maintenance to ensure it is up-to-date. Such an approach is not only expensive but also complex, often requiring dedicated resources and additional data center space. Cloud solutions like AWS help overcome these challenges by allowing businesses to use the cloud as their disaster recovery site. AWS Elastic Disaster Recovery significantly reduces downtime and data loss by enabling fast, reliable recovery of both on-premises and cloud-based applications. It utilizes cost-efficient storage, minimal compute resources, and point-in-time recovery to ensure that your applications stay operational even during a crisis. By leveraging AWS services such as S3, EC2, and EBS, organizations can quickly establish a disaster recovery site with minimal maintenance overhead. Components and Architecture To understand how AWS Elastic Disaster Recovery functions, consider the following key components in your on-premises and AWS environments: On-Premises (Source Servers): Install the AWS replication agent (downloadable from an S3 bucket) on every source server. Configure each replication agent to back up the selected disks on the servers. AWS Cloud Infrastructure: Staging Subnet: This is an AWS-managed subnet where an EC2 instance—acting as the replication server—handles data transfer from the replication agents to EBS volumes. Recovery Subnet: During a failover event, new servers are launched in this subnet. These servers boot from point-in-time snapshots of the EBS volumes, ensuring they have access to the latest data. In summary, the disaster recovery workflow involves: Installing the replication agent on source servers at the primary site. Continuously replicating data to AWS EBS volumes through a staging subnet. Launching recovery instances from snapshots in a dedicated recovery subnet during an outage. End-to-End Process The disaster recovery process using AWS Elastic Disaster Recovery follows these steps: On-Premises Setup: Install the replication agent on each of your servers (for example, two servers) and configure them to back up specific disks (e.g., two disks on server one and three on server two). These agents communicate with the DRS endpoint, informing the service of available source servers. Staging Area in AWS: Set up a staging area that includes: A replication server (an EC2 instance) responsible for managing data transfers. A corresponding set of EBS volumes for each disk being backed up. For instance, if you have five disks across your servers, there will be five associated EBS volumes. Note The replication server continuously transfers backup data from the replication agents to the configured EBS volumes. Failover Process: Upon a disaster or during a planned test: AWS launches the recovery subnet. Appropriate instances are started using snapshots taken from the EBS volumes. The recovery instances ensure your application runs in AWS with the latest replicated data. After the primary site is restored, data may be re-synced back from AWS if a failback is needed. Key Features AWS Elastic Disaster Recovery offers several notable features that help minimize downtime and ensure smooth recovery: Real-Time Data Sync and Point-in-Time Recovery: Data is continuously replicated so that the most recent snapshot is always available during failover. Automated Disaster Recovery Drills: Regular drills automatically validate that the replicated environment is ready and operational. Rapid Recovery: AWS can launch recovery instances within minutes, thereby reducing potential downtime. Integration with AWS Services AWS Elastic Disaster Recovery works seamlessly with key AWS services: EC2: Both the replication server (in the staging subnet) and the recovery instances are launched on EC2. S3: Snapshot storage and additional replication data are managed by S3. EBS: Acts as the replicating storage medium for the backed-up data. When recovery instances are initiated, they automatically attach the EBS volumes containing the latest copies of your data, thus ensuring continuity of operations. Use Cases AWS Elastic Disaster Recovery is best suited for several scenarios, including: Use Case Description Example Rapid Operational Recovery Quickly recovering your operations after unexpected events such as software issues or hardware failures. Launching recovery instances in minutes during a disaster event. Cross-Cloud Disaster Recovery Executing disaster recovery from non-AWS environments to AWS for enhanced resiliency. Migrating recovery operations from Azure or on-premises resources. Intra-Cloud Regional Failover Failing over between AWS regions to mitigate regional outages and improve service reliability. Switching operations from one AWS region to another seamlessly. This high-level overview demonstrates how AWS Elastic Disaster Recovery can reduce costs, minimize maintenance complexity, and provide a robust disaster recovery solution to quickly restore operations during critical events. Warning Ensure that your replication agents are consistently updated and correctly configured to avoid data loss during an unexpected failover. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,AWS Mainframe Modernization,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Migration-and-Transfer/AWS-Mainframe-Modernization,"AWS Solutions Architect Associate Certification Services Migration and Transfer AWS Mainframe Modernization Discover how AWS Mainframe Modernization helps you modernize as well as migrate your legacy mainframe applications to AWS managed runtime environments. This solution not only transitions applications from on-premises systems to the cloud but also updates their runtime environments, ensuring optimal performance and scalability. AWS Mainframe Modernization provides a comprehensive toolkit for planning and executing your migration and modernization strategies. It enables you to analyze existing mainframe applications, update or develop new code using languages such as COBOL or PL/I, and set up CI/CD pipelines for continuous integration and delivery. Depending on your client's needs, you can choose between automated refactoring for modernization or a straightforward replatforming method. Refactoring Refactoring modernizes your legacy application artifacts so they can seamlessly run in modern cloud environments. This process involves code and data conversion and often requires rewriting portions of your application to optimize it for the cloud. The AWS Blueprints service automates much of this refactoring process, offering end-to-end support for migrating and modernizing your mainframe applications. The refactoring process includes: Evaluating your application inventory Assessing system dependencies Automatically transforming legacy code Capturing and managing various test scenarios Note Automated refactoring with AWS Blueprints can significantly reduce manual effort and ensure consistency throughout your modernization journey. Replatforming Replatforming is another modernization approach where you transfer your mainframe application to a new computing platform without modifying its source code. This method allows a quick migration to the cloud while preserving the existing codebase. If your primary objective is to move your applications to the cloud swiftly, then replatforming is the ideal strategy. However, if you also need to modernize your application architecture and code, refactoring would be the better choice. Features of AWS Mainframe Modernization AWS Mainframe Modernization offers a robust set of features to support your entire modernization lifecycle: Assess : Conduct comprehensive evaluations of your project scope and plan both migration and modernization strategies effectively. Refactor : Leverage AWS Blueprints to convert legacy programming languages into modern macroservices or microservices, update user interfaces, and revamp software stacks. Replatform : Effortlessly migrate your applications to the cloud without altering source code or requiring recompilation. Developer IDE : Utilize an on-demand integrated development environment that accelerates coding with smart editing, debugging, instant code compilation, and unit testing. Managed Runtime : Enjoy a managed execution environment that continuously monitors clusters, providing self-healing compute and automated scaling to support enterprise workloads. CI/CD : Benefit from built-in continuous integration and delivery capabilities that help deployment teams update code reliably and reduce time-to-market. Warning Ensure that you assess your organization's readiness and choose the approach—refactoring or replatforming—that best aligns with your modernization goals to avoid potential pitfalls during the migration process. For additional details on AWS modernization and related topics, explore AWS Documentation and Mainframe Modernization Resources . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,The Snow Family storage focused,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Migration-and-Transfer/The-Snow-Family-storage-focused,"AWS Solutions Architect Associate Certification Services Migration and Transfer The Snow Family storage focused In this lesson, we explore the AWS Snow Family, a collection of physical storage devices designed to facilitate the secure and efficient transfer of massive data volumes between your on-premises environment and AWS. These devices act like portable hard disks, enabling you to bypass the bandwidth, cost, and security challenges often associated with large network-based data transfers. Transferring extensive amounts of data (multiple terabytes to petabytes) via traditional networks can be slow, expensive, and risky. The AWS Snow Family addresses these challenges by physically transporting your data, making it a robust alternative for large-scale migrations. Main Features of the AWS Snow Family Before diving into the specifics of each device, let’s review some of the key features and benefits: Rugged and Portable: Many of these devices are built with ruggedized cases to withstand rough handling. For instance, the AWS Snowcone is a compact device weighing only 4.5 pounds, while the AWS Snowball weighs less than 50 pounds. Their durability makes them convenient for transport in challenging environments. NFS Mount Point Support: The devices support NFS versions 3 and 4.1, which allows seamless integration with on-premises servers and file-based applications. The file system metadata remains intact until files are converted into objects when migrated to an Amazon S3 bucket. Onboard Compute Capabilities: With built-in computing resources, you can process and analyze data at the edge. The devices support running specific Amazon EC2 instances and AWS IoT Greengrass functions. During the ordering process, you can choose preferred EC2 AMIs (including those for IoT Greengrass) that are pre-loaded onto the device. Data Security: All data transferred is automatically encrypted using 256-bit encryption keys managed by AWS KMS. These keys are never stored on the devices, ensuring robust data security during transit. Offline Data Migration: The Snow Family devices support efficient offline data migration by shipping the hardware loaded with your data to your designated AWS region. Remember to configure your S3 buckets before ordering a Snowball device to streamline the transfer process. Tamper-Proof Design: Each device features a trusted platform module that establishes a hardware root of trust, protecting your data and preventing device tampering. End-to-End Tracking: Innovative shipping labels allow for real-time tracking of your data migration progress via the AWS console, Amazon SNS notifications, and text messages. After the transfer to AWS, the device is securely wiped clean to protect your data. Overview of AWS Snow Family Devices AWS Snowball Edge The AWS Snowball Edge is a 50-pound device that combines significant onboard storage with compute capabilities. It is ideal for local processing and running edge computing workloads while also transferring data between your on-premises environment and AWS Cloud. AWS Snowcone AWS Snowcone is an ultra-portable, secure, and rugged device designed for environments where space and power are limited. It supports both offline data transfer (via shipping) and online transfer using AWS DataSync, making it an excellent choice for remote locations such as construction sites, ships, or military deployments. AWS Snowball Distinct from the Snowball Edge, the standard AWS Snowball is a suitcase-sized device available in two configurations: Snowball Edge Compute Optimized: Offers up to 104 vCPUs for intensive compute applications. Snowball Edge Storage Optimized: Provides up to 80 terabytes of hard disk capacity alongside 210 terabytes of NVMe storage for large-scale data migration. This flexibility allows you to select a device that best meets your compute or storage requirements. AWS Snowmobile For organizations facing exabyte-scale data migrations, AWS Snowmobile is the solution. This truck-sized device can transfer up to 100 petabytes of data, making it ideally suited for extremely large-scale data transfers where traditional methods would be impractical. Use Cases for the AWS Snow Family Edge Computing and IoT: In environments with intermittent or limited connectivity, devices like the Snowball Edge and Snowcone act as local hubs that can process, analyze, and handle data—including image, video, or AI/ML analytics—directly at the edge. Note Leveraging edge computing can significantly reduce latency and improve responsiveness in remote applications. Large-Scale Data Migration: For enterprises needing to migrate vast amounts of data from on-premises data centers to AWS, both the standard Snowball and the Snowmobile offer secure and efficient solutions. Their robust design ensures that data is transferred reliably and securely at scale. Integration Capabilities While the primary function of the Snow Family is to transfer data to Amazon S3, these devices also integrate with AWS Lambda. This integration enables you to execute compute tasks directly on the devices, providing advanced processing capabilities for supported workloads. Tip Integrating AWS Lambda with the Snow Family can streamline workflows by processing data locally before or during migration. Summary The AWS Snow Family offers versatile solutions to physically transfer data into AWS, catering to a wide range of applications—from small-scale edge deployments to exabyte-level data migrations. With features such as rugged durability, onboard compute capabilities, robust security, and comprehensive tracking, these devices provide an efficient and reliable method for migrating large volumes of data while maintaining integrity and security throughout the process. For more detailed information on the AWS Snow Family and additional AWS storage solutions, be sure to explore the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,CloudWatch,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/CloudWatch,"AWS Solutions Architect Associate Certification Services Management and Governance CloudWatch In this lesson, you will learn about AWS CloudWatch, a robust monitoring service that provides real-time insights into AWS resources and applications. CloudWatch empowers you to collect, track, and analyze various metrics and logs from multiple sources, generate notifications when specific thresholds are breached, and centrally manage system logs for deeper analysis. CloudWatch automatically ingests logs and metrics from numerous AWS services such as EC2 , RDS , and Lambda . This integration offers instant visibility into the performance and overall operation of these services. Additionally, developers can use the CloudWatch SDK to send custom logs and metrics, which is especially useful for tracking application-specific data like API latency, response times, and error counts. With the CloudWatch Logs API, you also have programmatic control to push logs directly into the service. Once CloudWatch receives these logs and metrics, it extracts detailed monitoring data and triggers alarms based on user-defined thresholds. Serving as a centralized platform, CloudWatch allows you to view metrics, logs, and traces in one place, streamlining your monitoring and troubleshooting workflows. Key Components of CloudWatch Below is an overview of the primary components that make CloudWatch a comprehensive monitoring solution: Metrics Metrics are the foundational data points that indicate the performance of your services and applications. Common examples include CPU utilization and application latency. Alarms Alarms enable you to set thresholds on metrics. For example, you can configure an alarm to trigger when the average CPU utilization of an EC2 instance exceeds 80%. Logs CloudWatch Logs offer a centralized repository to store, view, and analyze logs from a variety of services and applications, both current and historical. Events (EventBridge) Formerly known as CloudWatch Events, EventBridge responds to state changes in AWS resources. With custom rules, you can automate responses to events, such as triggering a Lambda function when a new file is uploaded to an S3 bucket. Custom Dashboards Create personalized dashboards on CloudWatch’s customizable console homepage to display relevant metrics, logs, and data points for your operations. Insights Logs Insights provide the ability to perform advanced queries on your log data. This feature allows for sophisticated analysis and the extraction of complex metrics. Note CloudWatch also plays a critical role in the management of AWS Auto Scaling groups. For instance, alarms monitoring EC2 instance metrics within an autoscaling group can directly influence decisions that trigger scaling actions based on predefined thresholds. CloudWatch Capabilities CloudWatch empowers you to: Publish custom metrics in dedicated namespaces alongside AWS’s built-in metrics. Set up alarms to receive notifications when specific metric thresholds are exceeded. Collect and securely store logs from your infrastructure, resources, and applications. Stream near real-time system events using EventBridge. Create and configure interactive dashboards to consolidate your monitoring data. For more information on AWS CloudWatch and its extensive capabilities, please refer to the official AWS CloudWatch documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Database Migration Service,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Migration-and-Transfer/Database-Migration-Service,"AWS Solutions Architect Associate Certification Services Migration and Transfer Database Migration Service In this lesson, we explore the AWS Database Migration Service (DMS), a managed solution designed to help you quickly and securely migrate databases and analytics workloads to AWS with minimal downtime and zero data loss. DMS is to databases what Application Migration Service is to servers and applications – offering specialized tools that streamline the migration process. This service is ideal for moving databases from on-premises environments to the cloud, supporting over 20 different database and analytics platforms including MySQL, PostgreSQL, and more. Key Components of DMS DMS Fleet Advisor The DMS Fleet Advisor gathers data from multiple database environments to provide comprehensive insights into your data infrastructure. It supports popular databases such as Microsoft SQL Server, MySQL, Oracle, and PostgreSQL. A data collector is installed on your database server (for example, a MySQL on-premises server) and captures essential information, which is then reported to DMS. This data can be exported for further analysis. This process is similar to the discovery phase in other migration services, where an agent collects environment data to help you assess migration requirements. DMS Schema Conversion Tool The DMS Schema Conversion Tool is specifically designed to simplify migrations between different database types. It evaluates the complexity of the migration and converts database schemas along with associated code objects. For example, when switching from MySQL to PostgreSQL, the tool addresses subtle differences in schema definitions between the two systems. Note The Schema Conversion Tool is particularly beneficial when migrating between systems with differing SQL dialects, ensuring that your target environment is correctly configured. Replication Instance During the migration process, the replication instance continuously transfers data from your on-premises source to the AWS target environment. This ensures that the target always mirrors the most current data. AWS DMS connects to your data stores via endpoints, which function as access points for both the source and the target databases. When configuring these endpoints, details such as endpoint type (source or target), database engine (e.g., Oracle or MySQL), server address, port number, encryption options, and credentials must be specified to guarantee secure connectivity. After defining the endpoints, you create a replication task by selecting the replication instance and specifying the source (on-premises) and target (AWS) endpoints. This task then manages the migration process seamlessly. Migration Types DMS supports several migration strategies to suit different use cases: Full Load: Migrates all current data from the source to the target database while creating necessary tables. This method is suitable if you can allocate a brief period of downtime. Full Load plus CDC (Change Data Capture): Performs a complete initial data load and then captures any ongoing changes in the source database. This ensures that after the initial migration, the target database is continuously updated until final cutover. CDC Only: When a bulk initial load is handled by another method, DMS can be used solely for capturing and applying ongoing changes during the migration process, particularly in homogeneous environments using native tools. Continuous Data Replication and Schema Conversion One of the key strengths of AWS DMS is its ability to maintain continuous data replication between the source and target databases, keeping both environments in sync throughout the migration period. Additionally, AWS DMS provides automatic schema conversion. This feature adapts the source database schema to match the target. For example, it can map an Oracle VARCHAR2 data type to a PostgreSQL VARCHAR or convert schemas from various data warehouse formats to Amazon Redshift. Endpoints and Tasks Endpoints in AWS DMS represent source and target databases. For instance, an on-premises Oracle database serves as a source endpoint, while an Amazon Aurora database in the cloud serves as a destination endpoint. A migration task defines how data moves from the source to the target. These tasks can be started, stopped, paused, updated, and monitored to ensure real-time progress and receive actionable alerts. Warning Ensure that all endpoint configurations, including encryption and credential settings, are correct before initiating the migration to prevent potential data loss or security issues. Use Cases of Database Migration Service AWS Database Migration Service is a versatile tool that can be used in several scenarios: Use Case Description Example Data Migration Transfer databases from legacy systems or on-premises environments to AWS-managed cloud environments. Migrating an on-premise Oracle database to Amazon Aurora. Change Data Capture Continuously replicate and apply data changes between source and target databases in real time. Syncing a MySQL source with a PostgreSQL target. Integration with Data Lakes Facilitate the construction of data lakes and enable real-time processing of changes from various data stores. Ingesting and processing data for analytics on AWS. By leveraging its robust set of features, AWS DMS provides a seamless and efficient database migration experience with minimal disruption to your operations. For more information on database migration and other AWS services, explore the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Demo Create an S3 bucket with Python and the CDK,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/Demo-Create-an-S3-bucket-with-Python-and-the-CDK,"AWS Solutions Architect Associate Certification Services Management and Governance Demo Create an S3 bucket with Python and the CDK In this guide, you'll learn how to use the AWS Cloud Development Kit (CDK) with Python to create and manage an S3 bucket. We'll cover the entire process—from installing dependencies to deploying and cleaning up your AWS resources. ───────────────────────────── Installing Node.js and the AWS CDK CLI Before you begin, ensure that Node.js is installed on your machine. Node.js provides the JavaScript runtime required for the AWS CDK CLI, which is installed via npm. Download the appropriate installer for your operating system from the official Node.js website. Once Node.js is installed, open your terminal or command prompt and run the following commands: aws sts get-caller-identity
npm install -g aws-cdk
cdk --version If the output from cdk --help shows a list of commands and options, the installation was successful. ───────────────────────────── Overview of AWS CDK CLI Commands The AWS CDK CLI offers several commands for managing your infrastructure as code: cdk list : Lists all stacks in your CDK application. cdk synth : Synthesizes the CloudFormation template from your CDK app. cdk bootstrap : Sets up the CDK toolkit stack in your AWS account. cdk deploy : Deploys one or more stacks to your AWS account. cdk diff : Compares your local stack configuration with the deployed version. cdk destroy : Deletes the specified stack(s). cdk init : Initializes a new CDK project from a template. To explore all available commands, run: cdk --help For instance, to generate your CloudFormation templates from the CDK application, execute: cdk synth ───────────────────────────── Initializing a New CDK Project Kickstart your project using the CDK CLI by running: cdk init sample-app --language python This command generates a starter project that includes: A Python virtual environment. A requirements.txt file that lists dependencies like aws-cdk-lib and constructs . A basic project structure with a README , configuration file ( cdk.json ), and source files. Below is an excerpt from the generated app.py : #!/usr/bin/env python3

import aws_cdk as cdk
from cdk.cdk_stack import CdkStack

app = cdk.App()
CdkStack(app, ""CdkStack"")

app.synth() And here’s an example snippet from cdk_stack.py , which defines a sample stack: from constructs import Construct
from aws_cdk import (
    Duration,
    Stack,
    aws_iam as iam,
    aws_sqs as sqs,
    aws_sns as sns,
    aws_sns_subscriptions as subs,
)

class CdkStack(Stack):

    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        super().__init__(scope, construct_id, **kwargs)

        # Example: creating an SQS queue (currently commented out)
        # queue = sqs.Queue(
        #     self, ""CdkQueue"",
        #     visibility_timeout=Duration.seconds(300),
        # )

        # Example: creating an SNS topic and subscribing the above queue
        # topic = sns.Topic(self, ""CdkTopic"")
        # topic.add_subscription(subs.SqsSubscription(queue)) ───────────────────────────── Setting Up Your Python Environment To ensure that your dependencies remain isolated, set up a Python virtual environment: Create the virtual environment: python -m venv .venv Activate the environment: On macOS/Linux: source .venv/bin/activate On Windows: .venv\Scripts\activate.bat Install required packages from requirements.txt : pip install -r requirements.txt ───────────────────────────── Configuring an S3 Bucket in Your CDK Stack Enhance your CDK stack by adding an S3 bucket. Start by importing the S3 module and update your stack configuration as follows: from constructs import Construct
from aws_cdk import (
    Duration,
    Stack,
    aws_s3 as s3,
)

class CdkStack(Stack):

    def __init__(self, scope: Construct, construct_id: str, **kwargs) -> None:
        super().__init__(scope, construct_id, **kwargs)

        # Create an S3 bucket with KMS encryption enabled
        bucket = s3.Bucket(self, ""My-kodekloud-cdk-bucket"",
                           encryption=s3.BucketEncryption.KMS) Note Bucket names in AWS must be globally unique. If your chosen name is already in use, the AWS CDK may append extra characters to ensure uniqueness. After updating your stack, verify the CloudFormation templates by running: cdk synth The generated output should include the CloudFormation template with your S3 bucket details. ───────────────────────────── Configuring AWS Credentials Before deploying your resources, configure your AWS CLI with the necessary credentials: aws configure Enter your AWS Access Key ID and Secret Access Key when prompted. Although using full administrator access may be acceptable for demos, remember to follow the principle of least privilege in production environments. ───────────────────────────── Previewing Changes and Deploying the Stack Before deploying your changes, use cdk diff to inspect the differences between your local setup and the deployed stack: cdk diff If changes are detected—and if a ""bootstrap"" error occurs due to missing SSM parameters—execute the bootstrap command: cdk bootstrap Once bootstrapping is complete, deploy the stack: cdk deploy During deployment, AWS CloudFormation creates or updates your resources. A typical log output might resemble: CdKStack | 0/3 | CREATE_IN_PROGRESS | AWS::S3::Bucket | My-kodekloud-cdk-bucket
CdKStack | 1/3 | CREATE_COMPLETE    | AWS::CDK::Metadata  | CDKMetadata/Default
CdKStack | 2/3 | CREATE_COMPLETE    | AWS::S3::Bucket     | My-kodekloud-cdk-bucket
CdKStack | 3/3 | CREATE_COMPLETE    | AWS::CloudFormation::Stack | CdKStack Confirm that your S3 bucket appears in the S3 console: ───────────────────────────── Verifying and Cleaning Up Resources After deploying, run cdk diff again to validate that no discrepancies exist between your configuration and what has been deployed: cdk diff A message indicating ""There were no differences"" confirms that your stack is synchronized. When your testing is complete, clean up your resources by executing: cdk destroy Confirm the deletion in the prompt. Finally, verify in the CloudFormation and S3 consoles that all resources have been removed. ───────────────────────────── Conclusion In this tutorial, you've learned how to initialize an AWS CDK project with Python, set up an isolated virtual environment, configure an S3 bucket with KMS encryption, and deploy the stack using CloudFormation commands. The AWS CDK makes managing your infrastructure efficient by employing commands like synth, diff, deploy, and destroy. Happy coding, and see you in the next lesson! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,CloudFormation Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/CloudFormation-Demo,"AWS Solutions Architect Associate Certification Services Management and Governance CloudFormation Demo In this lesson, you’ll learn how to work with AWS CloudFormation by using a YAML template to deploy an EC2 instance. While CloudFormation supports both YAML and JSON, this demo focuses on YAML for its readability and simplicity. We'll walk through creating a file named stack.yaml that configures our CloudFormation stack. This template includes sections for defining resources such as EC2 instances and security groups, along with parameters and outputs to customize and retrieve important deployment details. Defining the EC2 Instance To start, we define an EC2 instance as a resource. First, we assign the resource a logical name (""Ec2Instance"") and specify its type as AWS::EC2::Instance , as outlined in the AWS documentation. Resources:
  Ec2Instance: Next, add the resource's Type and Properties. CloudFormation uses the Type field to identify the resource, and the Properties section allows you to specify configuration details for your instance such as security groups, tags, and AMI details. The example below highlights a standard configuration excerpt: Type: AWS::EC2::Instance
Properties:
  AdditionalInfo: String
  Affinity: String
  AvailabilityZone: String
  BlockDeviceMappings:
    - BlockDeviceMapping:
  CpuOptions:
    CpuOptions:
  CreditSpecification:
    CreditSpecification:
  DisableApiTermination: Boolean
  EbsOptimized: Boolean
  ElasticGpuSpecifications:
    - ElasticGpuSpecification:
  ElasticInferenceAccelerators:
    - ElasticInferenceAccelerator:
  InstanceId: String
  InstanceType: String
  IamInstanceProfile: String Following that, here’s a detailed configuration that specifies essential properties like the AMI, key pair, and tags: Resources:
  Ec2Instance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: ami-041feb57c611358bd
      KeyName: main
      Tags:
        - Key: Name
          Value: myEc2Instance For the appropriate AMI, navigate to the EC2 console, launch an instance, and choose a suitable Amazon Machine Image. Documentation Tip Refer to the AWS CloudFormation User Guide for a full list of configurable properties. Below is an image from the AWS documentation that illustrates the properties of an EC2 instance: The next image shows the EC2 console for instance selection: Adding a Security Group To enhance your deployment, you can add a security group resource that controls access to your instance. Begin by defining the security group with a logical name (""InstanceSecurityGroup""), its type ( AWS::EC2::SecurityGroup ), and a description. Resources:
  Ec2Instance:
    Type: AWS::EC2::Instance
    Properties:
      ImageId: ami-041feb57c611358bd
      KeyName: main
      Tags:
        - Key: Name
          Value: myEc2Instance
  InstanceSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Enable SSH access via port 22 Then, specify the ingress rules for this security group. In the example below, TCP port 22 is opened for SSH access from any IP address: Properties:
  ImageId: ami-041feb57c611358bd
  KeyName: main
  Tags:
    - Key: Name
      Value: myEc2Instance
InstanceSecurityGroup:
  Type: AWS::EC2::SecurityGroup
  Properties:
    GroupDescription: Enable SSH access via port 22
    SecurityGroupIngress:
      - IpProtocol: tcp
        FromPort: 22
        ToPort: 22
        CidrIp: 0.0.0.0/0 Port Range Tip If you intend to allow access over a range of ports, modify FromPort and ToPort accordingly. For a single port, both values remain the same. Finally, integrate the security group reference into the EC2 instance. CloudFormation enables you to refer to other resources using the !Ref intrinsic function: Resources:
  Ec2Instance:
    Type: AWS::EC2::Instance
    Properties:
      SecurityGroups:
        - !Ref InstanceSecurityGroup
      ImageId: ami-041feb57c611358bd
      KeyName: main
      Tags:
        - Key: Name
          Value: myEc2Instance
  InstanceSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Enable SSH access via port 22
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIp: 0.0.0.0/0 Adding Parameters for Dynamic Input To provide flexibility during deployment, you can introduce parameters for customizable values such as the EC2 instance name and key pair. The example below creates parameters for the key pair and instance name. By setting the type of KeyName to AWS::EC2::KeyPair::KeyName , CloudFormation displays a dropdown list of available key pairs. Parameters:
  KeyName:
    Description: The EC2 key pair
    Type: AWS::EC2::KeyPair::KeyName
  Ec2Name:
    Type: String
Resources:
  Ec2Instance:
    Type: AWS::EC2::Instance
    Properties:
      SecurityGroups:
        - !Ref InstanceSecurityGroup
      ImageId: ami-041feb57c611358bd
      KeyName: !Ref KeyName
      Tags:
        - Key: Name
          Value: !Ref Ec2Name
  InstanceSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Enable SSH access via port 22
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIp: 0.0.0.0/0 Adding Outputs Outputs enable you to extract and display key information after the stack is deployed. In this template, the public IP address of the EC2 instance is output using the !GetAtt function to access the PublicIp attribute. Outputs:
  PublicIp:
    Description: Server Public IP
    Value: !GetAtt Ec2Instance.PublicIp Complete CloudFormation Template Below is the final version of the CloudFormation template combining parameters, resources, and outputs: Parameters:
  KeyName:
    Description: The EC2 key pair
    Type: AWS::EC2::KeyPair::KeyName
  Ec2Name:
    Type: String
Resources:
  Ec2Instance:
    Type: AWS::EC2::Instance
    Properties:
      SecurityGroups:
        - !Ref InstanceSecurityGroup
      ImageId: ami-041feb57c611358bd
      KeyName: !Ref KeyName
      Tags:
        - Key: Name
          Value: !Ref Ec2Name
  InstanceSecurityGroup:
    Type: AWS::EC2::SecurityGroup
    Properties:
      GroupDescription: Enable SSH access via port 22
      SecurityGroupIngress:
        - IpProtocol: tcp
          FromPort: 22
          ToPort: 22
          CidrIp: 0.0.0.0/0
Outputs:
  PublicIp:
    Description: Server Public IP
    Value: !GetAtt Ec2Instance.PublicIp Deploying the CloudFormation Stack To deploy your CloudFormation stack: Open the AWS CloudFormation console . Click Create stack and choose to upload your template file. Select the stack.yaml file. Enter a stack name (e.g., ""my-deployment""). Specify the parameters: For the EC2 instance name, enter a desired value (for example, ""this is the server""). For the key pair, select the appropriate key from the dropdown. Click Next to configure additional options such as tags, rollback settings, or notifications. Review and submit the stack for deployment. The image below illustrates the AWS CloudFormation console during stack creation: After deployment, the stack status will change to ""CREATE_COMPLETE"". You can review the events and check resource details, including obtaining the EC2 instance's public IP address from the Outputs tab. The following image shows the stack deployment events in the CloudFormation console: Updating or Deleting the Stack If you need to make changes, update the stack using the template designer within the AWS CloudFormation console. To delete a stack, simply select it in the console and click Delete . This action removes all resources created by the stack. The image below shows the update interface in the AWS CloudFormation console: That concludes this lesson on AWS CloudFormation. Enjoy automating your infrastructure and check back for more detailed tutorials in our upcoming lessons! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,CloudFormation,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/CloudFormation,"AWS Solutions Architect Associate Certification Services Management and Governance CloudFormation In this article, we explore AWS CloudFormation and how it simplifies provisioning and managing AWS infrastructure as code. CloudFormation enables you to define your entire infrastructure in a template (in JSON or YAML), streamlining resource deployment, management, and updates. Traditionally, infrastructure was provisioned through a mix of scripts and manual processes—often documented in runbooks or stored in version control systems. This approach was prone to inconsistencies, unreliability, and a lack of repeatability when documentation or scripts were out of date. Infrastructure as Code Infrastructure as code (IaC) is a key DevOps practice that involves managing and provisioning infrastructure through code rather than manual processes. Think of it as the digital equivalent of constructing a building with detailed blueprints. Just like an architect uses standardized blueprints and toolkits to build consistent structures, a DevOps engineer uses code-based blueprints to deploy servers, databases, networks, and more. How CloudFormation Works AWS CloudFormation automates resource provisioning by reading your defined templates to create and manage stacks. Here’s a simplified workflow: Write the infrastructure code in a template. Submit the template to CloudFormation. CloudFormation reads the template and automatically provisions the defined AWS resources. A CloudFormation ""stack"" is a collection of AWS resources created and managed as a single unit. You can use separate stacks for different applications or environments (such as development, staging, and production). When you need to update your infrastructure—by adding new resources or modifying existing ones—you simply update your CloudFormation template. CloudFormation generates a change set that previews how your running resources will be impacted by the changes. After you review and approve the change set, CloudFormation applies the updates seamlessly. Key Features and Benefits AWS CloudFormation offers a range of benefits that streamline infrastructure management: Feature Benefit Example Usage Infrastructure as Code (IaC) Manage AWS resources with code-based templates that are easy to version and collaborate on. Storing templates in Git for version tracking and collaboration. Consistent and Repeatable Deployments Duplicate environments like development, staging, and production by reusing the same configuration. Creating a staging environment identical to production. Version Control Integration Maintain a clear history of configuration changes by storing templates in version control systems. Using Git to track and revert changes as needed. Resource Tracking Easily track, update, and manage groups of resources as a single unit. Managing a collection of resources as a CloudFormation stack. Efficiency and Cost Savings Reduce manual errors and save time by automating infrastructure deployment. Eliminating manual AWS console configurations for faster deployments and reduced operational costs. Integration with AWS Services CloudFormation integrates seamlessly with other AWS services and developer tools. For instance, you can store your templates in AWS CodeCommit and build CI/CD pipelines using AWS CodePipeline and AWS CodeBuild, enabling you to manage infrastructure changes just like application code. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Demo Move files from File Server to EFS using Datasync,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Migration-and-Transfer/Demo-Move-files-from-File-Server-to-EFS-using-Datasync,"AWS Solutions Architect Associate Certification Services Migration and Transfer Demo Move files from File Server to EFS using Datasync In this guide, we demonstrate how to use AWS DataSync to transfer files from an on-premises NFS server to an S3 bucket in AWS. The objective is to simulate a corporate data center environment and illustrate the seamless migration process using AWS DataSync. The lab setup emulates an on-premises scenario where an application server accesses an NFS server to read and write data. This data is then transferred to an S3 bucket via AWS DataSync. Although the simulation represents an on-premises environment, all components reside in AWS for simplicity. Below is the architecture diagram illustrating the data transfer flow from the corporate data center to AWS S3. The diagram shows an App Server, NFS Server, DataSync Agent, and the AWS DataSync service: Environment Overview An application server in our simulated environment connects to an NFS server running within an AWS VPC. The data destined for migration is stored on the NFS server and will ultimately be copied to an S3 bucket. The following diagram shows the deployed application server and its connectivity with the NFS server: To confirm connectivity, log in to the application server and run the command below to display mounted file systems: df -h The output indicates that the NFS server is mounted at /mnt/data with the source path 10.11.12.182:/media/data : Filesystem      Size  Used Avail Use% Mounted on
/dev             478M     0  478M   0% /dev
/dev/shm         486M     0  486M   0% /dev/shm
/run             486M   416M  485M   1% /run
/sys/fs/cgroup   486M     0  486M   0% /sys/fs/cgroup
/                8.0G   1.8G 6.3G  23% /
/run/user/1000   98M     0   98M   0% /run/user/1000
/media/data      8.0G   1.8G 6.3G  23% /mnt/data
user@ip-10-11-12-118 ~ $ Navigating into /mnt/data reveals a folder called images that contains numerous JPEG files destined for migration to the S3 bucket. For example: [ec2-user@ip-10-11-12-118 ~]$ cd /mnt/data
[ec2-user@ip-10-11-12-118 data]$ ls
images
[ec2-user@ip-10-11-12-118 data]$ cd images/
[ec2-user@ip-10-11-12-118 images]$ ls
00001.jpg  00018.jpg  00035.jpg  00052.jpg  00069.jpg  00086.jpg  00103.jpg  00120.jpg  00137.jpg  00154.jpg  00171.jpg  00188.jpg  00189.jpg  00199.jpg  00200.jpg
00002.jpg  00019.jpg  00036.jpg  00053.jpg  00070.jpg  00087.jpg  00104.jpg  00121.jpg  00138.jpg  00155.jpg  00172.jpg  00191.jpg  00200.jpg  00201.jpg
... Once the files are transferred, you can verify their presence by accessing the S3 bucket (named ""data sync - KodeKloud""), where the files appear under an ""images"" folder. Deploying the DataSync Agent The next step is to deploy the DataSync agent. While the agent can be deployed on any supported virtualization hypervisor (e.g., VMware, KVM, or Microsoft Hyper-V), this demonstration uses an EC2 instance in AWS. Refer to the AWS DataSync documentation for detailed information on deploying the agent in different environments, including VMware, EC2, AWS Snowcone, and AWS Outposts. For EC2 deployment, perform the following steps: Retrieve the latest AMI for the DataSync agent using AWS CLI: aws ssm get-parameter --name /aws/service/datasync/ami --region us-east-1 The output will be similar to: {
  ""Parameter"": {
    ""Name"": ""/aws/service/datasync/ami"",
    ""Type"": ""String"",
    ""Value"": ""ami-dd2f74d8a00ff34d"",
    ""Version"": 88,
    ""LastModifiedDate"": ""2023-10-13T19:30:54.383000+00:00"",
    ""ARN"": ""arn:aws:ssm:us-east-1:parameter/aws/service/datasync/ami"",
    ""DataType"": ""text""
  }
} Tip You can run this command in AWS CloudShell if your local AWS CLI is not configured. Launch an EC2 instance using the retrieved AMI. In the launch wizard, ensure that you update: Source Region: us-east-1 (or your respective source region) AMI: Paste the obtained AMI ID. Instance Type: Options like m5.2xlarge, m5.4xlarge, or c5.medium are recommended. For testing, a smaller instance might be sufficient. Key Pair: Select your designated key pair. VPC and Public IP: Choose the appropriate VPC and assign a public IP address. Security Group: For demonstration purposes, allowing all traffic is acceptable. In production, restrict access appropriately. After launching the instance, register the DataSync agent through the AWS DataSync service page: Click on ""Agents"" then ""Create Agent."" Select the EC2 option (since the agent is deployed on EC2) and leave the public service endpoint enabled. Retrieve the activation key by refreshing the EC2 instances list and noting the public IP address of the DataSync agent. Paste the public IP on the DataSync registration page, assign a name (e.g., ""data sync agent""), and create the agent. A successful registration output may look like: Successfully retrieved activation key from agent
Activation key
EVH1S-KH48T-0RF59-K77NG-V5MGL
Agent address
18.234.228.236 Creating DataSync Locations After deploying the agent, the next step is to define the source and destination locations where DataSync will copy the files. Source: NFS File Server Create a source location for the NFS file server by following these steps: Choose ""NFS"" as the location type. Select the DataSync agent deployed earlier. Specify the NFS server’s private IP address and set the mount path, which in this case is /media/data . An example of the connection test for the NFS mount is shown below: [ec2-user@ip-10-11-12-118 ~]$ cd /mnt/data
[ec2-user@ip-10-11-12-118 data]$ ls
images
[ec2-user@ip-10-11-12-118 data]$ cd images/
[ec2-user@ip-10-11-12-118 images]$ ls
00001.jpg  00018.jpg  00035.jpg  ...  00209.jpg Destination: S3 Bucket Define the destination location for your S3 bucket by following these guidelines: Select your S3 bucket (e.g., ""data sync - KodeKloud""). Use the ""standard"" storage class (or adjust as needed). Specify the bucket folder if required; for this demonstration, the root directory is used. Assign an IAM role for DataSync to access the S3 bucket. If none exists, you can opt for auto-generation of the role and policy. Creating and Running a DataSync Task With both source and destination locations configured, you can now create a DataSync task to perform the file migration: Click ""Create Task"" and select the previously configured source (NFS) and destination (S3) locations. Name the task (e.g., ""copy-nfs-to-s3""). Configure the task options, including copying all files and optionally setting up logging via auto-generated CloudWatch log groups. Review the configuration and create the task. Start the task using the default settings. Once the task starts, you can monitor its progress, observing metrics such as throughput, file count, and data transferred from the source to destination. When the task progresses from the launching phase to transferring files (as indicated by metrics reaching 1% and beyond), it confirms successful connectivity between the NFS server and S3 bucket. After task completion, verify the migrated files by checking the S3 bucket: Conclusion This guide illustrated the deployment of a DataSync agent on an EC2 instance, configuration of source and destination locations, creation of a DataSync task, and the verification process for transferring files from an NFS server to an S3 bucket using AWS DataSync. The streamlined integration between these services simplifies data migrations from on-premises environments to AWS, making it an excellent solution for modern data management. Happy syncing! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,CloudWatch Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/CloudWatch-Demo,"AWS Solutions Architect Associate Certification Services Management and Governance CloudWatch Demo In this article, we walk through a comprehensive demo of AWS CloudWatch, highlighting its key features and functionalities for monitoring your AWS environment. CloudWatch is more than just a single service—it’s a suite of monitoring tools that manage logs, metrics, alarms, dashboards, and more. To begin, access CloudWatch by searching for the service in the AWS console. Once on the CloudWatch page, explore the various features available on the left-hand panel. Each feature provides distinct capabilities for monitoring and managing your AWS resources. Logs CloudWatch's centralized log management lets you collect and analyze logs from different services and applications across your AWS account. Logs are organized into log groups based on your configurations or the specific service/resource generating them. This means you can have distinct log groups for your DataSync operations, each Lambda function, or services like AWS Macie. Viewing Log Streams Within each log group, inspect individual log streams. For example, when you navigate to the logs of a Lambda function (e.g., ""test one""), you'll notice several log streams that include timestamps of the last events. Selecting a log stream reveals detailed log data such as initialization messages, invocation details, and function output. Below is a snippet from a Lambda log stream: 2023-10-19T11:15:24.574Z  eec4ff36-6a6d-49ab-bbbe-4da06f561e77  INFO  {
    Records: [
        {
            eventID: 'c4c4238a0b92382dc590a675948',
            eventName: 'INSERT',
            eventVersion: '1.1',
            eventSource: 'aws:dynamodb',
            awsRegion: 'us-east-1',
            dynamodb: [Object],
            eventSourceARN: 'arn:aws:dynamodb:us-east-1:123456789012:table/ExampleTableWithStream/2015-06-27T08:48:05.899'
        },
        {
            eventID: 'c81e728d9d4c2f636f067f89b148c1460',
            eventName: 'MODIFY',
            eventVersion: '1.1',
            eventSource: 'aws:dynamodb',
            awsRegion: 'us-east-1',
            dynamodb: [Object],
            eventSourceARN: 'arn:aws:dynamodb:us-east-1:123456789012:table/ExampleTableWithStream/2015-06-27T08:48:05.899'
        },
        {
            eventID: 'ecc87742b5c2fe28a3075f9df2a7bfa',
            eventName: 'REMOVE', This snippet shows detailed Lambda output, capturing events from a DynamoDB stream. Each new invocation generates similar log entries. For example, Macie logs contain job event details: {
  ""adminAccountId"": ""841860927337"",
  ""jobId"": ""89bfa2335cdea88b3c1d38d78c12c"",
  ""eventType"": ""JOB_CREATED"",
  ""occurredAt"": ""2023-10-17T01:06:24.566653Z"",
  ""description"": ""The job was created."",
  ""jobName"": ""macie-test-job""
} {
  ""adminAccountId"": ""841860927337"",
  ""jobId"": ""89bfa2335cdea88b3c1d38d78c12c"",
  ""eventType"": ""ONE_TIME_JOB_STARTED"",
  ""occurredAt"": ""2023-10-17T01:06:29.618922Z"",
  ""description"": ""The job started running.""
} {
  ""adminAccountId"": ""841860927337"",
  ""jobId"": ""89bfa2335cdea88b3c1d38d78c12c"",
  ""eventType"": ""JOB_COMPLETED"",
  ""occurredAt"": ""2023-10-17T01:16:47.152607Z""
} CloudWatch also features a live tail mechanism for real-time troubleshooting. By selecting a specific log group (e.g., the ""test one"" Lambda group) and activating live tailing, you can stream new log entries as soon as they are generated. Running tests on your Lambda function will produce outputs similar to the following: START RequestId: 78d5a620-3f88-4ed2-aec1-150ee81539ae Version: $LATEST
2023-10-19T01:20:54.405Z 78d5a620-3f88-4ed2-aec1-150ee81539ae INFO  Records: [ { eventID: 'c4c4a238abf923820dc509a67f549b', eventName: 'INSERT', eventVersion: ...
END RequestId: 78d5a620-3f88-4ed2-aec1-150ee81539ae
REPORT RequestId: 78d5a620-3f88-4ed2-aec1-150ee81539ae Duration: 116.26 ms Billed Duration: 117 ms Memory Size: 128 MB Max Memory Used: 69 MB Note This live tail feature is invaluable for immediate insights during troubleshooting, allowing you to quickly pinpoint issues as they occur. Log Insights CloudWatch Log Insights provides a powerful query language similar to SQL, enabling you to filter and analyze logs efficiently. For instance, you can execute the following query to list the 20 most recent log entries: Fields @timestamp, @message, @LogStream, @Log
sort @timestamp desc
limit 20 The query output displays a graph of log events over time alongside detailed log entries that match the criteria. You can modify the query to sort entries in ascending order and reduce the limit, as shown below: fields @timestamp, @message, @LogStream, @Log
| sort @timestamp asc
| limit 5 Metrics CloudWatch collects a variety of metrics for your AWS resources. To view these metrics, navigate to the ""All Metrics"" section. You can filter metrics by service; for example, EC2 metrics include CPU utilization, status checks, network traffic, and EBS operations. Consider the following example for monitoring CPU utilization of an EC2 instance: In this graph, you can observe periods of minimal usage and sudden spikes. Customize the display by adjusting the time range (e.g., last hour or three hours) or by choosing different visualization formats. Creating Alarms CloudWatch alarms allow you to monitor specific metrics and trigger notifications when thresholds are exceeded. For instance, you can create an alarm for CPU utilization. When the average CPU usage exceeds a predefined static threshold—say 60% over a five-minute period—an alarm is triggered. After selecting the metric, configure the alarm conditions, and ensure notifications are sent via an SNS topic (such as an email alert). This setup guarantees that any sustained high CPU usage triggers an alert to the designated recipients. Verify your settings with the corresponding graph display. If everything is in order, save your configurations; otherwise, cancel the setup if modifications are needed. Additional Features CloudWatch offers several additional capabilities to enhance monitoring: X-Ray Traces: Monitor distributed applications with AWS X-Ray. (Note: This demo does not include an X-Ray example.) Service-Specific Insights: Leverage tailored pages such as Container Insights, Lambda Insights, and Application Insights. Events: Utilize Amazon EventBridge (formerly CloudWatch events) to define rules that trigger actions based on specific AWS environment events. Dashboards Dashboards in CloudWatch let you assemble metrics and logs into customizable visual displays. Creating a dashboard is straightforward: Navigate to the ""Dashboards"" section and click ""Create dashboard."" Name your dashboard (e.g., ""demo"") and add various widgets: Metric Widgets: Insert a line graph for CPU utilization by selecting the corresponding metric. Numeric Widgets: Include widgets that showcase numerical data, such as the number of network packets sent. Lambda Invocation Metrics: Display the number of invocations for your Lambda functions. Resize and rearrange the widgets as needed to optimize the layout. Once satisfied with the layout, save your dashboard under an appropriate name. Use multiple dashboards to monitor different environments or teams, such as separate dashboards for ECS clusters or individual applications. This overview of CloudWatch has covered log management, live tailing, log insights, metrics, alarms, and dashboards. Experiment with these features to tailor your AWS monitoring strategy to your specific requirements. Happy monitoring! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,CDK,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/CDK,"AWS Solutions Architect Associate Certification Services Management and Governance CDK AWS Cloud Development Kit (CDK) is a powerful Infrastructure as Code (IaC) tool that enables developers to define AWS resources using familiar programming languages such as Python, JavaScript, Java, or .NET. Unlike AWS CloudFormation—which requires JSON or YAML templates—CDK lets you leverage each language’s ecosystem for better flexibility, access to third-party libraries, and enhanced testing capabilities. With a CDK app, you employ pre-configured constructs that bundle best practice defaults, expediting the onboarding process. Although you author your infrastructure in code, CDK synthesizes this code into standard CloudFormation templates. In other words, running the synthesis command (CDK synth) translates your code into CloudFormation templates that can then be deployed using CDK deploy. Key Information CDK enables a more dynamic and maintainable approach to infrastructure management by bridging the gap between application code and cloud resource definitions. Features and Benefits of AWS CDK AWS CDK provides several advantages for managing your infrastructure declaratively through code: Transparency and Predictability: Codified infrastructure is transparent, repeatable, and predictable, ensuring reliable resource provisioning. Code Reusability: Reuse custom components across projects, share them with the community, and take advantage of community-provided constructs. Rich AWS Construct Library: Gain instant access to a diverse range of pre-built constructs that represent various AWS resources. Automated Synthesis: Automatically generate CloudFormation templates from your application code, simplifying deployment processes. Environment Agnosticism: Develop once and deploy across multiple environments by parameterizing your resources for different configurations. A summary of the key features is provided below: Feature Benefit Example Declarative Infrastructure Simplifies resource management and provisioning CDK constructs automatically generate CloudFormation templates Component Reusability Increases development efficiency and consistency Reuse constructs across multiple applications Rich AWS Library Provides access to a wide array of ready-to-use components Leverage constructs for Amazon S3, EC2, VPC, etc. Automated Synthesis Transforms code into deployable templates seamlessly Use CDK synth to generate CloudFormation templates Environment Agnosticism Ensures seamless deployment across different environments Parameterize constructs for development, staging, and production Integrating AWS CDK with CI/CD AWS CDK integrates seamlessly with AWS services to build robust CI/CD pipelines. For instance, after updating your CDK application, you can commit your changes to AWS CodeCommit. This action triggers a pipeline in AWS CodePipeline that performs the following steps: Executes CDK synth to generate CloudFormation templates. Runs unit tests to validate changes. Builds artifacts and packages your application. Deploys the resulting CloudFormation templates to provision AWS resources. This integration ensures a smooth, automated, and efficient deployment process for your CDK applications, taking full advantage of AWS CodeCommit, CodePipeline, CodeBuild, and CloudFormation. Deployment Note Always validate your CloudFormation templates (generated via CDK synth) in a test environment before deploying to production. This precaution helps prevent potential misconfigurations from causing downtime or resource misallocation. For more detailed information on AWS CDK and best practices, be sure to check out the AWS CDK Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,X Ray,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/X-Ray,"AWS Solutions Architect Associate Certification Services Management and Governance X Ray In modern distributed architectures, managing and troubleshooting applications can be quite challenging. With hundreds of independent services interacting simultaneously, it is often difficult to pinpoint the root cause of issues. AWS X-Ray simplifies this process by providing deep insights into each request and its entire journey through your system. Overview of AWS X-Ray AWS X-Ray is a tracing tool that collects detailed data about the requests made by your application. It captures not only your application’s internal processes but also its interactions with various AWS services such as RDS, DynamoDB, and others. By breaking down each request into detailed segments, X-Ray enables you to monitor timing, status indicators, and performance metrics across the entire call chain. Note Every incoming request is recorded as a trace—a collection of segments that chronicle the complete journey of that request through your distributed system. How X-Ray Works Consider a scenario where a user initiates a login request. AWS X-Ray logs the entire process by splitting it into segments. For example: The request enters the application, reaches an endpoint, and returns a 200 status code in 118 milliseconds. The application then interacts with an RDS database to process the login, taking 105 milliseconds with a status code of 200. Lastly, the request continues to an S3 bucket for file storage, which takes 88 milliseconds. This detailed breakdown allows you to pinpoint which segment is causing a delay. If your application starts experiencing significant slowdowns—for instance, a response time of 5000 milliseconds—reviewing the individual segments can help you quickly identify and resolve the bottleneck. Warning When diagnosing performance issues, ensure you analyze the complete trace. Missing any segment in the analysis might lead to an incorrect conclusion about the root cause. Key Concepts of X-Ray Component Description Trace A complete record of a single request as it flows through your system. Each trace consists of one or more segments. Segment A specific unit of work within a trace, such as processing handled by an EC2 instance. These segments capture detailed timing and status data. This structured breakdown of application requests is the primary advantage of AWS X-Ray. It not only simplifies the process of identifying and resolving performance issues but also provides a clear visual representation of how different components interact across your distributed system. AWS X-Ray is an essential tool for developers and architects aiming to enhance application performance and reliability. By utilizing the detailed trace data provided by X-Ray, you can quickly detect unusual delays, understand the flow of complex requests, and optimize the overall performance of your cloud-based applications. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Demo X ray your applications on AWS,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/Demo-X-ray-your-applications-on-AWS,"AWS Solutions Architect Associate Certification Services Management and Governance Demo X ray your applications on AWS In this lesson, we explore how to leverage AWS X-Ray for analyzing application traces. The demonstration outlines working with the AWS X-Ray console to visualize, filter, and diagnose issues in your application's performance. An application has already been created, deployed on AWS, and instrumented using X-Ray libraries to capture and send trace data. This guide explains how to navigate the X-Ray console, interpret trace segments, and filter trace data for troubleshooting purposes. Prerequisite Ensure your application is correctly instrumented with the AWS X-Ray SDK to capture trace data effectively. Getting Started Start by searching for ""X-Ray"" in the AWS Management Console, where you will discover that the X-Ray service is integrated within CloudWatch. If you navigate to CloudWatch, you will notice a dedicated section for X-Ray. This section is organized into two subsections: Service Map and Traces. Exploring the Service Map The Service Map provides a visual diagram detailing the interconnected components of your application. In the diagram below, the client (front-end user) sends a request to the scorekeep application running on an ECS container. This container then interacts with various components, such as: The scorekeep notification SNS topic Multiple DynamoDB tables for state, session, move, and game data These interactions are clearly represented in the trace. At the top of the interface, select a time frame (e.g., last 5 minutes or last 15 minutes). Clicking on a specific node will display all the traces involving that node. For instance, clicking a node reveals metrics like latency, request counts, total faults (including 500 status code failures), and response time distributions. The dashboard below highlights how metrics and traces are presented. Traces with failures or errors are emphasized, enabling rapid identification of problematic segments. Analyzing Individual Traces When you click on a trace, you will see the individual segments that form it. For example, a trace might begin with a request from your ECS container that interacts with the scorekeeping game table. In a simple trace, operations such as a ""get item"" on a DynamoDB table are segmented along with their durations. Consider the following query that captures a segment related to the scorekeep game table: service(id:{name:""scorekeep-game"", type:""AWS::DynamoDB::Table""}) This query indicates that the application sends a get item request to the scorekeep game table, in addition to performing operations like retrieving an item from the scorekeep state table and sending an SNS notification. By examining the duration of each segment, you can pinpoint operations that may be increasing overall latency. If a fault occurs (for example, a 500 error), the trace will highlight the location of the failure. The diagram below identifies a fault in the ""Scorekeep"" service, making it easier to isolate the problematic component. To view all traces within the selected time frame without filtering for a specific component, simply remove the current filter query and execute the query again. This will provide a comprehensive list of all captured traces. You can then click on each segment to view detailed information such as response codes and operation durations (e.g., 3 milliseconds, 2 milliseconds, etc.). Moreover, refine your queries by selecting specific nodes. For example, adding a filter for traces that involve both the SNS topic and the scorekeep game table may yield a detailed list of 43 traces, complete with response time distributions and additional metrics. Conclusion This lesson demonstrates how AWS X-Ray serves as a powerful tool for tracing and monitoring your applications. By instrumenting your application with X-Ray and analyzing trace data in CloudWatch, you gain valuable insights into the journey of a request, allowing you to identify bottlenecks and enhance performance. For more information on AWS X-Ray, consider exploring the following resources: AWS X-Ray Documentation CloudWatch Documentation Thank you for reading this demonstration. We hope you found this step-by-step guide helpful, and we look forward to sharing more advanced lessons in the future. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,AWS Transfer Family for FTPAS2,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Migration-and-Transfer/AWS-Transfer-Family-for-FTPAS2,"AWS Solutions Architect Associate Certification Services Migration and Transfer AWS Transfer Family for FTPAS2 AWS Transfer Family is a secure file transfer service designed to enable seamless transfer of files into and out of AWS storage services. It supports both Amazon S3 and Amazon EFS, making it an ideal solution for migrating data from on-premises data centers to the cloud. AWS Transfer Family is distinct from other services like DataSync. While DataSync primarily focuses on data migration, AWS Transfer Family delivers a fully managed, highly available FTP server. This means you no longer need to set up and manage your own FTP infrastructure. The service supports multiple protocols including SFTP, FTPS, FTP, and AS2, all while eliminating manual server configuration and maintenance. Managed Service Advantage AWS Transfer Family handles the infrastructure setup, ensuring high availability, scalability, and robust security. This allows you to concentrate on your core business needs instead of managing servers. When setting up AWS Transfer Family, AWS provides a public access endpoint for the FTP server, allowing access over the internet. If your requirements demand restricted access—such as internal resources within a specific VPC—you can configure the service to use a VPC endpoint. In addition, files can be directly transferred to S3 or EFS, and Amazon CloudWatch integration offers comprehensive logging and monitoring capabilities. The AWS SFTP Connector enhances the capabilities of AWS Transfer Family by enabling secure communication with remote servers—be they in the cloud or on-premises. This integration facilitates the merging of externally generated and stored data with AWS-hosted data warehouses, supporting robust analytics, business applications, reporting, and auditing. From an integration standpoint, AWS Transfer Family works seamlessly with S3 and EFS by directly copying files to these services. This smooth integration with AWS storage options, along with its support for multiple transfer protocols and robust CloudWatch logging, positions AWS Transfer Family as a versatile and efficient solution. Key Use Cases Some key use cases for AWS Transfer Family include: Modernizing File Transfer: Ideal for financial and healthcare institutions that require secure file transfers for regulated data under standards such as PCI and HIPAA. Real-Time Data Insights: Connect business applications and IoT devices to data lakes for real-time analysis and processing. Enhanced Collaboration: Improve connectivity among supply chain trading partners to drive real-time insights across various business applications like ERP and transportation management systems. Expanding Subscriber Reach: Offer diverse connectivity options with built-in fine-grained access controls that protect revenue across different channels. Summary AWS Transfer Family offers a fully managed platform for secure file transfer, simplifying the process of migrating data to AWS while ensuring high availability, scalability, and security. This cloud-native solution is powerful for organizations looking to modernize their file transfer processes without the overhead of managing traditional FTP servers. For more detailed information on AWS Transfer Family and its integration with AWS services, please refer to the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Datasync,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Migration-and-Transfer/Datasync,"AWS Solutions Architect Associate Certification Services Migration and Transfer Datasync In this article, we explore AWS DataSync—a powerful service engineered to facilitate rapid and secure data transfers between on-premises storage and AWS storage services. DataSync simplifies, automates, and accelerates the migration of large datasets by streamlining the transfer process to and from AWS. Core Components of AWS DataSync AWS DataSync comprises four essential components that work together to ensure efficient data migration: Agent The DataSync agent is a virtual machine appliance responsible for reading from and writing to storage during transfers. It can be deployed on various platforms such as VMware, KVM, or Microsoft Hyper-V. Location A location represents either the data source or destination. Every DataSync transfer involves two locations—for example, a source location in an on-premises environment and a destination location like an AWS S3 bucket or another AWS storage service. Task A task defines a DataSync transfer, specifying both the source and destination locations. It also outlines detailed instructions on how data should be copied, including how metadata, deleted files, and permissions are managed. Task Execution Task execution is the practical run of a task blueprint. Multiple executions can occur over time, each reflecting different transfer phases and statuses. Below is a diagram that illustrates the four key steps of DataSync: Task and Task Execution States Both tasks and their executions progress through specific states during the lifecycle of a transfer. Task States Available: The task is ready to initiate data transfer. Running: Data transfer is in progress. Unavailable: The DataSync agent is currently offline. Queued: Another task using the same agent is running. DataSync processes tasks sequentially, so an agent cannot execute more than one task at a time. Task Execution States Queued: The execution is in line, waiting for the agent if another task is active. Launching: The initialization phase begins when the agent becomes available and no queuing is required. Preparing: DataSync identifies which files need to be transferred. The duration of this phase depends on the file count and the performance of both the source and destination systems. Transferring: The selected data is actively transferred. Real-time updates for the number of bytes and files transferred are visible. Verifying: If configured, DataSync performs a data integrity check. A successful verification results in a ""success"" status; otherwise, an ""error"" state is displayed. The chart below illustrates the status icons and states for tasks and their executions: The following flow diagram visually represents the stages of task execution: AWS DataSync Discovery Similar to many AWS migration services, AWS DataSync offers a discovery process that provides critical insights before initiating data transfers. The DataSync agent connects to your on-premises storage and initiates a discovery job to gather system information. The agent sends the collected data to DataSync Discovery via a public service endpoint. Based on the gathered information, DataSync Discovery recommends the optimal AWS storage services for your data migration. The flowchart below outlines the DataSync Discovery process: Key Features of AWS DataSync DataSync is equipped with a range of features designed to ensure efficient and secure data transfers: High-Speed Data Transfer: Optimized for moving large volumes of data rapidly. Simplified User Interface: User-friendly management of tasks via an intuitive UI. Data Integrity Checks: Ensures accurate and complete data transfers. Automation and Scheduling: Enables frequent, automated data transfers without manual intervention. Multi-Protocol Support: Compatible with NFS, SMB, and S3 protocols. Incremental Transfers: Transfers only the updated or changed files to reduce redundancy. Security and Encryption: Data is encrypted during transit and at rest for maximum security. Monitoring and Logging: Provides real-time transfer progress tracking, comprehensive history logs, and detailed analysis for troubleshooting. The diagram below highlights these robust features: Common Use Cases for AWS DataSync AWS DataSync supports a wide variety of use cases, including: On-Premises to AWS Storage Transfers: Migrate data from your on-premises storage systems to AWS services such as Amazon EFS, S3, or FSx. AWS Region-to-Region Synchronization: Synchronize data between AWS regions to support data redundancy, disaster recovery, or regional migrations. This ensures optimal performance and availability while relocating data between regions. Third-Party Cloud Migrations: Seamlessly migrate data from other cloud providers such as Microsoft Azure or Google Cloud Platform (GCP) to AWS. Note AWS DataSync not only transfers files and objects securely but also supports data replication to AWS storage services. This ensures enhanced data protection, backup capabilities, and the opportunity to archive less frequently accessed data to cost-effective storage like Amazon S3 Glacier. By leveraging AWS DataSync, organizations can reduce on-premises storage costs and streamline data migrations while maintaining high levels of security and efficiency. DataSync integrates seamlessly with various AWS services, making it an ideal solution for a diverse range of data migration and synchronization projects. For further reading on AWS services and best practices, check out the following resources: AWS Documentation Amazon S3 AWS Storage Gateway Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Trusted Advisor,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/Trusted-Advisor,"AWS Solutions Architect Associate Certification Services Management and Governance Trusted Advisor In this lesson, we explore AWS Trusted Advisor—a vital tool designed to provide real-time guidance and help you adhere to AWS best practices. By continuously reviewing your environment, Trusted Advisor offers actionable recommendations in key areas such as cost optimization, security, fault tolerance, and performance enhancement. For instance, consider an EC2 instance deployed within a Virtual Private Cloud (VPC). AWS Trusted Advisor evaluates your instance configuration and suggests adjustments, such as implementing security group rules, to significantly strengthen your network security. Another common scenario involves managing an S3 bucket. Trusted Advisor recommends enabling lifecycle policies to archive data and shift objects to more cost-effective storage classes, thereby reducing your monthly expenses. Key Benefits The main advantages of using AWS Trusted Advisor include: Cost Optimization: Detect idle resources and underutilized services (e.g., redundant RDS instances or EBS volumes) to lower costs. Performance Improvement: Analyze usage patterns and configurations, such as EBS throughput or EC2 compute usage, to enhance efficiency. Enhanced Security: Identify potential security risks like exposed access keys or overly permissive S3 bucket policies, and implement recommended best practices. Fault Tolerance and Reliability: Receive advice on how to bolster the resilience of your deployments. Service Quota Monitoring: Get alerts when resource usage approaches 80% of preset limits. For exam preparation, it's important to remember that AWS Trusted Advisor is the service that provides prescriptive advice to optimize your deployments. When asked which AWS service offers recommendations to improve system configurations or deployments, the correct answer is AWS Trusted Advisor. For further details on cloud optimization and security best practices, consider checking out the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Launch Wizard,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/Launch-Wizard,"AWS Solutions Architect Associate Certification Services Management and Governance Launch Wizard In this article, we will explore AWS Launch Wizard and how it simplifies the deployment of popular third-party applications on AWS. With its pre-configured deployment templates, AWS Launch Wizard helps streamline the process while adhering to AWS best practices for security, scalability, and performance. What Is AWS Launch Wizard? AWS Launch Wizard is a managed service that accelerates the deployment of applications such as SQL Server, Microsoft Active Directory, and SAP on AWS. By providing pre-configured templates optimized for CPU, memory, and other critical configurations, it eliminates the need to manually configure complex settings. This ensures that your deployment is not only quick and efficient but also secure, scalable, and cost-effective. Note AWS Launch Wizard automatically applies recommended security measures during resource provisioning, allowing you to deploy applications with confidence. How AWS Launch Wizard Works The AWS Launch Wizard deployment process is divided into several easy-to-follow steps: Selecting an Application: Choose the desired application from AWS's extensive catalog, which features a range of software commonly used in enterprise environments. Inputting Application Specifications: Enter key details such as application size, type, and custom configuration parameters. This step ensures that the deployment meets your specific organizational needs. Resource Recommendations and Cost Estimation: Based on your input, AWS Launch Wizard recommends the optimal AWS resources and provides a cost estimate. This helps you manage budgets and optimize resource allocation. Resource Provisioning: Once you approve the recommended configuration, the service automatically provisions the selected AWS resources. It creates a highly available solution and generates reusable CloudFormation code templates for future deployments. Deployment and Integration: Finally, AWS Launch Wizard deploys the application and integrates it with AWS management and monitoring services such as CloudWatch and AWS IAM for enhanced security and access control. Benefits of Using AWS Launch Wizard Leveraging AWS Launch Wizard offers numerous advantages: Simplified Deployment Process: The service abstracts complex deployment details, ensuring that all configurations adhere to AWS best practices. Optimized Resource Selection: AWS Launch Wizard assists in choosing the right combination of AWS services, such as EC2 instance types and EBS volumes, tailored to your application’s requirements. Accurate Cost Estimation: Receive a transparent estimate of associated costs, making budget planning and expense management straightforward. Reusable CloudFormation Templates: Automatically generated CloudFormation templates serve as a reliable baseline for future deployments, significantly reducing setup time. Note There are no additional charges for using AWS Launch Wizard; you only pay for the AWS resources that are provisioned to run your solution. By harnessing the power of AWS Launch Wizard, organizations can deploy critical applications more efficiently, ensuring they align with both industry standards and AWS best practices. For further reading on AWS services and deployment strategies, refer to the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,IAM,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/IAM,"AWS Solutions Architect Associate Certification Services Security IAM In this lesson, we explore AWS Identity and Access Management (IAM), a service dedicated to securely managing authentication and authorization within an AWS environment. IAM verifies that users are who they claim to be (authentication) and determines what AWS resources they can access (authorization). For example, when a user initiates an operation — such as creating an S3 bucket — IAM first confirms the user's identity and then checks if they have the appropriate permissions to perform that action. This secure mechanism centralizes identity verification and permission management, ensuring compliance and robust audit trails based on the principle of least privilege. This principle restricts users to the permissions necessary for their tasks. Key Concept When granting AWS access, creating an individual IAM user is essential. An IAM user represents a single entity (whether a person or an application) and starts with no permissions by default. Administrators must explicitly assign permissions via IAM policies. In practice, if a team is only responsible for working with AWS RDS databases, they should be granted permissions solely for RDS—not for other services like S3 . IAM also supports grouping similar users. Groups allow you to assign a common set of policies to multiple users at once. For instance, if both Smith and Clark are part of the ""dev"" group, they automatically inherit all permissions associated with that group. Since users can belong to multiple groups, Clark might also gain additional permissions if he is a member of the ""audit"" group. Important Remember: New IAM users have no access to AWS resources until permissions are explicitly granted through IAM policies. IAM policies are defined in JSON format. Below is an example policy document that grants a user permission to list the contents of an S3 bucket and retrieve objects from that bucket: {
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Action"": [
        ""s3:ListBucket"",
        ""s3:GetObject""
      ],
      ""Resource"": [
        ""arn:aws:s3:::KodeKloud-bucket"",
        ""arn:aws:s3:::KodeKloud-bucket/*""
      ]
    }
  ]
} This policy document includes: The policy language version (""2012-10-17""). A statement that: Specifies the actions permitted (listing the bucket and retrieving objects). Sets the effect to ""Allow"", granting the specified permissions. Defines the specific S3 bucket and its contents to which these permissions apply. After creating such a policy, you can assign it to an IAM user or group, thereby enforcing the defined access controls. When studying for the AWS Solutions Architect Associate Certification , keep these IAM concepts in mind. Mastering authentication, authorization, and the proper structuring of IAM policies is crucial for ensuring AWS security best practices. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,IAM identity Center SSO Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/IAM-identity-Center-SSO-Demo,"AWS Solutions Architect Associate Certification Services Security IAM identity Center SSO Demo In this lesson, we will walk through the process of configuring and using IAM Identity Center, enabling you to manage user access across multiple AWS accounts efficiently. Accessing IAM Identity Center Begin by logging into the AWS Management Console and typing ""IAM Identity Center"" in the search box. Once selected, you will be directed to the service page. If this is your first time accessing IAM Identity Center, you may see an option to “Enable IAM Identity Center.” Follow the on-screen prompt to enable the service. After enabling, navigate to the settings page to customize your Identity Center configuration. Below the settings section, locate the access portal. This portal serves as the customized login page where users enter their username and password to access their assigned AWS accounts. Use the ""Actions"" menu to change the identity source by specifying the desired user directory. You can switch between the built-in Identity Center directory, Active Directory, or another external identity provider. In this demo, we are using the Identity Center directory. Tip For this demonstration, continue using the Identity Center directory and simply cancel any dialogs prompting for a change in identity source. IAM Identity Center manages users and groups in a manner similar to IAM, with the key difference being that user creation occurs directly within Identity Center. If you decide to integrate a different identity provider, note that user and group management will be handled on that external platform. Additionally, this service enables the management of multiple AWS accounts from a centralized location. Creating Users To begin, create a couple of users: Click on ""Add user."" Provide a name (for example, user one) along with a dummy email address. For this demo, you do not need to assign users to groups. After creation, a registration email will be sent so that the user can set their password. Repeat the steps to create a second user (for example, user two). When the registration email is received, the user should click on ""Accept invitation"" and set a password. This completes the user creation process. Creating Permission Sets Permission sets in IAM Identity Center allow you to define the scope of operations users can execute within your AWS accounts, functioning similarly to IAM policies. To create a permission set: Click ""Create permission set."" Choose between predefined permission sets (such as AdministratorAccess, Billing, or ViewOnlyAccess) or create a custom permission set. For example, to create a custom permission set that grants Amazon S3 full access: Select the managed policy ""Amazon S3 Full Access"" which includes permissions for creating, reading, and editing S3 buckets. Click ""Next,"" provide a name (e.g., s3.full.access), and then complete the creation process. You can also create a predefined permission set for view-only access. Once both permission sets are created, they can be assigned to users across different AWS accounts. Assigning Permission Sets to Users To assign permission sets, navigate to the AWS accounts section in Identity Center. For example, to grant user one full S3 access in multiple accounts: Select the relevant AWS accounts. Assign the ""S3 Full Access"" permission set to user one. Confirm the assignment to configure the policies accordingly. Next, open the access portal in a new browser tab and log in as user one. You will observe that user one has access to two different AWS accounts (for example, the main account and account two), each with the corresponding permissions. To confirm the assignment, log in as user one and attempt to create an S3 bucket in account two. After switching back to the main account, log in as user one and attempt to create another S3 bucket. Successful bucket creation in the respective accounts confirms that permissions have been correctly assigned. Configuring Distinct Permissions for User Two For user two, you might assign site-specific permissions. In this demonstration, user two will have: Full S3 access in account two. View-only access in the main account. To configure these settings in the IAM Identity Center console: For account two, assign the custom permission set with full S3 access for user two. For the main account, assign the predefined ViewOnlyAccess permission set for user two. Review the assignments for user two to ensure the correct permissions have been applied. Log in as user two and verify: In account two, full S3 access is available. In the main account, only view-only access is granted. Attempting to create a bucket in the main account should result in a permissions error. Conversely, bucket creation in account two should succeed due to the granted full S3 access. Conclusion This demo illustrates how IAM Identity Center streamlines access management across multiple AWS accounts by consolidating user and permission management into a single interface. By centralizing management, you can efficiently create users and groups, assign permission sets, and control account access without duplicative administrative overhead. By centralizing identity and access management tasks, IAM Identity Center minimizes administrative complexity and ensures that users have appropriate access levels across AWS accounts. This lesson has provided you with a clear understanding of the core functionalities of IAM Identity Center, equipping you to manage AWS account access with confidence. Transcribed by: otter.ai Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Demo SSO Audit and Security inside of a Control Tower account,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/Demo-SSO-Audit-and-Security-inside-of-a-Control-Tower-account,"AWS Solutions Architect Associate Certification Services Management and Governance Demo SSO Audit and Security inside of a Control Tower account In this guide, we demonstrate how AWS Control Tower simplifies multi-account management. You will learn how to set up your landing zone, configure organizational units, and provision new accounts using AWS Control Tower features. Setting Up the Landing Zone Begin by searching for the AWS Control Tower service in your AWS console. Click the designated button to start configuring your landing zone. During this process, you will see detailed information about the actions performed and associated pricing details. With AWS Control Tower, you only pay for the AWS resources that are created—Control Tower itself is provided at no extra charge. Under the ""Home Region"" section, select your preferred region (US East 1 is the default for this demo). You will also notice a ""region deny"" setting that restricts access to AWS services in regions not governed by Control Tower. For this demo, we leave the region deny setting disabled, but you can enable it if necessary. All these settings can be updated later. Configuring Regions for Control Tower Proceed by specifying which AWS regions will be managed by Control Tower. Although your home region is pre-selected, you have the option to add other regions (such as Europe and the Middle East) if required. For this demonstration, these additional regions remain unchecked. Click ""Next"" to move forward. Control Tower then prompts you to define the organizational units (OUs) to create. Typically, a primary OU for log archives and security audits is pre-defined (here, labeled ""Security""). You can also create an additional ""sandbox"" OU for development or testing accounts. In our demo, we create a sandbox OU and click ""Next"" to continue. Creating Service Accounts Set up your management and service accounts in the next step. Since you are already logged into the management account, you only need to create new accounts for the log archive and security audit functions. Provide a unique email address and account name for each account. If you already have an existing log archive account, you can opt to select it. After entering the required details, click ""Next"" to proceed. AWS Account Access and Logging Configuration Configure the AWS account access settings by leaving the default option selected. Next, set up CloudTrail logging and Amazon S3 log configurations. These configurations are optional and can be modified later if needed. When you are ready, click ""Next"" to review your configuration. Accept the required permissions, then initiate the landing zone setup. Note that resource provisioning in the landing zone may take several minutes. Review of the Landing Zone Configuration Once AWS Control Tower completes the landing zone setup, it configures organizational units, shared accounts, and provisions accounts for user requests. In this demonstration, AWS Control Tower sets up the following: Two custom organizational units (Security and Sandbox) Three shared accounts (management, log archive, and security audit) AWS IAM Identity Center for integrated identity management Preventative controls (20 in total) and detective controls (3 in total) to enforce best practices Review the summary displayed on the dashboard to see the details of your organizational structure and accounts. Scroll down to view the root organizational unit along with the two custom OUs (Sandbox and Security). The three AWS accounts—the management account, audit account, and log archive account—are clearly listed. You can also examine the 20 preventative and 3 detective controls that have been automatically applied. Selecting a preventative control displays further details from the controls library. For example, one control ensures that Amazon API Gateway (both REST and WebSocket APIs) has logging enabled, preventing API creation without proper logging. You can filter controls based on categories like cost optimization to verify, for instance, that EBS volumes are cost efficient or that stopped EC2 instances are terminated after a specified period. Email Notifications and Identity Center Setup After account registration, AWS sends email notifications for each newly created account. For example, you will receive confirmation emails for the log archive and audit accounts. Additionally, AWS IAM Identity Center issues an invitation email to activate your user account. Follow the instructions in the email to set your password and accept the invitation. Once your account is activated, sign in through the provided portal URL. In this demonstration, you will have access to three accounts (audit, log archive, and main management accounts) along with two different policies for the main account: full administrator access and a policy tailored for service catalog management. Creating a New AWS Account via Account Factory Navigate to the Account Factory within the AWS Control Tower dashboard to provision a new AWS account. This feature automatically configures the account with the necessary settings and default security controls. Within Account Factory, you can review and adjust network settings prior to account creation. For example, the default VPC setting prevents the creation of a public subnet when provisioning a new account. You can customize the number of private subnets and the CIDR block as needed—in this demo, we use the default settings. Click “Create New Account” and enter the following details: Account Email: Provide a unique email address for the new account. Display Name: For this demonstration, we name the account ""staging."" IAM Identity Center Username: Use ""staging"" for consistency. Organizational Unit: Assign the account to the ""sandbox"" OU. After entering the details, select ""Create Account."" The provisioning process will begin, and you can track the request in the AWS Service Catalog under “Provisioned Products.” The account creation process may take 5 to 10 minutes. Once complete, return to the Control Tower dashboard to verify that the new ""staging"" account is successfully enrolled under the sandbox OU. Inspect the enabled controls for the new account, and you will see that all best practice configurations and service control policies have been automatically applied. Note AWS Control Tower ensures that each new account complies with your organization's security and governance policies by automatically applying best practice controls. Conclusion AWS Control Tower significantly streamlines multi-account management by automating account creation, applying centralized security controls, and integrating AWS IAM Identity Center for effective user management. This automation keeps all your AWS accounts compliant with the latest best practices while minimizing the need for manual configuration. We hope this guide has been informative and that you are now ready to manage your AWS accounts with enhanced security and efficiency. Happy managing! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Compute Optimizer,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/Compute-Optimizer,"AWS Solutions Architect Associate Certification Services Management and Governance Compute Optimizer In this lesson, we focus on AWS Compute Optimizer—a powerful service designed to improve the efficiency of your compute resources by providing actionable optimization recommendations. Compute Optimizer uses machine learning to analyze various AWS services, including Amazon Elastic Compute Cloud (EC2) , EBS, Amazon Elastic Container Service (AWS ECS) , Fargate, and AWS Lambda . The insights generated help you identify underutilized or misconfigured resources, so you can make adjustments for significant cost savings and improved performance. How AWS Compute Optimizer Works AWS Compute Optimizer performs resource analysis in three key steps: Resource Analysis It begins by evaluating the utilization and performance of your compute resources. By analyzing multiple metrics and usage patterns, the service identifies whether resources are over-provisioned, under-provisioned, or already optimized. Recommendations Based on the analysis, Compute Optimizer provides tailored recommendations. For example, it might suggest resizing an Amazon EC2 instance or adjusting other configurations to better align with your workload requirements—helping you eliminate unnecessary costs and enhance overall performance. Implementation After reviewing the recommendations, you can make the necessary adjustments to your compute resources. This may involve transitioning to a more cost-effective instance type or modifying resource configurations to optimally support your applications. Key Benefit AWS Compute Optimizer not only identifies inefficiencies in your compute resources but also provides actionable insights, empowering you to optimize performance and reduce costs. Benefits of AWS Compute Optimizer The primary advantages of using AWS Compute Optimizer include: Performance Risk Analysis: Identifies under-provisioned Amazon EC2 instances and suggests larger instance sizes when needed. Cost-Saving Recommendations: Offers guidance on selecting smaller, more cost-effective EC2 instances when current resources are oversized relative to workload demands. Tailored Instance Recommendations: Provides optimal instance type suggestions that match your specific application requirements. EBS Optimization: Analyzes EBS volume configurations and throughput to suggest potential improvements. Fargate Task Analysis: Delivers insights on CPU and memory configurations for AWS ECS tasks running on Fargate, ensuring efficient performance. Cloud Optimization Features The image below illustrates the five key features associated with cloud optimization using AWS Compute Optimizer: Additional Resources For further reading and more detailed information on AWS Compute Optimizer and related services, please refer to the following resources: AWS Compute Optimizer Documentation AWS General Documentation Overview of Amazon EC2 This comprehensive overview should provide you with valuable insights into how AWS Compute Optimizer can enhance your cloud resource management strategy while optimizing for both cost and performance. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Systems Manager,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/Systems-Manager,"AWS Solutions Architect Associate Certification Services Management and Governance Systems Manager In this article, we delve into AWS Systems Manager, a robust service designed to simplify the management of your EC2 instances, on-premises servers, and various resources across multiple platforms and environments. Whether your servers run on AWS, another cloud provider, or on-premises, Systems Manager streamlines operations for large-scale infrastructures. AWS Systems Manager empowers you to collect software inventory, enforce configuration standards, and execute a wide range of tasks—from creating necessary users and configuring firewalls to running commands and applying OS patches. This centralized approach ensures consistent configurations, prevents drift, and maintains software compliance regardless of the underlying system. SSM Agent Overview To manage various systems seamlessly, Systems Manager relies on the SSM agent. This lightweight software runs on both EC2 instances and on-premises servers or virtual machines, ensuring secure communication between your servers and the Systems Manager service. With the SSM agent, you can efficiently handle configuration management, inventory collection, and remote command execution. You can interact with Systems Manager through the AWS CLI, the Management Console, or various SDKs. This flexibility allows you to push configuration changes across multiple servers simultaneously, group servers based on specific roles, and automate routine IT tasks efficiently. Systems Manager's key features and capabilities include: Centralized control via a unified user interface. Grouping resources to target specific subsets of servers. Automation of repetitive IT operations, such as patch management and configuration updates. Operational insights through Systems Manager Explorer and Insights. The State Manager, ensuring servers remain in their predefined configuration state. Secure hierarchical storage for configuration data and secrets via the Parameter Store. Remote management with Session Manager, offering browser-based shell access or CLI connectivity. Compliance scanning to verify adherence to defined configuration and patch policies. Systems Manager offers a broad array of features that can be grouped into several key categories: Application Management AWS Systems Manager enhances application management through two primary services: Application Manager Application Manager assists in investigating and mitigating issues within applications and clusters. For instance, if your e-commerce application (which might include resources like EC2, RDS, and Lambda functions) exhibits a spike in error rates, Application Manager helps visualize the architecture, pinpoint problematic components (e.g., a non-responsive EC2 instance), and leverage features like Run Command to resolve issues. Parameter Store Parameter Store provides a secure storage mechanism for configuration data and secrets. Instead of embedding credentials directly in your application code, store sensitive information such as database usernames and passwords securely in the Parameter Store with KMS encryption. Your application can then retrieve these credentials at runtime, simplifying credential rotation and enhancing security. Change Management Managing changes in dynamic environments is seamless with Systems Manager. Its suite of tools includes: Change Manager An enterprise framework that governs operational changes to application configurations and infrastructure. For example, when upgrading a critical database, Change Manager allows you to create a structured workflow with necessary approvals from involved teams before any modifications are implemented. Automation This tool automates repetitive IT tasks. For instance, if new EC2 instances require specific security configurations, Automation can attach the correct IAM role and apply necessary security groups automatically. Change Calendar Change Calendar helps schedule modifications to avoid critical business periods. For example, during peak shopping seasons like Thanksgiving or Christmas, you can prevent significant changes to sustain stability. Maintenance Windows Maintenance Windows are designed to schedule system tasks during off-peak hours. Whether patching or updating code, you can plan these activities during late nights or weekends to minimize impact on business operations. Node Management For effective day-to-day server management, Systems Manager includes several essential tools: Compliance – Scan your server fleet for patch compliance and configuration discrepancies. Inventory – Generate comprehensive reports on all servers managed by Systems Manager. Session Manager – Provides secure, remote connectivity to managed instances. Run Command – Execute commands and configurations across multiple servers without manual intervention. State Manager – Maintain defined configuration states across instances. Patch Manager – Automate patching processes for managed instances. Distributor – Seamlessly distribute software packages to your instances. Operations Management To effectively manage and resolve operational issues, AWS Systems Manager offers: Incident Manager Incident Manager aids in mitigating and recovering from critical incidents. For example, during website downtime at peak hours, it can detect outages through pre-set CloudWatch alarms, alert on-call engineers via SMS or email, and initiate predefined response plans that include diagnostic instructions and runbooks. OpsCenter OpsCenter consolidates alerts and findings from various AWS services onto a centralized dashboard, making it easier to investigate and resolve operational challenges. It can, for instance, help identify which EC2 instances need patching. Best Practices Reminder Always ensure that your Systems Manager configurations and permissions follow AWS best practices. Properly managing roles, policies, and security settings is crucial to safeguard your infrastructure. Conclusion AWS Systems Manager offers a comprehensive centralized solution to manage, automate, and secure your IT infrastructure across diverse environments. Its extensive feature set—including application management, change control, node management, and operations management—ensures that you maintain high levels of operational performance and reliability in a scalable and efficient manner. For further reading on AWS Systems Manager features and best practices, consider visiting the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,AWS Organizations,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/AWS-Organizations,"AWS Solutions Architect Associate Certification Services Management and Governance AWS Organizations In this lesson, we explore AWS Organizations and how it streamlines the management of multiple AWS accounts. This powerful service enables centralized governance, reducing administrative overhead and enhancing security while simplifying billing operations. When managing AWS environments, organizations often operate several AWS accounts. Different departments, teams, or individual applications might each use their own AWS account, resulting in various challenges such as: Consolidated Billing: Separate billing for each account increases administrative complexity. Management Overhead: Distinct logins, user management, and ongoing maintenance for each account can be cumbersome. Security Inconsistencies: Configuring IAM policies and security settings on a per-account basis can lead to vulnerabilities. Limited Resource Sharing: Without centralized control, resources may be underutilized, leading to unnecessary costs due to redundancy. AWS Organizations addresses these issues by creating a centralized management model for all your AWS accounts. Think of AWS Organizations as a multinational company where the master account functions similarly to a corporate headquarters. The master account sets high-level policies and oversees subsidiary accounts, ensuring that every account follows centralized guidelines while retaining a degree of autonomy. AWS Organizations allows you to: Centrally Manage Multiple Accounts: Administer several AWS accounts all from a single interface. Group Accounts with Common Policies: Organize accounts into groups, or Organizational Units (OUs), to simplify policy enforcement. Streamline Billing: Utilize consolidated billing to aggregate charges across accounts for reduced administrative effort. Enforce Service Control Policies (SCPs): Apply overarching policies that define permissible actions across accounts. Components of AWS Organizations An AWS Organization unifies multiple AWS accounts into a single, manageable unit. The key components are: Root Account The root account serves as the top-level container for all AWS resources. Policies applied at this level cascade down to all Organizational Units and subordinate accounts. An organization always starts with one root. Organizational Units (OUs) OUs allow you to group accounts based on common requirements. For example, you might group all development accounts together, enabling you to apply consistent policies to that group. Management Account The management account is responsible for administrative tasks within the AWS Organization. It facilitates creating permissions, inviting or removing accounts, applying policies, and integrating with other AWS services for enhanced functionality. Service Control Policies (SCPs) SCPs establish the schema for permitted or denied operations within your accounts. Operating similarly to IAM policies but on an organizational level, SCPs can restrict actions—for instance, preventing the launch of oversized EC2 instances in development environments. These policies can be applied organizationally, to specific OUs, or at the individual account level. Consider the example of a development account where an SCP might restrict users from launching EC2 instances above a certain size. This approach helps prevent over-allocation of resources in non-production environments. Such policies can be universally applied or tailored to specific accounts or groups. Benefits and Features of AWS Organizations AWS Organizations offer a range of features designed to improve operational efficiency and security: Centralized Management: Control multiple AWS accounts from one central dashboard instead of managing them individually. Consolidated Billing: Aggregate charges across all accounts for streamlined financial management. Service Control Policies: Enforce security and operational guidelines across accounts similarly to IAM policies. Seamless Integration with AWS Services: Integrate effortlessly with services such as AWS IAM, IAM Identity Center, and CloudTrail, enhancing both security and insight. Cost Efficiency: AWS Organizations itself does not incur extra charges; you only pay for the AWS resources you deploy. Integration Benefits Integrations like single sign-on through AWS IAM Identity Center simplify access by eliminating the need for multiple logins. Additionally, enabling CloudTrail across your organization offers comprehensive API activity insights, bolstering both security and operational oversight. In summary, AWS Organizations centralizes the management of multiple AWS accounts, simplifies billing, and reinforces security through SCPs. This integrated approach ensures a more efficient and secure cloud environment while fostering consistency across your entire organization. For further reading on AWS Organizations and additional AWS services, consider visiting the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,IAM Demo,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/IAM-Demo,"AWS Solutions Architect Associate Certification Services Security IAM Demo In this lesson, we demonstrate how to manage AWS Identity and Access Management (IAM) by creating users, groups, and roles, as well as by customizing permissions and policies. Using IAM, you can control which users have access to specific AWS services, ensuring secure and organized access management. When you visit the AWS Management Console and click the ""Sign in to the console"" button at the top right, you'll be prompted to log in. There are two ways to log in: as the root user or as an IAM user. When you create an account, a root user is automatically created with the email address you registered. On the root user sign-in page, enter your email address and password. If multi-factor authentication (MFA) is enabled, enter the MFA code when prompted. Once logged in as the root user, note that you have full access to all services. Because root credentials have complete privileges, it is best practice to use them sparingly. Instead, create a separate IAM user with specific permissions for everyday tasks. Best Practice For enhanced security, avoid daily use of root credentials. Create and use limited-permission IAM users instead. Accessing IAM and Creating a User To create a new IAM user: Navigate to IAM from the AWS Management Console (use the search bar or the recent services section). In the IAM dashboard, manage your groups, users, roles, and policies. To create a user: Click the Users section. Since there are no users yet, click Add users . Enter a user name (for example, ""Sanjeev Thiyagarajan"") and check the option to Provide user access to the AWS Management Console . Decide whether AWS should auto-generate a password or if you want to specify one. You can also choose if the user must change the password upon the first sign-in. In this demonstration, uncheck the ""Users must create a new password at next sign-in"" option as you are creating the account for yourself. On the Set permissions page, you have several options: Add the user to a group to inherit permissions. Copy permissions from another user. Attach policies directly to the user. For now, create the user without any permissions. Review your selections and click Create user . After creation, click Continue to return to the users list, where the newly created user ""Sanjeev Thiyagarajan"" is listed without any group associations and with MFA disabled. Testing the New User's Permissions Now, log in as the ""Sanjeev Thiyagarajan"" IAM user. Since this IAM user currently has no permissions, it will not be able to perform any actions. Open a new browser tab, incognito window, or use a session management extension for this purpose. On the sign-in page, select Sign in as IAM user and enter: The AWS Account ID (available from the root user’s account dropdown), The IAM user name, and The password you specified. Once logged in as Sanjeev, you'll notice that the user does not have permissions to create, modify, or delete resources. For example, if you try to create an Amazon S3 bucket: Click on S3 from the console search. Choose to create a bucket (name it and select a region as needed). Attempting to create the bucket will fail with an error stating that the ""S3 Create Bucket"" permission is required. This confirms that new IAM users have no permissions by default. Granting Permissions via Policies Since the Sanjeev user cannot modify any resources, log back in as the root user to grant permissions. Follow these steps: Navigate to the IAM Users page. Select the ""Sanjeev Thiyagarajan"" user. Go to the Permissions tab and click Add permissions . When adding permissions, you have multiple options: Create an inline policy. Copy permissions from another user. Add the user to a group. Attach AWS managed policies directly. For demonstration purposes, attach the AWS managed policy AdministratorAccess to grant full administrative permissions. You can view the policy contents by clicking its plus icon. The policy is defined as follows: {
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Action"": ""*"",
      ""Resource"": ""*""
    }
  ]
} This policy allows any action ("" "") on any resource ("" ""). Click Next and then Add permissions . Now, when you check the Sanjeev user details, the AdministratorAccess policy is applied. Switch back to your Sanjeev session. You should now see a list of existing S3 buckets and be able to create a new bucket successfully. To test, try creating a new bucket (for example, ""cloudtest12345""). The bucket should be created successfully, confirming that Sanjeev now has full administrative access. Using Groups to Manage Permissions Managing permissions individually can be tedious, especially when onboarding multiple employees. IAM groups simplify this by allowing you to assign a set of policies to multiple users at once. For example, after revoking direct permissions from Sanjeev, you could: Create a group named admin Add users (for instance, Sanjeev and a dummy user ""user1"") to the group Attach the AdministratorAccess policy to the group Users in the group inherit these permissions automatically. Review the group details to confirm that permissions are correctly inherited: In the Users section, if you select ""user1,"" you will see that the AdministratorAccess policy is attached via the group rather than directly. Back on the Sanjeev session, verify that he can create S3 buckets again: You can create additional groups for different departments. For instance, create a monitoring group with the AWS managed policy ReadOnlyAccess for users who only need to view resources but not modify them. Add Sanjeev to this group as well so he inherits permissions from both groups. Then, remove him from the admin group to limit his permissions to read-only actions. If an unauthorized action is attempted—such as deleting an S3 bucket—a permission error will be displayed. Creating and Assuming a Role Roles in AWS IAM allow users or services to assume temporary permissions that differ from their default permissions. This is useful when a user occasionally needs elevated access—for example, full S3 access. To create a role with temporary S3 permissions: Navigate to the Roles section in IAM and click Create Role . Select AWS account as the trusted entity if the role is to be assumed by an IAM user within your account. Proceed by clicking Next and select the permission policy S3FullAccess (an AWS managed policy) to grant full Amazon S3 access. Reviewing this policy, you will see it permits S3 actions as shown below: {
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Action"": [
        ""s3:*"",
        ""s3-object-lambda:*""
      ],
      ""Resource"": ""*""
    }
  ]
} Name the role, for example, S3FullAccess , and create it. To restrict which IAM users can assume this role, attach a custom inline policy to the specific user (Sanjeev): Navigate to the Sanjeev user's permissions. Click Add Inline Policy . For the service STS , allow the AssumeRole action only for the specific role's Amazon Resource Name (ARN). For example: {
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Sid"": ""VisualEditor0"",
      ""Effect"": ""Allow"",
      ""Action"": ""sts:AssumeRole"",
      ""Resource"": ""arn:aws:iam::<ACCOUNT_ID>:role/S3FullAccess""
    }
  ]
} Name this policy (e.g., AssumeS3Access ) and create it. This ensures that only Sanjeev can assume the S3FullAccess role. To test the role: In the Roles section, click on the S3FullAccess role and copy the Switch Role URL. In the Sanjeev session, paste the URL. The console will automatically populate the necessary details (account ID and role name), allowing you to specify a display name for the session (e.g., ""S3 role""). Click Switch Role to assume the role. A badge (e.g., ""S3 role"" in blue) will appear, indicating that the role is active. Test S3 operations, such as deleting a bucket. As the assumed role, you should be able to delete the bucket, confirming that the temporary permissions are in effect. To return to your original session, click Switch Back . Summary In this lesson, we covered how to: Create IAM users and provide them with console access. Grant permissions directly via policies. Use groups to manage permissions efficiently across multiple users. Create roles for temporary elevated access and control who may assume these roles. This comprehensive demonstration illustrates how AWS IAM enables you to securely manage access across your AWS environment while ensuring best practices for security and scalability. For further reading: AWS IAM Documentation AWS Best Practices for IAM Happy cloud computing! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Service Catalog,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/Service-Catalog,"AWS Solutions Architect Associate Certification Services Management and Governance Service Catalog In this lesson, we explore the AWS Service Catalog and its core components. Think of it as a carefully curated library of IT services. Just as a traditional library offers a well-maintained collection of books—with details such as title, author, publication date, and location—an AWS Service Catalog organizes and delivers IT services such as virtual machines, servers, software, and databases, including complete multi-tier application architectures. Just like a librarian manages the selection of available resources, a cloud or IT administrator configures the AWS Service Catalog. They decide which services to include and control user access to ensure consistency and compliance. In a library, strict rules determine who can borrow resources, what items can be borrowed, and for how long. Similarly, the AWS Service Catalog enforces role-based access control, granting users permission only to a specific subset of services. This prevents unauthorized access and ensures that each user interacts only with the resources that match their responsibilities. You might ask, why is a service catalog indispensable? Without a standardized process or consistent templates, different departments might configure resources differently, which can lead to inconsistent tagging, misconfiguration, and increased complexity in troubleshooting. Moreover, the absence of centralized governance could lead to non-compliant deployments, heightened security risks, and uncontrolled cloud spending. For enterprises handling multiple AWS accounts, manually managing these resources amplifies operational challenges and delays resource provisioning—prolonging time-to-market for new products and services. Key Benefit By using AWS Service Catalog, organizations streamline resource deployment, enforce governance, and ensure consistent configuration across cloud environments. How AWS Service Catalog Works The AWS Service Catalog helps organizations create and manage approved IT services on AWS. It functions as a repository of templates that define how to deploy these services. Each product in the catalog is an IT service defined by a CloudFormation template that specifies the necessary AWS resources, their relationships, and configurable parameters (such as security groups or key pairs). Deploying a product from the catalog ensures that it is provisioned exactly as specified, eliminating misconfiguration risks. Portfolios in the service catalog group related products and include configuration settings and access controls. Administrators can tailor portfolios to different user groups, selectively granting access. Once a new version of a product is added to a portfolio, authorized users gain immediate access to the updated version—and portfolios can even be shared across multiple AWS accounts. User Roles in AWS Service Catalog There are two primary user roles: Catalog Administrator: Configures the catalog by creating products using CloudFormation templates, organizing them into portfolios, and setting up user access permissions. End User: Utilizes the AWS Management Console to search for, select, and launch products based on their granted permissions. A typical workflow involves the catalog administrator creating a product, organizing it into an appropriate portfolio, and distributing it. End users then discover the product in the catalog, launch it when needed, and manage its lifecycle as required. Integration and Deployment AWS Service Catalog leverages AWS CloudFormation to deploy all underlying resources. Each product corresponds to a CloudFormation stack, ensuring that deployments remain consistent with the defined templates. Access to both products and portfolios is managed using AWS Identity and Access Management (IAM) policies, which ensure that only authorized users can perform deployment or modification tasks. For enterprises with multiple AWS accounts managed through AWS Organizations, the service catalog can be shared, enabling consistent and centrally managed service deployments. Key Components and Features Below is an overview of the essential AWS Service Catalog components: Products: Collections of AWS resources defined by a CloudFormation template. A product can be as simple as a single Amazon Linux compute instance or as complex as a full multi-tier web application. Portfolios: Groups of products managed together with IAM policies, ensuring that access is granted only to the appropriate users. CloudFormation Integration: Each product is deployed via an AWS CloudFormation stack, which maintains consistency in resource provisioning. Granular Access Control: Using IAM policies, administrators control who can view, launch, and modify products and portfolios. Service Actions: These enable end users to perform operational tasks such as troubleshooting or executing approved commands on provisioned products without requiring full AWS access. Summary This lesson has provided an in-depth overview of the AWS Service Catalog, outlining its components and how it enables organizations to deploy IT services efficiently, securely, and in a consistent manner. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Proton,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/Proton,"AWS Solutions Architect Associate Certification Services Management and Governance Proton In this lesson, we explore AWS Proton, a powerful service designed to simplify infrastructure management in a microservices-based environment. As organizations increasingly adopt microservices, managing and deploying infrastructure can become complex and time-consuming. Instead of dealing with disparate deployment practices and environments maintained by different teams, AWS Proton offers a standardized approach. AWS Proton standardizes the application stack by allowing administrators or platform team members to create environment and service templates. An environment template defines the shared infrastructure required across multiple applications or microservices, while a service template specifies the infrastructure for a specific application or service. These templates ensure consistent deployment protocols across teams, leading to uniform and reliable configurations. When developers need to deploy microservice infrastructure, they simply select a standardized service template. AWS Proton automatically creates the service, sets up a CI/CD pipeline, and manages the service instances running the source code. This significantly reduces the operational burden, allowing developers to focus on writing code rather than managing infrastructure. Benefits of Standardization Standardizing deployments with AWS Proton not only improves consistency across different environments but also minimizes troubleshooting challenges due to varied configurations. Key Benefits of AWS Proton Simplified Application Stack Templates: Streamline the creation of application stack templates, including ready-to-use CI/CD pipelines for developers. Flexible Service Provisioning: Supports service deployment with or without a pipeline, enhancing the overall developer experience. Enhanced Template Customization: Easily extend existing templates to support a broader range of use cases, enabling developers to create intricate components using infrastructure as code. Multi-Account Management: Manage all resources centrally from a single AWS account, simplifying administrative oversight. Built-In Template Management: Store and manage reusable versions of application stacks within AWS Proton, ensuring consistency and ease of updates. Conclusion AWS Proton is designed to standardize microservices deployments by providing a reusable and consistent infrastructure management framework. By automating the heavy lifting involved in infrastructure deployment, Proton allows development teams to concentrate on building and enhancing their applications. This results in improved reliability across environments and a reduced operational burden for developers. For more information about AWS Proton and other AWS services, refer to the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Control Tower,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/Control-Tower,"AWS Solutions Architect Associate Certification Services Management and Governance Control Tower In this lesson, we explore AWS Control Tower—a powerful service that simplifies the setup, maintenance, and security of multi-AWS account environments. AWS Control Tower serves as a centralized account orchestrator that streamlines the creation of AWS accounts while automatically applying the necessary configurations and best practices. When a team member requests a new AWS account, the process is as simple as clicking a button. The new account is provisioned with all the required permissions, guardrails, and policies, ensuring it is secure and compliant from the start. Built on top of AWS Organizations, AWS Control Tower leverages features such as centralized billing and account management. Unlike AWS Organizations, which requires manual configuration for creating and managing accounts, Control Tower automates these processes to help you quickly launch secure, production-ready environments. Control Tower also sets up a landing zone, a secure foundation for a well-architected, multi-account environment. This landing zone deploys a collection of best practices for services like AWS CloudTrail and AWS Organizations, eliminating the need for manual setup and enabling rapid deployment of new applications and services. Note If your organization lacks internal expertise for configuring multi-account environments according to AWS best practices, the preconfigured landing zone in AWS Control Tower provides a secure starting point. Guardrails in AWS Control Tower When you provision a new account, AWS Control Tower enforces several guardrails to maintain security and operational best practices. There are two types of guardrails: Preventive Guardrails These use IAM policies, AWS Config rules, and Service Control Policies (SCPs) to proactively block actions that do not comply with established standards. For example, a preventive guardrail can block the creation of a publicly accessible S3 bucket, protecting your data from unintended exposure. Detective Guardrails Instead of blocking actions outright, detective guardrails monitor and log potential issues. For instance, if a user launches an EC2 instance without a key pair, the detective guardrail will log the event, report it, and trigger an alert for further review. This approach supports thorough forensic analysis and incident response. All guardrails are preconfigured within AWS Control Tower, ensuring that every new account automatically aligns with your organization’s security baseline. Consider these examples: A user attempts to create a public S3 bucket. The preventive guardrail identifies this misconfiguration and blocks the action. A user launches an EC2 instance without specifying a key pair. The detective guardrail logs the activity and notifies administrators about the non-compliance. Account Factory AWS Control Tower simplifies the onboarding of new AWS accounts with its Account Factory. This feature automates the provisioning process by applying organizational policies, baselines, and the necessary guardrails consistently across all accounts. Key Features of AWS Control Tower AWS Control Tower enhances your cloud infrastructure management with the following benefits: Feature Description Simplified Multi-Account Management Automates the setup and governance of multi-account deployments. Reduced Risk of Human Error Minimizes manual configuration errors through automated account provisioning and policies. Automated Policy Enforcement Consistently applies security and compliance guardrails across all accounts. Improved Operational Efficiency Speeds up the deployment process and reduces management overhead. Continuous Monitoring Provides real-time visibility into your environment’s compliance with defined policies. Note AWS Control Tower is designed to integrate seamlessly with your existing AWS infrastructure, ensuring compliance and operational excellence while reducing administrative burden. Conclusion AWS Control Tower provides an automated, secure, and efficient method to manage multi-account AWS environments. With its robust features—including a well-architected landing zone, comprehensive guardrails, and the highly efficient Account Factory—this service is indispensable for organizations looking to enforce best practices while maintaining agile and scalable cloud operations. For more details on how AWS Control Tower can transform your cloud strategy, explore additional resources and AWS documentation on multi-account management and governance. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,IAM Demo Roles for EC2 instances,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/IAM-Demo-Roles-for-EC2-instances,"AWS Solutions Architect Associate Certification Services Security IAM Demo Roles for EC2 instances In this lesson, you’ll learn how to use IAM roles to provide your EC2 instance with the permissions it needs to interact with other AWS services, such as Amazon S3. We’ll start with a simple application that uses the AWS SDK to programmatically create an S3 bucket. Creating an S3 Bucket with Explicit Credentials The code snippet below demonstrates how to configure an S3 client using explicit credentials. The application conditionally adds credentials if a secret access key is provided. It retrieves the bucket name from a command-line argument and then creates the bucket using the S3 API. const accessKeyId = """";
const secretAccessKey = """";

const s3Config = { region: ""us-east-1"" };

if (secretAccessKey != """" || null) {
    s3Config.credentials = {
        accessKeyId,
        secretAccessKey,
    };
}

const s3Client = new S3Client(s3Config);

// Create the parameters for calling createBucket
var bucketName = process.argv[2];

// Call S3 to create the bucket
const main = async () => {
    try {
        const response = await s3Client.send(
            new CreateBucketCommand({ Bucket: bucketName })
        );
    }
}; To run the application and create a bucket (for example, named ""bucket123""), execute: node index.js bucket123 After uploading the code to an EC2 instance (named “SDK demo”), you can verify its contents by running commands such as ls and cat index.js on the instance. When you execute the application with: node index.js iam-role-kodekloud-demo you might see output similar to: 123, then that's going to create a bucket called bucket123. 
[ec2-user@ip-172-31-18-206 app]$ node index.js bucket123 This confirms that the instance is running the code and creating the bucket, assuming the provided credentials are valid. Handling Authentication Errors When you first run the application with provided (but incorrect) credentials, you might see an error like: [ec2-user@ip-172-31-18-206 app]$ node index.js iam-role-kodekloud-demo
Error: Code invalid access key ID. The AWS access key ID you provided does not exist in our records. Warning This error indicates that the access key ID is invalid. Before using IAM roles, our application used explicit access keys. For demonstration purposes, we then generated valid credentials by creating an IAM user. Creating an IAM User and Generating Credentials To generate valid credentials, follow these steps: Navigate to the IAM console and create a new user named “SDK demo.” Attach policies directly by searching for and selecting Amazon S3 Full Access . In the Security Credentials tab for the new user, create an access key. For this lesson, choose the Command Line Interface (CLI) option, then click ""Next"" and ""Create Access Key."" After copying the correct Access Key and Secret Access Key into your code, update the snippet as follows: const { S3Client, CreateBucketCommand, GetObjectCommand } = require(""@aws-sdk/client-s3"");

// Set the region and provide valid credentials
const accessKeyId = ""AKIAIAIWSJ5U7MTRXX52"";
const secretAccessKey = ""WW1UNLSS/bIa+V1qYpXRlC4vQpNb0EQGKrg7D73"";

const s3Config = { region: ""us-east-1"" };

if (secretAccessKey != """" || null) {
    s3Config.credentials = {
        accessKeyId,
        secretAccessKey,
    };
}

const s3Client = new S3Client(s3Config);

// Create the parameters for calling createBucket
var bucketName = process.argv[2];

// Call S3 to create the bucket
const main = async () => {
    try {
        const response = await s3Client.send(
            new CreateBucketCommand({ Bucket: bucketName })
        );
        console.log(response);
    } catch (e) {
        console.log(""failed to create bucket"");
        console.log(e);
    }
};

main(); Run the updated application with: node index.js iam-role-kodekloud-demo This produces an output similar to: {
    '$metadata': {
        httpStatusCode: 200,
        requestId: '3G32E56RK5V2Z3NH',
        extendedRequestId: 'yb9KufnGSDqX1DoM5/GdVQ+uImphU7RaqxludjBzIMDxsQipmJP8XiTWNZC+C2+x2fk1Gjhfno=',
        cfId: undefined,
        attempts: 1,
        totalRetryDelay: 0
    },
    Location: 'iam-role-kodekloud-demo'
} After verifying through the AWS S3 console, you should see the new bucket listed: Transitioning to IAM Roles To remove the need for managing access keys manually, we now transition to using IAM roles. When you remove credentials from your code and run the application, you’ll encounter an ""InvalidAccessKeyId"" error, as expected. To resolve this, create an IAM role for the EC2 instance by following these steps: In the IAM console, select Roles and click Create Role . Choose AWS service as the trusted entity type, since the role will be used by an EC2 instance. For the use case, select EC2 so the instance can perform actions on your behalf—specifically, interacting with S3. Attach the Amazon S3 Full Access policy to the role. Name the role (e.g., ""AWS SDK S3"") and use the following trust policy: {
    ""Version"": ""2012-10-17"",
    ""Statement"": [
        {
            ""Effect"": ""Allow"",
            ""Action"": [
                ""sts:AssumeRole""
            ],
            ""Principal"": {
                ""Service"": [
                    ""ec2.amazonaws.com""
                ]
            }
        }
    ]
} After creating the role, update your EC2 instance to use it. In the AWS EC2 management console: Locate your instance (SDK demo), select Security , then Modify IAM Role , and choose the newly created role “AWS SDK S3.” Now that the role is attached and your code no longer contains hard-coded credentials, run the application again: [ec2-user@ip-172-31-18-206 app]$ node index.js iam-role-kodekloud-demo The bucket is now created successfully. Verify its creation in the S3 console: Summary There are two primary methods for authenticating an application with AWS services: Authentication Method Description Explicit IAM User Credentials Uses generated access keys. Manual credential management is required. IAM Roles for EC2 Instances Automatically provides authentication by attaching a role to the EC2 instance; eliminates hard-coded keys. Using IAM roles simplifies security management by allowing your EC2 instance to assume a role with the correct permissions—enabling seamless interactions with AWS services like S3. Note This lesson demonstrated the transition from explicit credentials to using IAM roles, enhancing your application's security posture while reducing manual credential management. This concludes our lesson on using IAM roles for EC2 instances. For further reading, visit the AWS Documentation and explore the IAM User Guide . Watch Video Watch video content Practice Lab Practice lab"
AWS Solutions Architect Associate Certification,Resource Access Manager,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/Resource-Access-Manager,"AWS Solutions Architect Associate Certification Services Management and Governance Resource Access Manager In this article, we explore AWS Resource Access Manager (AWS RAM), a service that simplifies the process of sharing AWS resources across multiple accounts. By leveraging AWS RAM, you can avoid creating duplicate resources, improve cost efficiency, and maintain a secure environment. Key Use Cases AWS RAM supports various resource sharing scenarios. For instance: S3 Bucket Sharing: When you create an S3 bucket in one account, you can share it with authorized accounts using AWS RAM. This allows collaborative access, where users can securely view, modify, or retrieve objects. Any changes made—such as adding or removing items—are reflected automatically across all accounts with access. Subnet Sharing: If you create a subnet in one account, you can share it with other accounts. The recipient accounts can then deploy EC2 instances into the shared subnet, even though the physical infrastructure remains in the provider's account. This capability streamlines operations in multi-account AWS environments. Before AWS RAM, organizations often faced challenges such as fragmented resources—including VPCs, Transit Gateways, and subnets—spread across numerous accounts. This fragmentation led to increased costs, inefficiencies, and complex IAM policy configurations. Additionally, establishing complex network configurations like VPC peering or VPN connections was necessary to share resources, further complicating management and troubleshooting. Security Consideration Ensure that IAM policies are meticulously configured when sharing resources to avoid potential security vulnerabilities. Misconfigurations can lead to unauthorized access or data breaches. How AWS RAM Works The process to share resources with AWS RAM is straightforward: Create a Resource: Begin by creating the resource in your AWS account. Select the Resource: Identify the resource you wish to share. Choose Principals: Specify the AWS accounts (principals) with which you want to share the resource. Accept the Share: On the recipient account, accept the resource share to gain access. Once the resource share is established, you can continuously monitor, manage, or revoke access as needed. For more detailed information on managing AWS resources, refer to the AWS Documentation . Note AWS RAM not only simplifies resource sharing but also enhances collaboration among different teams by centralizing resource management. This makes it an essential tool for enterprises with multi-account AWS environments. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Resilience Hub,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/Resilience-Hub,"AWS Solutions Architect Associate Certification Services Management and Governance Resilience Hub In this lesson, we explore the AWS Resilience Hub—a robust solution designed for managing disaster recovery in AWS cloud environments. Disasters, whether natural (such as earthquakes), technical (system failures), or human-induced (including DDoS attacks), can disrupt operations, compromise data integrity, and impede business continuity. When a disaster occurs, data security, application availability, and overall business operations can be severely impacted. To address these challenges, AWS recommends following the Well-Architected Framework. This framework promotes strategies like: Reliable backup mechanisms Multi-regional deployments for critical components Robust security measures An effective disaster recovery plan These strategies are aimed at minimizing disruptions by ensuring rapid restoration of systems, applications, and data. Overview of AWS Resilience Hub AWS Resilience Hub simplifies and automates your disaster recovery process in the AWS cloud. It offers a centralized console from which you can manage resilient activities such as backup scheduling and setting recovery objectives—including Recovery Point Objective (RPO) and Recovery Time Objective (RTO). For clarity: RPO (Recovery Point Objective): The maximum tolerable timeframe in which data might be lost due to an incident. RTO (Recovery Time Objective): The targeted period within which systems and business processes must be restored to prevent unacceptable consequences. After deployment, Resilience Hub continuously monitors your application's resiliency posture. In the event of an outage, it offers detailed insights and swiftly initiates recovery procedures. Acting as a centralized overseer, it monitors services across your application, ensuring efficient failover when required. When a disruption occurs, Resilience Hub identifies the root cause, alerts operators, and facilitates execution of standard recovery procedures. Configuring and Assessing Resilience Configuring Resilience Hub begins by specifying which resources to protect. There are several methods to define these resources: Use the CloudFormation stack that created your resources Utilize a resource group If managing with Terraform, reference the Terraform state file After defining your resources, Resilience Hub analyzes them to identify potential weaknesses. You then set your application’s resilience objectives by attaching a resilience policy that outlines specific RTO and RPO targets for various disruption scenarios. With the application and policy in place, initiate a resiliency assessment. This assessment cross-checks your configuration against the defined resiliency policy and generates a report detailing where adjustments might be necessary. Based on the assessment report, you receive targeted recommendations to bolster your application's resilience. These recommendations may advise configuration changes, setting up alarms, implementing regular testing, or revising your recovery standard operating procedures (SOPs). Following updates to your application and policy, you can re-run the assessment. This iterative process continues until the desired resilience targets are met. Testing and Evaluating Resilience Once your resilience configuration is up-to-date, it's important to conduct tests that mimic real-world outages. This testing evaluates whether your AWS resources and applications restore within your target RTO and meet your RPO expectations. These simulated tests are critical for understanding and enhancing the overall resiliency and recovery speed of your systems. Note Regular testing of your disaster recovery plan ensures that your infrastructure remains resilient and compliant with business continuity requirements. Key Features and Benefits of AWS Resilience Hub The benefits of integrating AWS Resilience Hub into your disaster recovery strategy include: Centralized Resiliency Planning: Manage and monitor your recovery plans across multiple AWS services and accounts from a single interface. Automated Backup and Recovery: Streamline critical data backup and facilitate rapid restoration in the event of a disaster, thereby minimizing downtime and data loss. Compliance Adherence: Implement disaster recovery processes that meet legal and regulatory standards. Continuous Monitoring: Obtain real-time insights and alerts on potential issues, enabling proactive measures to prevent disruptions. Benefit Category Description Example Tools/Features Centralized Management Unified console for monitoring resiliency across AWS services AWS Resilience Hub Dashboard Automated Backup & Recovery Automation of backup processes and rapid recovery during outages Scheduled backups, automated failover mechanisms Compliance Adherence to regulatory standards and best practices Recovery policies aligned with legal requirements Continuous Monitoring Proactive insights and alerting for early detection of potential issues Real-time tracking, automated alerts AWS Resilience Hub provides a comprehensive ecosystem to ensure your AWS infrastructure is well-prepared to withstand, recover from, and ultimately prevent the adverse effects of disasters. For further reading on AWS disaster recovery strategies, consider reviewing the AWS Well-Architected Framework and related resources in the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Resource Group and Tag Manager,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/Resource-Group-and-Tag-Manager,"AWS Solutions Architect Associate Certification Services Management and Governance Resource Group and Tag Manager In this lesson, we explore how to efficiently organize and manage your cloud resources using Resource Groups and the Tag Manager. These tools simplify navigation and administration, saving time and reducing errors when handling your AWS resources. Resource Groups let you consolidate resources based on custom tags rather than the default grouping by AWS service (e.g., EC2, Lambda) in the AWS Management Console. Typically, you have to navigate through multiple sections—first checking EC2 instances, then Lambda functions, and so on—to locate all resources associated with a specific application. With Resource Groups, you can create custom views to display all related assets on a single page. For instance, you might assign tags like: Environment: Development Environment: Test Environment: Production Using these tags, you can create dedicated resource groups for each environment. This approach enables you to view and manage all related resources in one consolidated place instead of filtering through various service pages. Tip Utilize consistent tagging across your organization to improve visibility and management. Uniform tags help ensure that your resource groups accurately reflect your application's structure and the departments or teams that manage them. Resource groups are also invaluable when managing projects across different departments or teams. By assigning specific tags to each department’s resources, you can create individual resource groups that allow teams to focus solely on their assets, eliminating distractions from resources owned by other groups. Grouping resources together not only clarifies structure but also facilitates bulk operations. For example, if you need to update or patch several EC2 instances for an application, you can add those instances to a resource group and apply the changes simultaneously. Likewise, modifying network settings—such as adjusting port access for a set of instances—becomes much simpler with resource groups, as you can execute the change across all related resources in one go. Using Resource Groups can significantly streamline resource management by automating processes and reducing the need for manual navigation through multiple pages in the management console. This automation enhances efficiency and decreases the likelihood of configuration errors. For more detailed insights, check out the following resources: AWS Resource Groups Documentation AWS Tagging Best Practices Watch Video Watch video content"
AWS Solutions Architect Associate Certification,IAM identity Center SSO,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/IAM-identity-Center-SSO,"AWS Solutions Architect Associate Certification Services Security IAM identity Center SSO In this lesson, we explore IAM Identity Center—a service that extends traditional AWS IAM functionality by providing centralized authentication and authorization across multiple AWS accounts. By the end of this guide, you'll understand the operational differences and advantages of using IAM Identity Center over standard IAM. Imagine an organization managing several AWS accounts. In a traditional setup, if a user (for example, Bob) requires access to resources in accounts one, two, and three, you must manually create an IAM user in each account with the necessary permissions. This repetitive process becomes even more cumbersome when another employee, Mark, needs access to different sets of accounts. Managing authentication and authorization across multiple AWS accounts comes with several challenges: Duplicated user management tasks due to creating separate IAM users in each account. Inconsistent permissions, as changes in one account need manual replication in others, resulting in configuration drift. Complex auditing processes because of the absence of centralized tracking. Time-consuming role definitions and individual permission management for each AWS account and application. Centralized Management Advantage IAM Identity Center centralizes user management, allowing you to create users once and assign them to various AWS accounts with defined permission sets. This drastically reduces administrative overhead and minimizes errors. IAM Identity Center simplifies operations by letting you manage users from a single location. Instead of creating a new user in every account, you define a user once within the Identity Center and assign them appropriate access to specific accounts. This centralized approach streamlines access control and enhances security. The operational model of IAM Identity Center is similar to that of traditional IAM. Here’s how it works: Create users in the Identity Center. Assign users to specific AWS accounts. Define user permissions using a ""permission set,"" which is essentially a collection of one or more IAM policies indicating what actions the user can perform in the account. When a user initiates an action in AWS, IAM Identity Center handles both authentication and authorization. Unlike traditional IAM—where users are created and managed individually in each account—IAM Identity Center allows for integration with well-known cloud identity providers such as Active Directory, OneLogin, Okta, and Microsoft Active Directory. This integration facilitates Single Sign-On (SSO) access to AWS resources and various applications. IAM Identity Center also integrates seamlessly with business cloud applications like managed Grafana services, SageMaker, and Systems Manager, as well as custom SAML-enabled applications. This versatility makes it an effective solution for centralized workforce identity management. Key Benefits Centralized user management across multiple AWS accounts Streamlined process with permission sets Integration with popular cloud identity providers Simplified auditing and reduced configuration drift Scalable and free of charge for AWS access management In summary, Amazon Identity Center—formerly AWS Single Sign-On—is a powerful and centralized platform designed to connect or create workforce identities in AWS. It simplifies access management across multiple AWS accounts and applications, scales effortlessly, and even supports SSO for Amazon EC2. For more detailed information on managing AWS identities and access, check out the AWS Documentation and the IAM Identity Center User Guide . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,License Manager,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/License-Manager,"AWS Solutions Architect Associate Certification Services Management and Governance License Manager In this lesson, we dive into the AWS License Manager service, which simplifies the management of software licenses across your environments—from cloud-based deployments to traditional on-premises systems. Whether you're managing various operating systems, databases, or enterprise applications, having clear oversight of your license usage is essential to avoid costly compliance issues. Overview Organizations often face challenges such as a lack of visibility into license usage and entitlements. This can lead to inadvertent license violations and inefficiencies like over-purchasing licenses, which may incur unnecessary expenses. The Need for Efficient License Management Without effective license management, you risk breaching software license terms, potentially leading to fines and legal complications. Large organizations, particularly those with multiple teams and hundreds of engineers, might inadvertently purchase more licenses than needed. This creates wastage of resources and escalates costs. To tackle these challenges, AWS License Manager provides a centralized solution that manages licenses from various vendors—such as Microsoft, SAP, Oracle, and IBM—across both AWS and on-premises environments. This service not only enforces licensing rules but also tracks and reports usage to ensure compliance. How AWS License Manager Works The process begins with defining a license configuration that outlines your licensing terms. This configuration typically includes: The number of licenses purchased The scope of resources they cover (e.g., instances, cores, etc.) Expiration dates for the licenses Once these configurations are set, they are applied to your AWS resources—such as EC2 instances, RDS databases, and more. When an AWS resource is launched, License Manager evaluates the launch request against your predefined licensing rules. If a launch request violates these rules, the service can block the creation of the resource, ensuring you remain compliant. Important Failure to adhere to licensing rules can result in overspending or even legal repercussions. Always verify that your resource deployment plans align with the configurations set in License Manager. Additionally, AWS License Manager continuously monitors your license usage, producing detailed reports that help you avoid unnecessary license purchases while maintaining compliance with vendor agreements. Key Features of AWS License Manager License Manager serves as a centralized hub for all your licensing needs. Its robust features include: Centralized License Management: Oversee licenses across cloud and on-premises environments. License Tracking: Monitor usage to ensure that deployments stay within allocated limits. Enforcement of Licensing Rules: Prevent unauthorized resource deployments that could exceed license limits. Cross-Account Management: Consolidate license management for organizations with multiple AWS accounts. Discovery and Reporting: Automatically detect installed software and generate detailed usage reports. Conclusion AWS License Manager offers a comprehensive solution for managing software licenses. Its ability to enforce licensing rules, track usage, and consolidate management across multiple environments makes it an indispensable tool for organizations striving to optimize resource expenditure and ensure compliance. For further details on AWS licensing best practices and other AWS services, be sure to explore additional resources in the official AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Resource Explorer,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Management-and-Governance/Resource-Explorer,"AWS Solutions Architect Associate Certification Services Management and Governance Resource Explorer In this lesson, we explore AWS Resource Explorer—a powerful tool designed to simplify the discovery and management of your AWS resources. With Resource Explorer, you can efficiently locate resources across regions, ensuring optimal resource management and cost attribution while reducing compliance risks. Key Benefit By centralizing resource management, Resource Explorer helps in generating insightful reports and streamlines audits, saving valuable time. Overview of AWS Resource Explorer Before the introduction of Resource Explorer, managing numerous AWS resources spread over various regions was both inefficient and error-prone. Without a centralized system, cost allocation to specific projects or departments was cumbersome, and inconsistencies in tagging and configurations raised compliance risks. Resource Explorer addresses these challenges by enabling users to locate resources using tags, keywords, and other metadata. Setting up Resource Explorer is straightforward, requiring just a few minutes to enable and configure. User Roles in Resource Explorer There are two primary types of users for Resource Explorer: Resource Explorer Administrator The administrator is an IAM principal assigned the permissions necessary to manage Resource Explorer settings. Key responsibilities include: Enabling Resource Explorer in AWS regions. Updating index types. Creating views. Granting search permissions. Resource Explorer User These users have permissions to perform searches for resources. They can utilize the AWS Management Console, SDKs, or CLI to locate and manage resources effectively. Understanding Indexes in Resource Explorer In Resource Explorer, an ""index"" is the collection of information about AWS resources within a specific region. There are two types of indexes: Local Index: Exists within a single AWS region. Aggregator Index: Compiles local indexes from all regions where Resource Explorer is enabled. Consider a scenario where an administrator has enabled Resource Explorer in three AWS regions and selects the AP South Mumbai region as the aggregator index: Each region with a local index automatically replicates its data to the designated aggregator index (in this example, AP South Mumbai). For instance, both US East 1 and EU West 2 replicate their local indexes to AP South 1. As a result, users query the aggregator index in AP South 1 to access consolidated information from all enabled regions. Benefits of AWS Resource Explorer AWS Resource Explorer offers several key advantages: Benefit Description Unified Resource Search Easily locate resources across all regions with one centralized search tool. No Additional Charges Resource Explorer is free to use, eliminating extra costs. Time Efficiency Quickly identify and manage resources from a single, centralized location. Eliminates Custom Solutions No need to develop and maintain a custom search infrastructure. Enhanced Console Experience The unified search function in the AWS Management Console simplifies administration. Final Thoughts AWS Resource Explorer is a robust solution for efficient resource management. Its centralized approach not only simplifies operations but also ensures that administrators can monitor and manage assets across multiple regions with ease. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Cognito,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/Cognito,"AWS Solutions Architect Associate Certification Services Security Cognito Amazon Cognito is a powerful service designed to simplify user authentication and identity management in your applications. By leveraging Cognito, you can avoid the risks and complexities of building your own authentication system, ensuring secure password storage, seamless integration with third-party identity providers, rapid implementation, cost efficiency, and scalability. Note Using a proven service like Amazon Cognito allows you to offload critical security and maintenance tasks, letting you focus on core application features. Cognito is built for application-level user authentication rather than AWS resource access. While AWS IAM and IAM Identity Center manage AWS account authentication, Cognito handles key user actions such as sign-up, sign-in, and sign-out, which simplifies user management for developers. Cognito provides several clear benefits: Secure password storage that meets industry standards. Integration with multiple third-party identity providers such as Facebook, Google, or Apple. Rapid authentication setup to help you launch your application quickly. A pay-as-you-go pricing model where you only pay for the resources you use. Scalability to support millions of users and hundreds of transactions per second. Cognito User Pools AWS Cognito offers two main components, starting with Cognito User Pools, which provide a managed user directory and authentication service. User Pools support user registration, authentication, and token issuance for applications hosted on AWS, other cloud platforms, or on-premises. This makes it an excellent solution for applications that utilize services like API Gateway, Lambda functions, and DynamoDB. How Cognito User Pools work: Users register and authenticate against the Cognito User Pool. Upon successful authentication, the user receives a token. The token is then transmitted from the client to your application during subsequent operations. The application verifies the token to confirm the user’s authentication status. The diagram below represents a typical serverless application architecture using Cognito User Pools in conjunction with AWS API Gateway, Lambda, and DynamoDB: Cognito Identity Pools Cognito Identity Pools enable you to grant authenticated users temporary access to AWS resources. After a user authenticates via the User Pool and obtains a token, that token can be exchanged with an Identity Pool to receive temporary AWS credentials. With these credentials, users can securely interact with services such as Amazon S3 for file uploads and other AWS resources. This approach seamlessly integrates application-level authentication with AWS resource authorization. The following diagram outlines the authentication flow when using Cognito Identity Pools: Summary Use Cognito User Pools for managing application-level authentication, including registration, sign-in, and token issuance. Use Cognito Identity Pools to grant authenticated users temporary AWS credentials for accessing AWS resources. By adopting AWS Cognito, you can quickly build secure, scalable applications while leveraging the robust security and integration capabilities provided by AWS. For additional resources and guidance, check out: AWS Cognito Documentation AWS Lambda Overview Amazon DynamoDB Documentation API Gateway Documentation Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Verified Permissions,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/Verified-Permissions,"AWS Solutions Architect Associate Certification Services Security Verified Permissions In this lesson, we explore Verified Permissions—a scalable AWS service that streamlines permission management for custom applications. Verified Permissions centralizes authorization, allowing you to control user access after authentication is set up. Instead of hardcoding permissions in your application, AWS handles policy management for actions like viewing or deleting content. This centralized model enhances security, simplifies auditing, and decouples authorization from core business logic. It leverages the Cedar policy language to define granular permissions. With Verified Permissions, you can manage policies via the AWS console, CLI, or SDK. Billing is based on the number of authorization requests processed each month, encouraging efficient permission management. The process begins by creating a policy schema, where developers define the authorization model—specifying what actions users are permitted or forbidden to perform. These schemas and their associated policies are securely stored in Amazon Verified Permissions. When your application—whether running on AWS Lambda, Amazon ECS, or another platform—needs to verify a user's permission, it sends a request to the Verified Permissions service. The service cross-references the relevant policy documents and responds with whether the user is authorized to perform the requested action. Centralized Authorization Benefits Decoupling authorization from your application development allows you to focus on business logic while ensuring secure, centralized permission management. Verified Permissions embraces a zero-trust architecture, where every access request is authenticated in real time. This model prevents inadvertent permission escalation by ensuring that no user gains unauthorized access due to embedded logic in the application. There are two primary ways to create policies: During development, you can define baseline permissions that serve as the foundation for your application. End users can also create custom policies, enabling dynamic and flexible authorization based on evolving needs. Billing Consideration Be aware that you're charged based on the number of authorization requests processed each month. Monitor your workload to manage costs effectively. Transcribed by Otter.ai Watch Video Watch video content"
AWS Solutions Architect Associate Certification,CloudTrail,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/CloudTrail,"AWS Solutions Architect Associate Certification Services Security CloudTrail AWS CloudTrail is an essential service that records every API call made within your AWS account. Acting as a comprehensive audit trail, CloudTrail logs details such as the caller's identity, the action performed, and the event timestamp. It covers API calls initiated via the SDK, AWS Management Console, or CLI, ensuring that every interaction with AWS services is thoroughly documented. Log Retention and Analysis By default, CloudTrail stores event logs for 90 days. If you require a longer retention period, you can configure CloudTrail to deliver logs to an Amazon S3 bucket. Once logs are in S3, you have the flexibility to analyze them further using services like OpenSearch and Athena for enhanced querying and insights. Real-Time Monitoring with AWS CloudWatch AWS CloudTrail logs can be ingested in real time by AWS CloudWatch . This integration allows you to: Create alarms based on specific events or log patterns. Trigger notifications through Amazon SNS when defined thresholds are met. For instance, establishing an SNS topic can alert your operations team immediately if a CloudTrail event indicates a potential security breach. Tip Consider integrating CloudWatch alarms with automated notifications to ensure prompt response to security events. Automated Responses with AWS Lambda You can further enhance your security posture by setting up AWS Lambda functions to perform custom actions in response to specific CloudWatch alarms. For example, a Lambda function can automatically disable a compromised AWS resource when a security-related CloudTrail event is detected. Benefits of AWS CloudTrail AWS CloudTrail is critical for maintaining robust security, compliance, and operational troubleshooting. Its capabilities include: Monitoring API activity across all AWS services. Conducting thorough security audits. Performing detailed forensic analysis. Supporting compliance requirements with extended log retention in Amazon S3. Security Advisory Always ensure your log data is securely stored and access to these logs is tightly controlled to prevent unauthorized access. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,NACLs and SecGroups,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/NACLs-and-SecGroups,"AWS Solutions Architect Associate Certification Services Security NACLs and SecGroups In this article, we dive into Network Access Control Lists (NACLs) and security groups—two essential components for managing and securing your AWS Virtual Private Cloud (VPC). We will explore what NACLs are, how they compare to security groups, and how both work together to provide layered security. Understanding NACLs A Network Access Control List (NACL) serves as an additional firewall for your VPC by regulating traffic moving in and out of subnets. NACLs offer flexibility as you can associate one NACL with multiple subnets if they require similar traffic filtering. Each rule within a NACL is defined by: A rule number (lower numbers indicate higher priority) The traffic type (e.g., all traffic, TCP, or UDP) Port range (for TCP/UDP) Source IP address (for inbound traffic) Action (either allow or deny) Below is an example demonstrating a default NACL configuration with several rules: Note When you create a VPC and its corresponding subnets, a default NACL is automatically assigned that permits all traffic until you modify the rules to enforce more restrictive filtering. Comparing NACLs and Security Groups AWS employs two primary types of firewalls to secure your environment: NACLs : Operate at the subnet level, filtering both inbound and outbound traffic. Security Groups : Provide instance-level security, managing traffic to and from individual EC2 instances and other resources. Key Differences in Traffic Handling Feature NACLs Security Groups Level Subnet-level Instance-level State Stateless (requires explicit rules for both directions) Stateful (inbound rules automatically allow outbound responses) Rule Options Allow and Deny Allow only Note NACLs require you to configure rules for both directions since they are stateless, while security groups simplify management by automatically handling the response traffic. The diagram below summarizes these differences: Integrating NACLs and Security Groups NACLs and security groups complement each other by offering security at different layers of your VPC architecture: NACLs control traffic at the subnet level. Security groups protect individual instances. The following diagram illustrates how both security layers interact within a VPC that includes public and private subnets across multiple availability zones: Deep Dive: Security Groups Security groups act as virtual firewalls for individual instances, controlling inbound and outbound traffic. While every resource gets a default security group, you can tailor these groups to meet specific requirements. Managing Security Groups Effectively Consider a scenario with multiple web servers that need only HTTP (port 80) and HTTPS (port 443) access. Instead of assigning unique security groups to each server, create one comprehensive web security group with the necessary rules and assign it to all. You can also attach multiple security groups to an EC2 instance to merge rules—such as combining web traffic with management traffic (SSH on port 22). Configuring Security Group Rules Security group rules are divided into two sections: Inbound Rules : Control incoming traffic. Outbound Rules : Control outgoing traffic. Below is an example configuration for an inbound rule that permits SSH access: # Inbound rule configuration for SSH access
{
    ""Type"": ""SSH"",
    ""Protocol"": ""TCP"",
    ""PortRange"": ""22"",
    ""Source"": ""0.0.0.0/0""
} In this configuration, the rule allows SSH (TCP port 22) access from any IP address (0.0.0.0/0). Remember that security groups only support ""allow"" rules. Outbound rules follow a similar format but apply to traffic leaving the instance. The diagram below shows a user interface section for configuring these inbound rules specifically for SSH: Automatic Traffic Exceptions in Security Groups Certain traffic types are automatically permitted by security groups, ensuring essential communication is not interrupted. These include: Amazon DNS servers Amazon DHCP traffic EC2 instance metadata service ECS task metadata endpoints Windows license activation traffic Amazon Time Sync Service Reserved IP addresses used by the default VPC router The image below lists these exceptions: Note You do not need to create explicit rules for the above protocols; they are automatically allowed by AWS to ensure critical services run smoothly. This comprehensive guide has provided an in-depth look at how NACLs and security groups function, highlighting their individual roles and how they synergize to secure your AWS infrastructure effectively. For further reading and advanced configuration tips, be sure to check out the AWS Documentation and related resources. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Artifact,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/Artifact,"AWS Solutions Architect Associate Certification Services Security Artifact In this lesson, you'll explore AWS Artifact—a service that centralizes access to AWS third-party audit reports and helps you verify compliance with numerous regulatory frameworks. AWS Artifact simplifies your compliance efforts by aggregating audit documents from assessments such as HIPAA, PCI DSS, SOC, ISO, and many more. Note AWS Artifact provides a one-stop repository for all audit reports generated during AWS assessments, making it easier than ever to maintain a compliant infrastructure. Overview AWS undergoes regular audits by multiple governing bodies to confirm its adherence to global standards. The audit documents available through AWS Artifact offer detailed insights on security and compliance measures for various AWS services. These documents are essential when demonstrating compliance during organizational audits. User-Friendly Compliance Management AWS Artifact features an intuitive portal within the AWS Management Console. With a single click, you can download comprehensive audit reports on demand—greatly reducing the administrative overhead associated with traditional compliance documentation processes. In addition to providing easy access, AWS Artifact covers a wide range of global compliance standards. Whether you're looking for documentation to meet SOC, PCI, ISO, or other regulatory requirements, AWS Artifact delivers the necessary reports quickly and conveniently. Cost Efficiency and Benefits One of the standout advantages of AWS Artifact is that it is provided at no extra cost. This means you can leverage a robust compliance support framework without incurring additional fees, allowing you to focus resources on other critical areas of your infrastructure. Summary AWS Artifact not only centralizes access to compliance reports but also streamlines the process of managing and demonstrating regulatory adherence. This service is ideal for organizations looking to alleviate the complexities of compliance while maintaining rigorous security standards. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Config,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/Config,"AWS Solutions Architect Associate Certification Services Security Config In this lesson, we explore AWS Config—a comprehensive AWS service that continuously monitors, records, and tracks your AWS resource configurations along with any changes. AWS Config creates a historical timeline of resource states, making it indispensable for auditing, compliance, and security management. Think of AWS Config as a librarian in a busy library. Just as a librarian meticulously records who checked out a book and when it was borrowed and returned, AWS Config continuously tracks how each of your AWS resources is configured and used. Before AWS Config was introduced, organizations often encountered several challenges, including: Lack of complete visibility into resource configurations. Time-consuming, error-prone manual audits or reliance on ad hoc scripts. Configuration drift where resources deviated from their intended state. Increased security and compliance risks due to misconfigured resources. Difficulty understanding dependencies and relationships between resources. Key Advantages of AWS Config AWS Config addresses these challenges by: Keeping an inventory of all your AWS resources. Continuously monitoring and recording resource configurations. Capturing configuration changes over time. Reporting non-compliant resources. Enabling corrective actions through configurable rules. Sending notifications whenever a resource configuration changes. Analyzing relationships among different resources. The following table summarizes the key features of AWS Config and their benefits: AWS Config Feature Benefit Inventory Management Maintains an up-to-date list of all AWS resources. Continuous Monitoring Records real-time changes in resource configurations. Configuration History Stores historical configuration data for auditing purposes. Compliance Reporting Detects non-compliant resources through rule evaluations. Notification & Remediation Automatically triggers alerts and remediation actions. For example, if an employee modifies the security groups for an EC2 instance, AWS Config quickly detects and records the change. These configuration logs are then stored in an S3 bucket, and you can seamlessly integrate notifications or Lambda functions to trigger custom remediation actions when specific events occur. Furthermore, AWS Config features a robust rules-based system. With these rules, you define criteria and conditions to assess the compliance and overall configuration of your resources. AWS provides pre-defined rules that adhere to security best practices, or you can create custom rules tailored to your specific requirements. For instance, one rule verifies whether HTTP to HTTPS redirection is configured on all HTTP listeners of an Application Load Balancer. If one or more HTTP listeners lack this configuration, the rule marks the resource as non-compliant, thereby enforcing best practices and ensuring secure communication via HTTPS. In summary, AWS Config is a powerful automation tool that tracks AWS resource configurations, detects changes, and enforces security and compliance standards. Its continuous monitoring and rules-based evaluation help streamline resource management, address configuration drift, and mitigate potential compliance issues. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,GuardDuty,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/GuardDuty,"AWS Solutions Architect Associate Certification Services Security GuardDuty In this article, we explore AWS GuardDuty—a powerful security service that continuously monitors your AWS environment for suspicious and malicious activity. By leveraging machine learning and threat intelligence, GuardDuty detects potential threats, including unauthorized access, compromised instances, and harmful network traffic. When threats are identified, GuardDuty promptly generates alerts and provides actionable insights to help you respond effectively. GuardDuty analyzes logs from several critical sources: CloudTrail logs: Detect unusual API activity. VPC flow logs: Monitor for irregular internal traffic and suspicious IP addresses. DNS logs: Identify signs of compromised EC2 instances. EKS audit logs: Track unusual activity within your EKS cluster. Aggregating data from these sources allows GuardDuty to identify suspicious patterns and produce detailed reports on its findings. GuardDuty can also be integrated with other AWS services to automate remediation processes, such as triggering a Lambda function or notifying the appropriate teams when threats are detected. When a potential threat is detected, GuardDuty assigns a severity score ranging from 1 to 10: Low severity (1-3): The event is unusual but typically does not require immediate action. Medium severity (4-6): Suspicious activity deviates from the norm and may indicate a resource compromise. High severity (7-10): Indicates a critical issue where a resource is almost certainly compromised and demands immediate attention. This scoring system helps prioritize your response efforts effectively. GuardDuty also supports the use of custom datasets for enhanced threat detection. You can configure two sets of IP addresses: Trusted IP List (Whitelist): Identifies safe IP addresses within your AWS infrastructure, reducing false positives. Threat IP List: Contains known malicious IPs—supplied by third parties or defined internally—to ensure that any interaction with these addresses triggers an alert. Note Integrating custom IP lists can significantly enhance the detection accuracy by minimizing false positives and enabling a more targeted security posture. Detection Categories GuardDuty classifies detected threats into several categories. The table below provides an overview of each category and its common characteristics: Detection Category Description Reconnaissance Includes unusual API activity, port scanning, atypical login patterns or failures, and unauthorized port probing from known malicious IPs. Instance Compromise Indicates possible unauthorized cryptocurrency mining, the presence of malware that employs evasion techniques, malicious command and control, outbound denial of service attacks, or erratic network traffic. Account Compromise Encompasses suspicious activities such as API calls from unusual locations, use of anonymizing proxies, attempts to disable logging, changes that weaken password policies, unexpected resource deployments, region changes, or API calls from malicious IPs. Bucket Compromise Involves irregular S3 data access patterns, potential credential misuse, abnormal S3 activity from remote locations, or unauthorized access attempts from suspicious IP addresses. Warning High severity alerts should be prioritized immediately. Make sure your incident response plan is up-to-date and that your team is prepared to act swiftly in the event of a critical threat. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Inspector,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/Inspector,"AWS Solutions Architect Associate Certification Services Security Inspector In this article, we explore the robust security capabilities of AWS Inspector—a service designed to continuously scan your AWS workloads for software vulnerabilities and unintended network exposures. AWS Inspector automatically discovers and assesses EC2 instances, container images stored in the Elastic Container Registry (ECR), and AWS Lambda functions for potential security issues. One significant advantage of AWS Inspector is its continuous assessment capability throughout the resource lifecycle. Whether you're installing a new package, applying a patch, or addressing a new CVE disclosure, AWS Inspector will automatically re-scan the impacted resources to uncover vulnerabilities. When a vulnerability or open network path is identified, AWS Inspector generates a detailed finding. Each finding provides comprehensive information about the security risk, the affected resource, and actionable recommendations for remediation. Setting Up AWS Inspector Before initiating assessments, you need to define the AWS resources to be scanned by setting up an assessment target—a specific resource group. This allows you to target only production environments, for instance, while excluding development resources. To configure AWS Inspector effectively: Create an assessment target by grouping resources based on appropriate tags. Select the rules packages that align with your security requirements. Initiate the assessment to start scanning the defined resources. Note For detailed guidance on tagging resources for assessment targets, refer to the AWS Inspector documentation . Key Features of AWS Inspector AWS Inspector offers a centralized management approach through integration with AWS Organizations, which allows you to oversee vulnerability assessments across multiple AWS accounts. Its one-click activation and continuous monitoring provide a user-friendly yet powerful security solution. Key features include: Continuous and Responsive Scanning: AWS Inspector monitors for vulnerabilities in real time and triggers re-scans automatically after any significant system modifications. Detailed Findings and Severity Scoring: The service generates in-depth findings with severity scores, enabling you to prioritize remediation efforts based on risk. Customizable Centralized Dashboard: A user-friendly dashboard displays all findings, helping you to focus on resolving critical security issues. Types of Findings AWS Inspector classifies its findings into three primary categories: Package Vulnerability: These findings highlight vulnerabilities in software packages that could be exploited to compromise the confidentiality, integrity, or availability of your systems, potentially leading to unauthorized access. Code Vulnerability: These alerts identify exploitable segments in your code, which might result in injection flaws, data exposure, weak cryptographic practices, or insufficient encryption. AWS Inspector leverages automated reasoning and Amazon CodeGuru to assess the security compliance of your Lambda function code. Network Reachability: These findings point out open network paths to Amazon EC2 instances or overly permissive network configurations, including misconfigured security groups, access control lists, or Internet Gateways. AWS Inspector Workflow The AWS Inspector workflow is straightforward and can be summarized with the following steps: Define assessment targets by specifying the resources to be scanned. Select the relevant rules packages to identify vulnerabilities and ensure compliance. Launch the assessment to scan your configured environment. Review the findings generated by AWS Inspector. Investigate and remediate any issues detected. Note This continuous, responsive, and centralized scanning process makes AWS Inspector an essential security tool in dynamic AWS environments. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,CloudHSM,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/CloudHSM,"AWS Solutions Architect Associate Certification Services Security CloudHSM In this lesson, we explore CloudHSM, AWS's managed hardware security module service designed to centralize and secure cryptographic key management. While AWS Key Management Service (KMS) leverages encryption keys to store data in a secure and unreadable format, managing keys across various locations can sometimes increase the risk of accidental key exposure. CloudHSM mitigates this risk by consolidating key management into a single dedicated device. How CloudHSM Works A hardware security module (HSM) addresses key exposure by storing all your cryptographic keys in one secure device. This dedicated module is responsible for all cryptographic operations, including encryption and decryption. When you need to encrypt or decrypt data, you simply send it to the HSM, which processes the operation while keeping the keys securely stored within the device. This single point of key management minimizes the potential for key leakage, significantly enhancing the security of your cryptographic processes. Managed Service Advantages CloudHSM extends the traditional HSM model by offering a managed service in the cloud. Instead of investing in and maintaining your own HSM hardware, AWS manages the infrastructure for you. This cloud-based solution allows you to focus on your applications while securely processing encryption and decryption requests. Scalable and Resilient HSM Clusters CloudHSM supports deployment in clusters, enabling multiple HSM modules to work in tandem. This architecture not only improves performance by positioning the HSM as close as possible to your AWS services but also enhances scalability and availability. Unlike on-premises HSMs that require external communication from AWS services, cloud-deployed HSMs process requests more quickly and reliably. Client Integration To connect with a CloudHSM cluster, install the CloudHSM client on your EC2 instances or on-premises systems. This client facilitates cryptographic operations by securely communicating with the HSM. One key differentiation between CloudHSM and AWS KMS is the management of keys: with CloudHSM, only you have access to your keys. Although AWS manages the service, it does not have access to your encryption keys—a critical consideration for those requiring enhanced security. Key Benefits of CloudHSM Benefits of CloudHSM Dedicated HSM in the cloud that isolates and strictly controls access to your cryptographic keys Full control over encryption keys and operations, ensuring that only authorized users can access your data Scalable architecture that starts with a single HSM and can expand to clusters for improved performance and availability Seamless integration with multiple AWS services for robust data protection Enhanced compliance capabilities with a hardware-based key management solution CloudHSM offers a compelling solution for organizations seeking a highly secure, scalable, and managed hardware security module service in the cloud. By consolidating key management and providing exclusive control over encryption keys, CloudHSM significantly reduces the risk of key exposure while enhancing overall security and compliance. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Directory Service,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/Directory-Service,"AWS Solutions Architect Associate Certification Services Security Directory Service In this article, we explore the capabilities of AWS Directory Service and its seamless integration with Microsoft Active Directory (AD). By understanding how Active Directory functions as a centralized system to manage users, permissions, and access rights, you can better appreciate how AWS Directory Service simplifies directory management in the cloud. Active Directory is a directory service developed by Microsoft. It enables administrators to control access to applications, services, and network resources by centrally managing user permissions. Whether you're managing cloud applications, SaaS solutions, or on-premises applications, Active Directory is the backbone for secure and efficient access control. AWS Directory Service delivers a fully managed implementation of directory services similar to Active Directory. Just as Amazon RDS eliminates the complexities of database management, AWS Directory Service reduces the operational burdens associated with running your own Active Directory infrastructure across multiple availability zones. Key Benefit Using AWS Directory Service, organizations can achieve high levels of availability and scalability while ensuring secure directory operations in the cloud. Below, we outline the three primary modes in which AWS Directory Service can operate. Each mode is designed to meet distinct integration needs and use cases: Simple AD Mode In Simple AD mode, AWS Directory Service functions as a standalone directory within the AWS environment. This mode is ideal for AWS-compatible services that require basic directory functionalities. However, Simple AD is built using the Samba protocol and does not offer the full range of features available in Microsoft AD. It is intended for isolated deployments and does not support integration with existing on-premises directories. Managed Microsoft AD Mode For users who need advanced features and full compatibility with Microsoft Active Directory, AWS offers the Managed Microsoft AD mode. This service deploys a genuine instance of Microsoft AD in the AWS cloud, making it suitable for applications that depend on specific AD functionalities. Additionally, if you have an existing on-premises Active Directory, you can establish a trust relationship with your Managed Microsoft AD instance to create a seamless hybrid environment between your on-premises and cloud resources. AD Connector Mode Organizations that already maintain an on-premises Active Directory and prefer not to deploy a separate cloud instance can opt for the AD Connector mode. In this configuration, AWS Directory Service provides a proxy that connects AWS services, such as AWS WorkSpaces, directly to your on-premises AD. This solution avoids duplicating directory infrastructure in the cloud while still enabling secure integration with AWS services. Quick Comparison Table Directory Service Mode Description Key Use Case Simple AD Standalone directory in AWS using Samba; supports basic directory operations. Lightweight directory needs that do not require full AD features. Managed Microsoft AD Full-featured Microsoft AD deployed in AWS; supports trust relationships with on-premises AD. Applications that demand advanced AD functionalities and hybrid setups. AD Connector Acts as a proxy to connect AWS services to an existing on-premises Active Directory. Integrating AWS services with an existing on-premises directory without duplication. In summary, AWS Directory Service offers flexible modes—Simple AD, Managed Microsoft AD, and AD Connector—to address a variety of organizational needs. Whether you require a standalone cloud directory, an advanced Microsoft AD experience, or a proxy integration with your on-premises system, this managed service ensures enhanced performance, high availability, and seamless integration across your deployment scenarios. Further Reading For more in-depth information on AWS Directory Service and its features, refer to the AWS Directory Service Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Demo Setting up CloudTrail for the first time,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/Demo-Setting-up-CloudTrail-for-the-first-time,"AWS Solutions Architect Associate Certification Services Security Demo Setting up CloudTrail for the first time In this lesson, we demonstrate how to work with AWS CloudTrail effectively. You will learn how to view the past 90 days of events, create a CloudTrail trail to store and forward logs, and review detailed log records in both Amazon S3 and CloudWatch. Viewing CloudTrail Events To begin, search for the CloudTrail service in the AWS Console. Once in the CloudTrail dashboard, click on Event History to view recent events. For example, selecting a ""CreateUser"" event lets you examine critical details such as the event time, the user who initiated the action, the event type, the source IP address, and the access key used. Expanding a specific event (like the ""CreateUser"" event) provides a detailed view that includes: User identity Event time Action details Event metadata You also have the option to view the event record in JSON format. Consider the following example of a CloudTrail event for a user creation action: {
  ""eventVersion"": ""1.08"",
  ""userIdentity"": {
    ""type"": ""Root"",
    ""principalId"": ""841860927337"",
    ""arn"": ""arn:aws:iam::841860927337:root"",
    ""accountId"": ""841860927337"",
    ""accessKeyId"": ""ASIAIAAWSJ5UUDCXQU45"",
    ""sessionContext"": {
      ""sessionIssuer"": {},
      ""webIdFederationData"": {},
      ""attributes"": {
        ""creationDate"": ""2023-10-18T00:46:36Z"",
        ""mfaAuthenticated"": ""true""
      }
    }
  },
  ""eventTime"": ""2023-10-18T01:12:39Z"",
  ""eventSource"": ""iam.amazonaws.com"",
  ""eventName"": ""CreateUser"",
  ""awsRegion"": ""us-east-1"",
  ""sourceIPAddress"": ""173.73.184.248"",
  ""userAgent"": ""AWS Internal"",
  ""requestParameters"": {
    ""userName"": ""sdk-demo""
  }
} Note CloudTrail by default displays events for the past 90 days. For long-term log storage or integration with services like Amazon S3 and CloudWatch, you must create and configure a CloudTrail trail. Setting Up a CloudTrail Trail To extend the log retention period and enable additional integrations, follow these steps to set up a CloudTrail trail: Create a New Trail: Click on Create trail in the CloudTrail dashboard. Name Your Trail: Enter a trail name (for example, ""KodeKloud CloudTrail demo""). Select Event Sources: By default, the trail captures events from all regions in your account. (If you wish to capture events from all accounts or your organization, select the appropriate option. In this demo, the organization option is not configured.) Configure Storage Settings: Choose a storage location by either creating a new S3 bucket or selecting an existing one. A new bucket will be created with the suggested name unless renamed. Optional Encryption: Enable encryption for your log files if required. In this demo, encryption remains disabled. Enable Log File Validation: Decide whether to enable this feature to detect any modifications or deletions of log files. SNS Notifications (Optional): Optionally, set up SNS notifications for new CloudTrail events or log file deliveries. This is disabled in the demo. Forward Logs to CloudWatch: Enable the CloudWatch logs option. You can create a new log group or select an existing one. In this demonstration, the option ""Create new log group"" is selected with default configurations. Configure IAM Role for CloudWatch: Set up a role to grant CloudTrail permissions to forward logs to CloudWatch. Choose New and leave the default configuration. When prompted, assign a role name (e.g., ""CloudTrail CloudWatch role""). Specify Event Types: By default, management events are selected. Although you have the option to log data and insight events, these are left unchecked in this demo. After reviewing your configuration, create the trail. Once the trail is active, navigate to your selected S3 bucket to view the stored log files. Exploring Logs in Amazon S3 When you follow the link provided in the CloudTrail console, you will be directed to the corresponding path within your S3 bucket. Inside the bucket: You will see an ""AWSLogs"" folder. Inside ""AWSLogs"", there is a folder corresponding to your account ID (for example, ""841860927337""). Within your account ID folder, open the ""CloudTrail"" folder. Logs are organized into sub-folders by region (in this demo, only ""us-east-1"" is present), then by year, month, and date. Inside the appropriate date folder, you will find JSON log files. Opening a JSON file in a new browser tab displays the raw log data. Since JSON may appear unformatted, you can copy and paste the data into a JSON viewer tool (e.g., JSON Viewer) to enhance readability. For instance, the following JSON snippet is an event record for a ""CreateRole"" action stored in the S3 bucket: {
  ""eventVersion"": ""1.08"",
  ""userIdentity"": {
    ""type"": ""Root"",
    ""principalId"": ""841860927337"",
    ""arn"": ""arn:aws:iam::841860927337:root"",
    ""accountId"": ""841860927337"",
    ""accessKeyId"": ""ASIAIAJW5J5USLDMR7RZ"",
    ""sessionContext"": {},
    ""sessionIssuer"": {},
    ""webIdFederationData"": {},
    ""attributes"": {
      ""creationDate"": ""2023-10-21T17:04:29Z"",
      ""mfaAuthenticated"": ""true""
    }
  },
  ""eventTime"": ""2023-10-21T17:13:23Z"",
  ""eventSource"": ""iam.amazonaws.com"",
  ""eventName"": ""CreateRole"",
  ""awsRegion"": ""us-east-1"",
  ""sourceIPAddress"": ""173.73.184.248"",
  ""userAgent"": ""Coral/Jakarta"",
  ""requestParameters"": {
    ""path"": ""/service-role/"",
    ""roleName"": ""Cloudtrail-cloudwatch-role"",
    ""assumeRolePolicyDocument"": ""{\n  \""Version\"": \""2012-10-17\"",\n  \""Statement\"": [\n    {\n      \""Effect\"": \""Allow\"",\n      \""Principal\"": {\n        \""Service\"": \""cloudtrail.amazonaws.com\""\n      },\n      \""Action\"": \""sts:AssumeRole\""\n    }\n  ]\n}""
  }
} Storing logs in an S3 bucket guarantees that your logs are retained beyond the default 90-day retention period. Monitoring with CloudWatch Next, let’s review how CloudTrail logs are forwarded to CloudWatch for real-time monitoring. To access CloudWatch logs: Search for the CloudWatch service in the AWS Console. Navigate to Log Groups . Here, you will find a log group that contains all the forwarded CloudTrail logs. Click on a log stream within the group to see the events in a structured JSON format. Below is an example of a CloudTrail event forwarded to CloudWatch: {
  ""eventVersion"": ""1.08"",
  ""userIdentity"": {
    ""type"": ""Root"",
    ""principalId"": ""841869297337"",
    ""arn"": ""arn:aws:iam::841869297337:root"",
    ""accountId"": ""841869297337"",
    ""accessKeyId"": ""ASIA..."",
    ""sessionContext"": {
      ""sessionIssuer"": {},
      ""webIdFederationData"": {},
      ""attributes"": {
        ""creationDate"": ""2023-10-21T17:04:29Z"",
        ""mfaAuthenticated"": ""true""
      }
    }
  },
  ""eventTime"": ""2023-10-21T17:19:44Z"",
  ""eventSource"": ""notifications.amazonaws.com"",
  ""eventName"": ""ListNotificationHubs"",
  ""awsRegion"": ""us-east-1"",
  ""sourceIPAddress"": ""173.73.184.248"",
  ""userAgent"": ""Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/118.0.0.0 Safari/537.36"",
  ""requestParameters"": null,
  ""responseElements"": null,
  ""eventID"": ""7dcad2c4-c99a-42c2-3c2e3c1f81a"",
  ""readOnly"": true,
  ""eventType"": ""AwsApiCall"",
  ""managementEvent"": true,
  ""recipientAccountId"": ""841869297337"",
  ""eventCategory"": ""Management""
} With CloudTrail integrated to store logs in Amazon S3 and forward them to CloudWatch, you gain both the long-term data retention you need and the ability to monitor changes in real time. Conclusion In this demonstration, you learned how to: View recent CloudTrail events in the AWS Console, Set up a CloudTrail trail to extend log retention and enable integration with S3 and CloudWatch, Navigate the S3 bucket structure to locate JSON log files, Use CloudWatch to monitor real-time events. Pro Tip: For further reading on AWS CloudTrail and its integration with other AWS services, visit the AWS CloudTrail Documentation and explore related tutorials on AWS Blogs . Enjoy exploring AWS CloudTrail to enhance the security and monitoring of your AWS account! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Private Certificate Authority,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/Private-Certificate-Authority,"AWS Solutions Architect Associate Certification Services Security Private Certificate Authority In this lesson, we explore AWS Private Certificate Authority (PCA) and its pivotal role in securing and managing private certificates within an organization. Understanding Private Certificate Authorities Imagine a national mint tasked with producing trusted currency. For instance, the US Treasury issues US dollars that feature unique serial numbers, watermarks, and advanced security measures. Similarly, a Private Certificate Authority acts as this trusted entity—but instead of minting money, it issues digital certificates that authenticate internal users and systems. These certificates are strictly for internal communications, ensuring your organization’s resources are accessed securely. Note Certificates issued by a PCA are intended solely for internal use. They are not designed for public-facing services on the Internet. How AWS Private Certificate Authority Works AWS Private Certificate Authority is engineered as a scalable, secure, and cost-effective solution to manage the lifecycle of private certificates. Consider the following benefits: It eliminates the need to deploy and manage your own certificate authority infrastructure. It provides certificates that safeguard internal communications between servers, applications, and employees. It integrates natively with AWS Certificate Manager (ACM), enabling automatic certificate renewal to prevent downtime caused by expired certificates. For those preparing for the AWS Solutions Architect Associate Certification exam, it’s crucial to understand the difference between AWS Certificate Manager and AWS Private Certificate Authority. Whereas AWS Certificate Manager issues certificates primarily for Internet-facing services, the PCA focuses on securing internal networks. Key Features of AWS Private Certificate Authority AWS Private Certificate Authority offers numerous features designed to simplify certificate management, such as: Issuing certificates exclusively for internal use. Seamlessly integrating with AWS Certificate Manager for storage and management of certificates. Enabling comprehensive audit capabilities with logging through services like AWS CloudTrail. Automating the full lifecycle of private certificates—from issuance and renewal to revocation. Supporting a cost-effective pay-as-you-go pricing model. Key Takeaway By leveraging AWS Private Certificate Authority, your organization can enhance internal security and streamline certificate management without the complexity and overhead of maintaining your own certificate infrastructure. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,KMS,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/KMS,"AWS Solutions Architect Associate Certification Services Security KMS This article provides an in-depth look at AWS Key Management Service (KMS) and its role in data encryption within AWS environments. Introduction to Encryption Encryption plays a pivotal role in protecting sensitive data. When data is stored without encryption, it is in plain text and vulnerable to unauthorized access. For instance, if your confidential health records, social security numbers, or credit card details are stored in plain text, any unauthorized user or hacker who gains access to the server could easily read this data. Encryption transforms data into a form that appears as gibberish without the proper cryptographic key. These keys are essential; without the correct key, decryption is impossible, ensuring both confidentiality and data integrity. Additionally, encryption guards against data tampering and secures data during network transmission. What is AWS KMS? AWS KMS is a managed service designed to simplify the creation and control of cryptographic keys used for AWS services and applications. With AWS KMS, you no longer have to manage the entire key lifecycle—AWS KMS handles key generation, rotation, deletion, and policy enforcement. It supports cryptographic operations such as encryption, decryption, digital signing, and verification. Key Attributes and Policies Each KMS key is characterized by multiple attributes: Key ID and Key Material: This comprises the cryptographic material used for encryption and decryption. In AWS KMS, the key material is either generated internally or imported. States: Keys may be enabled, disabled, pending deletion, or pending import. Policy: Allows you to control which users have access to the keys and the specific operations they can perform. Tags, Creation Date, Key Usage, and Key Specs: These metadata elements track configurations such as permitted operations (encrypt/decrypt for symmetric keys or sign/verify for asymmetric keys) and rotation settings. Integration with AWS Services AWS KMS is designed to integrate seamlessly with various AWS services, ensuring a secure environment: Amazon S3: Objects stored in S3 can be encrypted automatically using a KMS key. Amazon Elastic Block Store (EBS): Encrypted volumes leverage KMS for their encryption. Amazon RDS: Underlying data for RDS instances can be encrypted with KMS. KMS also integrates with monitoring tools such as AWS CloudTrail and Amazon CloudWatch. CloudTrail logs all API activities and actions associated with KMS keys, while CloudWatch tracks metrics like the number of cryptographic operations, latency, and related performance metrics. Types of Keys in KMS AWS KMS supports several key types to meet diverse security and operational requirements: Customer Master Keys (CMKs) / KMS Keys: The primary resource used to encrypt, decrypt, and re-encrypt data. With customer managed keys, you have full control over creation, deletion, and policy management; note that these keys incur a cost. Data Keys: These are used for encrypting large volumes of data. Because CMKs have a size limitation (around 4 kilobytes), AWS KMS generates data keys in two versions—plaintext and encrypted. The plaintext version is used to encrypt your data, while the encrypted version is stored with the data to allow later decryption. Imported Key Material: This option lets you import your own key material, providing additional control over key lifecycle management. AWS Managed Keys: These keys are automatically created, managed, and rotated by AWS for services like S3, EBS, and Redshift. Unlike customer managed keys, AWS managed keys cannot be modified and do not incur additional costs. AWS Owned Keys: Fully managed by AWS, these keys are used across various AWS accounts to protect metadata and operational elements and are not visible to individual accounts. There is a clear distinction between customer managed keys (which require user management and come with associated costs) and AWS managed keys (which are maintained automatically by AWS and are integrated with various services). Symmetric vs. Asymmetric Encryption In AWS KMS, encryption methods are categorized into two primary types: Symmetric Encryption Symmetric encryption uses a single key for both encryption and decryption. This is the most common method within KMS, where the same key handles the complete encryption-decryption cycle. Asymmetric Encryption Asymmetric encryption involves a key pair—a public key and a private key. The public key is used to encrypt data or verify a signature, while the private key is used to decrypt data or to create a digital signature. This method allows the public key to be shared openly without exposing the private key. Using Data Keys for Encrypting Large Payloads Since CMKs are limited to encrypting data up to approximately 4 kilobytes, AWS KMS employs data keys to handle larger payloads. The process of using data keys typically follows these steps: AWS KMS generates a data key and returns both its plaintext and encrypted versions. The plaintext version of the data key is used to encrypt your larger dataset. The encrypted data key is then stored alongside the encrypted data (for example, in an S3 bucket) so it can be retrieved and used later for decryption. When data decryption is required, AWS KMS first decrypts the stored encrypted data key, which is then used to decrypt the actual data. Summary AWS KMS offers centralized key management with seamless integration across AWS services. By utilizing AWS KMS, you can: Create, manage, and enforce access policies for encryption keys. Automatically rotate keys and manage various key types to meet your needs. Securely encrypt both small payloads (using CMKs) and large amounts of data (using data keys). Leverage AWS CloudTrail and CloudWatch for comprehensive auditing and monitoring of cryptographic operations. Key Insight AWS KMS is essential for safeguarding your data by ensuring confidentiality, integrity, and compliance throughout your AWS environment. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Security Hub,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/Security-Hub,"AWS Solutions Architect Associate Certification Services Security Security Hub In this article, we explore AWS Security Hub and its significant role in centralizing and simplifying security management within your AWS infrastructure. Think of a building equipped with various security devices—security cameras, metal detectors, motion detectors, and smoke alarms. Normally, each device reports to a different system, making it difficult to monitor all security events efficiently. Now, imagine if every device sent its data to one centralized dashboard. This is exactly what AWS Security Hub does for your AWS environment. AWS Security Hub aggregates findings from multiple AWS security services, including Inspector, GuardDuty, Macie, Lambda, and CloudWatch Events, as well as integrated third-party tools. This consolidation eliminates the need to log into each service separately. Instead, all security alerts, vulnerabilities, and compliance findings are available in one place. This centralized approach enables you to prioritize critical issues and automate responses—such as triggering AWS Lambda functions to remediate detected vulnerabilities. Key Benefit One of the standout features of Security Hub is its automated compliance checks. It continuously monitors your AWS resources against best practices and various security standards, ensuring that your infrastructure remains secure as it scales. How It Works Consider the following scenario: During routine scans, AWS Inspector detects a vulnerability on an EC2 instance. The finding is immediately sent to Security Hub. Security Hub then triggers an EventBridge event that activates a specific rule to invoke a Lambda function designed to apply a necessary patch automatically. This seamless, end-to-end workflow moves from detection to remediation without manual intervention. The diagram below summarizes this comprehensive solution. Various AWS services send their findings to Security Hub. From there, notifications can be generated or additional events can be triggered via EventBridge. These events may call on services such as Lambda functions, Step Functions, or Systems Manager actions to remediate vulnerabilities or execute intermediary steps until a resolution is achieved. Architecture Flexibility The flexibility of this architecture is one of its greatest strengths. It accommodates various security protocols, whether that means automatically applying patches to EC2 instances or isolating compromised resources. AWS Security Hub offers a robust and scalable solution that aligns with your specific security requirements, streamlining operations across your entire AWS environment. For further reading, check out these resources: AWS Security Hub Documentation AWS Inspector Overview AWS GuardDuty User Guide By centralizing security findings and automating responses, AWS Security Hub empowers you to maintain a stronger, more resilient security posture in a dynamic AWS landscape. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Macie,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/Macie,"AWS Solutions Architect Associate Certification Services Security Macie In this article, we explore AWS Macie—a powerful security and privacy service designed to protect sensitive data stored in AWS S3 buckets. Understanding Personally Identifiable Information (PII) Before diving into AWS Macie, it's essential to understand what constitutes personally identifiable information (PII). PII refers to any data that can be used to identify an individual. This includes details such as: Name Date of birth Email address Phone number Home address Passport number And other related data PII can potentially be exploited, making its secure handling vital for compliance and risk management. What Is AWS Macie? Amazon Macie employs machine learning and pattern matching to automatically discover, classify, and protect sensitive data within your AWS S3 buckets. It scans objects in S3 for data types including: Personally identifiable information (PII) Credit card numbers Social security numbers Passport numbers Driver's license numbers If Macie detects sensitive information, it promptly notifies you, allowing immediate action to safeguard your data. Integration with AWS Services For example, upon detecting sensitive data, you can trigger an AWS EventBridge event. This event can then invoke a Lambda function or send an email notification, alerting your data protection team to take swift remedial steps. AWS Macie Versus Other Security Tools Similar to AWS Inspector—which scans EC2 instances and Lambda functions for vulnerabilities—AWS Macie focuses on S3 buckets. However, instead of searching for vulnerabilities, Macie is designed to: Detect sensitive data that should not be stored openly Provide an extra layer of security even for encrypted data at rest This automation ensures that any sensitive data does not go unnoticed, reinforcing your data protection measures. Key Benefits of AWS Macie Using AWS Macie brings numerous advantages for managing and securing your S3 data: Benefit Description Enhanced Security and Compliance Automatically identifies sensitive data to support compliance with data protection laws. Proactive Alerts Provides timely notifications to ensure a quick response to potential security breaches. Scalability Dynamically assesses an increasing number of AWS resources as your environment grows. Improved Data Governance Strengthens data management practices by securing sensitive information stored in S3. Reduced Operational Costs Automates data handling and security processes, lowering overall operational expenses. Why Choose Macie? By leveraging AWS Macie, organizations can bolster their security posture and streamline compliance processes, making it an essential tool in today's data-driven environment. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,WAF,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/WAF,"AWS Solutions Architect Associate Certification Services Security WAF AWS Web Application Firewall (WAF) is a robust security tool designed to protect your web applications from a variety of common attacks such as SQL injection, cross-site scripting, and other advanced threats at the application layer (Layer 7). When your web application is positioned behind resources like an Elastic Load Balancer or an API Gateway, AWS WAF acts as the first line of defense. It inspects incoming HTTP requests and determines, based on custom-defined rules, whether to allow, block, or count each request. Because AWS WAF operates at Layer 7, it can interpret the HTTP protocol and allow more sophisticated actions like issuing CAPTCHA challenges or redirecting users. How AWS WAF Works AWS WAF uses a series of rules organized within what is called a Web Access Control List (Web ACL). A Web ACL is essentially a collection of rules that evaluate incoming requests based on various conditions. These conditions can include properties like: IP addresses HTTP headers Request bodies URI strings Packet sizes Specific geographic locations Rate-based conditions to mitigate DDoS attacks How Web ACLs Work AWS WAF processes each web request as follows: The request is made to a resource (e.g., a CloudFront distribution or an Elastic Load Balancer). AWS WAF inspects the request and evaluates it against the defined rules in the Web ACL in order of priority. If a request matches a rule, AWS WAF takes the specified action—allow, block, or count the request. If no rule is matched, the default action specified in the Web ACL is applied, typically allowing the request to reach the protected resource. This consistent process applies whether AWS WAF is protecting an EC2 instance behind an Application Load Balancer, an API Gateway, or even Lambda functions. Key Use Cases for AWS WAF AWS WAF seamlessly integrates with other AWS services—such as Firewall Manager and CloudWatch—providing a comprehensive security and compliance management strategy. It is particularly useful for: Protecting Against Common Web Attacks: Safeguard your web applications from SQL injection, cross-site scripting, and cross-site request forgery. API Security: Secure internet-facing APIs from unauthorized access and potential data exfiltration risks. Enforcing Access Rules: Implement authentication and authorization rules to ensure that only legitimate users and applications gain access. Securing Serverless Applications: Integrate easily with API Gateway to protect serverless applications. Layer 7 Traffic Filtering: Filter HTTP traffic based on methods, headers, URI strings, and body content. Important Security Reminder Always review and update your Web ACL rules regularly to ensure they encompass the latest security threats and vulnerabilities. Regular monitoring and adjustments can help maintain the integrity of your application's defenses. By leveraging AWS WAF's advanced capabilities, organizations can not only defend against sophisticated web threats but also maintain a seamless user experience through intelligent, rule-based traffic handling. For more detailed information on AWS WAF and its integration with other services, consider browsing the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Secrets Manager,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/Secrets-Manager,"AWS Solutions Architect Associate Certification Services Security Secrets Manager In this lesson, we explore AWS Secrets Manager—a secure and efficient service for managing sensitive information required by your applications. When applications connect to resources like databases, they often need credentials such as usernames and passwords. Hard-coding these credentials in your source code, for instance in a GitHub repository, can expose them to unauthorized access. Security Alert Never store credentials directly in your source code. Always use secure methods such as environment variables or services like AWS Secrets Manager to safeguard sensitive data. The Risks of Hard-Coded Credentials Embedding credentials in your application code exposes your systems to potential attacks. If an attacker accesses your repository, they may extract these sensitive credentials, putting your database and valuable customer information at risk. How AWS Secrets Manager Works AWS Secrets Manager provides a reliable solution to this problem by allowing you to store, distribute, and manage sensitive data securely. This service handles various types of secrets including: Database credentials Passwords Third-party API keys Any other sensitive configuration data Each secret consists of a name and a JSON structure containing the secret data. AWS Secrets Manager encrypts these secrets at rest using AWS Key Management Service (KMS). When your application retrieves a secret, Secrets Manager decrypts it and securely transmits it over TLS to your runtime environment. Secure Secret Management Workflow Consider the following flow for managing credentials securely with AWS Secrets Manager: Your application requires credentials to connect to a database. Instead of embedding these credentials in the code, you store them as secrets in AWS Secrets Manager. The credentials are encrypted using AWS KMS. When the application starts, it retrieves the encrypted credentials from Secrets Manager. AWS Secrets Manager decrypts the credentials using KMS and securely transmits them over TLS. The credentials are only available at runtime, ensuring they remain secure and hidden from the source code. AWS Lambda functions can be scheduled to rotate secrets automatically at defined intervals, enhancing overall security. Pro Tip This workflow is applicable not only for database credentials but also for any sensitive data like API keys. Using AWS Secrets Manager helps centralize and secure all your critical secrets. Key Features of AWS Secrets Manager AWS Secrets Manager offers a range of benefits: Feature Description Secure Storage Encrypts secrets using AWS KMS, ensuring sensitive data is securely stored at rest. Easy Retrieval Simplifies secret management through straightforward API calls, reducing code complexity. Automatic Rotation Facilitates scheduled secret rotation, minimizing the risk of credential compromise. Fine-Grained Access Control Provides detailed access policies to control who can access specific secrets. AWS Secrets Manager eliminates the need to hard-code sensitive data, allowing your application to dynamically retrieve necessary credentials at runtime while maintaining a high security standard. This service not only protects your application but also simplifies credential management and enhances overall system integrity. By leveraging AWS Secrets Manager, you can strengthen your application's security posture and efficiently manage your sensitive data. Watch Video Watch video content Practice Lab Practice lab"
AWS Solutions Architect Associate Certification,Demo Showing Macie in Action,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/Demo-Showing-Macie-in-Action,"AWS Solutions Architect Associate Certification Services Security Demo Showing Macie in Action language: en In this lesson, we will demonstrate how to work with AWS Macie to identify sensitive data in your S3 buckets. AWS Macie is a powerful security service that uses machine learning to recognize personally identifiable information (PII) and other sensitive data types. Follow along as we walk through the process step-by-step. Step 1: Enable AWS Macie Begin by signing in to the AWS Management Console and searching for ""Macie"" in the search window. Select ""Get Started"" to enable AWS Macie. Click on Enable Macie to activate its functionality. Once enabled, you can configure Macie to scan your S3 buckets for sensitive data. Step 2: Generate Test Data To verify that Macie correctly identifies sensitive data, we will generate some test files. In this demo, we have three example files: addresses.txt This file contains dummy addresses from the United States. Each address represents personal information that ideally should not be stored in an S3 bucket. credit_cards.txt This file includes a list of credit card details with card numbers, expiration dates, and verification numbers. Below is a sample of the content: 1  VISA, 4916245657687972, 7/2030, 352 2  VISA, 4556276704904925, 6/2024, 616 3  VISA, 4929642655495499, 11/2029, 288 4  VISA, 4024007142950408, 8/2027, 897 5  VISA, 4716370077620671, 3/2025, 876 6  VISA, 4533220187054539, 9/2026, 102 7  VISA, 4539956991960712, 10/2030, 930 8  VISA, 4929085085149061, 6/2028, 674 9  VISA, 4532470873521771, 10/2027, 929 10 VISA, 4024007128445607, 10/2023, 732 11 VISA, 4485583681960705, 12/2025, 672 3. **random-text.txt**  
This file contains random text that does not include any sensitive information. It is used to validate that Macie does not flag non-sensitive data.

The expectation is that AWS Macie will flag the sensitive information in **addresses.txt** and **credit_cards.txt**.

## Step 3: Upload Files to an S3 Bucket

Create an S3 bucket to store these files. For this demonstration, we already have a bucket named **Macie - KodeKloud**. Upload the three files into this bucket.

## Step 4: Create and Configure a Macie Job

Return to the Macie console and navigate to the **Jobs** section, then click on **Create job**.  
![The image shows the Amazon Macie dashboard with a list of jobs, including their names, resources, job types, statuses, and creation times. A warning message at the top indicates that a repository for sensitive data discovery results needs to be configured.](https://kodekloud.com/kk-media/image/upload/v1752865782/notes-assets/images/AWS-Solutions-Architect-Associate-Certification-Demo-Showing-Macie-in-Action/amazon-macie-dashboard-jobs-list.jpg)

From the list of S3 buckets, select your bucket (KodeKloud) to create a job that scans the entire bucket and provides an estimated cost.  
![The image shows an Amazon Macie interface where a user is selecting S3 buckets for analysis. It includes details about a specific bucket named ""macie-kodekloud"" with its account ID, region, and other metadata.](https://kodekloud.com/kk-media/image/upload/v1752865783/notes-assets/images/AWS-Solutions-Architect-Associate-Certification-Demo-Showing-Macie-in-Action/amazon-macie-s3-bucket-selection.jpg)

After selecting the bucket, click **Next**. At this point, you can choose whether to schedule the scan or run a one-time scan. For this demo, we will run a one-time scan.  
![The image shows an Amazon Macie interface for creating a job to refine the scope of sensitive data discovery, with options for scheduling and sampling depth.](https://kodekloud.com/kk-media/image/upload/v1752865785/notes-assets/images/AWS-Solutions-Architect-Associate-Certification-Demo-Showing-Macie-in-Action/amazon-macie-sensitive-data-job.jpg)
,[object Object],

Click **Next** to proceed to the managed data identifier options. AWS Macie provides pre-built rules for detecting common sensitive data types such as social security numbers, credit card information, addresses, etc.  
![The image shows an Amazon Macie interface where sensitive data types and categories are listed, such as personal information and credentials.](https://kodekloud.com/kk-media/image/upload/v1752865786/notes-assets/images/AWS-Solutions-Architect-Associate-Certification-Demo-Showing-Macie-in-Action/amazon-macie-sensitive-data-interface.jpg)

For this demo, select the following two identifiers:
- **ADDRESS**
- **CREDIT_CARD_NUMBER**

You can select additional identifiers based on your security requirements.  
![The image shows an Amazon Macie interface where specific managed data identifiers are being selected, including ""ADDRESS"" and ""CREDIT_CARD_NUMBER,"" categorized under personal and financial information.](https://kodekloud.com/kk-media/image/upload/v1752865787/notes-assets/images/AWS-Solutions-Architect-Associate-Certification-Demo-Showing-Macie-in-Action/amazon-macie-managed-identifiers-selection.jpg)

Click **Next** to continue. If you wish to define custom data identifiers (using regular expressions), you can do so on the next screen; however, we will skip this step for now and click **Next**.

On the subsequent page, you have the option to add text patterns for Macie to ignore. Once you have reviewed those settings, click **Next**. Then, give your job a name—in this demo, we use **Macie test job**. Click **Next** to review your configuration, and then click **Submit** to initiate the job.

## Step 5: Monitor the Job and Review Findings

After submitting the job, AWS Macie begins scanning the designated S3 bucket. The job may take between 10 to 20 minutes to complete. Refresh the Macie console to check for a status update indicating the job has completed.  
![The image shows the Amazon Macie console with a list of jobs for analyzing sensitive data in S3 buckets. A notification indicates that a job was successfully created, but a repository for sensitive data discovery results needs configuration.](https://kodekloud.com/kk-media/image/upload/v1752865788/notes-assets/images/AWS-Solutions-Architect-Associate-Certification-Demo-Showing-Macie-in-Action/amazon-macie-console-s3-jobs.jpg)

Once complete, click on **Show findings** to view the detected sensitive data. A new tab will open displaying detailed findings, including severity levels (high or medium) for each piece of sensitive data.

For example, selecting the first finding reveals details for an S3 object labeled ""financial"" from the **credit_cards.txt** file, showing that 11 credit card numbers were detected along with specific metadata regarding the resource.

![The image shows an Amazon Macie dashboard with findings related to sensitive data discovery, highlighting high and medium risk levels for financial and personal data in S3 buckets. A warning at the top indicates a repository configuration is needed for sensitive data discovery results.](https://kodekloud.com/kk-media/image/upload/v1752865790/notes-assets/images/AWS-Solutions-Architect-Associate-Certification-Demo-Showing-Macie-in-Action/amazon-macie-dashboard-sensitive-data.jpg)

Scrolling down will reveal findings corresponding to the **addresses.txt** file, which in this demo include four identified addresses.  
![The image shows an Amazon Macie dashboard displaying findings related to sensitive data discovery. It lists two findings with different severity levels, indicating the presence of sensitive information in S3 objects.](https://kodekloud.com/kk-media/image/upload/v1752865791/notes-assets/images/AWS-Solutions-Architect-Associate-Certification-Demo-Showing-Macie-in-Action/amazon-macie-dashboard-sensitive-data-2.jpg)
,[object Object],

## Conclusion

This demonstration showcased how AWS Macie scans your S3 buckets—using either one-time or scheduled jobs—to identify and report sensitive data. By following these steps, you can quickly set up Macie to help safeguard your data storage and ensure compliance with data privacy standards. 

Thank you for following this lesson. We look forward to seeing you in the next article.

---

For more details on securing your data environment, be sure to check out the following resources:
- [AWS Macie Documentation](https://docs.aws.amazon.com/macie/latest/userguide/what-is-macie.html)
- [Sensitive Data Discovery with AWS](https://aws.amazon.com/macie/sensitive-data/)

Happy Securing!
,[object Object],"
AWS Solutions Architect Associate Certification,Certificate Manager,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/Certificate-Manager,"AWS Solutions Architect Associate Certification Services Security Certificate Manager In this article, we explore AWS Certificate Manager (ACM) and its essential role in securing web communications. Certificates create trust between clients and servers by verifying identities and ensuring secure data exchange. When you visit a website like google.com, a digital certificate confirms you are connecting with the authentic server. Certificates serve several critical purposes: Authentication: Confirms the identity of the connected machine. Data Encryption: Secures data as it travels between your device and the server using HTTPS. Data Integrity: Assures that the transmitted data remains unaltered throughout transit. These benefits are made possible by certificate authorities—trusted organizations that rigorously verify the requester’s identity before issuing a certificate. What Is AWS Certificate Manager? AWS Certificate Manager (ACM) is a managed service designed to simplify the generation and management of SSL/TLS certificates for secure web operations. Its key components include: AWS Private Certificate Authority (CA): A trusted entity responsible for issuing certificates. ACM: Facilitates the entire process—from generating certificates to managing their deployment across multiple AWS services. For example, when deploying an Elastic Load Balancer (ELB) in front of your web server, you can attach an ACM-provided certificate to enable secure communications. ACM also seamlessly works with other services such as CloudFront and API Gateway, offering a versatile solution for various deployment scenarios. Requesting and Validating Certificates ACM provides two main methods for certificate management: Request a new SSL/TLS certificate. Import an existing certificate. During the certificate request, you specify the domain names to be covered. Ownership of these domains is verified through either email or DNS validation. When using Amazon Route 53 as your DNS provider, DNS validation can be automated, streamlining the verification process and ensuring only authorized users receive certificates. Once validation is complete, ACM issues the certificate. You then attach it to an HTTPS listener on your Elastic Load Balancer, which uses the certificate to encrypt and decrypt client traffic. Although this secures communication over the internet, you can decide whether to also encrypt traffic between the load balancer and your backend servers. Service Integration and Limitations ACM integrates with several key AWS services such as: Elastic Load Balancer CloudFront API Gateway Note ACM is designed to simplify secure communications across your AWS infrastructure by centralizing certificate management. However, note the following limitations: EC2 Instances: ACM certificates cannot be directly installed on EC2 instances; they must be used with services like an ELB that manage external traffic. Amazon S3: To enable HTTPS for S3 static website hosting, route traffic through CloudFront using ACM. AWS Lambda: Direct support for ACM certificates is not available for Lambda functions. Additionally, ACM operates regionally. For example, a certificate issued in the US East (N. Virginia) region cannot be used in the US West (Oregon) region; separate certificates are required for each region. How ACM Works Using ACM involves several straightforward steps: Request a Certificate: Start by submitting a request for the desired domain names. Domain Ownership Verification: Confirm that you own or control the specified domains. Certificate Issuance: Once the domains are verified, ACM issues the certificate. Automatic Renewal: ACM automatically renews certificates, eliminating the need for manual tracking. Integration: Finally, integrate the certificate with an AWS service, such as attaching it to an HTTPS listener on an Elastic Load Balancer. Key Features of AWS Certificate Manager ACM offers several features that streamline certificate management and enhance web security: Automated Certificate Provisioning: Simplifies the process of acquiring certificates. Auto-Renewal: Automatically renews certificates to help avoid unexpected expirations. Seamless Deployment: Integrates natively with supported AWS services, reducing the risk of manual errors. Central Management: Provides a unified dashboard to manage all of your certificates efficiently. Deep AWS Integration: Works seamlessly with various AWS services for enhanced security. With just a few clicks, you can deploy ACM certificates across your AWS infrastructure to ensure all communications remain secure and encrypted. For more information on AWS security services, check out the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Security Lake,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/Security-Lake,"AWS Solutions Architect Associate Certification Services Security Security Lake In this article, we dive into AWS Security Lake—a centralized solution for managing logs across diverse infrastructures. Discover how AWS Security Lake streamlines log aggregation, normalization, and querying to simplify security and compliance monitoring. Modern organizations often deploy infrastructure across multiple environments, such as on-premises, several AWS accounts, and additional public clouds like Azure or GCP. Consequently, log data is dispersed across various locations, making administration and correlation a complex and inefficient process. AWS Security Lake resolves this challenge by consolidating logs and events from on-premises systems, AWS services, third-party providers, and other cloud environments into a single, centralized repository. Once ingested, logs are seamlessly managed and queried, enhancing efficiency and reducing operational overhead. Key Benefit: S3 Integration AWS Security Lake stores logs in an Amazon S3 bucket, leveraging S3's robust lifecycle policies, built-in encryption, and cost-effective storage solutions. After storage, logs are normalized—optimizing them for efficient querying with tools such as Amazon Athena using standard SQL. How AWS Security Lake Works The workflow of AWS Security Lake consists of the following steps: Log Collection: Gather logs from a variety of sources, including VPC flow logs, Route 53 logs, and CloudTrail logs that record user actions. Log Storage: Securely store the collected logs in an Amazon S3 bucket. Log Normalization: Convert logs into an optimized format, such as Parquet, and standardize them using the Open Cybersecurity Schema Framework (OCSF), making them easier to query. Data Querying: Utilize AWS tools like Amazon Athena to run ad hoc queries, enabling precise data extraction for investigations and analysis. Key Features of AWS Security Lake AWS Security Lake offers numerous features designed to simplify multi-environment log management: Data Aggregation: Consolidate data from various environments, supporting a wide range of events and third-party integrations. Data Transformation and Normalization: Automatically partition and convert incoming data into efficient formats, ensuring consistency with standards like the OCSF. Multi-Account and Multi-Region Support: Seamlessly operate across multiple AWS accounts and regions. Data Lifecycle Management: Manage retention policies and storage costs using automated tiering within S3. Benefits of Centralized Log Management Centralizing log collection and management through AWS Security Lake provides several advantages: Operational Efficiency: Eliminate the complexities of handling logs from disparate systems by maintaining a single source of truth. Enhanced Security: Streamline security monitoring and compliance audits with comprehensive data visibility. Cost Optimization: Leverage cost-effective storage solutions and automated lifecycle management to reduce expenses. Simplified Investigation: Use robust querying capabilities, such as identifying logs for a specific subnet at a particular time, to quickly trace events and mitigate issues. By consolidating logs under one umbrella, AWS Security Lake empowers administrators to efficiently query, analyze, and manage log data, leading to improved operational workflows, strengthened security, and enhanced overall compliance. For more detailed guidance on AWS services and log management best practices, consider exploring the following resources: AWS Security Documentation Amazon S3 Overview Embrace AWS Security Lake to streamline your log management strategy and enhance the security posture of your multi-environment infrastructure. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Tools for Well Architected Framework,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Bringing-it-all-together/Tools-for-Well-Architected-Framework,"AWS Solutions Architect Associate Certification Bringing it all together Tools for Well Architected Framework Welcome back, AWS Cloud Practitioner (CLF-C02) ! In this article, we explore several essential tools designed to help you understand, implement, and apply the AWS Well-Architected Framework. Whether you are preparing for certification or working as an architect or solutions architect, these constantly updated resources will enhance your AWS architectural strategy. Below are three key resources to consider: The AWS Well-Architected Framework, which was recently updated to cover all six pillars, including the additional Operational Excellence and Sustainability pillars that are not part of the Solutions Architect Associate Certification . The Well-Architected Tool available within the AWS Console that evaluates your workloads and provides best practice recommendations. A collection of specialized whitepapers — AWS Lenses and AWS Guidance — that offer in-depth insights and specific recommendations across various use cases. The AWS Well-Architected Framework The Well-Architected Framework comprises six pillars: Operational Excellence Security Reliability Performance Efficiency Cost Optimization Sustainability While Security, Reliability, Performance Efficiency, and Cost Optimization are core components, Operational Excellence is specifically beneficial for DevOps engineers, and the Sustainability pillar, although critical, is less frequently emphasized. Quick Tip Although memorizing every detail of the whitepapers is not necessary for certification, a solid understanding of these pillars and tools is highly recommended for real-world AWS architectural optimization. The AWS Well-Architected Tool Access the Well-Architected Tool through your AWS Console to evaluate your workloads effectively. By answering a series of targeted questions—such as details regarding a stock portfolio’s industry type, sub-industry, and regions—the tool assesses various aspects of your workloads including security, reliability, and cost-efficiency. The tool provides a clear overview with status indicators for answered and unanswered questions, risk assessments (categorized as high or medium risk), and recommendations for improvements. Additionally, you can generate comprehensive reports for your workloads. This powerful tool is invaluable for ensuring that your workloads adhere to AWS best practices and remain architecturally sound as your expertise as a solutions architect grows. AWS Lenses and Guidance Whitepapers In addition to the framework and tool, AWS offers targeted whitepapers categorized into two main types: AWS Lenses: These documents are tailored for specific technologies and industries including machine learning, data analytics, serverless computing, and high-performance computing. For example, you might have noticed that the Governance Lens was updated in August while the Machine Learning Lens received updates in July. AWS Guidance: These whitepapers cover broader architectural concepts and best practices. They address topics such as serverless architecture, operational readiness reviews, and disaster recovery strategies tailored for complex cloud environments. These whitepapers have been popular resources since 2021. They offer valuable insights for both industry-specific challenges and high-level strategic planning, making them essential reading as you continually refine your AWS architectural practices. Summary To wrap up, here are the key AWS tools that can reinforce your implementation of the AWS Well-Architected Framework: The updated Framework itself, which now incorporates six critical pillars. The Well-Architected Tool, an interactive resource to assess and improve your workload architectures. Specialized AWS Lenses and AWS Guidance whitepapers that provide focused, in-depth recommendations for various industries and technologies. Together, these resources empower you to design, maintain, and optimize robust, secure, and efficient cloud architectures on AWS. I'm Michael Forrester. Thanks for reading this article, and I look forward to connecting with you in the next lesson. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,MegaSection Introduction The Design for X Portion,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/MegaSection-Introduction-The-Design-for-X-Portion,"AWS Solutions Architect Associate Certification Designing for Security MegaSection Introduction The Design for X Portion Welcome everyone! I’m Michael Forrester, and in this lesson, I will introduce the Design for X portion—the second half of this course. In this section, we focus on designing systems with specific attributes that align with the AWS Well-Architected Framework. What Is Design for X? Design for X means creating solutions with an emphasis on key attributes. In this lesson, these attributes are: Security Reliability Performance Cost Optimization These core aspects are fundamental to the AWS Well-Architected Framework. For instance, designing for security and reliability is critical for mission-critical environments. Four Specific Domains The Solutions Architect Associate course covers the following four domains: Security Reliability Performance (also known as Performance Efficiency) Cost Optimization While the full AWS Well-Architected Framework includes six pillars (such as Operational Excellence and Sustainability), this certification specifically focuses on the four domains listed above. Overview of the AWS Well-Architected Framework The AWS Well-Architected Framework consists of six pillars: Security – Securing workloads, infrastructure, and establishing shared responsibilities between AWS and the customer. Reliability – Ensuring fault tolerance, leveraging high availability, disaster recovery plans, and resiliency. Performance Efficiency – Right-sizing resources, scaling appropriately, and avoiding over- or under-provisioning. Cost Optimization – Utilizing the right service efficiently to control costs. Operational Excellence – Emphasizing best operational practices. Sustainability – Focusing on environmentally friendly and sustainable solutions. Note While Operational Excellence and Sustainability are integral parts of the framework, they are considered out of scope for the Solutions Architect Associate exam. These pillars are more relevant in cloud and system operations certifications. Course Structure and Objectives This mega section is organized into five subsections: Security Reliability Performance Efficiency Cost Optimization Design Challenge For each of the first four pillars, you will encounter: A brief overview of the respective pillar. General design principles specific to that pillar. Detailed discussions on enhancing attributes like security for networking, storage, and compute services. Often, AWS’s built-in features provide a strong foundation for security, so the focus shifts to identifying and leveraging available settings to further improve each service's security posture. Additionally, exam-style questions and detailed diagrams will help reinforce these concepts for each pillar. Agenda for Each Domain For each domain—Security, Reliability, Performance Efficiency, and Cost Optimization—the following agenda will be covered: An initial overview and review of general principles. Examination of how these principles are applied across various service categories (such as networking, storage, and compute). A comprehensive design challenge that integrates all four pillars, enhancing your exam readiness. Preparing for the Design Challenge As you progress through the course, take detailed notes—every design decision, principle, and exam question is designed to enhance your expertise. The culminating design challenge will combine all aspects covered in the course: A summary of design principles for Security, Reliability, Performance Efficiency, and Cost Optimization. Walkthroughs of specific challenges in each domain. An integrative challenge that combines these concepts into a cohesive architectural solution. Summary In summary, this lesson teaches you how to design as a Solutions Architect by focusing on four key pillars derived from the AWS Well-Architected Framework: Security Reliability Performance Efficiency Cost Optimization Throughout the course, you will delve into domain-specific questions for various AWS services—whether it's applying security measures to VPCs, EC2, or EBS—and explore how these services interact through detailed diagrams and design challenges. This targeted approach is designed to prepare you for the AWS certification exam, ensuring you gain both the theoretical understanding and practical skills needed for success. Go forth and conquer this mega section. We look forward to seeing you apply these design principles in the upcoming design challenges. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Firewall Manager,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/Firewall-Manager,"AWS Solutions Architect Associate Certification Services Security Firewall Manager In this article, we explore AWS Firewall Manager and its powerful benefits for managing security across multiple AWS accounts. As organizations scale, manually configuring security measures such as Web Application Firewalls (WAF), network firewalls, and AWS Shield for each account becomes laborious and error-prone. When managing several AWS accounts, you often find that individual setups require you to configure security and firewall settings separately. For instance, setting up a WAF in a development account means you must manually duplicate similar configurations in the production account to maintain consistent protection. Over time, with an increasing number of accounts, this manual process leads to: Inconsistent rule configurations Complex overall management Time-consuming updates across accounts Slower response times to emerging threats due to manual rule propagation Challenges in enforcing and verifying corporate compliance standards Why Centralize? Centralized management eliminates the need to log into each account separately, making it easier to maintain consistency, improve efficiency, and respond faster to potential threats. Introducing AWS Firewall Manager To overcome these challenges, AWS designed Firewall Manager—a service that simplifies the management of various firewall and security services across your accounts. With Firewall Manager, you can: Configure and enforce WAF rules across all accounts from a single console Manage security groups, network ACLs, and AWS Shield Advanced consistently Streamline auditing and compliance through centralized logging with CloudWatch By setting up your security protections once, Firewall Manager automatically distributes your firewall and security rules to all associated AWS accounts. This ensures that your policies are uniformly enforced and that your environment remains secure without constant manual interventions. Important Security Reminder Always ensure that any changes to your central security configurations are thoroughly tested in a non-production environment before being broadly applied. This approach minimizes the risk of unintentional disruptions. Simplifying Multi-Account Security Management With AWS Firewall Manager, once you define your protection rules, the service automates their application across Production, Development, and other environments. This centralization allows you to: Define rules once for uniform enforcement across all AWS accounts Manage multiple security services from a single, intuitive console Leverage CloudWatch for real-time monitoring and logging to support auditing and compliance Potentially reduce costs by minimizing redundant configurations Key Benefits at a Glance Benefit Description Uniform Rule Management Define security policies once and enforce them across all your AWS accounts. Single Console Management Simplify oversight of services like AWS WAF, AWS Shield, and security groups. Enhanced Auditing and Compliance Utilize CloudWatch logs and alarms to support auditing and monitor security events. Operational Efficiency Reduce the manual overhead of updating multiple accounts, saving both time and resources. By leveraging AWS Firewall Manager, you can streamline your security operations, ensuring consistent protection and a more agile security posture across your entire AWS environment. For more details on managing security in AWS, check out the AWS Documentation and AWS Security Blog . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Storage Services Part 2,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Storage-Services-Part-2,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Storage Services Part 2 In this article, we explore how to secure Amazon Simple Storage Service (S3), the only true object storage service on AWS since 2006. We will cover best practices and strategies for securing your data, using measures such as private buckets, server-side encryption, and pre-signed URLs. A startup, for example, plans to use S3 to store user-uploaded images for a mobile application. They need to ensure these images are stored securely, are accessible only by authorized personnel, and can be retrieved quickly on demand. Consider the following approaches: Store images in a public S3 bucket and use IAM roles to restrict access from application servers. Store images in a private S3 bucket, enable server-side encryption, and use pre-signed URLs for temporary access. Store images in an S3 bucket with public read access and use bucket policies to restrict access based on IP addresses. Store images in a private S3 bucket without encryption and use embedded IAM credentials for access. The most secure option is #2 . Modern S3 buckets (as of 2024) are automatically encrypted by default, but you can enable or verify server-side encryption when needed. Keeping the bucket private while using pre-signed URLs adds an extra layer of security by allowing temporary access. How Pre-signed URLs Work Pre-signed URLs allow users to perform specific S3 operations for a limited time. The typical workflow is: A user calls an API endpoint (which can be an API Gateway backed by a Lambda function or a direct S3 call) to request a pre-signed URL. AWS generates a URL containing temporary credentials. The user uses this URL to securely upload or download a file. Server-Side Encryption Options in S3 Amazon S3 supports different server-side encryption methods: SSE-C (Customer-Provided Keys): You provide and manage the encryption keys. SSE-S3 (S3-Managed Keys): AWS manages the encryption keys by default. SSE-KMS (AWS KMS-Managed Keys): Keys are stored and managed in AWS Key Management Service (KMS), with options for using AWS-managed or customer-managed keys. When using SSE-KMS, AWS handles key management for S3, although you can choose to manage your own keys for additional control. In the case of SSE-C, encryption key details are provided with each request, allowing S3 to encrypt or decrypt objects as needed. Note that this differs from client-side encryption, where data is encrypted before it is sent to S3. Access Control Layers in S3 Amazon S3 evaluates permissions through multiple layers, ensuring robust security. The four main access control components are: Identity-based policies: Define what actions a user can perform. Bucket policies (resource-based): Attached to the bucket, specifying who can access the resource. Access Control Lists (ACLs): Although available, ACLs are generally minimized as they are disabled by default. Block Public Access settings: Prevent accidental exposure of buckets. Consider the following IAM policy that allows access to specific S3 actions: {
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Action"": ""s3:GetBucketLocation"",
      ""Resource"": ""arn:aws:s3:::*""
    },
    {
      ""Effect"": ""Allow"",
      ""Action"": ""s3:*"",
      ""Resource"": [
        ""arn:aws:s3:::YOUR-BUCKET"",
        ""arn:aws:s3:::YOUR-BUCKET/*""
      ]
    }
  ]
} A bucket policy example that grants public read access might look like this: {
  ""Version"": ""2008-10-17"",
  ""Statement"": [
    {
      ""Sid"": ""AllowPublicRead"",
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""AWS"": ""*""
      },
      ""Action"": [
        ""s3:GetObject""
      ],
      ""Resource"": [
        ""arn:aws:s3:::s3browser/public-folder/*""
      ]
    }
  ]
} Similarly, an ACL can grant specific read permissions to other AWS accounts, although it is less common: {
  ""Version"": ""2008-10-17"",
  ""Statement"": [
    {
      ""Sid"": ""AllowPublicRead"",
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""AWS"": ""*""
      },
      ""Action"": [
        ""s3:GetObject""
      ],
      ""Resource"": [
        ""arn:aws:s3:::s3browser/public-folder/*""
      ]
    }
  ]
} Access Control Evaluation If any one access control layer explicitly denies an action, further evaluation stops immediately. AWS assesses the layers in the following order: Block Public Access settings, then IAM policies, bucket policies, and finally ACLs. S3 Access Logging for Detection Monitoring and security also rely on detecting potential misuse. Amazon S3 access logging captures detailed information about requests made to your bucket. The logs include: Canonical user IDs for both the requester and bucket owner Operations performed Metadata such as object size and request timestamp These logs are vital for forensic analysis and auditing. A sample log entry might look like this: 79a59d9f900b949e55d96a1698bacedfd6e09d98acff8f8d5218e7cd47ef2be awsexamplebucket [06/Feb/2019:00:01:57 +0000] 192.0.2.3 79a59d9f900b949e55d96a1698bacedfd6e09d98acff8f8d5218e7cd47ef2be DD6CCT733EXAMPLE REST.PUT.OBJECT s3-dg.pdf ""PUT /awsexamplebucket/s3-dg.pdf HTTP/1.1"" 200 - 4406583 41754 20 ""-"" ""S3Console/0.4"" - 10S62zv81kBW7B6SX4XJA806kpc16LpwEoizZQ0xJd5qDSCTLX0Tgs37kYU BKQW3+PdRg1234- SigV4 ECDHE-RSA-AES128-SHA AuthHeader awsexamplebucket.s3.amazonaws.com TLSv1.1 Example Scenario: S3 Access Logging Consider an e-commerce company that hosts product images and static assets in S3. With rising traffic and the need for monitoring, the best approach is to enable S3 access logging and store these logs in a separate, private bucket. This method isolates log data effectively, unlike storing logs in the same bucket—which is not recommended—and CloudTrail, which is less suited for real-time S3 access logging. S3 Object Lock and Versioning For regulatory compliance and data integrity, S3 Object Lock enforces immutable storage. It can be configured in two modes: Governance Mode: Only users with special permissions can override the lock. Compliance Mode: No changes are allowed, even for users with full permissions, ensuring complete immutability during a specified retention period. For instance, a pharmaceutical company might adopt compliance mode for critical data and governance mode where some overrides are acceptable. Similarly, a legal firm storing sensitive court documents could use compliance mode of S3 Object Lock to guarantee that documents cannot be deleted or overwritten, aligning with strict regulatory mandates. S3 Replication and Immutable Data Storage Some architectures use replication between source and destination buckets across different accounts for backup purposes. By applying S3 Object Lock on the destination bucket, you ensure that once data is replicated, it remains immutable—even if changes occur in the source bucket. This approach is especially effective for preserving log integrity. Securing S3 Access with VPC Endpoints AWS provides VPC endpoints to ensure that service traffic remains within the AWS network, bypassing the public internet. For S3, there are two types of endpoints: Gateway Endpoints: Specifically support S3 and DynamoDB. Interface Endpoints: Powered by AWS PrivateLink, these can handle traffic for multiple services, including S3. For instance, a global e-commerce company with EC2 instances accessing an S3 bucket should create an S3 VPC endpoint and update the route table. This setup guarantees that data transfers remain within AWS's private network. Example Scenario: VPC Endpoints A healthcare organization has a critical application in a VPC that processes patient data and stores it in an S3 bucket. Compliance requirements dictate that the data does not traverse the public internet. The recommended solution is to create an S3 VPC endpoint (or an interface endpoint if PrivateLink is required) and update the VPC’s route table accordingly. For additional context, consider this diagram illustrating on-premises integration with AWS Cloud using endpoints: Summary In this article, we explored various strategies to enhance storage security with Amazon S3. Key takeaways include: Using private buckets with server-side encryption and pre-signed URLs for secure storage. Understanding and selecting among different server-side encryption options (SSE-C, SSE-S3, SSE-KMS). Implementing layered access control with IAM policies, bucket policies, ACLs, and Block Public Access settings. Leveraging S3 access logging for monitoring, auditing, and forensic analysis. Applying S3 Object Lock in both governance and compliance modes to meet regulatory requirements. Securing S3 access using VPC endpoints to maintain data within AWS’s private network. These security measures ensure that your data is protected in transit and at rest, helping you meet compliance requirements while maintaining operational security on AWS. Feel free to reach out via email or on social channels for further questions or clarifications. Happy securing! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Compute Services Part 2,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Compute-Services-Part-2,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Compute Services Part 2 In this article, we explore key AWS services for designing secure compute environments, focusing on AWS Image Builder and Elastic Beanstalk. We will discuss their benefits, scenarios where they excel, and best practices to enhance security in your AWS deployments. AWS Image Builder AWS Image Builder is a managed service that automates the creation of secure Amazon Machine Images (AMIs). It functions similarly to HashiCorp’s Packer and is reminiscent of tools like Acronis and Clonezilla . With Image Builder, you can integrate source images, various software components, and region-specific configurations (such as instance type or subnet restrictions) to produce a secure base image for your instances. Note When using AWS Image Builder, remember that while the service automates the image creation process, you must still incorporate secure software updates and regularly scan for known vulnerabilities (CVEs) to maintain a secure AMI. Scenario 1: Streamlining AMI Build and Deployment Consider a startup that wants to automate the build, test, and deployment process for their Amazon EC2 instances using EC2 Image Builder. For more complex requirements, you might also explore HashiCorp’s Packer . Evaluate the Benefits: Image Builder allows users to manually patch AMIs, assuring they’re always up to date. Manual patching contradicts the principle of automation. EC2 Image Builder provides a fully managed service that simplifies the creation, maintenance, validation, and sharing of virtual machine or container images. ✓ This is correct. (Note that Image Builder now also supports container images.) EC2 Image Builder is primarily designed to optimize the storage capacity of AMIs, reducing their size. This is not accurate. Image Builder can automatically convert AMIs into container images for use with EC. While image conversion is possible, it is not automated. The correct answer is statement 2 because EC2 Image Builder streamlines the entire lifecycle of image management. Scenario 2: Ensuring Compliance for Migrating Applications An organization migrating on-premises applications to AWS needs to update their EC2 instances regularly with patches and software updates while maintaining compliance. They are considering using Image Builder as part of a mutable infrastructure setup with potential integration of AWS Systems Manager or CodePipeline. Evaluate the Benefits: Image Builder allows users to integrate third-party applications directly into the AMI process. Customization is possible; however, direct integration is not automated. EC2 Image Builder provides automated pipelines to build, test, deploy, customize, and secure images based on defined recipes. While additional services may be combined for full automation, this accurately captures the benefit. Image Builder can replicate AMIs across multiple AWS accounts or regions without additional configuration. ✓ This statement is correct and highlights one of its key features. EC2 Image Builder offers a marketplace where users can purchase pre-built AMIs from other customers. AWS Marketplace exists, but it is not a feature of Image Builder. The best answer here is statement 3 for its replication capability across regions with minimal configuration. Remember: AWS Image Builder offers few direct security features—it's your responsibility to ensure that the underlying software is continuously updated and that known vulnerabilities are addressed. AWS Elastic Beanstalk AWS Elastic Beanstalk serves as an orchestration service that simplifies the deployment of web applications by provisioning underlying services with secure default configurations. While basic security settings such as enabling SSL, encryption, and auto-scaling are managed through these underlying services, Elastic Beanstalk integrates these best practices to reduce deployment complexity. Typical Elastic Beanstalk Architecture A common Elastic Beanstalk setup might include a cluster deployment with multiple standby instances (typically two, not one) and several web application servers organized within auto-scaling groups. This setup is further enhanced by additional services like CloudFront and S3 for content delivery and storage. Since Elastic Beanstalk leverages other AWS services for security, key security settings must be adjusted within those services. Despite Elastic Beanstalk featuring some checkboxes for best practices, robust security configurations typically reside in the integrated services. Scenario 3: Simplified Secure Deployment A startup developing a web application seeks a simplified, secure deployment process with the ability to easily apply future updates. They are evaluating Elastic Beanstalk for this purpose. Key Feature Evaluation: Elastic Beanstalk automatically encrypts all data. Encryption requires explicit configuration. Elastic Beanstalk allows for direct SSH access to running instances by default. This is not enabled by default. Elastic Beanstalk provides managed platform updates to ensure the runtime environment is updated with the latest patches. ✓ This is correct. Elastic Beanstalk offers a built-in web application firewall with customizable rules. WAF functionality is not provided by default. The best choice is statement 3, as managed updates help maintain a secure runtime environment. Elastic Beanstalk logs events such as instance additions or transitions in environment health. These notifications are critical for staying on top of potential issues. Enhanced Health Reporting in Elastic Beanstalk Enhanced health reporting in Elastic Beanstalk provides detailed metrics, logs, and real-time notifications about your application's health. It is particularly useful for rapidly growing e-commerce applications that require immediate visibility and issue resolution. Evaluate the Statements: Enhanced reporting only provides metrics beyond what the environment level offers and excludes instance-specific details. This is not true. Enhanced reporting automatically integrates with Amazon SNS to send real-time notifications about environment health. ✓ This statement is correct. Enhanced health reporting requires manual configuration on each EC2 instance launched by Elastic Beanstalk. This is incorrect as it is enabled by default. Enhanced health reporting provides detailed metrics and logs that can be viewed directly in the AWS Management Console for quick resolution of issues. ✓ This is also true. Thus, statements 2 and 4 are correct. Integrating AWS X-Ray with Elastic Beanstalk AWS X-Ray offers distributed tracing to monitor and troubleshoot applications by providing a detailed service map. In the service map, different performance colors help you quickly identify issues: green for acceptable performance, yellow for issues, and red for critical problems. For startups deploying microservices-based applications, integrating AWS X-Ray is straightforward. To ensure a secure and effective integration, follow these best practices: Manually installing the X-Ray daemon is not recommended as it is unnecessary. Enable X-Ray directly from the Elastic Beanstalk configuration settings and ensure that the necessary IAM roles (with adequate permissions) are in place. ✓ This is the recommended approach. Modify the application code to remove any logging mechanism. X-Ray complements existing logging; it does not replace it. Store X-Ray traces in an S3 bucket without encryption for easier access. Storing sensitive data without encryption is strongly discouraged. The correct approach is to enable X-Ray through the Elastic Beanstalk configuration and verify that the required IAM roles are properly configured. Conclusion This article covered essential aspects and scenarios for leveraging AWS Image Builder, implementing secure Elastic Beanstalk configurations, utilizing enhanced health reporting, and integrating AWS X-Ray. By understanding these components and best practices, you can significantly improve the security, management, and monitoring of your AWS deployments. For further details on these services, consider exploring additional resources and documentation available on the AWS Documentation page. Happy securing your cloud deployments! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Preparing for the SAA MockPractice Exam,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Bringing-it-all-together/Preparing-for-the-SAA-MockPractice-Exam,"AWS Solutions Architect Associate Certification Bringing it all together Preparing for the SAA MockPractice Exam Welcome, future AWS Solutions Architect Associates! This guide is designed to help you efficiently prepare for your practice exam by providing you with key strategies to review and sharpen your AWS knowledge before the big day. Follow these three main recommendations to ensure you’re ready for exam day: Review Your Summaries, Notes, and Study Materials Retake the End-of-Section Quizzes Practice Challenging Services and Concepts in a Hands-On Environment 1. Review Your Summaries, Notes, and Study Materials Before you attempt the practice exam, collect all your study resources and review them thoroughly. Focus on the following areas: Course Summaries and Notes: Rely on your recorded notes and summaries from both the course and external study materials. Consistent note-taking during study sessions can boost information retention by 20–30%. Course Outlines: Utilize these outlines to identify key service components such as security, performance, and reliability. AWS Exam Guide: The AWS Solutions Architect Associate exam guide is indispensable. It clarifies concepts such as the differences between Key Management Service (KMS) and CloudHSM, and reinforces design strategies like cost optimization (e.g., knowing that Lambda typically costs less than EC2) and performance enhancements. Review Tip Make sure to revisit every study resource. This comprehensive review will help you identify and fill any knowledge gaps before your exam. 2. Retake the End-of-Section Quizzes Consistently retaking the end-of-section quizzes is a proven method to test your knowledge and boost confidence. Focus on quizzes covering key topics such as: Core AWS Services: Networking, Storage, Compute, Database, and Application Integration. Advanced Topics: Data services, Machine Learning/AI services, Migration/Transfer processes. Management & Governance: This section integrates several topics for a holistic view. Security & Compliance: Fundamental for any AWS architecture. On average, each section consists of about 25 questions. With nine sections overall, you can expect a total of roughly 225 questions. Additionally, complete the design quizzes for sections like Design for Security, Design for Reliability, and the ultimate design challenge. This structured practice is essential since the actual exam is scenario-based and requires you to design for resiliency, cost optimization, and performance improvements. Quiz Strategy Be sure to retake the quizzes multiple times until you achieve a perfect or near-perfect score. This will boost your confidence and prepare you for the dynamics of the actual exam. 3. Practice Challenging Services and Concepts If you repeatedly struggle with questions related to a specific AWS service (for instance, CloudFront or SQS), gaining hands-on experience is key: AWS Playground by KodeKloud: Experiment with around 60 supported AWS services in a controlled environment. This is a great way to gain practical experience. Personal AWS Account: If the service isn’t available in the playground (such as Systems Manager, which consolidates around 22 services), test it out in your own AWS account. Remember to terminate resources after an hour or two to avoid unexpected charges. AWS Documentation: For deeper insights and real-world examples, consult the official AWS Documentation . Cost Management Warning When using your personal AWS account, be cautious about leaving resources active. Always ensure you shut them down promptly to prevent unexpected charges. In Summary Effective preparation for the AWS Solutions Architect Associate mock exam involves: A thorough review of all your study materials including notes, summaries, course outlines, and the exam guide. Retaking end-of-section quizzes (including design quizzes) to solidify your understanding and improve recall. Gaining practical experience on challenging AWS services through interactive labs, personal account experimentation, or detailed documentation studies. I'm Michael Forrester, and I hope you find these strategies beneficial as you prepare for your AWS Solutions Architect Associate exam. Best of luck, and I'll catch you in the next article! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,The Road ahead on your AWS Journey,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Bringing-it-all-together/The-Road-ahead-on-your-AWS-Journey,"AWS Solutions Architect Associate Certification Bringing it all together The Road ahead on your AWS Journey Welcome back, Certified Solutions Architect Associate! Congratulations—you did it! Whether you passed the exam on your first try or need a bit more review, take a moment to celebrate your success. If you didn't pass, please let us know on Slack so we can offer you positive feedback and guidance. Assuming you passed, what’s next on your AWS journey? Before diving into another certification—such as the Solutions Architect Professional (highlighted in purple on the certification chart)—it’s important to consider some essential steps. Instead of rushing to the professional exam, you may find it beneficial to explore additional associate-level certifications, especially if your role leans towards DevOps. Many developers and system administrators start with the Solutions Architect course and certification because it provides a comprehensive overview of AWS services and design decisions. You might also consider deepening your expertise by pursuing the Developer Associate and SysOps Associate certifications. Working with your AWS Free Tier account allows you to build a robust portfolio. Document your projects and architecture diagrams on GitHub to create tangible proof of your skills. Hands-On Experience is Key Engaging with AWS services on platforms like KodeKloud's AWS Playgrounds can significantly reinforce your understanding of challenging exam questions. The exam includes several ""throwaway"" questions intended to test new content, so practical experience is crucial. Getting real-world experience is essential. Experiment with AWS services both on KodeKloud and in your own projects to keep your skills sharp. Consider participating in programs such as KodeKloud Engineer, which allow you to work on live project tasks in authentic AWS environments—this experience is invaluable without the pressures of a live contract. Staying updated with the latest AWS developments is also essential. Apart from monthly updates on significant DevOps and AWS service changes, subscribing to the AWS Blog or even developing a chatbot to notify you about new updates can help maintain and strengthen your AWS expertise. Prepare Thoroughly Keep in mind that the Solutions Architect Professional exam is significantly more challenging than the associate-level exams. Ensure you are well-prepared before attempting it by deepening your knowledge and gaining extensive practical experience. In summary, take the time to celebrate your achievements and plan your next steps carefully. Consider expanding your qualifications through additional associate-level certifications before tackling the more challenging professional or specialty exams. Thank you for trusting us to guide you through the AWS Solutions Architect certification process. Join us on KodeKloud , YouTube, or Slack, and we look forward to seeing you in the next course. Take care and keep pushing forward on your AWS journey! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Network Firewall,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/Network-Firewall,"AWS Solutions Architect Associate Certification Services Security Network Firewall In this lesson, we explore AWS Network Firewall, a fully managed service that secures your Virtual Private Cloud (VPC) by filtering incoming and outgoing traffic. By leveraging granular control and deep inspection features, AWS Network Firewall ensures that only authorized traffic is allowed in or out of your VPC. Note When deploying AWS Network Firewall, always configure dedicated Firewall Endpoints within exclusive subnets. Avoid sharing these subnets with other resources to ensure comprehensive protection. Firewall Endpoints and Subnet Configuration To safeguard your VPC and its subnets, it is crucial to create dedicated Firewall Endpoints. These endpoints act as the primary points for traffic inspection. You must allocate a specific subnet for your firewall deployment because placing a Firewall Endpoint in a subnet with other resources could compromise their protection. In the diagram above, you can observe that separate subnets have been deployed across various availability zones to serve as Firewall Endpoints. Reserving an exclusive subnet for these endpoints ensures that your other VPC resources remain effectively protected. Key Features of AWS Network Firewall AWS Network Firewall offers a range of robust features designed to enhance your network security: Centralized Rule Management: Simplify administration with rule groups that ensure consistent policies across multiple VPCs. Granular Traffic Control: Define detailed rules based on IP addresses, ports, protocols, and other traffic attributes. Deep Packet Inspection & Intrusion Detection: Identify and block advanced threats at both network and application layers. Comprehensive Logging: Maintain detailed logs of network and firewall activity for security analysis, compliance, and troubleshooting. Rule Synchronization: Seamlessly synchronize rules across multiple firewall instances, ideal for complex network architectures and multi-VPC environments. Traffic Flow Process When AWS Network Firewall is enabled, traffic within your VPC is directed through a carefully managed inspection process: Inbound Traffic: Traffic from the Internet Gateway is first routed to the dedicated Firewall Endpoint in the firewall subnet. After being inspected and validated against the firewall rules, it is forwarded to the subnet hosting your resources. Outbound Traffic: Similarly, outbound traffic from your resources is sent to the Firewall Endpoint for inspection before exiting the VPC. It is essential to configure your route tables properly to ensure that traffic passes through the Firewall Endpoint. Without this configuration, inbound or outbound traffic might bypass the firewall inspection. Warning Improper routing configuration can lead to traffic bypassing the firewall inspection, potentially exposing your network to security risks. Deployment Models AWS Network Firewall supports two primary deployment models to suit different network architectures: VPC Deployment: Protects resources within a single VPC by directing traffic from the Internet Gateway to the Firewall Endpoint and then to the target subnet. Transit Gateway Deployment: Provides centralized protection across multiple VPCs or on-premises networks by connecting them through an AWS Transit Gateway, eliminating the need for deploying individual firewalls for each VPC. Rules Engines: Stateless vs. Stateful AWS Network Firewall employs two distinct rules engines that allow you to tailor traffic inspection based on your security needs: Stateless Rules Engine: Analyzes each packet independently without considering the traffic context. This engine processes rules in a user-defined order—similar to network ACLs—to determine if packets should be allowed or dropped. Stateful Rules Engine: Inspects packets within the context of their ongoing traffic flow. It recognizes the request-response pattern, supports complex rules, and logs traffic details. The engine processes pass rules first, followed by drop rules, and finally alert rules. It functions similarly to VPC security groups and is compatible with Suricata IPS. By leveraging either stateless or stateful inspection—or even a combination of both—you can customize AWS Network Firewall to meet your specific security requirements, ensuring efficient and comprehensive VPC protection. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Understanding the seven foundations of Security,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Understanding-the-seven-foundations-of-Security,"AWS Solutions Architect Associate Certification Designing for Security Understanding the seven foundations of Security Welcome to this lesson on the Well-Architected Framework. In this tutorial, Michael Forrester will guide you through seven critical areas of cloud security. Unlike broader design principles that help conceptualize your strategy, these seven areas are concrete categories you can actively work on to secure your cloud environment. Below are the seven areas of cloud security: Security Foundations Identity and Access Management Detection Infrastructure Protection Data Protection Application Security Incident Response There is significant overlap among these areas, but each one emphasizes a specific aspect of cloud security. 1. Security Foundations This section establishes the baseline for all subsequent security measures. It involves planning activities such as defining roles, identifying critical services, and determining the necessary protection levels—whether that means 24/7 monitoring or security during specific timeframes. By setting clear objectives for system updates, threat identification, and account management, the security foundations guide your overall strategy. 2. Identity and Access Management This category focuses on controlling who can access your cloud resources and what permissions are granted. In AWS, for example, this is achieved using IAM (Identity and Access Management). A robust identity and access management process not only prevents unauthorized entry but also facilitates secure operations for authorized users. 3. Detection Detection involves deploying tools and processes to identify security issues early. This includes monitoring for unexpected configuration changes, analyzing logs and metrics, and tracking vulnerabilities. The goal is to provide centralized and actionable information that can trigger timely alerts and remediation efforts. 4. Infrastructure Protection Infrastructure Protection is dedicated to defending the underlying cloud infrastructure. This area involves implementing various defenses—such as network security, continuous network inspections, and vulnerability assessments for compute assets. Protecting essential components like network switches, storage systems, and compute instances is vital to prevent exploitation. 5. Data Protection Data Protection focuses on safeguarding your data independently of the underlying infrastructure. This includes encryption of data at rest and in transit, key lifecycle management, and implementing data classification based on sensitivity. Maintaining data integrity and confidentiality—using built-in cloud features or additional security measures—is a top priority. 6. Application Security Application Security ensures that workloads and the software development lifecycle are protected at every phase. By integrating security measures into application design, development, deployment, and maintenance, you can mitigate vulnerabilities. Practices such as penetration testing, code reviews, and screening third-party components are key to maintaining a secure application environment. Note In addition to securing the infrastructure, focusing on application security helps ensure that your code remains safe from new vulnerabilities throughout its lifecycle. 7. Incident Response Incident Response is about preparing for and managing security breaches effectively. This involves creating a comprehensive incident response plan, conducting regular training sessions, and running simulations to test your procedures. The purpose is not only to remediate issues quickly when they occur but also to refine your processes continuously through lessons learned. Applying the Seven Areas of Cloud Security An understanding of these seven areas allows you to map specific security requirements to the appropriate focus area. For instance: To protect data stored in Amazon S3, emphasize Data Protection by enabling S3 bucket object encryption. To log user activity, implement Detection strategies such as using S3 access logs. To control resource access, focus on Identity and Access Management, utilizing AWS IAM policies. The purple-highlighted service features in the diagrams indicate which AWS service or feature to enable for meeting these security needs. These seven areas work together with cloud design principles to help evaluate and enhance the security posture of AWS services. For example, if you are strengthening data protection for a database, you might implement encryption and enable detailed access logs. Conversely, if your focus is on user authentication for S3, you would utilize IAM services for effective access control. Although these categories are not part of the formal exam, they are essential for conducting a thorough security analysis and designing robust solutions. If you have any questions, please feel free to reach out on Slack. Thank you for reading, and we look forward to seeing you in the next lesson. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Storage Services Part 1,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Storage-Services-Part-1,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Storage Services Part 1 Welcome, future solutions architects! In this article presented by Michael Forrester, we dive into designing secure storage services—starting with block storage. This guide is structured to provide detailed explanations supported by high-level diagrams and real-world scenarios. Block Storage Overview Block storage is considered the fundamental type of storage, functioning similarly to a hard drive. In this section, we begin our discussion with Amazon Elastic Block Store (EBS), which is detailed in the second section of this nine-part series. The diagram below offers a high-level visualization of block storage services operating across AWS regions. An EBS volume located within an Availability Zone (AZ) is depicted along with its snapshot stored in S3. Remember that this volume represents a virtual disk attached to an EC2 instance within the same AZ, although certain storage types can attach to two instances in the same zone. AWS provides four flavors of EBS volumes: Two magnetic hard drive variants (commonly known as “cold” and “throughput” optimized). Two SSD-based options (general purpose and provisioned IOPS, available as GP2 and GP3). There are also specialized options like Block I/O Express that deliver extremely high performance. Security for storage in AWS is primarily managed by the platform, with encryption being the critical setting to enable. While some storage systems come with encryption activated by default, others—such as EBS volumes and EFS file systems—require you to turn on encryption. Notably, FSx (for Windows or ZFS) has encryption enabled by default. For this discussion, our focus is mostly on EBS volumes (with additional coverage on instance storage and EFS). Recall that block storage attaches as a disk to an EC2 instance. The diagram below summarizes key relationships among EBS volumes, their snapshots stored in S3, EFS file systems for shared access, backups handled via AWS Backup, and connections with RDS transactional storage plus cross-region backup capabilities. EBS Volume Encryption Consider a leading e-commerce company migrating its database to an EC2 instance. With sensitive customer information at stake, the database must remain encrypted at rest. The company evaluates four approaches for securing its data on EBS: Create an EBS volume and manually encrypt it. Not Recommended Manual encryption is not feasible because AWS does not offer a “manual encryption” feature for EBS volumes. Create an EBS volume with encryption enabled from the start. Recommended Approach By enabling encryption during volume creation, you ensure that both the volume and its snapshots are encrypted. AWS Key Management Service (KMS) can be used with the default key or a customer-managed key. Data transfer between the volume and the instance is also automatically encrypted. Create an EBS volume and enable encryption after data has been written. Encryption settings cannot be applied retroactively on an existing unencrypted volume. Store the data on an unencrypted volume and use an encrypted AMI for the EC2 instance. This method only secures the operating system image without encrypting the data disk itself. The best practice is option 2—enabling encryption when the volume is created secures data at rest as well as any snapshots generated later. Below is a screenshot from the AWS console. Notice the checkbox for encryption and the section displaying the master key for a GP2 volume. Examining EBS Snapshot Encryption Imagine a global financial institution that uses EC2 instances with attached EBS volumes for transaction processing. With a strict data retention policy that includes regular snapshots, the institution must ensure that even snapshots of unencrypted EBS volumes are secure. The solution is to specify a KMS key during the snapshot creation process. This guarantees that, even if the source volume is unencrypted, the generated snapshot will be encrypted—securing data at rest. With AWS managing data in transit between the instance and its volume automatically, leveraging native encryption is the simplest and most secure option. Instance Storage and Its Limitations EC2 instance storage provides local storage that is directly attached to the physical host running your EC2 instance. However, keep in mind that if the instance is moved—say, due to a host failure—the data stored on instance storage is lost. This ephemeral nature means instance storage is best suited for scratch or temporary data. There are no AWS-managed controls for enabling encryption or long-term persistence on instance storage. If encryption is required, it must be managed at the client level. Without built-in replication across multiple instances, instance store volumes should not be used for critical data. For example, a gaming company using instance store volumes for high-speed, real-time processing must understand that data can vanish if the instance is stopped, terminated, or encounters a failure. In summary, while instance store volumes offer high IOPS and low latency, they lack persistence and AWS security controls such as built-in encryption. File and Network Storage Transitioning from block storage, this section covers file and network storage. Despite operating like disks, these storage types work over a network. Amazon Elastic File System (EFS) Amazon EFS offers a shared file system that spans an entire AWS region—unlike EBS volumes, which by default are confined to a single Availability Zone (unless additional steps are taken). EFS supports concurrent access from multiple EC2 instances, very similar to a traditional NFS setup. The diagram below shows an EFS file system mounted by multiple instances, which illustrates its capability as shared storage across several Availability Zones. EFS and Video Editing Workloads Consider a media production company evaluating EFS for video editing: EFS does not provide block-level storage—it functions as a network file system. It is a regional service with automatic replication across multiple AZs for high availability. It supports NFS (versions 4.2/4.3) and is engineered for low-latency access, ideal for intensive operations. It is not intended for cold data storage by default, though multiple storage classes (including one for infrequent access) are available. The correct understanding here is that EFS’s regional nature with built-in replication makes it ideal for such workloads. Amazon EFS Infrequent Access (EFS IA) For organizations needing to store large datasets that are infrequently accessed yet require optimal performance, EFS IA is a cost-effective solution. It acts as a storage class within the same file system, automatically managing synchronization between standard storage and the infrequent access tier. Key benefits of EFS IA include: No need to manually synchronize data between storage tiers. Cost savings on files that are accessed less frequently. Consistent performance compared to the standard EFS tier. Encryption must be enabled at creation time if required. Option 3 correctly highlights that EFS IA delivers a lower-cost storage class without compromising performance. Encryption Reminder Remember, EFS does not encrypt file systems by default. If encryption is needed, it must be enabled during the setup. For existing unencrypted file systems, migrating data to a new, encrypted file system is required. FSx for Windows FSx for Windows File Server is tailored for Windows-based applications and supports native Microsoft Windows file systems. Key features include: Encryption for both data at rest and in transit is enabled by default. Native integration with AWS Managed Microsoft AD for streamlined identity and access management. Secure configuration out of the box, minimizing the need for additional security controls. For instance, a multinational corporation migrating on-premise Windows applications to AWS would benefit from a managed, secure Windows file system integrated with Active Directory. FSx for Lustre FSx for Lustre is engineered for high-performance computing and large-scale data processing—ideal for research simulations or data analytics tasks. It can seamlessly integrate with S3, enabling rapid data ingestion, a feature utilized by services like Amazon Redshift for pulling S3 objects. For research institutions needing both top-tier performance and robust security, FSx for Lustre offers: Automatic encryption of data at rest with an option for customer-managed keys. FSx for OpenZFS Amazon FSx for OpenZFS leverages the proven capabilities of the OpenZFS file system, originally developed for Sun Solaris. It offers advanced file system features such as: Encryption both at rest and in transit. Seamless integration with Amazon VPC for robust access control. This makes FSx for OpenZFS an excellent choice for multinational corporations requiring stringent security measures during migrations. FSx for NetApp ONTAP FSx for NetApp ONTAP is a managed file system built for shared storage with advanced features, including: Data deduplication and compression. Default encryption for data at rest and in transit. Support for multi-protocol access such as NFS, SMB, and iSCSI. These capabilities make FSx for NetApp ONTAP particularly attractive for sensitive industries like healthcare, where managing patient data securely is paramount. Conclusion This article explored critical aspects of securing storage services in AWS, covering: Encryption best practices for EBS volumes and snapshots. The ephemeral nature and limitations of instance storage. Shared file system capabilities and cost management with EFS. Specialized managed file systems including FSx for Windows, FSx for Lustre, FSx for OpenZFS, and FSx for NetApp ONTAP. In upcoming articles, we will further examine security services, performance tuning, and advanced cost-management strategies for these storage solutions. Happy architecting! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Shield and Shield Advanced,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Services-Security/Shield-and-Shield-Advanced,"AWS Solutions Architect Associate Certification Services Security Shield and Shield Advanced In this lesson, we explore AWS Shield, Amazon's managed Distributed Denial of Service (DDoS) protection service. AWS Shield defends your applications from malicious attempts to disrupt network services by overwhelming them with excessive traffic. Understanding the basics of a DDoS attack is key: such an attack floods a network resource with illegitimate requests, effectively denying service to genuine users. AWS Shield is available in two distinct tiers: AWS Shield Standard and AWS Shield Advanced. AWS Shield Standard AWS Shield Standard is automatically activated for all AWS customers at no extra charge. It offers robust protection against common network layer (Layer 3) and transport layer (Layer 4) attacks such as SYN/UDP floods and reflection attacks. AWS Shield Advanced AWS Shield Advanced is a premium service, available for an additional fee of $3,000 per month, that provides enhanced defense against more sophisticated DDoS attacks targeting services including EC2, ELB, CloudFront, Global Accelerator, and Route 53. This tier delivers advanced capabilities such as: 24/7 Access to the AWS DDoS Response Team (DRT): Get custom mitigation strategies at any time during an attack. Financial Safeguards: AWS offers financial protection that helps mitigate unexpected billing spikes during a DDoS event. Important AWS Shield Advanced incurs an additional cost of $3,000 per month. Evaluate your application's risk profile to determine whether this enhanced security tier is necessary. Below is a diagram that illustrates the architecture of AWS Shield and Shield Advanced. The diagram demonstrates how AWS Shield defends against threats from malicious actors while ensuring uninterrupted access for legitimate users to AWS services and associated Virtual Private Clouds (VPCs): Key Features of AWS Shield Advanced AWS Shield Advanced enhances your security posture with a suite of advanced features: WAF Integration: Integrate with AWS Web Application Firewall (WAF) to achieve comprehensive Layer 7 protection. Automatic Application Layer DDoS Mitigation: Automatically respond to and mitigate Layer 7 attacks with configurable settings. Health Checks via Route 53: Leverage Route 53 for effective health monitoring, reducing false positives by accurately detecting anomalous events. Direct Access to the AWS Shield Response Team: Receive immediate assistance from the AWS Shield Response Team during DDoS incidents. Financial Safeguards: Protect your budget from unexpected charging spikes that occur during DDoS attacks. The diagram below summarizes these key features, highlighting AWS WAF integration, automatic application layer DDoS mitigation, and health-based detection: Additionally, the following diagram emphasizes the role of the AWS Shield Response Team, proactive engagement during DDoS events, and the available cost protection measures: For further details on implementing AWS security measures, consider reviewing the AWS Documentation and exploring additional resources on DDoS Protection Best Practices . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,AWS Shared Responsibility Model,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/AWS-Shared-Responsibility-Model,"AWS Solutions Architect Associate Certification Designing for Security AWS Shared Responsibility Model Welcome back, future AWS architects. In this article, we revisit the AWS Shared Responsibility Model—a critical framework that outlines the security responsibilities shared between AWS and its customers. Understanding this model is essential as it forms the foundation of security, resiliency, and compliance best practices in the cloud. The model clearly delineates roles. Responsibilities below the ""hardware line"" (e.g., compute, storage, databases, and networking components housed in AWS data centers) are managed entirely by AWS. In other words, if a component is part of the underlying infrastructure that you cannot access or configure, AWS is responsible for securing it. On the other hand, anything above that line falls under customer control. For instance, if you have the ability to log into and configure a compute instance or storage service, you are accountable for patching, managing, and securing these elements. This division of responsibilities evolves depending on the type of service: Infrastructure as a Service (IaaS): Customers maintain significant control and responsibility. Platform as a Service (PaaS): AWS manages a larger portion of the security, reducing customer responsibility. Software as a Service (SaaS): AWS handles almost everything, leaving customers with minimal security responsibilities. AWS recommends using managed services whenever possible to offload the operational burden, allowing your team to focus on higher-level tasks. Below is a diagram that illustrates the division of responsibilities across various service types: In this diagram, you can see that with infrastructure services such as Amazon Elastic Compute Cloud (EC2) , you are responsible for managing compute, storage, databases, and networking. Imagine an additional block representing data centers—this block would give you more direct control over your network security. Conversely, services like serverless computing offer very little control over the underlying infrastructure since AWS fully manages those environments. As you move from IaaS to Platform as a Service (PaaS) and finally to Software as a Service (SaaS), the customer's security responsibilities continue to diminish. For example: With AWS RDS (a PaaS offering), AWS handles most of the underlying management. With SaaS offerings like Amazon Simple Storage Service (Amazon S3) , DynamoDB, SNS, and SQS, your interaction is largely limited to an API, and you have virtually no insight into the servers or infrastructure details. Take a look at the following diagram for further clarity on how these responsibilities shift between AWS and the customer: Key Takeaways If you can access, configure, or manage a component, you are responsible for its security. If you cannot access or configure a component, AWS assumes responsibility for its security. IaaS places the most responsibility on the customer, while PaaS and SaaS shift more responsibility to AWS. This model acts as a filtering mechanism to help you identify which aspects of security management require your attention and which can be entrusted to AWS. Customers agree to these defined responsibilities when setting up an AWS account, bearing the duty to secure what they control while relying on AWS to secure everything else. As services become more managed, your overall security responsibilities decrease correspondingly. For additional documentation on AWS security, compliance details, or audit attestations, visit AWS Artifact . AWS Artifact provides a repository of certifications, attestations, and compliance documents that are especially useful during compliance audits. If you have any questions or need further clarification, feel free to reach out via KodeKloud.com, join our Slack community, or participate in our forums. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Compute Services Part 1,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Compute-Services-Part-1,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Compute Services Part 1 Welcome back, Future Solutions Architects! I'm Michael Forrester, and in this lesson we’ll explore how to enhance security on AWS compute services. After covering networking and storage, we're now shifting our focus to a range of security ""knobs"" specific to compute services in AWS. These configurations play a critical role in bolstering the overall security of your compute environment. The Shared Responsibility Model Before we dive into compute services, let's revisit the AWS Shared Responsibility Model. In this framework, AWS is responsible for the security ""of"" the cloud—covering infrastructure, hardware, and managed services—while you are responsible for security ""in"" the cloud, which includes configuring your applications, data, and security settings. For example: Infrastructure Services (e.g., EC2): You secure the virtual machines. Container Services (e.g., ECS, EKS): Security responsibilities are more evenly distributed. Managed Services (e.g., Lambda): AWS manages most security aspects on your behalf. A Scenario on the Shared Responsibility Model Imagine a rapidly growing startup migrating its infrastructure to AWS. The Chief Information Security Officer (CISO), who is new to cloud computing, wants to adhere to best practices under the AWS Shared Responsibility Model. Which of the following best describes this model? AWS is responsible for the security of the cloud while customers are responsible for security in the cloud. AWS is responsible for both security of the cloud and security in the cloud. Customers are responsible for security of the cloud while AWS is responsible for security in the cloud. AWS and customers share equal responsibility for all aspects of security. The correct interpretation is option 1: AWS secures the underlying infrastructure, while you manage security within your environment. Securing EC2 and the Role of Bastion Hosts Amazon EC2, AWS’s virtual machine service, is a key component of your compute environment. A common practice is to use a bastion host (or jump box) to manage remote access securely. In basic setups, an EC2 instance located in a public subnet uses a bastion host secured by a security group that allows inbound SSH (port 22). IAM roles further restrict access, granting specific permissions to services like CloudTrail and S3. When implementing bastion hosts, consider these best practices: Place the bastion host in a public subnet. Restrict SSH access to known IP addresses and use the bastion host as a gateway for accessing instances in private subnets. Minimize direct exposure of numerous instances to the internet by consolidating access points. Bastion Host Security Options An e-commerce company deploying a multi-tier application on EC2 is evaluating the best approach to secure administrative access. Which option best enhances security? Deploy the bastion host in a public subnet, restrict SSH access to known IP addresses, and use it as a jump box to access private application servers. Deploy the bastion host in a private subnet and allow direct SSH access from the internet. Deploy multiple bastion hosts across different public subnets while distributing SSH access load with unrestricted access. Avoid using a bastion host that allows open SSH access to all EC2 instances. Option 1 is the most secure and resilient, ensuring that only defined access points provide a pathway to private resources. Managing EC2 Key Pairs Securely Secure key pair management is crucial when deploying EC2 instances across multiple regions. Consider the following approaches: Use a single EC2 key pair across all regions. Generate a unique EC2 key pair for each region and store them in a centralized, encrypted storage solution with strict access controls. Share the private key among team members via email. Avoid using EC2 key pairs and rely solely on usernames and passwords. Option 2 is best practice. Using unique key pairs per region with centralized, encrypted storage not only separates keys but also enhances access control and accountability. Tip In many deployments, a dedicated key pair is used during initial setup. Afterward, assign each user their unique key pair to enforce one-to-one accountability. Replacing Bastion Hosts with AWS Systems Manager AWS Systems Manager Session Manager offers a modern alternative to traditional bastion hosts. This service allows administrators to securely access EC2 instances without the need to open inbound ports or manage SSH keys. For instance, a healthcare company managing critical applications with sensitive patient data can leverage Systems Manager. Which statement is accurate regarding this service? Systems Manager enables direct access to the underlying hardware for diagnostics. Systems Manager provides a unified interface to access EC2 instances without opening inbound ports or managing SSH keys. Using Systems Manager requires EC2 instances to be publicly accessible. Systems Manager replaces EC2 virtual machines. The correct answer is option 2. With Systems Manager, secure access is managed without the traditional pitfalls of jump boxes, thereby reducing both administrative overhead and exposure risk. Additional benefits include integration with AWS Identity Center (formerly Single Sign-On) and detailed role-based permissions. Securing Access to DynamoDB from EC2 For applications utilizing DynamoDB, it is imperative to avoid embedding AWS credentials in your code. Instead, leverage EC2 instance roles, which securely grant the necessary permissions on demand. Consider these approaches: Store AWS access and secret keys in the user data of an EC2 instance. Store the keys in user data and retrieve them during application initialization. Use the EC2 instance role to provide necessary permissions without requiring long-term credentials. Manually generate temporary access keys. Option 3 is best practice, as it uses AWS’s built-in mechanism to securely provision temporary credentials without hardcoding sensitive information. A supporting diagram demonstrates an EC2 instance with an attached IAM role that uses the Security Token Service (STS) to provide temporary credentials for interacting with DynamoDB. IAM Roles Anywhere for External Workloads For workloads running outside AWS, AWS IAM Roles Anywhere offers a solution for obtaining temporary security credentials safely. Consider these approaches: Use long-term AWS credentials and bypass X.509 certificates. Register your certificate authority with IAM Roles Anywhere as a trust anchor. Use self-signed X.509 certificates with a trust anchor. Avoid using IAM roles by managing separate IAM users with static credentials. Option 2 is recommended. Registering a trusted certificate authority establishes a secure trust relationship, allowing external workloads to obtain temporary credentials without risking exposure through long-term, static keys. Public versus Private Subnets and Routing Considerations Understanding the difference between public and private subnets is essential when architecting your EC2 environment. In a typical VPC setup: Public Subnets: These subnets have a route directing 0.0.0.0/0 traffic to an Internet Gateway (IGW), and instances here usually receive a public IP address. Private Subnets: In these subnets, traffic does not route directly to an IGW; rather, it passes through a NAT gateway or NAT instance for internet access. For a startup deploying multiple EC2 instances, best practices include: Tighten security group rules to disable unnecessary inbound and outbound traffic. Regularly update operating systems and software patches. Avoid practices such as storing sensitive data on the root volume or using the root user for day-to-day operations. Enhancing Security with Private Subnets and NAT To mitigate threats like DDoS attacks, consider re-evaluating your routing strategies. Instead of exposing every instance to the internet, you can place instances in private subnets and direct traffic through a NAT gateway or NAT instance. This strategy minimizes your public exposure and significantly enhances security. EBS Encryption and Data Security Data stored on EBS volumes must be secured, as these volumes are not encrypted by default. To protect data-at-rest, consider the following practices: Enable encryption when creating your EBS volume. Choose between AWS-managed keys or customer-managed keys via AWS KMS. Leveraging built-in encryption minimizes operational overhead and avoids modifying application-level code to handle encryption. Instance Storage versus EBS For workloads that demand high IOPS and low latency—such as an online multiplayer game—EC2 instance store volumes might be considered. However, keep in mind: Instance store volumes are intended for temporary storage. Data is lost if the instance stops or fails. Instance store volumes do not come encrypted by default using AWS-managed keys; any encryption would need to be implemented at the application level. The key takeaway is that instance store volumes are designed for temporary data processing and lack persistence if the instance is interrupted. SSL Termination at the Load Balancer An Application Load Balancer (ALB) can manage SSL termination, which offloads the decryption process from your EC2 instances. Key points include: Traffic between the client and the load balancer is encrypted. Communication between the load balancer and the EC2 instance can be configured to be unencrypted or re-encrypted using a different certificate. Offloading SSL termination improves EC2 performance by reducing the processing load. For example, a healthcare application can benefit from SSL termination at the load balancer to ensure that sensitive patient data is securely transmitted. The optimal configuration involves offloading SSL termination at the load balancer, ensuring secure transmission between client and load balancer while reducing the decryption burden on EC2 instances. Securing EC2 with Patch Management Ongoing patch management is critical for maintaining the security of your EC2 instances. AWS Systems Manager Patch Manager allows you to automate the patching process by: Defining patch baselines that specify approved updates. Patching both operating systems and application software. Automating the patching process (including reboots) without manual intervention. Eliminating the need for exposing instances to the public internet during patching. The recommended approach uses Patch Manager to enforce patch baselines that secure both the operating system and application layers. Integrating CloudTrail and CloudWatch Integrating AWS CloudTrail with CloudWatch is essential for monitoring API calls and system changes: CloudTrail logs API calls made on EC2 instances. These logs can be forwarded to CloudWatch for real-time monitoring and analysis. For instance, an e-commerce company can integrate these services to audit system activities without requiring instances to have public IP addresses. Option 2 is correct: CloudTrail captures API calls and streams logs to CloudWatch for real-time analysis. UEFI Secure Boot and Nitro TPM AWS EC2 integrates advanced security features such as UEFI Secure Boot and Nitro TPM. Here’s how they enhance security: The UEFI secure boot process validates the bootloader’s signature. After validation, the bootloader securely measures and loads the kernel. Nitro TPM unseals the boot volume only when the measurements match expected values, ensuring the integrity of the operating system. This process protects against unauthorized modifications during boot, thereby enhancing overall system security. AWS Nitro Enclaves AWS Nitro Enclaves offer a way to process highly sensitive data within a highly isolated environment. Key considerations include: Nitro Enclaves create an isolated, secure computing environment within your EC2 instance. Data processed inside an enclave remains inaccessible to the parent instance. Communication between the enclave and the parent instance is limited to a virtual socket (vsock). For instance, a healthcare company processing sensitive patient information can use Nitro Enclaves to meet strict compliance and data protection requirements. Option 2 is correct: Nitro Enclaves create a highly isolated environment that communicates with the parent instance only through a virtual socket. EC2 Instance Types Overview When choosing an EC2 instance, AWS offers several instance families to suit different workloads: General Purpose: Includes T and M family instances. Compute Optimized: Denoted by instance types starting with C (e.g., C5, C6, C7). Memory Optimized: Designed for memory-intensive tasks; instance types typically begin with R. Storage Optimized: Often indicated by instance types such as I, D, or H. Accelerated Computing: Equipped with GPUs (e.g., G and P families) for tasks like rendering or machine learning. For example, an animation studio looking to optimize rendering performance might benefit from accelerated computing instances. Consider this statement: Accelerated computing instances are optimized for memory-intensive applications. They come with pre-installed animation software. They leverage hardware accelerators (GPUs) to perform computations more efficiently than CPUs. They are primarily designed for storage-intensive workloads. Option 3 is correct—accelerated computing instances utilize hardware accelerators to efficiently handle complex computations such as graphics processing. This lesson has covered key aspects of securing compute services on AWS—from understanding the shared responsibility model to configuring secure access for EC2, managing key pairs, leveraging Systems Manager, ensuring data encryption on EBS, handling instance storage correctly, and integrating advanced features such as Nitro TPM and Nitro Enclaves. By applying these best practices and understanding AWS’s security mechanisms, you can significantly enhance the security posture of your compute workloads. Happy architecting! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Network Services Part 1,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Network-Services-Part-1,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Network Services Part 1 Welcome, future solutions architects! Presented by Michael Forrester, this article is the first installment in our ""Designing for X"" series. In this session, we focus on enhancing security for network services on AWS. Prepare to take detailed notes as we cover a substantial amount of material at a brisk pace. The objective is to demonstrate how to bolster security on familiar AWS services by fine-tuning the service configurations—essentially, adjusting the ""knobs"" that control key security features. Introduction to Security Design Conversations Security design discussions often begin when a stakeholder makes a specific request. For example: ""Can we secure the data stored in AWS?"" ""Please ensure that Java on our systems is patched to a specific version."" Consider the scenario where someone inquires about S3 encryption. You might explain that S3 data is encrypted by default. Similarly, if the request involves RDS encryption, you would note that encryption was enabled when the service was provisioned. The key takeaway is that when you receive a request—whether it's for updated software patches (using Systems Manager, for example) or to enable encryption—you must first verify if the AWS service supports that feature. If not, alternative solutions or third-party configurations may be required. Requests are rooted in technical requirements, but they usually serve broader business needs. Your role is to align the requested security feature with the corresponding AWS service capability. For example, while S3 encryption falls under data protection, other areas include identity access management, threat detection, application security, and incident response. Categorizing requests helps you quickly determine whether a service natively offers the required feature or if additional measures are needed. Matching Security Features to AWS Services Let's delve into some AWS examples to illustrate how security features align with services: Data Protection For S3, enabling encryption is a key data protection measure. Additionally, versioning is another feature that safeguards data integrity. Detection Activating detailed logging on an S3 bucket can be viewed as a detection mechanism—helping you monitor object access. Likewise, VPC flow logs serve as an event detection tool rather than solely preventing unauthorized access. Identity and Access Management (IAM) Adjustments to access control lists (ACLs) and enhanced logging for detailed access insights tie into IAM and broader application security. When preparing for the AWS certification exam, analyze the security request by identifying the target category: application layer, network infrastructure, or data security. The solution should maximize security while minimizing cost and operational overhead. For certification, you should be familiar with essential task statements such as: Designing secure access to AWS resources. Designing secure workload and application environments. Determining appropriate data security controls. Consider a scenario from the fictional ""Chubby Unicorn Company,"" where the requirement is to block unneeded ports for the application ""Glitterbomb."" In this case, network ACLs, security groups, or dedicated network firewalls might be utilized. When evaluating options, always balance enhanced security with cost-effectiveness and operational simplicity. Deconstructing Complex Networking Diagrams Understanding how to deconstruct intricate network security diagrams is an essential skill. Consider the following narrative inspired by Chubby Unicorn Company: 1. Multiple Campuses in a Region Imagine an AWS account as a collection of campuses (regions). Think of AWS regions and accounts as numerous office locations, each segmented by groups—like different floors in a building representing public and private subnets. 2. Segmentation Within a Building Within a VPC: The first floor (public subnet) is accessible via an Internet Gateway. Upper floors (private subnets) host sensitive operations. Security measures include: Public subnets managed by route tables and protected by Network ACLs (NACLs). Private subnets that rely on NAT Gateways for outbound communications. Instance-level firewalls enforced by security groups. 3. Routing and Address Resolution An Internet Gateway allows external traffic to reach public subnets. Route tables then ""direct traffic"" much like a receptionist, ensuring that data reaches the appropriate destination. Private subnets typically route via NAT Gateways to maintain secure outbound connections. 4. Availability Zones and Redundancy Availability Zones (AZs) can be thought of as separate buildings that are geographically isolated yet interconnected by routing tables. This design maximizes redundancy and security by ensuring isolated and secure traffic flows across distinct locations. 5. Additional Security Measures To further secure your AWS infrastructure, consider the following: VPC Endpoints provide private connections to AWS services (such as S3), obviating the need for public internet routes. Load Balancers distribute network traffic across multiple instances or subnets to ensure both performance and security. For IPv6 traffic, an egress-only Internet Gateway functions similarly to a NAT Gateway by allowing only outbound connections. Each component—from the Internet Gateway to security groups and NAT gateways—plays a critical role in your overall AWS network security architecture. Simplifying Complexity for Secure Design To summarize your approach: Understand the security request (e.g., secure data, enforce patch compliance, restrict network access). Identify the AWS features and services that address the request. Deconstruct complex diagrams by analyzing each component individually (VPC, subnets, NACLs, security groups, gateways, and route tables). Design solutions that strike an optimal balance between robust security, cost-effectiveness, and operational simplicity. By methodically breaking down your architecture, you will be well-equipped to answer design questions and ensure that your solutions meet the stringent security standards required by the AWS certification exam. Note Keep in mind that while these examples pertain to exam scenarios, staying current with AWS documentation is crucial in real-world applications. Happy architecting as you continue to secure network services and advance your design career! Explore more resources: AWS Certified Solutions Architect – Associate Exam Guide AWS Security Best Practices Kubernetes Documentation Docker Hub Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Agenda and Introduction for Designing for Security,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Agenda-and-Introduction-for-Designing-for-Security,"AWS Solutions Architect Associate Certification Designing for Security Agenda and Introduction for Designing for Security Welcome back, Future Solutions Architects. I’m Michael Forrester, and in this lesson we dive deep into designing for security—also known as the ""Citadel of Security."" In this session, we will cover AWS-recommended design principles, explore the four foundations of cybersecurity, and review the AWS Shared Responsibility Model. This practical guide is aligned with the AWS Solutions Architect Associate Certification . After the cost optimization module, your first design challenge will focus on security. The course format includes diagrams, analytical questions, and in-depth answers, so be sure to take thorough notes to reinforce your learning. Below is the agenda diagram for the security design section: Section 1: Design Principles In this section, we introduce the key design principles that form the backbone of the Citadel of Security. The topics include: Maintaining Traceability Applying Security at All Layers Automating Best Practices These foundational concepts set the stage for the more detailed discussions that follow. Section 2: Categories for Security in Design This section defines the four fundamental categories that govern all security actions in AWS: Identity and Access Management (IAM): Controlling and managing access. Detection: Identifying issues as soon as they occur. Protection: Defending against potential threats. Response: Taking corrective action to resolve issues. Understanding these categories helps pinpoint the focus areas for enhancing security within AWS. Section 3: The Shared Responsibility Model Next, we explore the AWS Shared Responsibility Model, which clearly delineates security responsibilities between the customer and AWS. Key points include: Customer Responsibilities (Security “in” the Cloud): Tasks that customers must handle, such as data protection and access management. AWS Responsibilities (Security “of” the Cloud): AWS covers the security of the underlying infrastructure. We will also review new services and their specific responsibilities within this model. Sections 4 to 12: Enhancing Security Across Services In these sections, we delve into security specifics for various AWS service categories including network, compute, storage, management, data, and machine learning. Topics include: Adjusting security on management and compute services. Enhancing network and storage security. Exploring available configuration options (""knobs"") for each AWS service. A detailed diagram below illustrates these layers and highlights opportunities for fine-tuning security settings. Section 13: Designing for Security Challenge The final section features an interactive design challenge. You will be presented with a diagram similar to the one below, where one or more elements are intentionally missing. Your task is to select the correct AWS services and features (such as Route 53, WAF, DynamoDB, Amazon EC2, or Lambda) and drag them to their appropriate positions in the diagram. Lesson Summary To recap, this lesson on designing for security includes: A refresher on AWS Security Fundamentals based on the Well-Architected Framework. An introduction to key security design principles and the four essential security categories. A detailed examination of the AWS Shared Responsibility Model. Insights into security configurations across various AWS service categories. An engaging design challenge that reinforces the concepts discussed. Note This comprehensive approach is tailored to strengthen your architectural analysis skills and prepare you for the AWS Solutions Architect Associate Certification while honing practical security design abilities. Thank you for joining this session. Be sure to take detailed notes as you progress through the lesson. I’m Michael Forrester, and I look forward to seeing you in the next lesson. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Preparing for the real Exam,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Bringing-it-all-together/Preparing-for-the-real-Exam,"AWS Solutions Architect Associate Certification Bringing it all together Preparing for the real Exam This guide walks you through the essential steps to prepare for the AWS Solutions Architect Associate exam. By following these clear, actionable steps, you will reinforce your AWS knowledge, address gaps in your understanding, and build the confidence to pass the exam. Step 1: Master the Mock Exam After completing your studies, your first priority is to focus on the mock exam. Retake it repeatedly until you consistently score 95% or until you fully understand every error. If you only achieve an 85th percentile, revisit the challenging areas until the concepts become clear. Tip Regularly reviewing your mock exam performance helps highlight any misconceptions—such as confusing the number of available EBS storage tiers—so you can adjust your mental model accordingly. Step 2: Dive Deep into the “Design for X” Sections The “Design for X” section is packed with scenario-based questions designed to test your ability to optimize various aspects of AWS architecture, such as security, reliability, performance, and cost efficiency. While the end-of-section quizzes are useful, the detailed design questions provide a deeper understanding of best practices. Consider a common scenario: a retail company deploying an e-commerce application on AWS. The application must securely handle customer transactions and manage sensitive data. To achieve this, segregate the application and business logic from the database layer to minimize unauthorized access. A recommended strategy is to use a single VPC with multiple subnets—assigning public subnets for the web tier and private subnets for the application and database layers—while implementing strict firewall configurations. Understanding the reasoning behind why one solution is correct—and why the others are not—is crucial. For example, placing the web tier in a public subnet while restricting the application and database tiers to private subnets effectively limits public access and promotes security through controlled, encrypted communication across layers. Occasionally, the section may also include reference diagrams provided by AWS. These diagrams offer valuable visual context and a deeper understanding of how AWS services integrate, making complex concepts more accessible. Step 3: Seek Guidance and Clarify Doubts on Slack If reviewing the mock exam and design scenarios still leaves you with questions, connect with the community on Slack at kodekloud.slack.com . Both Michael Forrester and Sanjeev Thiyagarajan are available to answer your questions, offer insights, and help pinpoint any topics that need further review. Community Support Engaging with experienced peers and experts can provide additional support and ensure you have covered all vital areas before the actual exam. Summary To prepare effectively for the AWS Solutions Architect Associate exam, follow these three critical steps: Practice Intensively: Retake the mock exam until you reach a consistent 95% score. This rigorous practice is essential, even though the actual exam has different questions and an 80% pass threshold. Review Design Scenarios: Focus on the “Design for X” sections to master scenario-based questions related to security, reliability, and cost optimization. Engage with the Community: Leverage the Slack channel ( kodekloud.slack.com ) to connect with experienced coaches for advice and additional support. Once you build your confidence by following these steps, sign up for the exam and celebrate your success. We are here to provide all the guidance and support you need on your journey to becoming an AWS Solutions Architect Associate. Good luck on your exam journey! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Network Services Part 2,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Network-Services-Part-2,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Network Services Part 2 In this article, we explore advanced AWS networking design strategies that enhance security while ensuring optimal performance. We cover global architectures, multi-VPC deployments, secure subnet design, and specialized networking features—all aimed at keeping customer data compliant with regional regulations and streamlining internal communications. Designing for Global Architecture and VPC Deployment Imagine a global e-commerce company that must serve customers in North America, Europe, and Asia with low-latency access while ensuring local storage of customer data. To achieve this, it deploys applications in multiple VPCs—one per region (e.g., Virginia, Frankfurt, and Tokyo). A single VPC approach would risk cross-region data flow, even with replication in place. Using Route 53, requests are directed to the nearest region to comply with data regulations like GDPR and minimize latency: Isolated Multi-VPC Architecture for Multinational Corporations Multinational corporations often require isolated VPC environments for each subsidiary to meet data sovereignty and regulatory standards. Although deploying separate VPCs with encrypted internet communication is an option, AWS VPC Peering provides a more secure alternative by leveraging AWS’s private backbone. Default VPC vs. Custom VPCs for Startups For startups launching web applications on AWS, time is of the essence. Using the default VPC can speed up deployment, provided that the security groups and network ACLs are modified to enforce the principle of least privilege. Keep in mind that the default VPC comes preconfigured with broad access, so manual adjustments are necessary for enhanced security. By default, AWS provides each region with one default VPC and five additional customer VPCs, with more available upon request: Subnet Design for Layer Segregation For a retail company deploying an e-commerce application, separating the application and database layers reduces the risk of unauthorized access. Instead of isolating these layers across different VPCs, the best practice is to use multiple subnets within a single VPC. One subnet handles the application layer while another, secured by distinct firewall rules, serves the database layer. For more complex setups like three-tier architectures, the web tier resides in a public subnet while the application and database tiers are isolated in private subnets with carefully configured routing and access control. A foundational diagram illustrating public and private subnets within a VPC is shown below: Routing Between Tiers For multinational corporations running multi-tier web applications, configuring routing tables is crucial. Typically, only the web tier has a route to an Internet Gateway, while the application and database tiers remain isolated from direct internet exposure. Even if the primary focus is on protecting the database, restricting the application tier from direct internet access is a good security practice. Internet Gateways and Controlled Access A software development company might deploy an application on AWS for testing that must be accessible through an Internet Gateway—exclusively to developers. While attaching an Internet Gateway is simple, security groups need to restrict inbound access to specific IP addresses. In many cases, companies enhance this configuration with remote hybrid connectivity. NAT Gateways for Outbound Traffic For FinTech and financial analytics companies, applications in private subnets may need secure access to public APIs. Deploy a NAT Gateway in a public subnet and update the private subnet’s routing to direct outbound traffic through it. This configuration allows secure data retrieval from the internet without exposing the internal instances. Egress-Only Gateways for IPv6 Traffic In situations where an IPv6 application must download external datasets without accepting incoming IPv6 traffic, deploying an egress-only Internet Gateway is the ideal solution. Direct all IPv6 outbound traffic from private subnets to the gateway, ensuring that the application remains shielded from unsolicited inbound requests. Secure DNS Resolution within a VPC For multi-tier applications, ensuring that EC2 instances communicate using private DNS names is fundamental to security. Enable DNS resolution and hostname settings within your VPC so that internal DNS queries do not traverse the public internet. One effective approach uses the Route 53 Resolver to connect an on-premises data center with AWS, allowing granular control over DNS query rules. Elastic IP Addresses for Persistent Connectivity For web applications hosted on EC2 instances, maintaining a consistent public IP address is critical, even after instance restarts. Assigning an Elastic IP (EIP) guarantees persistent public connectivity, unlike dynamically assigned public IPs that can change upon reboot. An additional diagram illustrates an EC2 instance with a dedicated Elastic IP, ensuring continuous connectivity: Enhancing Network Performance with Specialized Adapters High-performance applications, such as multiplayer game backends or financial analytics platforms, demand low latency and high throughput. AWS offers two specialized networking options: Elastic Network Adapter (ENA): Provides consistent throughput and low latency for demanding applications. Ensure security groups and NACLs are tuned to optimize traffic flow. Elastic Fabric Adapter (EFA): Ideal for high-performance computing (HPC) environments, offering isolated, low-latency communication perfect for HPC workloads. Network Access Control Lists (NACLs) NACLs provide stateless filtering at the subnet level. Consider the following use cases: Securing Database Servers: In a multi-tier application, configure NACLs on private subnets so that only traffic from web servers (within a specified IP range) is allowed, with outbound rules permitting necessary responses. (Remember that NACLs evaluate rules in order; deny rules should precede allow rules to be effective.) Note Ensure your NACL rules are strictly ordered, as misconfiguration may inadvertently allow unauthorized access. Blocking Unauthorized Access: For FinTech companies facing repeated attacks, implement top-priority deny rules in your NACL configurations to block known malicious IP ranges. Allowing Outbound Connections: For gaming companies, configure NACLs to allow all outbound traffic while restricting inbound traffic to ephemeral ports, ensuring only response traffic is accepted. Additional diagrams illustrate broader VPC architectures featuring public/private subnets, NAT gateways, and load balancers: Security Groups for Stateful Firewall Protection Security Groups act as stateful firewalls and are essential for controlling traffic between instances within a VPC. Consider these scenarios: Inter-Instance Communication: For backend applications that need to communicate with database and cache layers, assign all related instances to a single security group. This allows free intra-group communication while blocking external access by default. Layered Security Model: For applications with multiple tiers (e.g., healthcare applications), create separate security groups for the web, application, and database layers. Limit communication between layers to only essential interactions to reduce cross-tier vulnerabilities. Application Load Balancers (ALBs) and SSL/TLS Termination Applications that process sensitive data, such as FinTech payment gateways, benefit from offloading SSL/TLS termination to an Application Load Balancer (ALB). By terminating the SSL/TLS connection at the ALB, the backend servers receive decrypted traffic, thereby reducing their computational load. AWS Certificate Manager simplifies public certificate management, ensuring smooth certificate rotation. A diagram below illustrates an architecture with ALBs in public subnets and internal load balancers or application servers within private subnets: This deployment demonstrates how VPCs, subnets, and load balancers work together to provide secure and efficient communications. This article has provided a comprehensive overview of AWS networking security strategies—from multi-VPC and subnet designs to specialized adapters, NACLs, and security groups. By configuring these components effectively, organizations can build architectures that are secure, scalable, and compliant with regional data regulations. For more details on AWS security best practices and network design, explore the following resources: AWS Security Best Practices Understanding AWS VPCs AWS Networking Fundamentals Happy architecting! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Design Principles of the Security,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Design-Principles-of-the-Security,"AWS Solutions Architect Associate Certification Designing for Security Design Principles of the Security Welcome back to this lesson. I’m Michael Forrester, and today we will cover several core security design principles encouraged by AWS. These principles are essential not only for passing the exam but also for designing robust and secure solutions on AWS. When implementing, deploying, or designing services on AWS, it's critical to ask questions such as: What aspects of security enhance the service? Which design principles strengthen overall security—and which could possibly degrade it if misapplied? What is the ultimate security goal we are trying to achieve? For example, when evaluating a service's security posture, identify the elements that bolster its protection. Similarly, for reliability, you may ask what factors enhance or diminish it. In security, the following key questions arise: What principles increase security? What principles might compromise it? What is our security end goal? Design principles act as filters to determine whether a particular approach strengthens, remains neutral, or undermines your security posture. Encryption Example Storing data in raw format without encryption exposes it to unauthorized reading by anyone with access. Encrypting data not only ties into robust access controls but is a fundamental security measure in itself. A foundational security requirement is strong identity management. This involves robust processes to manage users, groups, and roles, and to enforce strict conditions for resource access. In AWS, this responsibility is typically handled by IAM and AWS Organizations. Imagine an AWS account where a user, role, or group requires specific permissions. Securing that access begins with strong identity management. Next, ensure that security is implemented at every layer of your infrastructure and applications. Effective security extends beyond the network perimeter—it must incorporate public firewalls, network defenses, host-level protection, and application-level controls. Layered security, or defense in depth, ensures that if one control fails, additional controls continue to protect your system. A well-defined response to security events is another critical principle. Early in my career, I made the error of deleting a compromised machine immediately, which inadvertently erased crucial forensic evidence. A comprehensive incident response (IR) plan enables systematic handling of security incidents while preserving vital forensic data. Tracking all relevant data is also essential. Even seemingly insignificant data or activities should be logged. Collect metrics, logs, configuration details, and state information in a centralized and secure location with access restricted to authorized personnel. This practice promotes integrity and accountability. Reducing direct human contact with sensitive processes and data is vital. When intervention is necessary, rely on controlled tools that restrict direct access and help mitigate the risk of errors. For instance, rather than granting direct access to raw data, use tools that enforce role-based controls. Automating best practices is the next step. Tasks such as patching, security scans, and even auto-scaling should be automated. By integrating security scans into CI/CD pipelines, you ensure that these checks occur automatically during code merges and deployments, thereby reducing human error and improving reliability. Finally, protect your data in all states: at rest, in transit, and in use. Encrypt data on your storage devices, secure network transfers, and, where feasible, encrypt data even during processing. This approach is critical not only for databases but also for code repositories, files, and other sensitive information. The SAPTRAP Mnemonic To summarize these principles, remember the mnemonic ""SAPTRAP"": Principle Description Strong Identity Management Robust user, group, and role management through services like IAM and AWS Organizations. Apply Security at All Layers Extend security measures from public firewalls to host and application-level controls. Prepare for Security Events Develop and follow an incident response plan to handle security breaches effectively. Track Everything Relevant Log and monitor all metrics, access logs, and configuration details centrally. Reduce Direct Contact Minimize manual interventions by using automated tools that enforce role-based access. Automate Best Practices Integrate automation in security checks, patch management, and scalability processes. Protect Data in All States Encrypt data at rest, in transit, and during processing to ensure complete data protection. Real-World Example: Amazon S3 Let’s apply these principles to a real-world scenario involving Amazon S3 . Suppose a client requires enhanced protection for their S3 data. They would benefit from: Enhanced Encryption: Protect data at rest and in transit. While AWS now enables encryption by default for new buckets, older buckets might require manual intervention. Access Logging: Enable S3 access logging to capture critical details like IP addresses and access patterns, which help in detecting potential security issues. Strong Identity Management: Use IAM policies (instead of access control lists) to enforce strict access control and maintain robust security practices. Summary Design principles serve as guiding values that help balance security, reliability, and cost in your solutions. Incorporating practices like: Automating best practices, Protecting data in all states, Enforcing strong identity management, and Applying layered security ensures that your AWS environment remains secure and resilient against threats. Key Takeaway While these principles may not appear exactly as-is on the exam, applying concepts like defense in depth and stringent identity management is critical for your security strategy. Review the SAPTRAP mnemonic to reinforce these concepts and integrate them into your design methodologies. Thank you for joining this lesson on security design principles. We’ll move on to the next section shortly. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Storage Services Part 3,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Storage-Services-Part-3,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Storage Services Part 3 In this lesson, we delve deeper into secure storage by exploring storage manipulation services. Beyond merely storing files, these services back up, replicate, and manage your data—actively enhancing both reliability and security. AWS Backup AWS Backup is a relatively recent addition to the AWS portfolio (approximately five years old) that automates the backup process for data, such as encrypted EBS snapshots and volumes, using customer-managed keys. In a typical scenario, one AWS account backs up data to a vault that supports cross-account copy jobs to another AWS Backup vault. For instance, backups taken in US East 1 can be replicated to EU West 1. AWS Backup is secure by design, but for added protection, consider these best practices: Encrypt all items stored in the backup vault. Configure the backup job’s encryption settings to use a specific AWS Key Management Service (KMS) customer-managed key (CMK) when required by regulatory standards. Avoid relying solely on the default S3 encryption (SSE-S3) if a CMK is necessary; instead, opt for SSE-KMS. Adjust IAM policies for AWS Backup to grant the proper permissions over the CMK. Note that this does not automatically attach the CMK to your backup job. Example for Regulatory Compliance For example, a financial institution that needs to comply with regulatory requirements can select the desired CMK during job creation to enforce encryption at both the source and in the backup vault. Legal Holds and Vault Locks Imagine a media company that receives a legal notice requiring certain backups to remain unaltered for a specified period. AWS Backup provides two mechanisms to support this requirement: Legal Hold: Applies across multiple vaults, securing backups throughout the entire AWS Backup deployment. Vault Lock: Secures a single backup vault by applying compliance or governance controls, thereby preventing any modifications. While a legal hold spans multiple vaults, a vault lock functions similarly to an S3 object lock but is confined to one vault. For scenarios such as a pharmaceutical company needing to lock data for seven years—even from a root user—apply a legal hold when multiple vaults are involved or use a vault lock for a single vault. Securing Data Transfers with VPC Interface Endpoints AWS Backup traffic can be secured using VPC interface endpoints. These endpoints assign an IP address from your subnet, ensuring that traffic flows over AWS’s private backbone instead of the public internet. This configuration is particularly beneficial for multinational corporations that require secure backup operations between their VPCs and AWS Backup. The recommended approach is to configure a VPC interface endpoint for AWS Backup. Keep in mind that only S3 and DynamoDB support gateway endpoints; all other services, including AWS Backup, require interface endpoints. Elastic Disaster Recovery (EDR) Formerly known as CloudEndure (or Elastic Disaster Recovery), EDR replicates storage from any location—including non-AWS environments—into AWS on a block-by-block basis. This near-real-time recovery capability is essential for mission-critical applications during a disaster. The EDR console is a dedicated interface that provides detailed insights into job executions, recovery instance details, and overall disaster recovery operations. It is used during both the initial setup and subsequent management phases. A typical EDR process involves: Data sourced from an external data center (or another cloud provider) is identified. An AWS Replication Agent installed at the source transfers the data to replication servers. Replication servers pre-stage the data in volumes before copying it to the target virtual machines in AWS. Synchronization is maintained on a block-by-block basis, ensuring that even the smallest changes are securely replicated. For example, a healthcare organization using EDR might conduct a failover test and then perform a ""failback"" to restore systems to the primary environment after a disaster. Storage Gateway Storage Gateway is one of AWS’s longstanding services that seamlessly integrate on-premises data with AWS cloud services. It is available in several modes: S3 File Gateway: Provides a file system mount on Amazon S3 that supports standard file system protocols (NFS and SMB) with encryption enabled by default. Volume Gateway: Offers iSCSI-based storage volumes. Volume Gateway can operate in: Cached Mode: Frequently accessed data is stored on-premises, while the complete dataset remains in Amazon S3. Stored Mode: The entire dataset is stored locally and in Amazon S3, with asynchronous backups. Tape Gateway: Emulates a physical tape backup infrastructure using a virtual tape library (VTL), storing backups in S3 and optionally archiving them in Amazon Glacier. FSx File Gateway: Specifically designed for Windows-based file systems using SMB. This gateway integrates with FSx for Windows File Server and supports on-premises caching with SSDs for optimal performance. S3 File Gateway A media company considering an S3 File Gateway will benefit from industry-standard file protocols that simplify integration with existing applications while ensuring data encryption. FSx File Gateway The FSx File Gateway enables low-latency access to Windows file systems via the SMB protocol. It connects on-premises systems to an FSx for Windows File Server in the cloud and uses both data and metadata caches to optimize performance. Successful implementation requires proper setup of components such as DNS, time servers, and secure VPC endpoints. This gateway can be deployed on-premises as a virtual machine using standard hypervisors (e.g., ESXi, Hyper-V, or KVM) and can join a Microsoft domain, ensuring seamless integration with enterprise environments. Tape Gateway Tape Gateway replaces traditional tape backup infrastructure with a virtual tape library (VTL). Data from on-premises backup applications is transmitted via iSCSI to the tape gateway appliance, which then stores virtual tapes in S3 or archives them in Amazon Glacier. When evaluating Tape Gateway, consider the following features: Supports a virtual tape library (VTL) interface. Seamlessly integrates with your existing backup infrastructure. Can be deployed virtually on-premises or as an EC2 instance. Fully supports archiving with both S3 Glacier Flexible Retrieval and S3 Glacier Deep Archive. Volume Gateway Volume Gateway delivers cloud-backed iSCSI storage for on-premises servers, configurable in two modes: Cached Mode: Frequently accessed data is cached on-premises while the complete dataset remains in Amazon S3. Stored Mode: The entire dataset is maintained locally as well as in Amazon S3 with asynchronous backups. This gateway is engineered for fast provisioning and efficient snapshot creation, ensuring encryption both in transit and at rest. For example, a global e-commerce company seeking both scalability and security might choose Volume Gateway in stored mode to maintain a complete local copy of the data while using S3 as the primary storage medium. Although some sources might mention a maximum volume size of 64 terabytes, the current supported limit is typically 32 terabytes. Summary This module focused on storage services that enable manipulation of data through backup, replication, and disaster recovery. Newer AWS storage services include robust logging and built-in data-at-rest protection by default. Nearly all storage services support encryption through AWS KMS or customer-managed methods. AWS provides comprehensive monitoring through CloudWatch, with additional oversight possible via CloudTrail, AWS Config, and Trusted Advisor for premium support plans. Thank you for joining this comprehensive 90-minute discussion on storage services. In upcoming sessions, we will explore additional topics, including compute services. Happy securing your data! Watch Video Watch video content Practice Lab Practice lab"
AWS Solutions Architect Associate Certification,Turning up Security on Network Services Part 3,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Network-Services-Part-3,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Network Services Part 3 In this lesson, we explore advanced security design strategies for network services on AWS. We examine topics including networking load balancers, gateway load balancers, transit networking, secure endpoints with PrivateLink, and DNS security with Route 53. By leveraging these approaches, organizations can ensure that only authorized traffic reaches their critical resources while maintaining high performance and availability. Networking Load Balancers Consider a scenario where a gaming company deploys a real-time multiplayer game on AWS. The backend is hosted on EC2 instances within a VPC, and it is crucial that only legitimate gaming traffic on a specific UDP port reaches the servers, with traffic distributed evenly across instances. Since UDP demands low latency and high throughput, a Network Load Balancer (NLB) is an excellent choice. In previous implementations, security groups were not supported on load balancers; only Network ACLs (attached to subnets) were available. In this scenario, the recommended approach is to assign a security group to the NLB that permits traffic solely through the specified UDP port, while configuring a network ACL to allow all inbound and outbound traffic. Although reverse configurations or sole reliance on network ACLs are possible, security groups offer greater control and flexibility. Gateway Load Balancers Gateway load balancers are built to filter and inspect traffic when managing virtual appliances—such as firewalls—used for in-depth security inspections. For example, a global e-commerce company might deploy several virtual appliances in its VPC to inspect and filter traffic. By using an AWS Gateway Load Balancer, the company can scale the deployment uniformly while satisfying the security team’s requirements. When implementing a gateway load balancer, ensure that routing and target groups are configured correctly to map application components and dependencies. The load balancer distributes incoming traffic across firewall appliances, which then perform deep packet inspection and enforce security policies. Reviewing the architecture, the application subnet routes traffic to a gateway load balancer endpoint. This endpoint directs traffic to the inspection appliances and then forwards approved traffic to the Internet Gateway. Transit Networking Transit networking ensures secure data transfers between on-premises data centers and AWS environments. Below are several key strategies: VPN for Secure Data Transfer A healthcare organization uses AWS to host its patient management application while storing sensitive patient data on-premises. To securely transfer data for processing in the cloud, the organization plans to use an AWS site-to-site VPN with dynamic routing via BGP and encryption enabled. Although VPN connections occur over the internet, they provide encryption by default. A detailed diagram shows the VPN terminating at a Transit Gateway, which centralizes and distributes the connection to various VPCs for production, test, development, and shared purposes. Direct Connect and Enhanced Security For a multinational corporation transferring large amounts of data between on-premises data centers and AWS, AWS Direct Connect offers a dedicated connection without relying on the public internet. For even greater security, a VPN can be established over the Direct Connect connection, or MACsec can be used to encrypt data in transit. In the accompanying diagram, the corporate data center (customer gateway) connects via a Direct Connect partner to a virtual gateway (not a Transit Gateway), which then routes traffic to a specific VPC and its subnets. VPC Peering and Transit Gateway A large company manages three VPCs (Dev, UAT, and Prod) in separate AWS accounts with non-overlapping CIDR blocks. Although UAT is peered with both Prod and Dev, transitive routing is not allowed. Therefore, a direct VPC peering connection between Prod and Dev is necessary for effective communication. Note that VPC peering only supports direct point-to-point connections; transitive routing across peered VPCs is not supported by AWS. Each communication channel must be explicitly established. A transit gateway diagram illustrates how flexible routing and proper routing table permissions facilitate communication among multiple VPCs in a multi-VPC architecture while supporting disaster recovery. Endpoints and PrivateLink Secure Access to External APIs For a healthcare application that requires secure communication between microservices and an external API, utilizing a VPC endpoint through PrivateLink creates a secure route without exposing traffic on the public internet. When the external API is hosted within AWS (for example, by a partner), PrivateLink ensures that traffic remains private. Accessing AWS Secrets Manager Securely A fintech company developing a microservices-based application on AWS needs one of its services to securely retrieve configuration data from AWS Secrets Manager without exposing it publicly. The solution is to create a VPC interface endpoint for Secrets Manager and associate it with a security group that restricts access to required IP addresses and ports. Remember that gateway endpoints are available only for S3 and DynamoDB. The following diagram details VPC endpoints by distinguishing between gateway endpoints (for S3 and DynamoDB) and interface endpoints (which leverage PrivateLink for a variety of AWS services). Edge Networking with CloudFront Field-Level Encryption A media streaming company using Amazon CloudFront collects sensitive user information (such as credit card details) via web forms. They require encryption of specific fields as data travels from the user to the origin server. CloudFront’s field-level encryption feature, combined with a customer-managed KMS key, addresses this need by protecting sensitive data while in transit. CloudFront not only distributes content globally but also caches files and communicates with origin servers based on well-defined path configurations. This ensures that the sensitive data remains encrypted throughout its journey. CloudFront Functions and Lambda@Edge For a global e-commerce platform, deploying lightweight serverless functions at the edge can enhance both security and user experience by modifying HTTP requests or responses. When using CloudFront Functions, ensure that the functions run with the least privileges necessary to limit risk. Likewise, Lambda@Edge functions allow companies to deliver customized content directly from CloudFront without having to manage servers. It is crucial to assign only the minimal set of permissions necessary to perform their designated tasks. An illustration reveals the request-response flow in CloudFront, detailing how viewer requests are processed via edge caches and regional edge caches. Functions like CloudFront Functions or Lambda@Edge can be invoked to modify requests as needed. Additionally, a diagram details the secure implementation of Lambda@Edge functions alongside CloudFront, emphasizing the use of minimal permissions while AWS manages much of the inherent security infrastructure. DNS Security with Route 53 Amazon Route 53 provides global DNS routing for critical applications. For a multinational corporation, protecting DNS configurations against threats—like distributed denial-of-service (DDoS) attacks—is paramount. Implement strict access controls and routing policies using Route 53 features. Recommended practices include restricting write access and validating DNS responses through well-crafted policies. A diagram illustrates a Route 53 setup with a primary web server in one region and a standby server in another, utilizing various routing policies such as latency-based, failover, and geoproximity routing. This design enhances both application availability and resilience. Route 53 Application Recovery Controller For a financial technology company, the Route 53 Application Recovery Controller assists in rapid recovery from failures while ensuring robust security and resiliency. The recommended configuration includes establishing recovery readiness checks and grouping related application components. Correctly mapping dependencies and securing connections with strict IAM policies is essential. Another diagram displays the architecture of an Application Recovery Controller setup with active and standby zones across multiple AWS regions. It leverages global services, such as replicated DynamoDB tables, to maintain data consistency and enable rapid failover. Conclusion This lesson provided a comprehensive overview of security strategies for AWS network services—from load balancers and VPNs to Direct Connect, VPC endpoints, CloudFront, and Route 53. Each section emphasized best practices and proper configurations, ensuring robust security while maintaining optimal performance and high availability. Further Information For more insights into AWS network security best practices, be sure to visit the AWS Documentation and explore detailed case studies on secure cloud architectures. For any questions or further discussions, reach out via email at [email protected] , join our forum discussions, or connect on Slack. Thank you for participating in this lesson—stay tuned for the next session. — Michael Forrester Watch Video Watch video content Practice Lab Practice lab"
AWS Solutions Architect Associate Certification,Turning up Security on Compute Services Part 3,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Compute-Services-Part-3,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Compute Services Part 3 In this article, we explore AWS Lightsail and its relevance in cloud deployments. Although Lightsail is not covered in the AWS Solutions Architect Associate exam, it remains important to understand its use cases. Essentially, Lightsail is AWS’s entry-level offering for virtual private servers (VPS). For just a couple of dollars a month, you receive a single-processor instance with 2 GB of RAM—a solution ideal for hosting WordPress sites or small-scale applications, though it is not designed for enterprise-grade applications. Remember that Lightsail consolidates several costs into one package and does not match the enterprise-level infrastructure provided by EC2. Therefore, if Lightsail is mentioned during your studies or in exam scenarios, keep in mind it is considered out of scope. Consider this scenario: A startup evaluates Lightsail for deploying its web application due to its simplicity and affordability. However, because the application handles sensitive customer data, the startup must ensure robust security measures are in place. The recommended security approach for Lightsail is to utilize automated backups and snapshots to protect data integrity. Relying solely on default firewall settings or storing unencrypted sensitive data is not advised. Best Practice Ensure that you enable automation and avoid embedding sensitive information directly on the instance. Designing for Security with Containers This section focuses on container security using AWS Elastic Container Service (ECS) and Elastic Container Registry (ECR). ECS offers two deployment options: using EC2 launch types or the Fargate launch type. Fargate provides a serverless container experience by abstracting the management of the underlying infrastructure, enabling you to define tasks that launch containers effortlessly. ECS operates by defining tasks to launch containers according to your specifications. A load balancer typically routes traffic across multiple Availability Zones, and container images are securely pulled from a registry and deployed within your VPC. Securing ECS Tasks Securing ECS tasks is similar to EC2 security but with container-specific nuances. Consider this scenario: A media streaming company deploys a video processing application on ECS. The application processes large video files and stores the outputs in an S3 bucket, scaling based on the processing queue length. Security is achieved by dynamically scaling the service and applying appropriate IAM roles to tasks to grant secure access to S3 resources. Deploying an ECS service that scales with the processing queue and assigning specific IAM roles to tasks is the most effective configuration. Manual task launching or fixed task counts do not provide the desired scalability and security automation. ECS uses a container agent with a designated task execution role. This role ensures that tasks can securely access resources such as private container images from ECR, secrets from AWS Secrets Manager, or parameters stored in Systems Manager Parameter Store—without embedding credentials. Security Tip Always ensure ECS tasks are assigned the proper IAM roles to prevent unauthorized access to critical resources. Managing Secrets and Credential Security Imagine a FinTech company deploying a microservices application on ECS. To securely access other AWS services, ECS tasks should be assigned appropriate IAM roles instead of embedding AWS credentials in container images. This method, recommended by AWS, enhances security by eliminating the risk of exposing sensitive data. ECS in Practice: Designing Scalable, Secure Architectures A detailed example from ECSworkshop.com illustrates an ECS architecture designed for multiple services. In this architecture, a load balancer directs traffic to front-end services that communicate securely via service meshes like App Mesh (with AWS Lattice on the horizon). This design guarantees encryption in transit and resilient infrastructure through load balancer scaling and automatic task replacement. Consider another scenario: A media streaming company integrates a load balancer with its ECS service to efficiently distribute incoming traffic. The optimal solution is to use an Application Load Balancer (ALB) configured with HTTPS listeners, ensuring encryption for communications between clients and ECS tasks. Deploying the ECS service with an ALB under HTTPS is the best practice for security, while alternatives like a network load balancer for SSL termination or relying solely on service discovery fall short of fully securing the traffic. Scaling Microservices with ECS Services Consider a scenario where a FinTech startup deploys a microservices-based application using ECS. Each microservice (such as ordering, product management, or customer profiles) needs to scale independently based on demand, with automatic task replacement for failures. The best practice is to deploy each microservice as a separate ECS service with defined minimum and maximum task counts, leveraging CloudWatch metrics for autoscaling. Using the ECS service abstraction, rather than deploying all microservices within a single ECS task or service, provides the necessary management functions for scaling and health monitoring. The diagram above depicts two ECS clusters—one for development and one for production—running on Fargate in public subnets across different Availability Zones. A CI/CD pipeline ensures that new code is both tested and deployed automatically, maintaining consistency between environments. Migrating to ECS with Fargate For organizations seeking a serverless container experience without the burden of managing underlying infrastructure, ECS with Fargate is an excellent choice. Fargate abstracts the management of EC2 instances and provides a scalable, secure environment where AWS handles server provisioning, patching, and other maintenance tasks. The key advantages of using ECS with Fargate include running containers without the need for managing underlying EC2 instances and focusing solely on application development. Monitoring ECS with CloudWatch A robust monitoring strategy is essential for any cloud deployment. For a FinTech company deploying microservices on ECS, AWS CloudWatch provides detailed metrics for tasks, services, and containers in real time. CloudWatch enables operations teams to set alarms, identify issues early, and visualize performance and system health—all without the need for manual agent installation on each container. Final Thoughts on Security and Compute Services Securing compute services on AWS requires a comprehensive strategy that includes: Using Lightsail for basic VPS needs while understanding its limitations. Leveraging ECS for containerized applications with proper IAM roles to avoid embedding credentials. Deploying load balancers—preferably an Application Load Balancer with HTTPS—to secure traffic between clients and ECS tasks. Scaling microservices independently using ECS service autoscaling. Utilizing AWS Fargate for a serverless container experience that removes the complexity of managing EC2 instances. Employing AWS CloudWatch for continuous, real-time monitoring. This approach ensures that your AWS deployments are secure, scalable, and efficient, ultimately strengthening your overall cloud architecture. Key Takeaway Implementing the above best practices will help maximize your security posture across AWS compute services while simplifying management and scaling challenges. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Compute Services Part 6,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Compute-Services-Part-6,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Compute Services Part 6 In this lesson, we dive into the serverless domain with a focus on AWS Lambda. Introduced in 2013–2014, Lambda has quickly become a key component in modern applications by enabling serverless computing and event-driven architectures. Consider the following architecture diagram. A CloudFormation stack creates multiple resources where a user interacts with APIs (for product, basket, and order management) through an API Gateway. Depending on the URL or endpoint accessed, the gateway routes the request to the corresponding Lambda function that communicates with its dedicated DynamoDB backend. Within this microservices setup, individual functions (e.g., product handling, basket management, ordering) trigger events in EventBridge or dispatch messages to SQS queues. This diagram illustrates a straightforward microservices product predominantly powered by AWS Lambda. Key Concept AWS Lambda simplifies deployment and management by handling much of the heavy lifting such as infrastructure management, allowing you to focus on your application's business logic. Lambda in Media Applications Consider a media company using Lambda to process and transcode large video files. One major challenge is ensuring that Lambda functions complete processing within the maximum execution time limit of 15 minutes (up from 5 minutes earlier). When facing the risk of transcoding jobs exceeding this limit, several strategies can be implemented: Increase the Lambda memory allocation (which boosts CPU and memory but may not guarantee completion). Split the video file into smaller chunks, processing each chunk with a separate Lambda invocation. Set the Lambda function timeout to the maximum allowed limit and hope for timely completion (this is not advisable). Store video files in Amazon S3 and trigger processing via S3 events (useful but does not guarantee completion within 15 minutes). The optimal strategy is to split the video into smaller chunks, ensuring individual invocations complete within the execution window. AWS Lambda Shared Responsibility AWS Lambda is a highly managed service. Similar to other managed services, AWS minimizes the customer’s responsibilities regarding data encryption, integrity, authentication, application management, internet access, and monitoring/logging. The diagram below illustrates the shared responsibility model for Lambda: For example, a fintech startup building a serverless application to handle sensitive financial transactions must focus on implementing proper IAM roles and permissions for their Lambda functions. Customers are responsible for access control configurations while AWS manages physical security and infrastructure updates. Best Practices for Environment Variables Lambda allows you to set environment variables (e.g., database host, database name) by default. However, it is best not to place sensitive information directly in these variables. Instead, consider using services like AWS Secrets Manager or the Systems Manager Parameter Store to securely retrieve sensitive data. Even though many databases now offer IAM integration—meaning permissions are defined in the Lambda runtime role—storing sensitive data directly in environment variables should be avoided. For instance, a healthcare company processing patient data with Lambda might choose to use encrypted environment variables to securely manage database credentials and API keys. Lambda’s built-in encryption mechanism ensures that stored data is encrypted at rest, providing an additional layer of security. Securing Lambda Execution and Permissions It is critical to assign minimal necessary permissions using IAM roles when configuring Lambda functions. Never embed access keys or credentials directly in your code. The recommended approach is to create a role with the least privileges required and associate it with your Lambda function. Enhance your Lambda’s security by enabling monitoring and logging with CloudWatch, CloudTrail, and X-Ray. The use of AWS X-Ray provides detailed tracing of individual components during function execution. For example, a fintech company must configure roles with only the necessary permissions to access AWS services like S3 or DynamoDB, rather than using excessive privileges. Leveraging AWS X-Ray and GuardDuty Integrate AWS X-Ray with your Lambda functions by ensuring that the execution role has the permissions to send trace data. There is no need to manually embed the X-Ray SDK in your code—Lambda automatically handles much of the integration. This native capability offers an end-to-end view of your application’s performance and activity. Additionally, AWS GuardDuty can monitor CloudWatch logs and other data sources to detect potential vulnerabilities and threats. GuardDuty’s capabilities extend to Lambda, checking for any security anomalies and ensuring enhanced protection. AWS Step Functions: Orchestration and Security AWS Step Functions provide orchestration for workflows by coordinating multiple AWS services. Think of these as the modern evolution of the Simple Workflow Service (SWF). For example, a media company might utilize Step Functions to manage a video processing pipeline, where the state machine handles the sequence of tasks. When designing secure state machines, avoid storing sensitive data directly within the state machine. Instead, implement roles with fine-grained permissions for each state to ensure that only authorized services are accessed. For financial transactions and other event-driven architectures, consider using Amazon EventBridge to capture external events and trigger the appropriate Step Functions state machine. This push model is both reliable and cost-effective compared to continuous polling. AWS Serverless Application Model (SAM) The Serverless Application Model (SAM) simplifies the deployment and management of serverless resources. SAM leverages AWS IAM for permission management, and many security features—such as data encryption, logging, and monitoring—are inherited from AWS services. For example, a healthcare startup managing patient records via SAM should avoid embedding sensitive configuration data directly into SAM templates. Instead, use SAM’s support for fine-grained IAM permissions to secure each resource. When developing with the SAM CLI, ensure that your local testing environment mirrors production closely. This reduces the risk of deployment issues by preventing direct development-to-production pushes without proper staging and testing. AWS Serverless Application Repository (SAR) and Logging The Serverless Application Repository (SAR) serves as an image registry for serverless solution patterns. When deploying applications from SAR—especially in sensitive environments like healthcare—thoroughly review the source code, documentation, and associated permissions to ensure compliance with industry regulations. It is also crucial to log all activities for monitoring and auditing using CloudWatch, CloudTrail, and AWS Config. AWS Amplify: A Managed Deployment Option AWS Amplify offers a simplified deployment experience, especially for web and mobile applications backed by Lambda functions. Amplify automatically secures data both in transit and at rest, providing significant security advantages over self-managed systems. Additionally, its native integration with CloudWatch facilitates comprehensive logging of user activity and error metrics. For startups developing mobile e-commerce apps, using AWS Amplify with its built-in CloudWatch logging ensures that user activities and errors are monitored effectively, enhancing overall application security. Hybrid Computing with AWS Outposts Hybrid computing integrates on-premises hardware with AWS services. AWS Outposts extends AWS infrastructure into your data center, allowing you to run AWS services locally while maintaining secure connectivity with the cloud through Direct Connect or VPN. Outposts requires connectivity for updates and configuration changes, ensuring continuous integration with AWS security tools. For instance, when deploying a Kubernetes cluster with Amazon EKS on Outposts, it is vital to use IAM for worker nodes, ensuring that the nodes only access necessary AWS services securely. When securing EKS on Outposts, follow these best practices: Use IAM roles that grant least-privilege permissions. Avoid creating full administrative users. Prevent storing credentials on hardware. Furthermore, secure IAM integration on Outposts by assigning roles with specific, least-privilege permissions for each resource rather than relying on broad administrative rights. ECS, EKS Anywhere, and Container Management For organizations looking to extend container workloads on-premises, ECS and EKS Anywhere offer integrated solutions. These services allow deployment and management of containers in traditional data centers while maintaining AWS-level security and IAM integration. For example, a global retail company running containerized applications with ECS must ensure that the ECS Anywhere agent has proper permissions for secure communication with AWS. Similarly, when using EKS Anywhere, leverage AWS connectors and IAM to manage Kubernetes clusters securely so that integration with AWS services remains seamless. VMware Cloud on AWS VMware Cloud on AWS is a unique solution where most security responsibilities are managed through VMware’s suite of tools (vRealize, vCenter, vSAN, and vSphere) rather than AWS-specific configurations. This approach benefits organizations by providing a familiar IT environment alongside AWS’s global infrastructure. For multinational companies migrating on-premises VMware environments to the cloud, the seamless integration between VMware tools and AWS services eliminates the need for major refactoring. AWS Snow Family The AWS Snow Family, which includes devices like Snowcone and Snowball, offers hardened solutions for edge computing and temporary on-site data processing under limited connectivity conditions. Although these devices support compute functionalities similar to EC2, they only offer a subset of instance types. For a global research organization needing remote data processing, the compute version of the Snowball device is particularly well-suited for on-site processing and secure data transfer back to the cloud. Summary Securing compute services on AWS spans various environments—from virtual machines and containers to serverless and hybrid setups. For fully managed services like AWS Lambda, Step Functions, SAM, Amplify, and hybrid solutions such as Outposts, ECS/EKS Anywhere, and the Snow Family, the focus is on leveraging AWS’s built-in security features (encryption, IAM, logging, and tracing) and following best practices. Although traditional infrastructure like EC2 offers more granular control (e.g., OS-level security and patching), managed services relieve you of such burdens, allowing you to concentrate on proper configuration, access management, and monitoring. If you have followed along, you are now better equipped to design and secure diverse compute environments on AWS. In upcoming sections, we will explore additional services and design considerations to further enhance your cloud security expertise. Learn more about these AWS services in the official AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Application Integration Part 1,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Application-Integration-Part-1,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Application Integration Part 1 Welcome back, Solutions Architects. In this article, we explore designing secure application integration services within AWS. We start with scaling and auto-scaling as key enablers in the AWS ecosystem and then delve into security-enhancing services such as Elastic Load Balancers, Gateway Load Balancers, and API Gateway. Auto-Scaling and Its Security Implications Auto-scaling is a prominent feature in the AWS environment, particularly for EC2. Although its primary purpose is dynamic resource management rather than security, understanding its interplay with AWS security services is crucial. For example, when considering which IAM entity supplies the permissions needed for EC2 auto-scaling, the answer is service-linked roles. These built-in roles grant the necessary privileges to launch instances without adding extra security layers. When configuring an auto scaling group, aligning the minimum and maximum settings with business continuity, redundancy, and cost requirements is vital. Avoid setting the minimum and maximum to the same number; allow auto-scaling to adjust dynamically with load. Monitoring these events is typically done with CloudWatch metrics and logs, though these tools do not provide intrinsic infrastructure protection. Elastic Load Balancers and Their Security Features Elastic Load Balancers (ELBs) are available in three types. Each type offers distinct security functionalities to protect and manage traffic. Below, we review the key characteristics of Application Load Balancers (ALB) and Network Load Balancers (NLB). Application Load Balancers (ALB) Operating at Layer 7 of the OSI model, ALBs provide application-level security features. Key benefits include: Security Groups: Applied to control inbound and outbound traffic. Authentication Support: Integration with Amazon Cognito to authenticate incoming requests. SSL Termination: Uses SSL certificates sourced from AWS Certificate Manager with automated renewal. Logging: Supports access logs (sent to an S3 bucket), API call logs via CloudTrail, and operational metrics through CloudWatch. Consider this question when asked about the OSI layer at which ALB operates—the answer is Layer 7, highlighting its role in ensuring application-level security. Network Load Balancers (NLB) Operating at Layer 4, NLBs are optimized for handling TCP and TLS traffic. Their key features include: TLS Support: Provides TLS termination ensuring in-transit encryption. Security Groups: Now supports security groups for controlling traffic, in addition to subnet-level NACLs. Access Logging: Available for TLS listeners, similar to ALB. Combining NLBs and ALBs can augment performance while maintaining secure TLS processing. For instance, TLS termination can occur at the NLB, with traffic subsequently forwarded to an ALB for application-level path-based routing. Gateway Load Balancer (GLB) The Gateway Load Balancer (GLB) functions as a security appliance, intercepting all incoming traffic at its endpoint. It forwards traffic to one or more virtual appliances (up to 1,500), allowing these appliances to analyze and decide whether to permit or block the traffic. This mechanism is particularly useful when secure, private connectivity across VPCs is required. Key features include: Endpoint Security: Although GLBs lack direct security groups, endpoint access can be managed through routing tables and sometimes endpoint-attached security groups. Multi-account Restrictions: You can limit which AWS accounts are authorized to create endpoints by configuring the GLB to allow only specific principals. Geneve Protocol: The GLB leverages the Geneve protocol (Generic Network Virtualization Encapsulation) to intercept and encapsulate traffic, adding metadata for network virtualization and specialized appliance functions. Monitoring: Integration with CloudWatch and support for VPC flow logs enables comprehensive monitoring of traffic. Note For detailed monitoring of traffic on specific network interfaces, enable VPC flow logs. This provides low-level network data compared to CloudTrail, which logs only API calls. API Gateway Security Features API Gateway serves as the entry point for accessing backend services and offers distinct security features compared to load balancers. Its capabilities include: Encryption: Endpoints are secured using SSL/TLS, and cached responses can also be encrypted. Mutual TLS: Ensures both the client and server mutually authenticate each other. Custom Authorization: Offers several authentication options, including Lambda Authorizers for custom logic. Usage Plans and Throttling: Protects backend resources by supporting quotas and rate limiting. Logging and Tracing: CloudWatch Logs, detailed metrics, and X-Ray tracing provide comprehensive visibility into API activity. Consider deploying APIs with tightly controlled access by using API keys, usage plans, and custom authorization methods. This setup helps service third-party developers while protecting backend systems. A common scenario is exposing a set of APIs to third-party developers while restricting access with API keys and usage plans. Throttling, along with detailed logging via CloudWatch and X-Ray tracing, ensures both security and performance. Looking Ahead In the next section, we will introduce AppFlow and explore its role in enhancing security and integration within the AWS ecosystem. This concludes our detailed exploration of auto-scaling, load balancers, and API Gateway security features. Each service plays a unique yet complementary role in ensuring robust, scalable, and secure application integration in AWS. Happy architecting! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Database Services Part 6,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Database-Services-Part-6,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Database Services Part 6 In this article, we explore advanced methods to enhance security on AWS database services with a particular focus on immutability and time-series databases. We dive into Amazon Quantum Ledger Database (QLDB) and Amazon Timestream, discussing their security features, network integration, and performance insights. Amazon Quantum Ledger Database (QLDB) Amazon QLDB is a ledger database that captures every data modification with an immutable record, ensuring that no changes are ever lost or altered. At first glance, QLDB might appear similar to a blockchain, but it is designed solely as a tamper-proof ledger. For instance, if you update a value from 1 to 2, the history of that change is permanently recorded. QLDB maintains a complete, tamper-resistant history of all modifications. This is particularly useful when an immutable record of transactions is required. For example, a logistics company might choose QLDB if they need SQL query support along with ACID properties—atomicity, consistency, isolation, and durability. It is important to note that while QLDB supports SQL-like queries, it is not intended for complex transactional queries like traditional relational databases. Note If you are looking for a traditional blockchain service, AWS offers separate options for that purpose. QLDB is strictly a distributed ledger database that provides cryptographically verifiable transaction logs in a centralized system. For access management, QLDB integrates seamlessly with AWS Identity and Access Management (IAM), allowing you to assign roles and enforce granular permissions for specific ledger operations. For network security, QLDB can be deployed within a Virtual Private Cloud (VPC) using a VPC endpoint. While the service itself does not attach a security group directly, the VPC endpoint applies standard firewall rules, providing robust network-level protection. Encryption is enforced by default on QLDB for both data at rest and in transit, adding an extra layer of security. Additionally, QLDB integrates with AWS CloudWatch and CloudTrail to provide detailed monitoring and an audit trail for API calls and system metrics. Amazon Timestream Amazon Timestream is a serverless time series database designed to efficiently store, process, and analyze high volumes of IoT and real-time analytics data. Although Timestream allows data updates, it is optimized for sequential storage of time-series data. Timestream is ideal for IoT applications, capable of handling up to a trillion events per day, and does so at a fraction of the cost of traditional databases. The integration with IAM enables you to manage access through roles, enhancing overall security with fine-grained permissions. IAM integration in Timestream facilitates secure access control, which is essential when connecting with other AWS services such as TwinMaker or Lambda. Moreover, Timestream supports secure network communications through VPC endpoints and adheres to standard firewall configurations. Security Tip Always use VPC endpoints to isolate your database services and enforce standard firewall rules for tighter network security. Timestream enforces encryption by default, ensuring data is secure during transit and while at rest. Additionally, Timestream integrates with CloudWatch and supports API calls for effective data reporting and monitoring. This feature is particularly useful for manufacturing companies that need to track the rate of incoming records—a critical performance metric. Summary This article has provided an in-depth look at two critical AWS database services and their security features: Amazon QLDB: Provides an immutable ledger with cryptographic verification and a complete history of changes. Integrates with IAM for granular access control and utilizes VPC endpoints for network security. Ensures data protection with encryption enabled by default, both at rest and in transit. Amazon Timestream: Serves as a cost-effective, serverless time series database optimized for IoT and real-time analytics. Offers robust security through IAM integration, VPC endpoints, and default encryption. Leverages CloudWatch for monitoring key performance metrics, particularly the rate of incoming data records. Understanding the unique features and security capabilities of these AWS services is crucial for designing secure, efficient, and scalable systems in today's complex technological landscape. Thank you for reading. In our next article, we'll shift our focus from databases to application integration, auto-scaling, and other essential components of modern cloud architectures. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Application Integration Part 2,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Application-Integration-Part-2,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Application Integration Part 2 In this lesson, we continue our discussion on securing application integrations with a focus on Amazon AppFlow. AppFlow facilitates bi-directional data transfers between SaaS applications and AWS services. Because most of its operations involve API calls with third-party applications, its security mechanisms mirror those used in IAM. It is important to note that AppFlow is a fully managed service, unlike an ETL service such as AWS Glue, and is geared toward long-term data storage—much like Amazon Glacier—with secure data transfers. All connection details for AppFlow are safely stored in Secrets Manager, and encryption is applied to the data context. There have been questions regarding the enhancement of security through integration with Secrets Manager. Rest assured, AppFlow seamlessly integrates with Secrets Manager to securely handle connection details. :::note Logging and Monitoring For logging, AppFlow utilizes CloudTrail and CloudWatch to track API calls and monitor data flows. For instance, a multinational corporation may synchronize data between cloud applications and AWS services, then use CloudWatch for metrics and flow execution tracking. Supplementary tools like AWS X-Ray, AWS Config, and AWS GuardDuty can also be considered for tracing and detecting issues. ::: Messaging and Events Next, we delve into messaging and event services, starting with the Simple Notification Service (SNS). SNS is a robust notification system that dispatches messages to a topic. Depending on the subscription configuration, these messages can be delivered to one or more endpoints such as SQS queues, email addresses, or SMS numbers. For example, a message might be replicated across three SQS queues as dictated by an application workflow. SNS secures data by encrypting both topics and queues using server-side encryption (SSE) via AWS Key Management Service (KMS). Another diagram demonstrates SNS integrated with two queues managed by a billing Lambda function and a scheduling function, with data processed and, in some cases, routed into S3. Importantly, SNS offers Message Data Protection, which allows you to set policies that audit, redact, or block sensitive information such as Personally Identifiable Information (PII). Moreover, SNS leverages resource policies alongside IAM to enforce fine-grained access control, ensuring only authorized entities can publish or subscribe to a topic. A common method is to manage access through IAM roles or groups, rather than relying solely on resource policies. SNS further integrates with CloudTrail and CloudWatch to provide key metrics (e.g., the number of messages published and delivery success rates) to monitor performance and ensure timely deliveries. A separate diagram outlines various monitoring solutions for SNS, emphasizing that CloudWatch Logs and Metrics play a fundamental role in tracking SNS performance—including practices for handling dead-letter queues when messages fail. Amazon Simple Queue Service (SQS) Now let’s discuss Amazon Simple Queue Service (SQS). Launched shortly after S3, SQS is a highly managed queuing service available in both standard and FIFO versions. It provides built-in encryption at rest via KMS and supports private traffic through VPC interface endpoints powered by AWS PrivateLink. Although SQS offers CloudWatch metrics and API activity tracking through CloudTrail, note that it does not log individual queue messages natively without CloudTrail integration. For detailed insights into API call interactions, ensure that CloudTrail logging is enabled. Amazon MQ Moving on to Amazon MQ—a managed message broker service based on ActiveMQ (with support for RabbitMQ). Unlike SQS, Amazon MQ is tailored for scenarios where industry-standard messaging protocols (such as MQTT) and classic broker functionalities are required. Amazon MQ ensures security using KMS for encryption at rest and supports encrypted data in transit. Amazon EventBridge Amazon EventBridge is a fully managed event bus that efficiently routes events based on defined rules. It encrypts data by default using the AES-256 algorithm and provides a seamless connection between applications without acting as a traditional broker, streaming service, or managed workflow service. To monitor EventBridge, leverage CloudWatch metrics and CloudTrail, along with optional tools such as AWS X-Ray and AWS Config. Amazon Simple Email Service (SES) Amazon Simple Email Service (SES) differs from SNS in its focus on sending bulk emails. SES supports multiple actions such as sending emails, storing them on S3, and processing content via Lambda (for tasks like language detection before resending). SES applies AES-256 encryption by default for data in transit and supports encryption at rest. Its standard logging capabilities are available through CloudWatch or Kinesis Data Firehose, and event notifications can be configured with SNS. In one scenario, emails from an SES domain are processed via an SNS fan-out, stored in S3, and managed by additional AWS services. SES also features an event publishing function that relays data on deliveries, bounces, complaints, and more. Tools such as the Reputation Dashboard help track email performance and ensure compliance with industry standards. Workflows and Orchestration AWS Step Functions AWS Step Functions serves as an orchestrator and state machine for coordinating tasks, often managing multiple Lambda functions. Security is primarily enforced through IAM policies that restrict both invocation and execution rights, thereby preventing runaway workflows. Monitoring is achieved via CloudWatch, CloudTrail, and AWS X-Ray, while integration with SNS provides timely notifications to users. Amazon Simple Workflow Service (SWF) An older orchestration service, Amazon SWF, is used for coordinating complex tasks (such as credit card processing or order management) through both sequential and parallel steps. SWF requires the invocation of underlying resources, and similar to other AWS services, it supports standard encryption and logging via CloudWatch and CloudTrail. Additional monitoring for SWF is accomplished using CloudWatch metrics, while changes in configuration are tracked with CloudTrail and AWS Config. AWS Managed Workflows for Apache Airflow (MWAA) The final service covered in this lesson is AWS Managed Workflows for Apache Airflow (MWAA). This fully managed solution abstracts away the complexities of managing Apache Airflow components (such as schedulers, workers, and the metadata database), enabling you to focus on designing and running complex ETL workflows. MWAA leverages Amazon Aurora for metadata storage and integrates seamlessly with AWS services for logging and security via CloudWatch and IAM. Summary In this lesson, we covered various AWS application integration services designed to enhance operational scaling by serving as intermediaries between compute and data storage. Key takeaways include: Built-In Encryption: Most managed services offer encryption by default for data at rest and in transit. Robust Logging and Monitoring: CloudWatch and CloudTrail are typically used for logging and monitoring. In some cases, AWS X-Ray and AWS Config provide deeper insights. Managed and Secure Integration: These services reduce the need for manual configuration while ensuring strong security and operational oversight. We look forward to exploring additional topics in our next lesson. Thank you for following along. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Compute Services Part 5,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Compute-Services-Part-5,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Compute Services Part 5 In this lesson, we focus on designing secure architectures for AWS container services. We will specifically explore the Elastic Container Registry (ECR) as well as related services such as App Runner and Proton, providing insights into best practices and security configurations. Elastic Container Registry (ECR) ECR is a versatile container registry that can be configured as either public or private. It easily integrates with AWS services such as IAM for permission management, CI/CD pipelines, and VPC endpoints for secure, private connectivity. Additionally, ECR supports repository replication and can serve multiple purposes including pull-through caching, development registries, and production repositories. Consider a scenario where a global e-commerce company uses ECR to store container images for its microservices application. To ensure images are securely stored, only authorized entities can push or pull images, and vulnerabilities are automatically detected, implement the following measures: Configure IAM policies to grant precise permissions to specific users or roles. Enable automatic image scanning upon push (leveraging tools like Trivy or similar). For enhanced security, integrate logging services such as CloudWatch, CloudTrail, and AWS Config. Using VPC endpoints further ensures that ECR traffic does not traverse the public internet. AWS Logging and VPC Connectivity in ECR ECR leverages standard AWS logging mechanisms to help you monitor and audit activities: CloudWatch for real-time monitoring. CloudTrail to capture and record API calls. AWS Config for tracking and recording configuration changes. Additionally, ECR supports VPC interface endpoints that provide secure, private connectivity. For instance, if a Docker instance in VPC-A pulls an image from a test registry, the traffic will flow through the dedicated VPC endpoints, including those for S3 and any other required services. Keep in mind that gateway endpoints are available only for S3 and DynamoDB, while services like ECR require interface endpoints. Note For organizations such as healthcare providers that mandate comprehensive auditing and quick troubleshooting, combining IAM resource-level controls with automatic scanning significantly enhances both security and operational efficiency. Securing App Runner AWS App Runner is a fully managed service that simplifies running web applications by deploying code or container images directly from your source repository. It is especially beneficial for rapidly growing startups that need fast deployments without the overhead of managing infrastructure. Consider a scenario where a startup prepares to deploy a web application using App Runner with the expectation of a user surge. The recommended approach is to leverage App Runner’s built-in features, which include: Direct code or container deployment from source repositories. Automatic scaling and integrated monitoring. Streamlined security features using CloudWatch, CloudTrail, and AWS Config. A standard App Runner workflow typically involves: Creating the service. Integrating source repositories. Deploying containerized applications. Monitoring service performance and security. Integrating App Runner with a VPC For organizations such as e-commerce companies migrating backend services to App Runner, maintaining private communication between the VPC and App Runner is essential. To avoid routing traffic over the public internet, configure VPC interface endpoints. This setup ensures that communications remain securely within the AWS network. It’s best to use interface endpoints rather than alternatives like VPC peering, Direct Connect, or VPN. Note that gateway endpoints are applicable only to S3 and DynamoDB. A detailed VPC-connected App Runner architecture typically includes components such as: An internet-facing load balancer. A request router. Multiple network interfaces directing traffic to tasks. A NAT Gateway for controlled internet access by private VPC resources. Data Encryption in App Runner For startups and enterprises that require encryption in transit and at rest, App Runner comes with built-in support for data encryption and HTTPS communications. To ensure robust security: Rely on App Runner’s encryption mechanisms for data at rest. Enforce HTTPS to secure data in transit. Warning Do not disable encryption if you operate in an environment with stringent data protection and compliance standards. AWS Batch Overview and Security Configurations AWS Batch simplifies the processing of large-scale jobs by efficiently orchestrating compute resources such as EC2, ECS, and EKS (with Fargate and Spot instances available). It divides workloads into smaller jobs that run at scale and typically stores outputs in S3. Securing AWS Batch When designing a secure AWS Batch environment, consider the following best practices: Mix on-demand instances (for critical jobs) with spot instances (for non-critical jobs) to optimize cost and performance. Assign specific IAM roles to limit Batch jobs to only necessary resource actions. Integrate monitoring and logging with CloudWatch Logs, CloudTrail, and AWS Config. For example, a pharmaceutical company using AWS Batch to process extensive datasets should adopt a hybrid approach to balance cost efficiency and workload prioritization. Ensure effective IAM permission management by assigning distinct policies to different teams. This ensures that each team only accesses its designated job queues and definitions. Monitoring and Troubleshooting Batch Jobs For companies running complex simulations, integrating CloudWatch Logs to monitor error patterns and setting up alarms is highly recommended. This approach provides granular visibility into the job lifecycle and accelerates issue resolution. Batch and Container Services Integration AWS Batch, while operating with containerized jobs (using ECS, EC2, EKS, and Fargate), is an essential part of the broader container services ecosystem. Batch leverages IAM roles for securing job executions and integrates with standard AWS logging and monitoring services. A typical workflow might involve: A user uploads a file to S3, triggering an event. An AWS Lambda function is invoked by the event, which then initiates a Batch job. The Batch job runs on Fargate, accesses an application image from a registry, processes data retrieved via a gateway endpoint, and writes the results to DynamoDB. This sequence demonstrates how AWS Batch seamlessly integrates with other AWS services to ensure secure data management and operational efficiency. Transition to Serverless After exploring the security configurations for AWS Batch and container registries like ECR, we now transition to serverless architectures. In the next part of this lesson, we will examine how similar security design principles apply to serverless services, ensuring that your applications are both secure and scalable. For more details on AWS security best practices, examine the official AWS Documentation and AWS Security Blog . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Database Services Part 2,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Database-Services-Part-2,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Database Services Part 2 In this article, we explore Amazon RDS Aurora as an extension of the standard Amazon RDS offerings (hereafter referred to as “plain vanilla RDS”). Aurora, available in PostgreSQL and MySQL variants, also offers serverless options; however, from a security perspective, the differences are minimal. For instance, RDS PostgreSQL (displayed on the left) employs synchronous replication across two EBS volumes. In contrast, an Aurora cluster deploys three EBS volumes (one primary and two secondaries) and manages replication at the storage layer rather than at the database level. As shown on the right, the replication is handled by the storage node. Although these methods vary, their overall impact on security remains consistent. Aurora Architecture Quiz Fintech startups assessing a migration to Aurora often review its architecture to take advantage of high availability and enhanced performance. Consider the following statements regarding Aurora’s architecture: Aurora automatically replicates data across multiple Availability Zones in a single region with two copies per zone. (This description oversimplifies the replication process.) Aurora uses a single master node for both reads and writes without support for read replicas. (This is incorrect; Aurora supports up to 15 or 16 read replicas.) Aurora divides the database volume into 10 GB segments distributed across many disks, with each segment replicated six times across three Availability Zones. (This is the most accurate portrayal, where each 10 GB chunk is replicated six ways regardless of overall data size.) Aurora is a serverless database service that automatically starts up, shuts down, and scales based on demand. (While Aurora Serverless does introduce automatic scaling, it simplifies the startup and shutdown process too much.) Clearly, statement 3 most precisely describes Aurora's architecture. IAM Integration and Authentication Options Aurora supports both IAM-based and native database authentication methods. It integrates seamlessly with AWS IAM and can also synchronize with AWS SSO if required. For example, a healthcare company migrating to Aurora must secure patient data with robust authentication. Only the authentication option that supports both IAM-based and native database authentication will meet these rigorous security requirements. Additionally, Aurora integrates efficiently with AWS Secrets Manager. For example, in a MySQL Aurora cluster deployed in the AP South 1 region with one master and replication across two zones, you can secure credentials via Secrets Manager—even if IAM-based or native database authentication is not enabled. When developing applications (such as those by companies like Intech), it is crucial not to store sensitive data in plain text or unsecured locations such as public S3 buckets. Instead, choose AWS Secrets Manager to generate, store, and automatically rotate database credentials. Networking and VPC Configurations Aurora’s networking setup follows a familiar pattern seen in other RDS products: one primary writer node with multiple reader instances. These reader nodes can scale within security groups, reside in private subnets, or in some cases, be publicly accessible (though they are typically kept private). Metrics and monitoring data are primarily emitted through CloudWatch. For healthcare companies deploying patient management systems on Aurora, isolating the database environment and safeguarding access is essential. The best practice is to deploy Aurora within a VPC, assign it to a private subnet, and restrict network access via tightly controlled security group rules that allow only specific application instances (hosted on AWS Lambda, ECS, EKS, or EC2) to connect. Furthermore, to prevent direct internet access, it is recommended to disable public accessibility and ensure that the application servers remain in the same VPC as the Aurora database. Aurora also supports VPC endpoints. Note that only interface endpoints are available for Aurora, as gateway endpoints are reserved for DynamoDB and S3, ensuring secure and private connectivity to other AWS services. Monitoring, Auditing, and Encryption Aurora provides advanced monitoring and auditing capabilities. A notable feature is Aurora Database Activity Streams, which delivers real-time database activity data for enhanced monitoring and compliance. This is particularly valuable for fintech companies subject to strict financial regulations. In addition to activity streaming, Aurora offers encryption for data at rest and in transit. Utilizing AWS KMS, you can enable the built-in encryption features and configure SSL/TLS for database connections to maintain data integrity and confidentiality. To ensure robust encryption: Enable the built-in encryption-at-rest feature. Configure the database to enforce SSL/TLS connections. Note Implementing these encryption measures ensures that sensitive data—such as patient records—is securely stored and transmitted. Aurora Serverless and Security Considerations Aurora Serverless delivers scalability equivalent to traditional Aurora deployments while maintaining rigorous security standards. When using Aurora Serverless, it is imperative to enable built-in encryption and enforce SSL/TLS for all connections, just as you would with non-serverless deployments. A fintech startup opting for Aurora Serverless should adhere to the same security best practices—leveraging built-in encryption and secure connectivity—instead of employing insecure methods such as storing sensitive data in plain text. RDS Proxy Overview and Its Security Benefits RDS Proxy is designed to efficiently manage connections between applications and RDS databases. It helps mitigate connection overloads during sudden traffic spikes by acting as an intermediary, thereby reducing the load on the RDS instances. Additionally, RDS Proxy offers enhanced logging, caching (if enabled), and improved failover handling. RDS Proxy has its own endpoint and security group, is deployed within a VPC, and uses the same underlying engine as the RDS instance. For example, an application may retrieve database credentials securely from AWS Secrets Manager and connect through RDS Proxy without requiring significant code changes. Warning Always use AWS Secrets Manager in conjunction with RDS Proxy to securely manage database credentials. Avoid insecure methods such as storing credentials in plain text or using unsupported third-party tools. RDS Proxy employs dedicated security groups to control network traffic. By configuring these security groups to allow communication only with the associated RDS instance’s security group, you ensure restricted and authorized access. Enhanced logging is another valuable feature of RDS Proxy. It provides detailed insights into SQL statements for debugging and auditing. However, note that enabling enhanced logging can affect system performance and typically disables automatically after 24 hours. In summary, both Aurora and RDS Proxy deliver enhanced high-availability, scalability, and security features. By leveraging multi-AZ replication, integrated IAM and Secrets Manager support, secure VPC deployments, and advanced logging capabilities, organizations ranging from healthcare providers to fintech startups can protect their databases and maintain robust performance across varying workloads. For further details, consider exploring related topics such as Kubernetes Basics , Kubernetes Documentation , Docker Hub , and the Terraform Registry . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Database Services Part 5,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Database-Services-Part-5,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Database Services Part 5 In this article, we explore the security designs implemented in various AWS database services. We cover MemoryDB for Redis, DocumentDB, Keyspaces for Apache Cassandra, and Neptune. Each service leverages unique AWS-managed features—such as VPC isolation, IAM-based authentication, encryption, and detailed monitoring—allowing you to architect secure solutions tailored to your needs. MemoryDB for Redis MemoryDB for Redis is a fully managed, persistent in-memory database service that runs a Redis-compatible engine. It employs a primary-replica architecture where a primary node handles write operations while replica nodes serve read requests through a configuration endpoint. The design follows recognized AWS patterns: Replication across multiple availability zones. Microsecond read latencies comparable to ElastiCache. Support for Redis Access Control Lists (ACLs) to enforce command- and key-level permissions. Note that while IAM policies control resource-level access (e.g., console/API operations), Redis ACLs strictly manage access within the database. IAM policies provide resource-level access control, while Redis ACLs define granular command and key permissions independently of encryption. For network security, MemoryDB for Redis relies on VPC interface endpoints (as opposed to gateway endpoints) for private connectivity via PrivateLink. These endpoints, positioned behind a network load balancer, enable integration with services such as Lambda and SNS. Key aspects regarding VPC endpoint integration include: Support for VPC interface endpoints for private connectivity. Non-support of gateway endpoints. Utilization of standard AWS data transfer paths without bypass or acceleration. Security Features MemoryDB for Redis offers robust encryption and security: Encryption at rest using AWS Key Management Service (KMS). Encryption in transit via TLS. Use of Redis ACLs for detailed command and key-level control. For event monitoring, MemoryDB for Redis emits cluster events that can be captured with CloudWatch, CloudTrail, and delivered via SNS notifications. DocumentDB Amazon DocumentDB is a MongoDB-compatible document database service that provides native username/password authentication for client connections. For administrative operations, access is governed by IAM roles. DocumentDB operates in a primary-replica configuration within a VPC, leveraging security groups, Network ACLs, and VPC isolation. Note that the default port for MongoDB (and DocumentDB) is 27017. Client-Side Field Encryption DocumentDB supports client-side field encryption processed by the MongoDB driver. When querying an encrypted field, the process involves: Detecting the encrypted field. Requesting the required encryption key from an external key manager (typically AWS KMS). Encrypting the field before query submission. Decrypting the response on the client side using the decryption key. To implement this encryption: Create an encryption key using AWS KMS. Associate the key with an appropriate IAM role. Configure your application to interact with AWS KMS during CRUD operations. DocumentDB provides encryption both at rest and in transit via TLS. Monitoring and Detection DocumentDB leverages CloudWatch for performance monitoring. CPU utilization is typically the first metric that indicates cluster activity. Additionally, a built-in profiler logs slow operations to diagnose performance issues. Audit logs further support detailed tracking of database events. Keyspaces for Apache Cassandra Amazon Keyspaces is an AWS-managed service that is compatible with Apache Cassandra. It exclusively uses IAM for both authentication and authorization, in contrast to DocumentDB’s native username/password approach. For example, the following snippet shows how Secrets Manager is used to manage credentials in the setup script: --entrypoint aws-sm-cqlsh.sh Keyspaces relies on IAM roles for secure, granular access rather than long-term static credentials. AWS manages infrastructure components such as guest OS patching, freeing customers to focus on data and application management. Security and Encryption Key security features for Keyspaces include: Encryption at rest using AWS KMS (with AWS-managed or customer-managed keys). Encryption in transit by default via TLS. Monitoring is handled by AWS CloudWatch and CloudTrail for capturing logs and metrics. AWS X-Ray and additional custom solutions are typically unnecessary for routine monitoring. Neptune Amazon Neptune is a managed graph database service similar to Neo4j. It supports both IAM database authentication and local user authentication for application-level management. For access control, it is recommended to use IAM for fine-grained, role-based authentication rather than a custom solution. The cluster endpoint of Neptune is located in a private subnet and secured by security groups. Neptune follows a maintenance paradigm similar to RDS and DocumentDB. Patching occurs during configured maintenance windows. The encryption capabilities in Neptune include: Encryption at rest via AWS KMS. In-transit encryption using TLS. Monitoring and Logging Neptune leverages CloudWatch for monitoring and CloudTrail for auditing API calls. Although native slow query logs are not available, error logs and cluster status events play an essential role in troubleshooting. Note Audit logs are available in Neptune, which assist in tracking database events for compliance and performance troubleshooting. For event notifications, Neptune supports various delivery channels (including SNS) without the need for Lambda integration. Patching for Neptune databases is conducted during scheduled maintenance windows, ensuring controlled updates and minimizing manual intervention. This comprehensive overview has covered the security design and operational details for MemoryDB for Redis, DocumentDB, Keyspaces for Apache Cassandra, and Neptune. By leveraging AWS-managed features such as VPC isolation, IAM, encryption, and monitoring, you can design robust and secure database architectures on AWS. For more detailed information, consider exploring: AWS Database Services Overview Amazon MemoryDB for Redis Documentation Amazon DocumentDB Documentation Amazon Keyspaces Documentation Amazon Neptune Documentation Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Database Services Part 1,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Database-Services-Part-1,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Database Services Part 1 Welcome, future solutions architects. In this lesson, we explore design attributes for enhancing security in database services within AWS. We begin by examining the strengths of SQL and then move into securing AWS services like Amazon RDS by leveraging built-in security controls. Remember the shared responsibility model: platform services like RDS reduce your operational overhead and share security responsibilities compared to managing your own servers on EC2. This difference is even more pronounced with highly managed services such as DynamoDB, where much of the security is handled for you. The standard conversation for securing managed services typically centers on controlling IAM access and monitoring logs with CloudWatch (particularly CloudWatch Logs), CloudTrail for API calls, and Config for tracking configuration changes. As services become increasingly managed, many security tasks are abstracted away. We have already covered networking, storage, and compute—each a component of infrastructure-based or infrastructure-managed services. Now, our focus shifts to database services, which primarily fall into the platform services category or even the highly managed group when advanced use cases like machine learning are involved. Amazon RDS and IAM Authentication Amazon RDS supports multiple authentication methods. A notable option is IAM authentication, which allows you to use your IAM user credentials to log in to an RDS database (supported for engines such as MariaDB, MSSQL, or Oracle). This avoids the need to manage separate database-specific credentials, especially when every database instance maintains its own user store. To use IAM authentication, enable the option during the database launch or modification process, then create an IAM policy that grants RDS DB connect permissions linked to the corresponding database user. Even though you might consider modifying the RDS instance or creating an IAM role directly, the technically correct approach is to craft an IAM policy with the necessary connection permissions. Consider this real-world case study question: MariaDB on RDS wants to leverage AWS IAM for database authentication. What steps are needed? Options include modifying the instance to enable IAM authentication, creating an IAM role with permissions, or generating IAM access keys for a MariaDB user password. The correct approach is to create an IAM policy that grants RDS DB connect permissions and associate it with the database user. Note Even though IAM authentication can be enabled at launch, remember that establishing proper permissions is crucial. Authentication Methods Across Database Engines Different RDS engines offer different authentication methods. For example, MySQL and MariaDB support native authentication but do not offer Kerberos. On the other hand, MSSQL, PostgreSQL, and Oracle support both native and Kerberos authentication. In multinational scenarios requiring both authentication types, PostgreSQL (or MSSQL/Oracle if available) stands out as a strong candidate. Secrets Management and Credential Rotation Amazon RDS is integrated with AWS Secrets Manager, which automates the rotation of database credentials. This presents a significant advantage over storing credentials as secure strings in Systems Manager, which does not support the auto-rotation feature. When you require automatic password rotation without modifying your application, Secrets Manager is the clear choice. RDS and VPC Considerations While RDS is fully managed, it operates on virtualized infrastructure much like EC2. Therefore, you must consider standard EC2 security practices such as firewalls, VPC isolation, and the impact of deploying your database across public versus private subnets. Key considerations include: Which DB security groups to use Whether to attach an SSL/TLS certificate Whether the instance is publicly accessible (assigned a public IP) or remains in a private subnet For example, if a database is deployed in a public subnet without a public IP address, the associated Internet Gateway cannot reach it. This detail is crucial: even in a public subnet, absence of a public IP maintains isolation from direct internet access—a vital aspect of securing your database. Consider this scenario question: Global financial firms use Amazon RDS to host transactional databases accessed worldwide. To ensure that only legitimate traffic reaches their RDS instance, which security measure should be implemented? A web application firewall (WAF) is unsuitable for typical database traffic (e.g., ports 1433 or 3306), and AWS SHIELD is designed for DDoS protection rather than access control. The best approach is to configure RDS security groups to allow only specific IP addresses or CIDR blocks. Choosing the Right RDS Instance Type for Performance and Security For rapidly growing e-commerce businesses migrating to RDS, instance type selection has implications for both performance and security. Burstable instances may not sustain prolonged high loads effectively, whereas memory-optimized instances are engineered for workloads that demand large in-memory data sets. In such contexts, a memory-optimized instance is typically the recommended option. Additionally, when integrating with IAM, it is best practice to assign roles to EC2 instances rather than embedding sensitive database credentials directly in application code. Using IAM roles ensures secure access to RDS without the risks associated with hardcoding credentials. Consider this scenario: Fintech companies deploying web applications on EC2 need to access an RDS database securely without hardcoding credentials. Which approach meets this requirement? The best practice is to store RDS credentials in AWS Secrets Manager and assign an appropriate role to retrieve them. This setup supports automatic password rotation while eliminating hardcoded credentials. Monitoring and Performance Insights Monitoring is essential for managing any transactional database. Amazon RDS integrates with CloudWatch to track metrics such as: Number of active database connections IO operations (which can reveal disk throughput bottlenecks) Freeable memory (a key performance indicator) Enhanced monitoring provides real-time OS-level metrics—covering CPU usage, memory consumption, and load averages—and includes a process list. For deeper analysis of slow queries or SQL performance, leverage RDS Performance Insights to visualize load distribution by SQL statements, hosts, or users. Consider this question: Fintech is using RDS for its primary database and is experiencing performance issues. They are considering enhanced monitoring to gain detailed OS-level insights. What does enhanced monitoring provide over standard CloudWatch metrics? Enhanced monitoring delivers real-time access to operating system-level metrics for the RDS instance, offering granularity that standard CloudWatch metrics lack. Below is an example SQL code snippet relevant to performance tuning and troubleshooting. Note that similar blocks have been consolidated for clarity: WITH cte AS (
  SELECT id FROM authors LIMIT ?
)
UPDATE authors s
SET email = ?
FROM cte
WHERE s.id = cte.id;

SELECT count(*) 
FROM authors 
WHERE id < (SELECT max(id) - ? FROM authors)
  AND id > (SELECT max(id) - ? FROM authors);

DELETE FROM authors
WHERE id < (SELECT max(id) - ? FROM authors)
  AND id > (SELECT max(id) - ? FROM authors);

-- Optional union example for aggregated queries
SELECT count(*) 
FROM authors 
WHERE id < (SELECT max(id) - ? FROM authors)
  AND id > (SELECT max(id) - ? FROM authors)
UNION 
SELECT ...;

DELETE FROM authors
WHERE id < (SELECT max(id) - ? FROM authors)
  AND id > (SELECT max(id) - ? FROM authors)
UNION 
SELECT ...; Accessing and Analyzing RDS Logs Amazon RDS provides convenient access to various database logs directly through the AWS Management Console. You can view and download logs for immediate analysis or export them to an S3 bucket for long-term storage and analysis. For most use cases, accessing logs via the RDS console is the fastest approach. RDS Event Notifications Event notifications play a vital role in security detection and operational awareness. RDS can send real-time notifications for critical events (for example, when a DB instance stops) by using event subscriptions that deliver messages to an SNS topic. This method is more efficient than polling APIs or solely relying on CloudWatch alarms. Below is an example of an RDS event notification payload: {
  ""metaData"": {
    ""version"": ""2014-07-01"",
    ""sent"": ""2021-08-10T05:12:52.000+0000"",
    ""source"": {
      ""accountId"": """",
      ""region"": ""us-east-1"",
      ""source"": ""aws.rds"",
      ""sourceIdentifier"": ""dbtest1"",
      ""sourceArn"": ""arn:aws:rds:us-east:dbtest1"",
      ""sourceType"": ""DB_INSTANCE"",
      ""eventID"": ""RDS-EVENT-0087"",
      ""time"": ""2021-08-10T05:12:52Z""
    },
    ""tags"": {
      ""name"": ""dbtest1""
    }
  },
  ""keyData"": {
    ""programName"": ""RDS"",
    ""eventName"": ""RDS Event Notification - [notification]""
  },
  ""baseData"": {
    ""occurred"": ""2021-08-10T05:12:00.000+0000"",
    ""State"": ""Alarm"",
    ""summary"": ""RDS DB Instance Event - DB identifier: dbtest1 EventID: RDS-EVENT-0087"",
    ""message"": ""Message: DB instance stopped""
  }
} For example, if a core banking application requires real-time notifications for significant RDS events, the ideal solution is to configure RDS event subscriptions that forward these events to an SNS topic. RDS Recommendations and Trusted Advisor AWS Trusted Advisor and RDS Recommendations offer valuable insights into optimization in performance, security, and cost. While Trusted Advisor may highlight underutilized instances or configurations that need attention, it does not directly change instance configurations. For example, a healthcare company using RDS for storing patient records may receive cost-optimization recommendations from Trusted Advisor, although no direct modifications to the database instances are made. Encryption: Data at Rest and in Transit Each Amazon RDS database engine supports encryption of data at rest. You can enable disk encryption with AWS Key Management Service (KMS) during instance creation. Certain engines, such as Oracle and MSSQL, also offer Transparent Data Encryption (TDE). In addition to encryption at rest, it is essential to secure data in transit. Enabling SSL/TLS encryption ensures that all communications between your client and the RDS instance are secure. Each database engine manages transit encryption through specific SSL parameters and certificates. For a fintech startup with stringent security requirements for sensitive financial data, the recommended strategy is to enable encryption at rest (using KMS or TDE where applicable) and enforce SSL/TLS for all client connections to secure data in transit. This lesson has covered several key aspects of securing database services on AWS—from authentication and IAM integration to monitoring, event notifications, and encryption. By understanding and applying these best practices, you can ensure that your database services are both high-performing and secure. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Compute Services Part 4,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Compute-Services-Part-4,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Compute Services Part 4 In this lesson, we continue our exploration of container security on AWS by focusing on the Elastic Kubernetes Service (EKS). In this article, we discuss secure access and management of EKS, covering key mechanisms for identity, access, and network security within your cluster. Access and IAM Integration in EKS Establishing a trust relationship between your Kubernetes cluster and AWS IAM is a critical challenge when accessing EKS. Historically, pods acquired permissions via service accounts annotated with roles. Today, emerging pod identity services allow you to assign an IAM role directly to a pod, eliminating the need to manage separate service roles for each containerized application. Although this new approach may eventually appear on certification exams, we focus on the established method for now. For secure interactions with AWS, the entire cluster must trust IAM. To achieve this, you create a trust policy between the cluster and AWS IAM and assign specific permissions to the cluster (often with administrative privileges for infrastructure tasks) as well as to individual pods (with the minimum required privileges). When a pod needs to access AWS services (for example, S3), it presents a JSON Web Token (JWT) along with its role information. The Secure Token Service (STS), an integral component of IAM, validates the pod’s credentials before issuing temporary access keys. Consider the flow below that illustrates how a pod uses its JWT and role information to acquire temporary credentials from STS: Once the cluster establishes trust with IAM, the pod (using its service account and JWT token) sends its credentials to STS. After validation against IAM policies, temporary credentials are issued to grant access to the required AWS resource. OIDC Integration with IAM Global retail companies deploying applications on EKS can enhance security and simplify role management by integrating an OpenID Connect (OIDC) identity provider with AWS IAM. By registering your Kubernetes cluster as an OIDC identity provider in IAM, you can associate specific IAM roles with service accounts, which in turn are attached to pods and other Kubernetes resources. The typical process involves: Enabling OIDC for the EKS cluster. Providing the cluster’s URL during IAM OIDC provider creation, which allows automatic discovery of necessary details. Other alternatives, such as AWS Single Sign-On (SSO) or directly associating IAM roles with nodes, may not be as secure. While AWS is moving toward direct pod-level IAM associations with the Pod Identity Service, the current best practice is to use Kubernetes service accounts to map roles. IAM Integration and Role Assignments in EKS Proper IAM integration involves several distinct role assignments that work collectively to secure your EKS environment. Below is a summary of the key roles and their responsibilities: Role Type Description Example Use Case Cluster IAM Roles Grant the control plane permissions for managing AWS resources like EC2 instances and ELBs. Enabling the control plane to manage auto-scaling and network tasks. Node IAM Roles Allow worker nodes (typically EC2 instances) to register with the control plane and access services like ECR and CloudWatch. Pulling container images and logging activities. Pod Execution Roles Enable individual pods to assume IAM roles independently from the worker node’s role. Providing limited AWS access for specific containerized applications. IAM Connector Role Bridge Kubernetes RBAC and AWS IAM to allow management of the cluster through the AWS Management Console. Integrated visual management from the AWS Console. Cluster IAM Roles Before creating an EKS cluster, assign an IAM role that includes the Amazon EKS cluster policy. This role enables the control plane (e.g., cloud controllers) to interact with AWS resources such as EC2 instances and Elastic Load Balancers. For instance, if new EC2 instances are launched to host additional containers, the cluster role must have the appropriate permissions to manage these resources. Node IAM Roles Worker nodes (typically EC2 instances in your EKS cluster) require an IAM role for registration with the control plane as well as to access other AWS services—such as ECR for pulling container images and CloudWatch for logging. These roles should be precisely scoped to allow only the required permissions. Pod Execution Roles The pod execution role allows individual pods to assume an IAM role and access AWS services independently of the worker node’s role. Even though advances such as the Pod Identity Service are changing the landscape, understanding pod execution roles remains important. IAM Connector Role The IAM connector role integrates the Kubernetes RBAC system with AWS IAM, allowing the Kubernetes cluster to be managed and monitored via the AWS Management Console. This role is essential for achieving seamless visual management. Note These distinct roles—cluster, node, pod execution, and connector—work together to ensure your Kubernetes workloads have necessary permissions while maintaining robust security boundaries. Node Types and Security Responsibilities When deploying EKS, you must choose the appropriate type of worker nodes depending on your administrative preferences and security requirements: Self-Managed Nodes: You have full control over compliance, patching, and security updates, but this control increases administrative overhead. Managed Node Groups: AWS automatically applies operating system security patches and supports features like auto-scaling, reducing management effort. Fargate Pods: AWS manages the underlying operating system entirely, further reducing your security responsibilities. For most deployments—especially those focused on reducing management overhead while maintaining security—managed node groups offer an attractive option. Consider a scenario where a healthcare company opts for managed node groups to benefit from automated security patching and compliance alignment—a key advantage in highly regulated environments. Conversely, self-managed nodes offer the flexibility needed for applications that require custom compliance configurations, albeit at the cost of increased management. Logging, Monitoring, and Data Protection Logging and Auditing Regulatory compliance and security best practices require detailed logging of all control plane activities. For EKS environments, consider the following steps: Enable logging for the API server, controller manager, and scheduler. Configure CloudWatch Logs to capture these events, simplifying auditing and monitoring. Data Protection To secure data both in transit and at rest: Integrate with AWS Key Management Service (KMS) to encrypt Kubernetes secrets. Configure your storage class with the encrypted parameter set to true for EBS volumes. Use encrypted EFS configurations for secure shared file storage. Warning Without KMS integration, Kubernetes secrets are simply base64 encoded and do not provide true encryption—ensuring proper encryption is critical for protecting sensitive data. Persistent Storage with CSI Drivers For containerized storage: EBS Volumes: Use the EBS CSI driver with your Storage Class defined to enable encryption by default when persistent volumes are dynamically provisioned. EFS: Ensure storage classes or PersistentVolume definitions are configured to enforce data encryption. FSx for Lustre or OpenZFS: Although many FSx solutions are encrypted by default, review the CSI driver configuration to confirm proper encryption parameters. Multi-Tenancy in EKS Kubernetes is not inherently multi-tenant because its control plane is shared by all workloads. However, you can simulate a multi-tenant environment by: Separating workloads using Kubernetes namespaces. Implementing role-based access control (RBAC) to isolate tenant resources. Applying resource quotas and limit ranges to manage resource consumption effectively. For enhanced security in multi-tenancy, it is recommended to use a single cluster with logically isolated namespaces combined with strict RBAC controls rather than operating multiple clusters or segregating by instance type. Network Security and Policies By default, Kubernetes allows all pod-to-pod communication within the cluster. To enforce restricted network boundaries, consider the following best practices: Leverage the Amazon VPC Container Network Interface (CNI) plugin that assigns each pod its own IP address. Implement Kubernetes Network Policies to limit both inbound and outbound communications between pods. For example, if fine-grained pod-level network restrictions are required, use the VPC CNI integration which allows security groups to be associated with pod ENIs: When designing your network controls: Avoid assigning individual security groups directly to each pod (this is not yet the norm). Instead, group pods by namespace and combine Kubernetes Network Policies with VPC-level security groups. In-Transit Data Encryption with Service Mesh To secure data in transit between services, many organizations deploy a service mesh. AWS App Mesh is one solution that implements mutual TLS (mTLS) for secure service-to-service communication within EKS. Sidecar proxies, such as Envoy, handle encryption seamlessly and protect all intra-cluster traffic without requiring significant changes to your application configurations. When configuring a service mesh: Enable AWS App Mesh and set up mTLS across all services. Avoid deploying without TLS support; relying solely on EKS-level encryption may not offer comprehensive protection. For applications with high throughput and low latency requirements: Use an Application Load Balancer (ALB) with the ALB Ingress Controller for standard HTTP/HTTPS traffic with intelligent routing. For ultra-high throughput and minimal overhead, consider a Network Load Balancer (NLB), though ALB is generally recommended for its advanced routing capabilities. For example, a global retail company using ALB can benefit from the automatic provisioning of the ALB Ingress Controller, which directs traffic efficiently to the correct pods. CSI Drivers and Storage Encryption When working with Container Storage Interface (CSI) drivers for persistent storage, consider the following best practices: EBS Volumes: Ensure your Storage Class is defined with encryption enabled. The EBS CSI driver will provision encrypted volumes when the encrypted parameter is set to true. EFS: Configure the Storage Class or PersistentVolume definitions to require encryption for shared file systems. FSx for Lustre or OpenZFS: Although many FSx solutions are encrypted by default, verify the CSI driver configuration to ensure encryption settings are correctly specified. Comprehensive Cluster Architecture Review the following comprehensive diagram that combines all of the discussed components—EKS, its control plane, worker nodes, associated AWS services (like load balancers, databases, and storage), and various security and networking configurations. Key aspects include: Deployment across multiple Availability Zones with both public and private subnets. An EKS control plane and nodes with designated IAM roles. Integration with essential components like load balancers, NAT gateways, endpoints, and AWS services such as S3, SES, and CloudTrail. Inclusion of security services like AWS Config, Systems Manager, and Certificate Manager. This integrated architecture demonstrates the layered security and scalability provided by EKS. A multinational corporation evaluating EKS versus self-hosted Kubernetes will note several advantages: Automatic patching and management of the control plane. Native integrations with CloudWatch, IAM, and VPCs for enhanced security and operational simplicity. Conclusion In this lesson, we covered the following key topics: The evolution of pod identity in EKS and the role of Secure Token Service (STS). Best practices for IAM integration for clusters, nodes, pods, and the connector role. Options for managing worker nodes: self-managed nodes, managed node groups, and Fargate pods—and their impact on security responsibilities. Techniques to secure data at rest and in transit using CSI drivers, encryption strategies, and service meshes. Methods to enforce network segmentation and secure multi-tenancy in an EKS environment. Logging, monitoring, and architectural best practices to ensure your EKS cluster meets security and compliance requirements. By understanding these components and their interactions, you can design a secure, scalable, and compliant solution on EKS that not only meets your current needs but also accommodates future enhancements. For more detailed configuration guidance, be sure to consult the Amazon EKS Documentation . Happy securing and deploying! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Management Services Part 1,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Management-Services-Part-1,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Management Services Part 1 Welcome to this comprehensive lesson on Designing for Security. In this article, we delve into security measures for various AWS management and governance services. We focus on the importance of secure provisioning and observability, highlighting best practices and key considerations when working with these highly managed services. Provisioning Services and CloudFormation Security AWS CloudFormation, the leading infrastructure-as-code tool, allows you to create and manage AWS resources based on a template you author. Typically, you craft a CloudFormation template, store it locally or in an S3 bucket, load it into the CloudFormation engine, and then create the associated stack. When deploying infrastructure with CloudFormation, pay close attention to the following security aspects: Secure storage of sensitive information. Assigning appropriate permissions to CloudFormation during execution. For example, if CloudFormation is tasked with creating 15 resources but is granted permissions for only 14, the stack creation will fail. Ensure the IAM roles and policies specified in your template align with the minimum permissions required for each resource. A best practice is to define a tailored IAM role (or a set of roles) that grants only the necessary permissions for each component of your template. Remember, if no specific user or role is provided, CloudFormation operates with the permissions of the initiating user. Also, avoid embedding sensitive credentials directly in your template; instead, utilize services like AWS Secrets Manager or the Systems Manager Parameter Store . Sensitive data should be referenced using secure strings from the Parameter Store or managed via Secrets Manager with automatic rotation. Additionally, CloudFormation logs stack creation events in both its Events tab and AWS CloudTrail, making monitoring and review straightforward. CloudWatch Events and CloudTrail further enhance security by tracking all stack-related changes. Cloud Development Kit (CDK) and Its Security Considerations AWS Cloud Development Kit (CDK) version 2 similarly transforms your code into CloudFormation templates. Security in the CDK framework hinges on the careful definition of roles and permission assignments within your constructs. When designing resources in languages such as TypeScript or Python, ensure that the IAM roles you define provide only the minimum necessary privileges. The CDK offers the flexibility to define resource roles and permissions as part of your application code. This approach not only streamlines infrastructure provisioning but also embeds security best practices from the onset. Furthermore, CDK allows you to associate roles for the deployed resources directly within your deployment script, ensuring that both the provisioning process and the running services adhere to defined permissions. Observability: CloudWatch, X-Ray, and Data Protection CloudWatch for Monitoring and Logging AWS CloudWatch is central to collecting metrics, logs, alarms, events, X-Ray traces, and insights. It is essential for ensuring performance monitoring, health assessment, and timely alarm notifications. Although data is encrypted by default in CloudWatch, exercise caution to avoid unintentional exposure of sensitive information in logs or canaries. Data protection policies within log groups can help mask or filter sensitive elements such as email addresses, social security numbers, or credit card details. Set up data identifiers and designate secure destinations for log data—such as S3, Kinesis Firehose, or CloudWatch Logs—to enforce filtering and masking rules. AWS X-Ray for Tracing AWS X-Ray offers end-to-end tracing capabilities for monitoring user sessions across your AWS environment. With data encryption enabled by default via AWS KMS, secure access to trace data should only be granted to authorized users via well-defined IAM roles. If any trace segments contain sensitive information, configure sampling rules to exclude those segments as necessary. Personal Health Dashboard The AWS Personal Health Dashboard (PHD), accessible via Systems Manager, provides a real-time status board for monitoring AWS service events that could impact your resources. Although the dashboard may display sensitive information, all data is encrypted and securely managed by AWS. Ensure that only users with specific permissions have access to this dashboard. Managed Prometheus and Grafana Amazon Managed Service for Prometheus AWS recently introduced a managed service for Prometheus, which competes with CloudWatch by gathering metrics in a highly managed environment. Standard security measures—including encryption, logging, and IAM-based access—are in place. Access is secured via VPC endpoints, and integrating its API calls with CloudTrail enables comprehensive auditing. CloudTrail logs API calls made by Prometheus to ensure operational transparency and compliance. Amazon Managed Grafana Amazon Managed Grafana offers rich data visualization by integrating with multiple AWS services and data sources. Unlike other AWS services that depend on IAM for authorization, Managed Grafana uses Amazon Identity Center or third-party identity providers such as Okta. This model means that dashboard access is controlled externally from IAM policies, though AWS CloudTrail and CloudWatch still assist in auditing and monitoring access. Managed Grafana supports multiple permission models, including service-managed and customer-managed permissions. For robust, fine-grained access control, customer-managed permissions are recommended. Trusted Advisor, Launch Wizard, and Compute Optimizer AWS Trusted Advisor AWS Trusted Advisor offers proactive recommendations to help optimize cost, performance, fault tolerance, and security. For security-focused guidance, Trusted Advisor highlights issues like overly permissive IAM policies or the absence of multi-factor authentication. While it does not directly resolve these issues, it provides actionable insights that can help close security gaps. AWS Launch Wizard AWS Launch Wizard guides you through the deployment of applications by providing a step-by-step setup for environments such as Kubernetes or SQL Server. When using Launch Wizard, verify that necessary IAM roles are either specified or created during the process, ensuring that deployments are performed securely. AWS Compute Optimizer AWS Compute Optimizer provides recommendations to optimize resource usage—including instance types and sizes—for both cost and performance improvements. Although Compute Optimizer carries minimal inherent security implications, proper IAM permissions are required for cross-account access via Trusted Access mode, which extends its recommendation capabilities. Summary In this lesson, we reviewed key elements to secure your AWS management services: Ensure proper IAM role management and safeguard sensitive data in CloudFormation templates. Use the AWS Cloud Development Kit (CDK) to embed security best practices directly into your code. Leverage observability tools like CloudWatch and X-Ray to monitor performance while enforcing data protection measures. Explore the security models of new services such as Amazon Managed Prometheus and Amazon Managed Grafana. Adopt AWS Trusted Advisor, Launch Wizard, and Compute Optimizer to gain actionable insights and optimize both security and efficiency. Key Takeaway By following these practices, you can design an AWS infrastructure that is secure, compliant, and resilient, ensuring that your cloud environment remains robust against potential threats. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Database Services Part 4,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Database-Services-Part-4,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Database Services Part 4 In this article, we explore various security considerations for database services on AWS. Following our discussion on DATS as an add-on for DynamoDB, we now focus on security configurations for OpenSearch and OpenSearch Serverless. Both services offer similar security mechanisms worth understanding. OpenSearch supports various resource-based policies along with IP-based policies for controlling access. Note that IP-based policies essentially function as identity-based policies by evaluating the user's IP address. In the diagram above, security groups function like a private link to a data node, which processes incoming data. OpenSearch is widely used for indexing and search processes. Here are some key technical aspects of OpenSearch Service's architecture: OpenSearch Service is a managed service supporting OpenSearch and providing Kibana. Clusters are not solely composed of a single node type; a distributed search environment is recommended. The optimal setup includes dedicated master nodes and the Kibana interface. OpenSearch Service handles data persistence internally instead of relying only on external services. For instance, consider a resource-based policy that incorporates an IP condition. Notice the ""Principal"" element in the policy below: {
  ""Version"": ""2012-10-17"",
  ""Statement"": [
    {
      ""Effect"": ""Allow"",
      ""Principal"": {
        ""AWS"": ""*""
      },
      ""Action"": [
        ""es:ESHttp*""
      ],
      ""Condition"": {
        ""IpAddress"": {
          ""aws:SourceIp"": [
            ""192.0.2.0/24""
          ]
        }
      }
    }
  ],
  ""Resource"": ""arn:aws:es:us-west-1:987654321098:domain/test-domain/*""
} This policy restricts access so that only requests originating from the IP range 192.0.2.0/24 are allowed. The policy is intentionally repeated to emphasize its importance. To integrate IAM with OpenSearch for access management, you can use IAM users and groups with tailored policies attached at the resource entry point (the domain endpoint). Additionally, OpenSearch supports fine-grained access control, enabling administrators to specify permissions right down to the index or document level. The fine-grained access control in OpenSearch works alongside security groups to validate client requests within the VPC. The typical access control sequence is as follows: A client sends a request that enters the VPC. The security group authorizes the request to reach the domain. IAM credentials are verified. The access policy confirms the user's permission for the targeted URL or URI. Fine-grained access control applies specific permissions, returning either the full or a partial dataset. When deploying search applications, implementing fine-grained access control is essential to ensure that users have the minimum permissions required to access specific indexes or documents. OpenSearch fine-grained access control also supports multiple authentication methods, including native local user access, Cognito, and IAM. Although Cognito may sometimes be omitted as an option, IAM remains the primary method for secure authentication. Endpoints and private links continue to play critical roles. Only VPC interface endpoints are appropriate for connecting to OpenSearch Service. Remember, gateway endpoints are exclusively used with services like S3 and DynamoDB, while VPC peering and Direct Connect do not attach to specific services. Reviewing the architecture: To secure data as it travels between nodes, enable node-to-node encryption on OpenSearch Service. For data at rest, the service provides AES-256 encryption through the Key Management Service, applicable to components such as data nodes and log storage. A key feature of OpenSearch is its security analytics, which monitors infrastructure and ingests data from various sources to detect anomalous activities in real time. This feature triggers alerts (via Slack, email, SNS, etc.) based on pre-configured security rules. Note that security analytics is supported in OpenSearch 2.5 and later. Enhanced security analytics in OpenSearch can be visualized through dashboards that display security metrics and trends, offering valuable insights via anomaly detection in your logs. The discussion now turns to open source databases and their security on AWS. We will now focus on ElastiCache, AWS’s managed version of Redis or Memcached, optimized for caching and ephemeral data storage. ElastiCache is primarily designed for transient data rather than long-term storage. Its basic architecture for Redis includes a primary node handling write operations and replica nodes for read requests. Furthermore, it supports automatic partitioning across multiple shards when operating in Redis cluster mode. Security for ElastiCache involves the use of IAM for managing access. For example, an EC2 client armed with a designated role can access ElastiCache using temporary security credentials. While IAM policies define permissions for ElastiCache operations, data-level security within Redis is managed separately through the Redis authentication mechanism. When evaluating IAM integration with ElastiCache, it is important to recognize that while IAM controls access to the cache service, the Redis authentication mechanism governs operations within the Redis environment. Network isolation is crucial in AWS. For services like Redis on ElastiCache, deploy them within a VPC and utilize either VPC peering or an interface endpoint (not a gateway endpoint) to achieve private connectivity between services. For encryption, ElastiCache supports both in-transit encryption (leveraging Redis auth) and encryption at rest. These measures ensure that data remains secure both as it moves between nodes and when stored on disk with help from AWS Key Management Service. ElastiCache also offers extensive logging options that enhance security and troubleshooting: Redis Slow Log The Redis slow log captures commands that exceed a specified execution time, which is useful for troubleshooting latency. It logs only the commands that exceed the threshold, rather than providing a full real-time feed. The ElastiCache events log records changes such as modifications to cache clusters and parameter groups. These events can be subscribed to via Amazon SNS and are retained for a set period. Engine logs offer insights into the Redis engine's behavior, logging errors or capacity issues such as evictions. These logs integrate seamlessly with CloudWatch Logs. Additionally, AWS provides real-time monitoring via CloudWatch, and the ElastiCache console supports automated service updates. Users can schedule or trigger engine version upgrades according to recommended apply dates. This self-service update feature ensures clusters consistently run the latest security fixes. In summary, ElastiCache supports encryption for data in transit and at rest, provides comprehensive logging and monitoring capabilities, and offers a straightforward mechanism for applying service updates through the AWS console or API. These features are essential for managing both the security and performance of your cache clusters. This concludes our discussion of the security features for OpenSearch and ElastiCache. In the next section, we will transition to exploring security options for open source databases such as MongoDB and Cassandra, and how AWS enhances security for these platforms. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Database Services Part 3,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Database-Services-Part-3,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Database Services Part 3 In this article, we explore the security aspects of Amazon Redshift and Redshift Serverless. Redshift is a data warehousing service that can operate in two modes—provisioned and serverless. Although performance differences between these modes will be discussed later, this article focuses exclusively on security. The diagram above outlines several critical components including JDBC connections, APIs, live data queries, and machine learning integrations. Data flows into a central warehouse that can be either a provisioned, cluster-based system or a serverless one. The main challenge is determining the best method to secure these architectures. Securing Amazon Redshift Security in Redshift is managed uniformly regardless of the deployment mode. Consider the scenario of a data analytics company deploying Redshift for big data processing. The company needs to determine which cluster configuration best describes the primary node type in Amazon Redshift. The correct answer is the configuration where the cluster is divided into leader nodes and compute nodes. In this setup, the leader node coordinates query execution while the compute nodes handle the heavy processing tasks. Although a single-node deployment is available for development or testing purposes, it is not considered the primary cluster mode. Redshift IAM Authentication Redshift supports IAM authentication, similar to other SQL-based databases. Over time, Redshift has evolved to incorporate several security enhancements. This process maps database users and groups to IAM users and groups. With AWS Single Sign-On (now known as the Identity Center), administrators can set up these mappings to streamline access control. Using IAM authentication allows users to generate temporary database credentials to log into Amazon Redshift. Keep in mind that while IAM authentication supports secure access, it does not eliminate the need to configure individual database users and permissions—it simply maps these permissions to IAM roles for more efficient management. Consider this question regarding IAM authentication in Redshift: The correct answer confirms that IAM authentication enables users to obtain temporary database credentials for login, leveraging role-based access rather than direct management console credentials. Security Groups and Network Access Both Redshift and Redshift Serverless support the use of security groups, which function as stateful firewalls to control inbound and outbound traffic. You can configure these rules to specify which ports are accessible and from which IP addresses or VPCs, supplementing them with Network ACLs (NACLs) depending on your VPC design. A related security question asks about the functionality of security groups: The correct statement is that security groups function as a stateful firewall allowing you to specify both inbound and outbound rules. Contrary to some misconceptions, a single Redshift cluster can be associated with multiple security groups, and by default, all inbound traffic is denied unless explicitly allowed. VPC Endpoints for Redshift VPC interface endpoints powered by AWS PrivateLink are the industry standard for connecting securely to Redshift. These endpoints facilitate a private connection between your VPC and Redshift, keeping all traffic within the AWS network. Review the following question on VPC interface endpoints: Do they allow you to connect your cluster without using a public IP address, ensuring all traffic remains within the AWS network? Yes, they do. The other options, which suggest performance enhancements or support for gateway endpoints, are incorrect; Redshift only supports interface endpoints. Logging, Auditing, and Compliance Redshift offers extensive logging capabilities. Audit logs can be exported directly to an S3 bucket, later analyzed using tools like Amazon Athena or visualized via QuickSight. Moreover, integration with CloudWatch and CloudTrail facilitates monitoring of API calls and system performance. To enable audit logging, simply configure Redshift to use your desired S3 bucket. This feature is built into the Redshift configuration, negating the need for auxiliary services like AWS Lambda or Redshift Spectrum (the latter is used for querying data in S3, not for logging). Another scenario involves a financial institution that must meet regulatory compliance by monitoring and retaining all cluster activity logs: The recommended approach is to enable Redshift audit logging and push the logs directly to CloudWatch Logs where you can convert them into actionable metrics with defined alarm thresholds. For example, you might run a query like this to view sample audit logs: SELECT * FROM ""default"".""redshift_audit_logs"" LIMIT 10; Encryption in Amazon Redshift Amazon Redshift supports encryption for both data at rest and data in transit. You can utilize AWS Key Management Service (KMS) or an external hardware security module such as CloudHSM. Options include using AWS’s default key or a custom key from another account. Data in transit is encrypted via TLS, and integration with third-party tokenization solutions is available—note that tokenization is managed by the third-party service, not natively within Redshift. In summary, secure your Redshift clusters by: Enabling audit logging. Configuring encryption for data at rest using KMS. Employing TLS and tokenization (with a trusted third-party service) for data in transit. Securing NoSQL with Amazon DynamoDB Shifting focus to NoSQL, Amazon DynamoDB is AWS's flagship managed NoSQL database known for its minimal operational overhead. DynamoDB automatically scales capacity based on demand, and it supports point-in-time backups without requiring manual patching or third-party tools for backup and restore. For access control, DynamoDB relies exclusively on IAM for table and row access. Since it does not employ traditional native database users, access is strictly managed through policies that include specific conditions to restrict actions on designated tables. A common security question for DynamoDB is: • How do you ensure that only specific IAM users or roles can access particular tables and actions? The answer is by using IAM policies with condition keys that restrict access to designated DynamoDB tables. VPC Endpoints and Access for DynamoDB Although DynamoDB does not run within a VPC, it supports both gateway and interface endpoints to provide secure, private access without going over the public internet. Simply configure the appropriate VPC endpoint in your route table to set up secure connectivity. Monitoring and Change Tracking in DynamoDB DynamoDB integrates seamlessly with CloudWatch and CloudTrail, and it also offers DynamoDB Streams—a feature enabling near real-time change data capture. This capability is particularly useful for applications such as fraud detection where immediate processing of table changes is critical. The correct choice for real-time processing is enabling DynamoDB Streams. Data Protection and Encryption in DynamoDB DynamoDB automatically encrypts data at rest using AWS KMS and secures data in transit with TLS 1.2. This built-in encryption ensures data protection without necessitating additional configuration. A related question asks which statement about DynamoDB encryption is accurate. The correct answer is that DynamoDB supports comprehensive encryption for both data at rest and data in transit. DynamoDB Accelerator (DAX) DAX is a managed, distributed in-memory cache that boosts DynamoDB read performance by reducing latency. Despite its performance benefits, DAX has unique security requirements. Because DAX requires its own set of permissions, granting full access to DAX clusters might inadvertently widen access beyond what is intended. Always ensure that assigned IAM roles and policies for DAX are as restrictive as necessary. For IAM integration with DAX, the correct approach is to use policies that appropriately grant or deny access to DAX actions and resources. Fine-grained access is managed via IAM for the cluster as a whole rather than through DAX directly. Monitoring DAX is achieved through CloudWatch, which logs metrics and events in a manner similar to DynamoDB. While enabling X-Ray can provide additional request flow insights, CloudWatch remains the primary monitoring tool. Finally, encryption for DAX must be enabled during cluster creation via a provided checkbox; this setting cannot be modified later. Ensure that server-side encryption is enabled at the time of configuration for secure operations. Conclusion This article has covered key security considerations for both Amazon Redshift (provisioned and serverless) and Amazon DynamoDB. For Redshift, best practices include using robust IAM authentication, setting up security groups and VPC endpoints, enabling comprehensive audit logging, and implementing encryption for data at rest and in transit. In the realm of DynamoDB, emphasis is placed on ease of management, the use of IAM for strict access control, configuring VPC endpoints for secure communication, leveraging DynamoDB Streams for near real-time data processing, and relying on built-in encryption capabilities. Additionally, DynamoDB Accelerator (DAX) offers a caching layer that boosts read performance while incorporating its own security measures. Key Takeaways Use IAM to manage access for Redshift and DynamoDB. Configure security groups and VPC endpoints to limit network exposure. Enable audit logging and encryption features to ensure compliance and data protection. By following these guidelines and best practices, you can securely deploy and manage your AWS database services while ensuring robust protection for your data. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on MigrationTransfer Services,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-MigrationTransfer-Services,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on MigrationTransfer Services Welcome Solutions Architects! In this article, we dive into the security measures for AWS migration and transfer services. Although these managed services require less extensive configuration compared to DIY solutions, proper security practices remain essential. This article provides an overview of key services including Migration Hub, Application Discovery, Application Migration Service, Database Migration Service (DMS), Elastic Disaster Recovery, Mainframe Modernization, Data Transfer Services, and the Snow Family, with a particular focus on ensuring robust security controls. Migration Hub and Application Discovery Migration Hub centralizes migration data from various tools and displays workflows from services such as application migration, Systems Manager, and database migration service. Its main concern is Identity and Access Management (IAM), so ensuring proper access controls is crucial due to the sensitive nature of migration data. Security Best Practice Only grant Migration Hub access to users who truly need it. While Migration Hub aggregates workflows, the underlying services are already highly secure. Although Migration Hub displays workflows from different services, it provides minimal added security beyond what the source services already implement. For logging and monitoring, AWS CloudTrail tracks API calls, while CloudWatch is used sparingly for output display. Application Discovery Service The Application Discovery Service (ADS) collects metadata from your on-premises servers via agents, agentless methods, or vCenter outputs, and transfers the data securely to AWS. This service is focused on discovery rather than migration. Agents installed on your servers gather details about running applications and send them securely for analysis. For large enterprises, understanding the functionality of these agents is essential. ADS manages access through IAM roles and policies, and logging via CloudTrail ensures that all API calls are recorded. IAM integration within ADS enforces strict roles and policies. Enhanced logging using CloudTrail (and optionally CloudWatch) provides a comprehensive audit trail. Application Migration and Database Migration Service (DMS) Application Migration Service (AMS) The Application Migration Service (AMS) replicates and transitions your entire environment to AWS. Replace the retired Server Migration Service with Elastic Disaster Recovery. AMS installs an agent on your source environment, replicates the setup to AWS, and then conducts a cutover. It utilizes IAM for permission management and integrates with CloudWatch and CloudTrail for real-time logging. AMS supports both lift-and-shift and replatforming scenarios (for databases or containers) while logging all actions through CloudWatch logs in real time. Database Migration Service (DMS) DMS is designed for migrating large, sensitive database workloads. The security profile of the migration depends largely on the target destination. For example, using an encrypted Aurora cluster or an S3 bucket with server-side encryption ensures that the data remains secure. Key DMS features include: • Support for full copy loads and continuous data replication via change data capture (CDC) • Comprehensive logging, capturing all DML commands and operations, with time travel logs to review historical changes Consider the SQL command below, which reveals the structure of a migrated table: MySQL [dmstarget]> SHOW CREATE TABLE product;
+---------+-------------------------------------------------+
| Table   | Create Table                                    |
+---------+-------------------------------------------------+
| product | CREATE TABLE `product` (                        |
|         |   `product_id` bigint NOT NULL,                 |
|         |   `product_code` varchar(20) CHARACTER SET utf16 COLLATE utf16_general_ci NOT NULL, |
|         |   `product_name` varchar(50) CHARACTER SET utf16 COLLATE utf16_general_ci DEFAULT NULL, |
|         |   `product_price` decimal(28,6) DEFAULT NULL,   |
|         |   PRIMARY KEY (`product_id`, `product_code`)    |
|         | ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci |
+---------+-------------------------------------------------+
1 row in set (0.00 sec) Troubleshooting Tip Enabling CDC is vital for synchronization between source and target databases. Detailed logs assist in resolving replication issues. DMS supports encryption at rest, SSL/TLS for in-transit security, and full encryption of replication streams. Multiple log types (task logs and time travel logs) provide robust oversight during migration. Furthermore, DMS ensures continuous replication of incremental data changes while maintaining robust encryption and detailed change logs. Elastic Disaster Recovery Service The Elastic Disaster Recovery Service replicates entire suites of virtual machines using block-level replication. It transfers data securely from a source to a target, inheriting encryption settings from the source instance. Network security is maintained by applying security groups during the transfer. Replication agents extract data from disks, stage it on replication servers, and transfer it at a low level to recovery instances, preserving the configuration and security settings. Encryption settings on the source are automatically applied to target EBS volumes. Network access continues to be enforced with security groups during the replication process. Mainframe Modernization Mainframe Modernization involves analyzing, refactoring, and replatforming legacy mainframe processes into an automated DevOps pipeline. Security best practices include implementing strong access policies, ensuring encryption at rest and in transit, and following best practices from the underlying services used during modernization. Additional measures, such as enforcing perfect forward secrecy through TLS protocols with appropriate cipher suites, may be required in certain environments. Data Transfer Services AWS DataSync AWS DataSync uses an on-premises agent to transfer storage data into AWS storage services such as S3, EFS, or FSx. As a serverless and managed service, DataSync leverages TLS to secure data in transit. However, when transferring data to an unencrypted S3 bucket, enable server-side encryption to maintain data security. When transferring files from on-premises to EFS, DataSync can directly copy the data and logs can be monitored via CloudWatch. AWS Transfer Family The AWS Transfer Family provides secure file transfers using protocols such as SFTP and FTPS, while excluding FTP due to its inherent security weaknesses. Custom security policies can be configured to meet specific organizational requirements. Exercise caution when setting cryptographic algorithm options; an overly strict policy might conflict with other security requirements. For legacy protocols like AS2, the AWS Transfer Family accepts the inbound HTTP payload, stores it in an S3 bucket, and creates an audit trail via CloudWatch. Snow Family (Storage Perspective) The Snow Family consists of rugged hardware devices designed for secure, local data transportation in harsh environments. The Snowball Edge, a key device in this family, utilizes AWS IAM for user access management. Data stored on these devices is automatically encrypted; in-transit encryption is also enforced. For large-scale data transfers (for example, transferring 60 terabytes), the Snowball Edge is a suitable solution. With support for up to 72 terabytes of usable transfer data, it emphasizes encryption and network controls alongside varying capacity and processing capabilities. Summary This article highlighted the key security considerations across AWS migration and transfer services. Below is a summary of the essential security themes: Security Theme Description IAM Controls & Access Policies Strictly manage who can access these services with granular IAM roles. Encryption (At Rest & In Transit) Ensure continuous encryption for data stored or transmitted. Comprehensive Logging Use AWS CloudTrail and CloudWatch for complete audit trails and issue diagnosis. Managed Services Leverage AWS managed services that simplify security configurations. In summary, migration and transfer services in AWS—from Migration Hub and Application Discovery to AMS, DMS, Elastic Disaster Recovery, Mainframe Modernization, DataSync, Transfer Family, and the Snow Family—are designed with strong security features. Nevertheless, maintaining high awareness regarding configuration, encryption, logging, and access control is paramount to safeguard your data during migration. Thank you for reading, and we look forward to seeing you in the next study session. Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Data Services Part 1,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Data-Services-Part-1,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Data Services Part 1 Welcome to this detailed lesson on designing secure data and machine learning services. In this article, we explore how security considerations evolve as services become more abstracted from the underlying infrastructure. Our focus will shift toward IAM permissions, access control, and monitoring with tools like CloudWatch. While some services, like SageMaker, still maintain many of the traditional configuration options found in EC2 or container-based systems, the primary discussion here centers on managing data inputs and outputs securely. The Power of Data Ingestion with Amazon Kinesis Amazon Kinesis is a managed streaming service similar to Apache Kafka. It operates as a secure streaming bus where records are encrypted by default. When data is ingested, it is temporarily stored in Kinesis stream storage before being moved to an alternative storage solution. Kinesis supports both server-side encryption to protect data at rest and SSL/TLS to secure data in transit, ensuring robust data protection. Kinesis offers three service types—data streams, video streams, and Firehose. All leverage CloudTrail and CloudWatch for monitoring while supporting various producers and consumers via SDKs. Note that the Kinesis agent is a standalone software component designed to send data to Kinesis Data Streams and Firehose (it does not support video streams). For instance, a global e-commerce platform might use a Java-based Kinesis agent to stream real-time clickstream data from web servers. The agent aggregates and forwards this data to either Kinesis Data Streams or Firehose. It is essential to understand that the agent is simply a software forwarder, not a piece of hardware or a standalone cloud service. Managed Service for Kafka (MSK) Managed Service for Kafka (MSK) on AWS delivers a similar streaming solution focused on open-source Kafka. Unlike Kinesis with its shard-based model, MSK uses a more complex architecture involving Kafka brokers and ZooKeeper nodes for cluster management. AWS manages much of the heavy lifting—provisioning, configuration, and scaling—allowing you to focus on specifying the appropriate cluster size. MSK enforces encryption at rest and in transit by default. Data at rest is secured using AWS KMS, while TLS encryption safeguards data as it moves between Kafka brokers and clients. When TLS is enabled for your MSK cluster, encrypted connections automatically extend to both brokers and ZooKeeper nodes, ensuring secure communication throughout the cluster. Broker traffic logs can be exported to Amazon S3 for in-depth analysis by services such as AWS Glue, Athena, or Redshift. This capability distinguishes MSK from Kinesis, where detailed broker logging is not available. Data Extraction, Transformation, and Load with AWS Glue AWS Glue is a robust service for ETL (Extract, Transform, Load) operations powered by PySpark. Glue comprises multiple components: A data crawler that scans various sources (like S3) to populate a data catalog. An ETL job engine to execute data transformation tasks. Glue Studio, which offers a graphical interface for creating and managing ETL jobs (with security managed by AWS). Security Best Practice When securing AWS Glue, it is advisable to use VPC endpoints for secure, private connectivity, restrict access with specific IAM roles and policies, and avoid storing sensitive data in plaintext within scripts. Glue job bookmark data is encrypted using the default AWS KMS key, ensuring that job progress tracking remains confidential. Data Transformation with Glue DataBrew Glue DataBrew allows users to visually transform and prepare data without the need to write code. Its intuitive drag-and-drop interface simplifies data cleaning, normalization, and preparation for analysis. DataBrew runs in a highly managed environment with security enforced by core AWS services, meaning encryption and logging rely on the settings of services such as S3 and VPC endpoints. DataBrew integrates with both gateway endpoints for S3 and VPC interface endpoints for Glue, enhancing network isolation and security. It also supports encryption for both data in transit and at rest. When addressing questions about Glue DataBrew’s encryption, the correct explanation is that it leverages AWS KMS for data at rest and adheres to standard logging protocols. Designing for Security in Data Storage with Lake Formation Lake Formation provides a robust security and governance layer over AWS data lakes. It integrates closely with IAM (and AWS SSO/Identity Center) to manage access control and permissions. Lake Formation supports fine-grained, cell- and row-level security while leveraging logging tools such as CloudWatch, CloudTrail, and AWS Config to track data access and compliance. In addition, Lake Formation supports data classification and tag-based access control, which enforce security policies across databases and tables. Data Presentation and Query with Amazon Athena Amazon Athena is a serverless SQL engine perfect for ad hoc analysis on data stored in locations such as S3, Redshift, or data cataloged by Glue and Lake Formation. Rather than storing data, Athena accesses it externally, leveraging pre-existing encryption settings (for example, server-side encryption with KMS or S3-managed keys). Athena relies on IAM for access control and integrates with CloudWatch and CloudTrail for monitoring and logging activity. Athena supports querying data encrypted with AWS-managed keys, whether using S3’s SSE-S3 or AWS KMS. It’s important to remember that once data is loaded into memory for query execution, its security configuration is dictated by the source. Elastic MapReduce (EMR) and Its Security Considerations Amazon EMR is a powerful tool for data processing and batch analysis that runs on clusters of EC2 instances. An EMR cluster typically consists of master nodes (for coordination), core nodes (for storage and processing), and task nodes (for additional, ephemeral compute capacity). Although a serverless option is now available, most exam-relevant deployments involve traditional server-based clusters. For security, EMR clusters are deployed within a VPC, combining security groups with IAM permissions to enforce access control. Although EMR passively handles operating system patching by replacing old instances with new, patched ones, managing your own servers requires using tools like AWS Systems Manager for continuous patch management. EMR also integrates seamlessly with CloudWatch, CloudTrail, and custom logging solutions to provide comprehensive operational insights. In terms of data encryption, EMR offers HDFS transparent encryption (managed via KMS) and disk-level encryption for EBS volumes. These features, combined with robust logging via CloudWatch Logs and CloudTrail, ensure end-to-end security and monitoring of your data processing environment. This comprehensive overview has covered the security designs integrated into various AWS data services. By understanding encryption, access controls, and monitoring mechanisms—from Kinesis to EMR—you are better equipped to architect secure data pipelines and processing frameworks on AWS. For more information on best practices and service-specific guidance, be sure to explore the official documentation and related resources. Happy architecting! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Data Services Part 2,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Data-Services-Part-2,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Data Services Part 2 In this lesson, we review the security aspects of several AWS data and machine learning services. We cover visualization tools, data warehousing solutions, and inference services, along with natural language processing, transcription, and translation. Each section describes the security features of the service, the encryption methods used, and complements the discussion with relevant diagrams. Amazon QuickSight Amazon QuickSight is a visualization tool that lets you build dashboards by pulling data from sources such as Lake Formation, Athena, and the AWS Glue Data Catalog. As a fully managed service, QuickSight natively encrypts data both at rest and in transit. This encryption covers stored data, temporary storage within its SPICE engine, and all data movement. In addition, QuickSight integrates with CloudWatch for monitoring execution flows and metrics without requiring extra configuration. Amazon Redshift and Data Warehousing Amazon Redshift is an SQL-based data warehousing solution that offers performance insights with automatic logging. Its tight integration with IAM and encryption mechanisms makes it a secure choice for managing large volumes of data. For more detailed aspects of its security features, refer to the dedicated sections later in this lesson. Amazon SageMaker Amazon SageMaker provides a platform for building, training, and deploying machine learning models. Its security features are designed to protect sensitive data during the entire machine learning lifecycle. Key aspects include: Notebooks and Studio: SageMaker Notebooks (essentially EC2 instances running Jupyter) automatically encrypt data at rest and in transit. Training Jobs: Enable inner-container traffic encryption during training by specifying the appropriate encryption parameter in the API. VPC Access and Storage Encryption: Deploy SageMaker within a VPC to restrict public Internet access, and enforce storage encryption using custom keys. For secure configurations, ensure that: Inner container traffic encryption is set to true. SageMaker is deployed in a VPC to isolate instances from the Internet using security groups and specific VPC endpoints. Monitoring SageMaker is integrated with CloudWatch for logging and metrics, and AWS CloudTrail provides comprehensive auditing. Amazon Rekognition Amazon Rekognition analyzes and recognizes objects within images. This service encrypts input media and associated metadata by default. For additional security, client-side encryption is supported, though the data must be decrypted prior to processing. To maintain secure access, deploy VPC interface endpoints (PrivateLink). Amazon Polly Amazon Polly converts text to lifelike speech. As a managed service, its security largely relies on the backing storage used for audio outputs (usually Amazon S3). When evaluating Polly’s security, it is important to consider the encryption settings and access policies configured on S3, as well as logging through CloudWatch. Amazon Lex Amazon Lex, which powers conversational chatbots, combines automatic speech recognition (ASR) with natural language understanding (NLU) to convert speech to text and analyze intent. It encrypts data by default and uses IAM for fine-grained access control. Lex can also be accessed securely by leveraging VPC interface endpoints. Note When securing Lex interactions, be sure to employ PrivateLink to privatize traffic as needed. Amazon Comprehend Amazon Comprehend is a natural language processing service that extracts key insights such as entities, key phrases, language, sentiment, and topics from text. Fully managed and leveraging pre-trained machine learning models, Comprehend simplifies NLP tasks. Users can enable encryption for both input data and outputs using AWS KMS. Amazon Forecast Amazon Forecast builds time-series forecasting models using historical data. While it remains a newer addition to AWS’s machine learning portfolio, its security practices align with standard AWS services: data at rest is encrypted (with options for user-managed keys) and access is controlled by IAM. Logging support is available via CloudTrail and CloudWatch. Augmented AI (A2I) Amazon Augmented AI (A2I) adds a human review step to machine learning predictions when confidence is low. This helps improve the accuracy of results by integrating human verification via internal teams or AWS Mechanical Turk. Key security considerations include protecting submitted data and review processes with IAM policies and KMS encryption. Amazon Fraud Detector Amazon Fraud Detector identifies anomalous and potentially fraudulent activities within transactions. It integrates with IAM for secure access management and leverages AWS KMS for encryption of data at rest. Additionally, the service supports comprehensive logging and monitoring through CloudTrail and CloudWatch. Transcription and Translation Services Amazon Transcribe Amazon Transcribe securely converts audio and video content into text. It encrypts data automatically in transit and at rest while also supporting customer-managed keys. Its security framework ensures that media files remain protected throughout the transcription process. Amazon Translate Amazon Translate provides language translation services with strong security measures. It encrypts data in transit using TLS and secures information at rest (typically using AES-256). Moreover, Amazon Translate complies with industry standards such as ISO 27001, PCI DSS, and HIPAA. Amazon Textract Amazon Textract extracts text and structured data from scanned documents while ensuring robust security. It leverages server-side encryption for data stored in S3 and relies on IAM for authenticated access and user management. Note Remember that ensuring the security of outputs from Textract also depends on the configurations of connected services such as the S3 buckets storing the document data. Summary This lesson covered the security features of an array of AWS data and machine learning services—from data ingestion and visualization to inference and transcription. Key themes include: Default encryption both at rest and in transit. Use of AWS KMS for customer-managed encryption. Robust access controls using AWS IAM. Monitoring and logging through AWS CloudWatch and CloudTrail. While most of these services are designed to be highly secure by default, it remains essential for architects to apply additional settings—like inner container encryption in SageMaker or restricted access via VPC endpoints—to align with specific security requirements. Thank you for following this lesson on AWS security considerations. In the next section, we will explore additional security topics and best practices. For further reading, consider these resources: AWS Security Documentation Amazon Web Services – Compliance AWS Best Practices for Security, Identity, & Compliance Watch Video Watch video content Practice Lab Practice lab"
AWS Solutions Architect Associate Certification,Turning up Security on Management Services Part 2,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Management-Services-Part-2,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Management Services Part 2 In this article, we explore designing secure architectures around AWS management services. Building on our previous discussion on Compute Optimizer and Trusted Advisor, we now delve deeper into management tools, focusing on AWS Organizations, IAM, and logging. These strategies help enhance security across multi-account environments while ensuring centralized monitoring and governance. AWS Organizations AWS Organizations allows you to group multiple AWS accounts into organizational units, simplifying policy management and security control enforcement across an enterprise. A crucial feature is Service Control Policies (SCPs). SCPs serve as an account-level permission guard: when an SCP denies access to a service, no account-level configuration can override that decision. This approach enforces consistent IAM policies and logging practices. Centralizing logging is also simplified with AWS Organizations. For example, a centralized CloudTrail in the master account can capture events from all member accounts, ensuring consistency in log retention, formatting, and storage—typically directing all logs to a single S3 bucket. This process is further enhanced by AWS Control Tower, which automates best practices for multi-account governance. CloudTrail and Centralized Logging Centralizing CloudTrail logging across AWS Organizations simplifies the management of global events. With a single trail capturing events from all accounts, compliance and auditing are streamlined, and uniform security monitoring is maintained. Integration with AWS Control Tower further improves the automated enforcement of security standards. For example, a Solutions Architect might recommend configuring CloudTrail in the master account to audit activities throughout the organization. This centralized approach reduces operational overhead and strengthens security controls. AWS Control Tower and IAM Identity Center AWS Control Tower simplifies the setup and governance of multi-account environments by integrating AWS Organizations with AWS Identity Center (formerly SSO). Control Tower automatically configures IAM Identity Center to enable centralized user management, automated account provisioning, and governance. This setup not only activates CloudTrail but also enforces both proactive prevention and detection controls via AWS Config. For organizations requiring secure, centralized management, AWS Control Tower seamlessly integrates IAM Identity Center with supporting services to ensure that all activities are logged and policies consistently enforced. AWS Systems Manager AWS Systems Manager is a comprehensive management suite designed for routine operational tasks such as patching, inventory management, session management, and compliance reporting for both EC2 and on-premises servers. By installing the Systems Manager (SSM) agent, you can perform secure operations at scale. Key Components of Systems Manager Systems Manager Explorer and OpsCenter Use Explorer for gaining operational insights, while OpsCenter facilitates incident management. Together with CloudTrail, CloudWatch, and IAM, these components secure and log operational actions. Systems Manager Diagrams Diagrams illustrate how several subservices, including AWS Config, OpsCenter, and Inventory, work together to provide secure logging and management. Incident Manager Incident Manager enables the creation and management of response plans for operational events and failures, making it ideal for coordinating incident response efforts in complex environments. Application Manager and AppConfig Application Manager provides alarms and operational insights for custom applications and services. AppConfig enables you to securely manage, validate, and deploy application configurations using IAM controls. Parameter Store Parameter Store is a secure repository for environment variables, credentials, and configuration parameters. With native KMS encryption, sensitive data is protected using SecureString. Change Manager and Automation Change Manager allows you to define and automate operational workflows with runbooks, ensuring that changes are consistent and auditable. Its automation processes, secured by IAM and logged by CloudTrail, help streamline repetitive tasks such as patch updates and configuration changes. Maintenance Windows Maintenance Windows, in conjunction with Patch Manager, schedule patching during approved windows. This ensures that updates occur in a controlled and secure manner, grouping instances by tags regardless of the operating system. Fleet Manager and Compliance Fleet Manager provides a centralized console to monitor and troubleshoot a fleet of EC2 or on-premises servers. With Compliance and Inventory features, it reinforces security by tracking configuration and patch adherence. Hybrid Activations Hybrid Activations serve as a registration engine for on-premises servers, allowing them to securely register with AWS Systems Manager using activation codes. Session Manager and Run Command Session Manager enables secure remote access (via SSH or RDP) without public endpoints, while Run Command allows execution of one-off commands across multiple instances. Both integrate with IAM, CloudTrail, and CloudWatch for robust security and logging. Additionally, State Manager ensures that instances maintain their desired configuration state—for example, enforcing that a web server only listens on port 443. Patch Manager and Distributor Patch Manager automates system patching based on predefined baselines and maintenance windows. In parallel, SSM Distributor securely distributes custom software packages across your environment using controlled IAM permissions. SSM Documents SSM Documents are AWS-hosted, encrypted scripts designed to standardize operational tasks. When used with the automation engine and State Manager, they help enforce consistent configurations and manage routine tasks across your environment. AWS Service Catalog and License Manager Service Catalog AWS Service Catalog allows you to offer pre-configured AWS resources packaged as CloudFormation or Terraform templates. You maintain security by managing access through strict IAM policies and roles. License Manager AWS License Manager helps track and manage licenses, including notifications for license overages or expirations while adhering to AWS security best practices to safeguard sensitive information. AWS Proton AWS Proton is designed for automated container and serverless deployments. While Proton abstracts much of the deployment process with limited direct security controls, it integrates CloudTrail for thorough logging. Secure deployments with Proton require ensuring that logs are stored in encrypted S3 buckets and that built-in security controls for environment templates are fully leveraged. A best practice is to utilize Proton’s built-in security features for both environments and templates, including regular vulnerability assessments and automated scans. Additional Management Tools Other management services such as Tag Editor, Resource Explorer, Resource Groups, Resource Access Manager, and Resilience Hub primarily rely on IAM for security. Typically, these services do not offer additional logging mechanisms beyond standard AWS managed security practices. All management and governance tools include encryption in transit and at rest by default, ensuring that sensitive operations remain protected. Conclusion In summary, we reviewed a variety of AWS management services and examined how security is integrated across each. From AWS Organizations with SCPs and centralized CloudTrail logging to AWS Control Tower’s automated setup and the comprehensive feature set of AWS Systems Manager, AWS offers robust security and governance mechanisms primarily anchored by IAM, CloudTrail, and native encryption. Next Steps Stay tuned for our next article, where we continue to explore additional aspects of security and operational excellence. For more detailed guidance, refer to the AWS Documentation . Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Security Services Part 2,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Security-Services-Part-2,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Security Services Part 2 In this article, we explore a range of AWS security services that safeguard your data—whether at rest or in transit. We explain how these services integrate to simplify certificate management, secure key infrastructures, and offer centralized threat protection. Certificate Management and AWS Private Certificate Authority When protecting stored data, services like AWS Key Management Service (KMS) and AWS CloudHSM are commonly used. For data in transit, particularly with TLS/SSL certificates, AWS Certificate Manager (ACM) is the preferred choice. ACM not only manages public-private certificates but also supports importing certificates from external vendors. For certificates created within ACM, automatic renewal further simplifies the certificate lifecycle – a process similar to certificate management in Kubernetes. For organizations hosting websites where secure communications are critical, AWS Certificate Manager automatically provisions and renews certificates: In addition, AWS Private Certificate Authority (PCA) is a key component of ACM. AWS Private CA allows you to establish your own certificate hierarchies without having to build an on-premise public key infrastructure. It issues private certificates for various organizational units while eliminating the need to develop and maintain internal key infrastructures: This feature is particularly beneficial if you plan to host your own Private Certificate Authority, enabling seamless creation and management of certificate hierarchies: Secrets Management To securely manage credentials, API keys, and tokens, AWS Secrets Manager is the recommended service. It encrypts sensitive data and enforces access through appropriate IAM permissions. A standout feature of Secrets Manager is its automated credential rotation which is not available in the Systems Manager Parameter Store (which does offer secure strings but lacks auto-rotation). Tip If automated credential rotation is a priority for your application, AWS Secrets Manager should be your go-to solution. Network-Level Security: Security Groups and NACLs Within the AWS network, security groups and network access control lists (NACLs) act as built-in firewalls. Security groups provide stateful security at the network interface level by filtering both inbound and outbound traffic: For stateful security controls, remember that security groups are purpose-built for this task: In contrast, NACLs act at the subnet level as stateless firewalls, using ordered, rule-based filtering to offer an extra layer of security for your VPC: Centralizing Security Data with AWS Security Lake AWS Security Lake serves as a centralized repository for collecting, normalizing, and analyzing security data from multiple AWS services. By aggregating information from services like IAM Access Analyzer, GuardDuty, Inspector, AWS Config, and CloudTrail, it enables advanced threat detection and continuous security monitoring. For enterprises that require deep security analysis, consider complementing Security Lake with services like Amazon GuardDuty for a robust defense mechanism: Web Application Firewall and DDoS Protection AWS Web Application Firewall (WAF) protects your applications from common web exploits such as SQL injection and cross-site scripting. It integrates seamlessly with services like API Gateway and CloudFront, safeguarding your applications at the edge. Although WAF rules can be detailed, the AWS certification exam typically emphasizes its basic functionality. For DDoS attack protection, AWS Shield is available in both standard (free) and advanced (paid) versions. Shield Advanced offers enhanced integration with services like AWS Firewall Manager, providing deeper security controls: Security Best Practice Combining Shield Advanced with WAF and using Firewall Manager enables real-time dynamic adjustment of firewall rules to combat sophisticated attacks. Network Firewall and AWS Firewall Manager AWS Network Firewall delivers traditional firewall capabilities at the VPC level. It includes features such as deep packet inspection, web filtering (including SNI filtering), and protocol detection, giving you granular control over inbound and outbound traffic: Completing the security landscape, AWS Firewall Manager provides centralized management for firewall rules across various services such as WAF, security groups, and Network Firewall. This centralized approach is ideal for large enterprises where compliance and uniform security policies are essential: Final Thoughts In summary, we reviewed several AWS security services that are designed with robust security features enabled by default. Understanding how these services integrate—for instance, feeding logs into CloudWatch, recording API calls in CloudTrail, and tracking configuration changes with AWS Config—is essential for building a secure AWS environment. As you progress to topics like reliability, performance, and cost management, remember that many security services are optimized for ease of setup and operation. This foundational knowledge is a key step on your path toward AWS certification success. Take a moment to absorb these concepts—they will underpin your ongoing journey to AWS mastery. Catch you in the next article! Watch Video Watch video content"
AWS Solutions Architect Associate Certification,Turning up Security on Security Services Part 1,https://notes.kodekloud.com/docs/AWS-Solutions-Architect-Associate-Certification/Designing-for-Security/Turning-up-Security-on-Security-Services-Part-1,"AWS Solutions Architect Associate Certification Designing for Security Turning up Security on Security Services Part 1 Welcome back, Solutions Architects! In this lesson, we delve into the final section on security by focusing on secure design principles and enhancing security within AWS services. This guide highlights how to leverage AWS security tools for robust access control, comprehensive auditing, proactive threat detection, and advanced encryption. The Power of Access Control Access control is a cornerstone of cloud security. In AWS, Identity and Access Management (IAM) is your first line of defense. While IAM provides secure defaults and industry standard practices, there are additional measures that can further harden your environment. IAM enforces industry standards such as tracking API calls via AWS CloudTrail and reviewing permissions through IAM policies. To boost security further, consider the following best practices: Enable multi-factor authentication (MFA) to add an extra layer of protection. Apply the principle of least privilege by assigning roles that grant only the essential permissions. A practical example involves using CloudFormation templates to enforce role assignments and require MFA—ensuring that access depends on both something the user knows and something they possess. A common practice is to define specific IAM roles for individual AWS resources—such as EC2, S3, or DynamoDB—so that each service only has the minimal permissions it requires. Tips for Secure Credential Management Avoid embedding sensitive credentials like secret keys directly in CloudFormation templates or in Git repositories. Use AWS Secrets Manager or AWS Systems Manager Parameter Store to securely manage and store sensitive data. Consider the management of database credentials, for example: securely store them using Secrets Manager or Parameter Store. In addition, IAM’s integration with CloudTrail and its Access Analyzer assists in verifying permissions and ensuring they adhere strictly to your security policies. CloudTrail is your primary tool for monitoring API calls and any changes to IAM or CloudFormation resources. For detailed tracking, review CloudTrail event logs regularly. Additionally, AWS IAM Identity Center (formerly Single Sign-On) is a robust option for managing secure access across multiple AWS accounts. Integrated with AWS Organizations, it enforces permission boundaries and verified access consistently. Expanding IAM capabilities into web and mobile applications is achieved with AWS Cognito. Cognito user pools store user profiles and identity pools provide roles and permissions to access services such as API Gateway or DynamoDB. Continuously apply the least privilege approach and enable CloudTrail logging for enhanced auditability. When integrating with external identity providers like Microsoft Active Directory, security fidelity hinges on proper configuration and enforcing trust relationships. Ultimately, the objective remains to enforce strict and secure access policies irrespective of the platform. AWS Verified Permissions introduces a language-based method for enforcing security policies using Cedar. This innovative approach validates that granted permissions do not exceed defined policy templates and offers streamlined policy testing and validation. Auditing Services Auditing is essential for tracking changes and monitoring configurations across your AWS environment. Key services include CloudTrail, CloudWatch, and AWS Config. AWS CloudTrail CloudTrail logs API call activities and is capable of exporting these logs to Amazon S3—a process that should always be paired with encryption via AWS Key Management Service (KMS). Additionally, CloudTrail Lake and Security Lake enhance log analysis and query capabilities. AWS Config AWS Config continuously captures changes within your environment and encrypts data both at rest and in transit by default. Although the control options are limited, you can customize rules and notifications to ensure continual compliance with your security policies. AWS Artifact AWS Artifact provides on-demand access to AWS compliance reports and certification documents, including ISO 27001 and PCI DSS reports. While Artifact does not directly influence security controls, it is vital for auditing and maintaining compliance standards. The Power of Detection Detection tools help identify threats and vulnerabilities before they can be exploited. AWS offers several detection-focused services that provide continuous monitoring and insightful analytics. AWS GuardDuty GuardDuty employs machine learning and threat intelligence to analyze DNS logs, VPC flow logs, and CloudTrail events. It detects anomalies and potential threats, automatically forwarding findings to AWS Security Hub for centralized management. GuardDuty continuously monitors network activity, providing critical insights into suspicious behavior and reinforcing your overall security posture. AWS Inspector AWS Inspector is a vulnerability assessment service that scans EC2 instances, containers, and Lambda functions for known vulnerabilities. It automatically conducts scans and reports findings, making it a valuable service for regular security assessments. Amazon Macie Amazon Macie specializes in detecting and classifying sensitive data within S3 buckets, such as personally identifiable information (PII). As Macie continuously monitors S3 buckets, additional security configurations are typically not required. AWS Security Hub AWS Security Hub centralizes and aggregates security findings from multiple AWS services like GuardDuty, Inspector, and Macie. While the service itself is inherently secure, ensure that IAM access restrictions to the Security Hub dashboard are properly configured. The Power of Encryption Data encryption is vital in protecting information both at rest and in transit. AWS provides robust encryption solutions to safeguard your data. AWS Key Management Service (KMS) and CloudHSM AWS Key Management Service (KMS) and CloudHSM are foundational for data encryption in AWS. KMS is a multi-tenant service that integrates seamlessly with many AWS services, while CloudHSM offers a dedicated single-tenant hardware security module for organizations with stringent security requirements. Both solutions support key rotation and fine-tuned access control via IAM policies. For scenarios like enforcing strict key controls for S3 encryption, KMS is typically the ideal solution. If dedicated hardware separation is required, migrating to CloudHSM is the recommended approach. This lesson provided an in-depth overview of enhancing security across various AWS services—from strengthening access control using IAM and Cognito, auditing environment changes with CloudTrail and AWS Config, detecting potential threats with GuardDuty and Inspector, to securing data via KMS and CloudHSM. Each service offers a range of built-in security features along with advanced options to further harden your AWS environment. Keep these principles in mind as you design and implement secure architectures in AWS. Happy architecting! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Certification Details,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/First-Section/Certification-Details,"Certified Kubernetes Application Developer - CKAD First Section Certification Details Hello and welcome to this comprehensive guide on the Certified Kubernetes Application Developer (CKAD) program. In this article, we explore what the CKAD certification is, why it is valuable, and how you can get started. With Kubernetes adoption rapidly increasing—as depicted in the Google Trends graph below—establishing your credibility in the market has never been more important. The CKAD certification, developed in collaboration between the Cloud Native Computing Foundation and the Linux Foundation, validates your ability to design and build cloud-native applications using Kubernetes. This credential not only distinguishes you from your peers but also enhances your marketability in the rapidly evolving tech landscape. Further Information For more detailed insights about the certification, please visit CNCF's CKAD page . Additional resources are available in the references section below. The exam fee is 300 US dollars and includes one free retake. This policy means that if you do not pass on your initial attempt—which, with adequate preparation, should not be a concern—you are entitled to one additional free attempt within the next 12 months. The CKAD exam is delivered online, enabling you to take it at your convenience from any location. An online proctor oversees your exam session to ensure a secure testing environment. For exam environment details—including room setup, system requirements, and network connectivity—please consult the candidate handbook available on the certification website. Unlike many other certification tests, the CKAD exam is performance-based rather than multiple-choice. This hands-on approach assesses your practical skills by requiring you to complete real-world tasks within a two-hour time frame. Rather than simply memorizing facts, the exam emphasizes understanding Kubernetes operations and effective utilization of its features. During the exam, you are permitted to reference the official Kubernetes documentation. In the following sections of this article, we guide you on how to efficiently navigate these resources to quickly locate the information you need. Good Luck! We wish you the best of luck in preparing for and successfully completing the CKAD certification exam. With thorough practice and a deep understanding of Kubernetes, you’ll be well-equipped to pass the exam with flying colors. Additional Resources and References Kubernetes Basics Kubernetes Documentation Cloud Native Computing Foundation Linux Foundation Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Recap Architecture,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Core-Concepts/Recap-Architecture,"Certified Kubernetes Application Developer - CKAD Core Concepts Recap Architecture Welcome to this lesson. In this article, we review the key components of Kubernetes architecture, focusing on nodes, clusters, master nodes, and essential Kubernetes command-line tools. This guide is ideal for beginners and professionals alike who want to understand the inner workings of Kubernetes. Nodes and Clusters A node is a machine—physical or virtual—on which Kubernetes is installed. Previously known as minions, nodes serve as the worker machines where containers run. Relying on a single node can lead to application downtime in the event of a failure. To mitigate this risk, nodes are grouped into clusters, ensuring high availability and load distribution. Even if one node fails, the application continues to run on other nodes without interruption. The Master Node and Cluster Management Managing a Kubernetes cluster requires robust coordination and monitoring. The master node plays a pivotal role by overseeing the entire cluster. Configured with Kubernetes, the master node stores critical cluster information, monitors node health, and redistributes workloads when necessary. Key Components Deployed with Kubernetes When installing Kubernetes, several core components are automatically deployed: API Server: Serves as the front end, processing commands from users and interfaces. etcd: A distributed key-value store that holds all cluster data and ensures consistency. Kubelet: An agent on every node that ensures containers are running as expected. Container Runtime: Software (such as Docker) that runs the containers. Controllers: Monitor cluster state and take corrective actions, like replacing containers when nodes fail. Scheduler: Distributes container workloads across nodes by assigning new containers to the most suitable node. Distribution of Components In a Kubernetes cluster: The master node hosts the API server, controller manager, scheduler, and the etcd key-value store. The worker nodes run the kubelet agent and the container runtime (like Docker) to host and run containers. The master node continually communicates with worker nodes to monitor their health and manage container deployment, while all operational data is securely stored in etcd. Managing the Cluster with kubectl The Kubernetes command-line tool, kubectl, is essential for deploying and managing applications within the cluster. It allows you to retrieve cluster information, control nodes, and perform various administrative tasks. Here are some fundamental commands: kubectl run hello-minikube
kubectl cluster-info
kubectl get nodes The first command launches an application. The second displays cluster information. The third lists all nodes in the cluster. Next Steps As you advance in your Kubernetes journey, you will explore additional kubectl commands and more complex cluster management tasks. That concludes this lesson on Kubernetes architecture. Stay tuned for upcoming lessons where we delve deeper into advanced commands and concepts. For additional resources, check out the following: Kubernetes Documentation Kubernetes Basics Docker Hub Terraform Registry Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Kubernetes Series of Courses,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/First-Section/Kubernetes-Series-of-Courses,"Certified Kubernetes Application Developer - CKAD First Section Kubernetes Series of Courses Welcome to our comprehensive Kubernetes course series. In this series, you'll gain insights into one of today's most powerful cloud computing technologies—Kubernetes. Supported on all major cloud platforms, Kubernetes enables the management of sophisticated, scalable applications across diverse environments. Mastering Kubernetes requires a solid grasp of containers, application development, YAML configuration, and more. This series covers a wide range of topics including architecture, networking, load balancing, monitoring, auto-scaling, configuration, security, and storage. Because Kubernetes attracts a diverse audience—from beginners to seasoned professionals and application developers—our training is designed to be both comprehensive and targeted. To best serve your needs, we offer a three-course series tailored to different levels and career paths: Kubernetes for Absolute Beginners Certified Kubernetes Administrators (CKA) Certified Kubernetes Application Developer (CKAD) Each course is carefully curated to address specific topics and certification requirements. Rather than overwhelming you with all topics at once, we have segmented the content so you can focus on what matters most for your role and career growth. Course Overviews Kubernetes for Absolute Beginners This course is perfect for individuals new to containers and orchestration. It provides a high-level overview of Kubernetes without diving too deep into technical complexities. Key learning outcomes include: Setting up a simple lab environment for hands-on Kubernetes experimentation Grasping the prerequisites and basic concepts of Kubernetes Deploying applications using fundamental Kubernetes components such as Pods, ReplicaSets, Deployments, and Services Note This course is also ideal for non-technical team members who want to gain a working understanding of Kubernetes to participate in strategic discussions. Kubernetes for Administrators Designed for professionals aiming to manage production-grade Kubernetes clusters, this course delves into advanced topics, such as: Detailed scheduling and monitoring techniques for optimal cluster performance Cluster maintenance, security, and robust storage management practices Effective troubleshooting techniques in production environments This course is particularly beneficial for those preparing for the Certified Kubernetes Administrator (CKA) exam, offering the blend of theoretical insights and practical skills needed to excel as a Kubernetes Administrator. Kubernetes for Developers Targeted at application developers, this course covers how to design, build, and deploy cloud-native applications on Kubernetes. While prior experience with development platforms like Python or Node.js is helpful, the course emphasizes managing applications rather than complex coding challenges: Managing application configurations with ConfigMaps, Secrets, and Service Accounts Working with multi-container Pods Implementing Readiness and Liveness Probes for robust application performance Effective logging, monitoring, and managing Jobs, Services, and networking This course acts as a strong foundation for those aspiring to earn the Certified Kubernetes Application Developer (CKAD) credential. Hands-on Learning Each course is packed with coding exercises and quizzes to reinforce your understanding of developing and deploying applications using Kubernetes. While some topics span across multiple courses, these areas are revisited as necessary to ensure clarity and comprehension. Learning Path Flexibility You do not need to enroll in each course sequentially. For example, administrators can begin with the Kubernetes for Absolute Beginners course, then advance to the Certified Kubernetes Administrators (CKA) course. Developers aiming for the CKAD credential may choose a similar path. If you're ready to elevate your Kubernetes skills, embark on the course that best fits your career objectives and start your learning journey today. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Recap Demo Creating Pods with YAML,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Core-Concepts/Recap-Demo-Creating-Pods-with-YAML,"Certified Kubernetes Application Developer - CKAD Core Concepts Recap Demo Creating Pods with YAML Welcome to this comprehensive guide on creating a Kubernetes pod using a YAML definition file. In this article, you will learn how to write a YAML file from scratch and deploy it into a Kubernetes cluster. You can create YAML files using any text editor, such as Notepad, Notepad++, or Atom, or even an integrated development environment (IDE) like PyCharm. I recommend PyCharm for its excellent YAML support. If you're interested, visit PyCharm by JetBrains to download the Professional or free Community edition. Below is a step-by-step walkthrough to guide you in creating a pod definition file and deploying it on your Kubernetes environment. Creating the Pod Definition File 1. Create a New Project and File Begin by creating a new project called pod . Inside this project, create a file named pod-definition.yaml . Every Kubernetes YAML file typically includes the following root-level properties: apiVersion , kind , metadata , and spec . For a pod, set the apiVersion to v1 and kind to Pod . 2. Define the Metadata In the metadata section, provide essential details such as the pod's name and associated labels for grouping. For instance: apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec: When using PyCharm, you'll notice a tree-view panel that displays the YAML file structure. This visualization confirms that apiVersion , kind , metadata , and spec are at the same root level. Make sure the name and labels entries have identical indentation. Incorrect indentation might mistakenly nest labels under name , leading to errors. 3. Adding Optional Labels Enhance your pod's metadata by adding extra labels, such as cost center or geographical location, to better organize your resources. Here’s an example: apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    costcenter: amer
    location: NA
spec: This flexible labeling approach enables more efficient management of your deployments. 4. Define the Pod Spec Under the spec section, specify the container details. The containers field accepts an array; each container is introduced by a dash ( - ) followed by its configuration, such as name and image . The example below defines a pod with a single container running an Nginx image: apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
    - name: nginx-container
      image: nginx For scenarios with multiple containers, add another item to the list. For example: apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
    - name: nginx-container
      image: nginx
    - name: backend-container
      image: redis Note For this demonstration, we are only using one container. Deploying the Pod After finalizing your pod-definition.yaml , follow these instructions to deploy it to your Kubernetes cluster. 1. Setup Folder Structure on the Kubernetes Master Node Log in to your Kubernetes master node and create a directory structure for your demos: root@kubemaster:/home/osboxes# mkdir demos
root@kubemaster:/home/osboxes# cd demos
root@kubemaster:/home/osboxes/demos# mkdir pod
root@kubemaster:/home/osboxes/demos# cd pod 2. Create the YAML File Paste the contents of your pod-definition.yaml into a new file on the master node. For instance, execute: root@kubemaster:/home/osboxes/demos/pod# cat > pod-definition.yml
apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec:
  containers:
    - name: nginx-container
      image: nginx After creating the file, verify its contents using the cat command. 3. Clean Up Existing Pods Before deploying your new pod, ensure there are no conflicting pods already running. If a pod was previously deployed with a command like kubectl run , delete it by executing: root@kubemaster:/home/osboxes/demos/pod# kubectl get pods
NAME                           READY   STATUS    RESTARTS   AGE
nginx-8586cf59-wf5r4           1/1     Running   0          59m
root@kubemaster:/home/osboxes/demos/pod# kubectl delete deployment nginx Confirm the deletion by listing pods again: root@kubemaster:/home/osboxes/demos/pod# kubectl get pods
No resources found. 4. Create the Pod Now, create the pod with the following command: root@kubemaster:/home/osboxes/demos/pod# kubectl create -f pod-definition.yml
pod ""myapp-pod"" created You can monitor the pod’s creation by running: root@kubemaster:/home/osboxes/demos/pod# kubectl get pods
NAME        READY   STATUS             RESTARTS   AGE
myapp-pod   0/1     ContainerCreating   0          8s After a few moments, check the pod status again: root@kubemaster:/home/osboxes/demos/pod# kubectl get pods
NAME        READY   STATUS      RESTARTS   AGE
myapp-pod   1/1     Running     0          19s This confirms the pod has been successfully created and is now running in your cluster. That concludes our guide on creating a Kubernetes pod using a YAML file. In the next article, we’ll explore additional tips for managing YAML files in PyCharm. Happy coding and deployment! Additional Resources Kubernetes Documentation Kubernetes Basics Docker Hub Terraform Registry Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,A note on Docker Deprecation,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Core-Concepts/A-note-on-Docker-Deprecation,"Certified Kubernetes Application Developer - CKAD Core Concepts A note on Docker Deprecation Every time we mention Docker in this article, we receive questions like: Why discuss Docker if it's deprecated? This article aims to clarify the situation and dispel any confusion surrounding Docker's deprecation in Kubernetes. Previously, Docker was the only supported container runtime for Kubernetes. Docker combined several essential components into one platform, including the Docker CLI, API, image build tools, volume support, security features, the container runtime (runc), and the daemon (containerd). However, to support a variety of runtimes, Kubernetes introduced the Container Runtime Interface (CRI). The key takeaway is that the component containerd is CRI-compatible and operates directly with Kubernetes, independent of Docker’s other components. This means containerd can function as a standalone runtime. With containerd managing container operations, Kubernetes no longer depends on Docker-specific tools such as its CLI, API, or volume management, as these needs are now handled internally by Kubernetes. Clarification Kubernetes' deprecation of Docker as a runtime does not mean Docker is obsolete. It simply indicates that Kubernetes now uses containerd directly through CRI, bypassing Docker-specific integrations. Docker remains the most popular container solution among developers for daily development and build processes. In discussions throughout this article, Docker is often used as an example to illustrate container concepts. This is acceptable because beginners typically start with Docker before transitioning to container orchestration with Kubernetes. For those who use containerd exclusively or do not have Docker installed, tools like nerdctl provide equivalent commands with a Docker-compatible interface. For further reading on container runtimes and Kubernetes integrations, consider exploring these resources: Kubernetes Basics Kubernetes Documentation nerdctl GitHub Repository Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Introduction,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Core-Concepts/Introduction,"Certified Kubernetes Application Developer - CKAD Core Concepts Introduction Before you begin the practice test in this lesson, please review this guided walkthrough of the practice test portal. After watching this lecture, you will be directed to the labs. Click the ""Start"" button to launch the lab. For an optimal experience, make sure you access the practice test on a laptop or desktop, as a physical keyboard is essential for the hands-on tasks. Allow a few moments for the lab environment to load. Although it usually takes less than 30 seconds, loading times may occasionally extend to a few minutes. Once the lab is active, you will notice that the interface is split into two main components: On the left: the quiz portal displaying practice test questions. On the right: a live terminal that lets you run commands and interact with the system. Depending on the focus of your study, the terminal environment may differ: A Linux terminal when learning Linux, shell scripting, or Git. A Docker host when exploring Docker. The Kubernetes control plane when working with Kubernetes. This live terminal empowers you to execute commands and immediately apply your knowledge on assigned tasks. Lab Components The lab environment is divided into two key sections: Terminal Located on the right, this is where you execute commands, view logs, and configure the Kubernetes cluster. Quiz Portal Present on the left, this section displays practice test questions. The total number of questions is visible at the top, and your progress is continuously updated as you complete each question. Questions vary in format. Some require you to search for information in the environment and select the correct answer, while others involve configuration tasks such as deploying Pods or services based on a provided specification and then verifying the setup. For example, you might receive a task to deploy resources using a provided config file, after which your solution will be automatically tested for accuracy. At times, you may be given Kubernetes definition files and instructed to use them for creating configurations. It is recommended to utilize the vi editor, one of the supported editors within the exam environment. Tip Familiarizing yourself with the vi editor before starting can greatly improve your efficiency during the exam. Some scenario-based multiple-choice questions are designed to test your theoretical knowledge and reinforce practical skills. Keep in mind that these questions are not part of the actual certification exam, but they provide valuable hands-on experience. When you receive a specification, be sure to align your configuration precisely with the provided requirements. You can skip a question by clicking the designated button at the top of the portal. However, remember that questions must be answered in sequence, and once skipped, you cannot return to any previous question. If your deployed application features a web interface, you can access it via the web portal link above your terminal. For example: root@controlplane:~# kubectl expose pod nginx --port 80 --type NodePort
service/nginx exposed
root@controlplane:~# If you need an additional terminal window, simply click the corresponding button at the top. Keep in mind that this environment is strictly for practice and does not replicate the actual Kubernetes certification exam interface. The emphasis is on the tasks you perform and the command line interactions. You are welcome to retake these tests as many times as needed to build your confidence. Important The lab environment is temporary—available for one hour (or as specified by the lab instructions) before it resets, and your work will not be saved. If you refresh the lab window, you should be returned to the same environment, but closing your browser or leaving the session idle for too long may terminate your session. In such cases, refresh the page to start a new session from the first question. Do not use or store any personal credentials or persistent work in this environment. Finally, these practice tests are crafted not only to assess the skills covered in the lecture but also to give you practical, hands-on experience. Some topics might extend beyond the lecture content. If you encounter any challenges, consult the relevant documentation for further guidance and answers. This is also an excellent opportunity to become more familiar with navigating documentation and extracting the necessary information. For additional clarification or assistance on any topic, please feel free to reach out. Hints and solutions for every question or task are available in the ""Hints and Solutions"" tab next to your question. Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Recap Pods,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Core-Concepts/Recap-Pods,"Certified Kubernetes Application Developer - CKAD Core Concepts Recap Pods Hello and welcome to this lesson on Kubernetes Pods. This guide assumes that you have already met the following prerequisites: Your application has been developed, containerized as Docker images, and pushed to a Docker repository (e.g., Docker Hub). A Kubernetes cluster is up and running—whether it’s single-node or multi-node, all essential services must be operational. Our objective with Kubernetes is to deploy your application within containers running on a cluster of worker nodes. However, instead of running containers directly, Kubernetes encapsulates them within an object called a pod. A pod is the smallest deployable unit in Kubernetes and represents a single instance of your application. In a simple scenario, a single-node Kubernetes cluster will run one instance of your application inside a Docker container encapsulated within a pod. When your application’s user base grows, you scale up by adding more instances of your application. Instead of launching multiple container instances in the same pod, Kubernetes creates new pods—each running a fresh instance of your application. This design ensures that every instance operates in its own pod. For example: If the current node reaches its capacity due to increased user load, additional pods can be deployed on new nodes that are added to the cluster. In summary, pods maintain a one-to-one correspondence with application containers: to scale up, new pods are created; to scale down, pods are deleted. It is important not to add extra containers to an existing pod solely for scaling. Note While a pod typically contains a single container, Kubernetes also supports multiple containers within a single pod for cases such as adding helper containers that perform supporting tasks. Consider a scenario where you might include a helper container alongside your main application container. Both containers share the same pod, meaning they are created and terminated together. They communicate over localhost and can easily share storage. When a helper container is present, the combined set (application and helper) benefits from shared storage, network namespace, and lifecycle management. Kubernetes Pods vs. Docker Containers To further understand the benefits of Kubernetes pods, consider a similar Docker scenario. Suppose you deploy your application on a Docker host with: docker run python-app This command launches your application and makes it accessible. As user demand increases, you might run the command multiple times: docker run python-app
docker run python-app
docker run python-app
docker run python-app
docker run helper --link app1
docker run helper --link app2
docker run helper --link app3
docker run helper --link app4 In this Docker setup, you must manually manage networking between application and helper containers (using links or custom networks), create shareable volumes, and monitor container states. If an application container fails, you need to manually stop its corresponding helper container, and vice versa. Kubernetes Advantage Kubernetes simplifies this process by automatically managing network connectivity, shared storage, and container lifecycles within pods. You only define the set of containers that comprise a pod, and they are always created and terminated together. Deploying Pods Using kubectl To deploy pods in Kubernetes, you use the kubectl command-line tool. The kubectl run command creates a pod by deploying a Docker container. For example, to deploy an instance of the nginx Docker image, run: kubectl run nginx --image=nginx By default, Kubernetes pulls the specified image (here, nginx) from Docker Hub , a public repository for various Docker images. Kubernetes can also be configured to pull images from a private repository within your organization. After deploying a pod, list the active pods in your cluster using: kubectl get pods The output might look like this: C:\Kubernetes>kubectl get pods
NAME                    READY   STATUS              RESTARTS   AGE
nginx-8586cf59-whssr    0/1     ContainerCreating   0          3s

C:\Kubernetes>kubectl get pods
NAME                    READY   STATUS    RESTARTS   AGE
nginx-8586cf59-whssr    1/1     Running   0          8s Note that although the pod is created, the nginx server is not yet exposed externally—it is only accessible within the node. To expose the service externally, further network and service configurations are required. That concludes this lesson on Kubernetes Pods. Let's move on to the demo. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Docker vs ContainerD,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Core-Concepts/Docker-vs-ContainerD,"Certified Kubernetes Application Developer - CKAD Core Concepts Docker vs ContainerD In this article, we explore the evolution of container runtimes—from Docker to ContainerD—and clarify the roles of key command line tools: ctr, nerdctl, and crictl. This discussion explains why Docker and ContainerD coexisted for a period and what eventually led Kubernetes to change its runtime support. A Brief History of Container Runtimes Initially, Docker emerged as the leading container tool with a user-friendly interface that simplified container management. Its early dominance meant that Kubernetes was built exclusively around Docker container orchestration. However, as Kubernetes grew in popularity, support for additional container runtimes became necessary. Runtimes like Rocket began adhering to the Open Container Initiative (OCI) standards, which define an image specification and a runtime specification for container operations. To enable multiple runtimes, Kubernetes introduced the Container Runtime Interface (CRI). This interface allowed any runtime compliant with OCI standards to integrate with Kubernetes. Because Docker was developed before the advent of CRI, it did not natively support these standards. Kubernetes resolved this by implementing the Docker Shim—a temporary bridge that enabled Docker to work with the new CRI framework. Docker is not simply a container runtime; it is a comprehensive suite of tools that includes the Docker CLI, Docker API, image building tools, volume management, authentication, and integrated security features. A notable component within Docker is containerd—a daemon that manages containers using runc under the hood. Containerd is now fully CRI-compatible and can operate independently. Although Kubernetes supported Docker directly for a period, the Docker Shim was eventually removed in Kubernetes version 1.24. It is important to note that images built with Docker remain compatible with ContainerD, as they adhere to the OCI image specification. Diving Deeper into ContainerD ContainerD has matured into an independent project and is a graduated project within the CNCF. This means you can install ContainerD as a standalone container runtime without relying on Docker. To install ContainerD, extract the archive using the following commands: $ tar Cxzvf /usr/local containerd-1.6.2-linux-amd64.tar.gz
bin/
bin/containerd-shim-runc-v2
bin/containerd-shim
bin/ctr
bin/containerd-shim-runc-v1
bin/containerd
bin/containerd-stress Once installed, ContainerD includes the command line utility ""ctr"", primarily designed for debugging. Its limited feature set means that for regular container management, a more user-friendly tool is preferred. Running Containers: ctr vs. nerdctl Traditionally, Docker users employed commands such as docker run to start containers. With ContainerD installed independently, similar operations can be performed using the ""ctr"" tool. For example, to pull an image and run a container, you would execute: $ ctr images pull docker.io/library/redis:alpine
$ ctr run docker.io/library/redis:alpine redis Tip For more user-friendly operations, consider using nerdctl—a Docker-like CLI developed for ContainerD. Nerdctl not only mimics Docker command syntax but also supports enhanced features like encrypted images, lazy pulling, peer-to-peer image distribution, image signing, and Kubernetes namespace integration. Using nerdctl, running containers resembles Docker commands: $ nerdctl run --name redis redis:alpine
$ nerdctl run --name webserver -p 80:80 -d nginx Interacting with CRI-Compatible Runtimes: crictl crictl is a versatile command line tool developed by the Kubernetes community. Unlike ctr or nerdctl, which are specific to ContainerD, crictl is designed to work with any CRI-compatible runtime, such as ContainerD and CRI-O. Its primary focus is on debugging and inspecting container runtimes, not on extensive daily container management. For example, you can use crictl to pull images, list available images or containers, and execute commands within containers: $ crictl pull busybox
$ crictl images
$ crictl ps -a crictl also offers functionality to execute commands inside running containers or view logs, similarly to Docker. One significant difference is its awareness of Kubernetes pods—a concept that is not native to Docker commands. A comparison between Docker and crictl reveals that many commands such as attach, exec, images, info, inspect, logs, ps, stats, and version are similar. This familiarity eases the transition for Docker users. Configuration Changes in Kubernetes 1.24 Before Kubernetes 1.24, CRI tools attempted connections to runtime endpoints in a default order, including: unix:///var/run/dockershim.sock unix:///run/containerd/containerd.sock unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock With the release of Kubernetes 1.24, the support for the Docker socket was deprecated. This deprecation necessitates manually specifying the container runtime endpoint. For instance, you can configure the endpoint as shown: $ crictl --runtime-endpoint
$ export CONTAINER_RUNTIME_ENDPOINT=unix:///run/containerd/containerd.sock Other common endpoints include: unix:///run/crio/crio.sock unix:///var/run/cri-dockerd.sock For a deeper understanding of these changes, refer to discussions in the Kubernetes CRI tools repository (see pull request 869 and issue 868). Summary To summarize, there are three key command line tools associated with container runtimes: ctr – A debugging-oriented utility included with ContainerD that is used mainly for troubleshooting. nerdctl – A Docker-like CLI developed for ContainerD, offering a more comprehensive feature set for regular container operations. crictl – A versatile CLI tool from the Kubernetes community that works with any CRI-compatible runtime (including ContainerD and CRI-O) and is designed for debugging and inspection. Each tool serves a specific purpose, and the choice depends on whether you require advanced debugging options or a full-featured CLI for everyday container management. As container technologies continue to evolve, nerdctl is poised to become the preferred tool for general container operations in environments using ContainerD. Thank you for reading this article. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solution ReplicaSets optional,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Core-Concepts/Solution-ReplicaSets-optional,"Certified Kubernetes Application Developer - CKAD Core Concepts Solution ReplicaSets optional In this guide, we review the ReplicaSets lab exercise by detailing each command, expected output, and the rationale behind every step. Follow along to understand how to diagnose issues, update configurations, and scale your ReplicaSet efficiently. Checking the Initial State Before proceeding, verify there are no existing Pods or ReplicaSets in your cluster. List all Pods: kubectl get pods Expected output: No resources found in default namespace. This confirms that there are currently zero Pods running. List any existing ReplicaSets: kubectl get replicaset The command similarly shows no ReplicaSets at this point. Verifying the New ReplicaSet After applying some changes, a new ReplicaSet is created. Confirm its presence using: kubectl get replicaset The output should display something similar to: NAME             DESIRED   CURRENT   READY   AGE
new-replica-set  4         4         0       9s This indicates that the new ReplicaSet is configured to have 4 Pods. Running the command a second time reaffirms the same output. Determining the Image Used by the ReplicaSet To inspect the details of the ReplicaSet and determine which image is assigned to its Pods, execute: kubectl describe replicaset new-replica-set Within the Pod Template section under Containers, look for the image field: Image: busybox777 Also, the command section appears as: Command:
  sh
  -c
  echo Hello Kubernetes! && sleep 3600 This verifies that the ReplicaSet is initially using the image ""busybox777"". Examining the Pod Readiness Issue If the Pods are not transitioning to the Ready state, further investigation is required: Check the replica status: kubectl describe replicaset new-replica-set You should see an excerpt like: Replicas: 4 current / 4 desired
Pods Status: 0 Running / 4 Waiting / 0 Succeeded / 0 Failed Inspect one of the Pods to identify the issue: kubectl describe pod new-replica-set-7r2qw Notice the container is in a Waiting state with the reason: Reason: ImagePullBackOff And an associated event message: Failed to pull image ""busybox777"": ... pull access denied, repository does not exist or may require authorization Warning The error indicates that there is no image named ""busybox777"" available in the repository. Use the standard BusyBox image instead. Deleting a Pod Managed by the ReplicaSet ReplicaSets are designed to maintain the desired number of Pods automatically. Even if you delete a Pod, the ReplicaSet immediately replicates a new one. List the Pods: kubectl get pods Example output: NAME                     READY   STATUS            RESTARTS   AGE
new-replica-set-wkzjh    0/1     ImagePullBackOff   0          2m59s
new-replica-set-vpkh8    0/1     ImagePullBackOff   0          2m59s
new-replica-set-hr2zqw   0/1     ImagePullBackOff   0          2m59s
new-replica-set-tn2mp    0/1     ImagePullBackOff   0          2m59s Delete one of the Pods: kubectl delete pod new-replica-set-wkzjh Confirmation output: pod ""new-replica-set-wkzjh"" deleted List the Pods again to note that the ReplicaSet has recreated the missing Pod. Creating ReplicaSets Using YAML Definition Files Two YAML definition files are located in the /root directory: replicaset-definition-1.yaml replicaset-definition-2.yaml Fixing the First Definition File The initial content of replicaset-definition-1.yaml is: apiVersion: v1
kind: ReplicaSet
metadata:
  name: replicaset-1
spec:
  replicas: 2
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: frontend
    spec:
      containers:
      - name: nginx
        image: nginx When running: kubectl create -f /root/replicaset-definition-1.yaml you encounter the error: error: unable to recognize ""/root/replicaset-definition-1.yaml"": no matches for kind ""ReplicaSet"" in version ""v1"" Inspecting the available API versions with: kubectl explain replicaset reveals that the correct API version is apps/v1 . Update the file accordingly. After the change, creation should succeed. Fixing the Second Definition File The content of replicaset-definition-2.yaml is: apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replicaset-2
spec:
  replicas: 2
  selector:
    matchLabels:
      tier: frontend
  template:
    metadata:
      labels:
        tier: nginx
    spec:
      containers:
        - name: nginx
          image: nginx Creating it results in the error: The ReplicaSet ""replicaset-2"" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{""tier"":""nginx""}: selector does not match template labels This occurs because the selector ( tier: frontend ) does not match the Pod template labels ( tier: nginx ). Correct the Pod template labels to use tier: frontend so they match the selector. After this fix, create the ReplicaSet again using: kubectl create -f /root/replicaset-definition-2.yaml The ReplicaSet ""replicaset-2"" should now be successfully created. Deleting Newly Created ReplicaSets After verification, remove replicaset-1 and replicaset-2 from your cluster. List the current ReplicaSets to check their status: kubectl get rs Example output: NAME                DESIRED   CURRENT   READY   AGE
new-replica-set     4         4         0       10m
replicaset-1        2         2         2       3m4s
replicaset-2        2         2         2       22s Delete the ReplicaSets: kubectl delete rs replicaset-1
kubectl delete rs replicaset-2 Confirmation messages will indicate that both ReplicaSets have been removed. Updating the Original ReplicaSet's Image Since the original ReplicaSet (""new-replica-set"") uses an incorrect image (""busybox777""), update it to use the proper BusyBox image. Edit the ReplicaSet: kubectl edit rs new-replica-set Locate the container section and update the image field: containers:
  - name: busybox-container
    image: busybox Save the changes and verify by describing the ReplicaSet: kubectl describe rs new-replica-set Despite the update, existing Pods may still reflect the previous error because updating the ReplicaSet does not restart the existing Pods. To resolve this issue, manually delete the problematic Pods so that the ReplicaSet creates new ones with the corrected image: kubectl delete pod new-replica-set-vpkh8 new-replica-set-tn2mp new-replica-set-7r2qw After a short wait, list the Pods again: kubectl get pods New Pods should appear and transition to the Running state. Scaling the ReplicaSet Scaling Up To increase the ReplicaSet to five replicas, run: kubectl scale rs new-replica-set --replicas=5 Then verify the updated Pod count: kubectl get pods Expected output: NAME                       READY   STATUS    RESTARTS   AGE
new-replica-set-f5gth      1/1     Running   0          55s
new-replica-set-nsbgx      1/1     Running   0          55s
new-replica-set-8z7z5      1/1     Running   0          55s
new-replica-set-whhll      1/1     Running   0          55s
new-replica-set-mwpnz      1/1     Running   0          4s Scaling Down To scale the ReplicaSet down, you can edit the resource directly: Open the ReplicaSet for editing: kubectl edit rs new-replica-set Modify the replicas value from 5 to 2 in the YAML: spec:
  replicas: 2 Save your changes. Verify the update with: kubectl get rs new-replica-set The ReplicaSet will now adjust to maintain only two Pods. Final Commands Recap Below is a summary of the commands used throughout this lab exercise: # Check initial state
kubectl get pods
kubectl get replicaset

# After changes, verify new ReplicaSet and its details
kubectl get replicaset
kubectl describe replicaset new-replica-set

# Determine the image used and check Pod error
kubectl describe pod new-replica-set-7r2qw

# Demonstrate automatic Pod replacement
kubectl delete pod new-replica-set-wkzjh
kubectl get pods

# Create ReplicaSets from YAML and fix errors
kubectl create -f /root/replicaset-definition-1.yaml
kubectl create -f /root/replicaset-definition-2.yaml

# Delete newly created ReplicaSets
kubectl delete rs replicaset-1
kubectl delete rs replicaset-2

# Update the image in the original ReplicaSet
kubectl edit rs new-replica-set

# Delete faulty Pods so new ones are created with the correct image
kubectl delete pod new-replica-set-vpkh8 new-replica-set-tn2mp new-replica-set-7r2qw

# Scale the ReplicaSet upward
kubectl scale rs new-replica-set --replicas=5
kubectl get pods

# Scale the ReplicaSet downward using editing
kubectl edit rs new-replica-set Happy learning and successful Kubernetes management! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Recap ReplicaSets,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Core-Concepts/Recap-ReplicaSets,"Certified Kubernetes Application Developer - CKAD Core Concepts Recap ReplicaSets Hello and welcome to this Kubernetes controllers lesson. I’m Mumshad Mannambeth, and today we will explore how controllers manage your applications’ availability and scalability. Controllers are the brains behind Kubernetes—they monitor objects and respond to any change in the cluster. In this lesson, we will focus on the replication controller and its more advanced successor, the ReplicaSet. Imagine you have a single Pod running your application. If that Pod crashes, your users immediately lose access. To overcome this, you can run multiple instances (Pods) of the application. The replication controller ensures that a specified number of Pods are running at all times, providing high availability even during failures. Even when running a single Pod, a replication controller is beneficial because it automatically replaces a failed Pod, ensuring continuous availability. For instance, if one instance fails, another is promptly created to maintain the required Pod count. Another important use of the replication controller is load distribution. As user demand grows, additional Pods can be deployed under the controller’s management. If one node runs out of resources, Kubernetes schedules new Pods across other nodes. This efficiently balances the load and scales the application dynamically. It’s important to understand the difference between a replication controller and a ReplicaSet. While both ensure the desired number of Pods are running, the replication controller is an older technology that is gradually being replaced by the more advanced ReplicaSet. In our examples and demos going forward, we will focus on ReplicaSets, though the core concepts apply to both. Creating a Replication Controller Let’s start by creating a replication controller definition file named rc-definition.yaml . Every Kubernetes definition file comprises four main sections: API version, kind, metadata, and spec. For our replication controller: apiVersion: Set to v1 because the Replication Controller is supported under this version. kind: Set to ReplicationController . metadata: Contains the name ( myapp-rc ) and labels ( app and type ) for identification. spec: Defines the desired state, including the number of replicas and a Pod template. Move the Pod definition (excluding the API version and kind) into the template section. Ensure the Pod details are indented correctly under template to nest them properly as the replication controller's child. Below is the combined definition: # rc-definition.yaml
apiVersion: v1
kind: ReplicationController
metadata:
  name: myapp-rc
  labels:
    app: myapp
    type: front-end
spec:
  replicas: 3
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx-container
          image: nginx Once you’ve created the file, run the following command to create the replication controller: kubectl create -f rc-definition.yaml You should see an output confirming that the replication controller “myapp-rc” has been created. To verify, use these commands: kubectl get replicationcontroller
kubectl get pods The Pods created by the replication controller will have names starting with myapp-rc , indicating their management origin. Introducing ReplicaSets The ReplicaSet is the modern, recommended approach for ensuring a specified number of Pod replicas. In a ReplicaSet definition: apiVersion: Use apps/v1 (instead of v1 ). kind: Set to ReplicaSet . metadata and template: Similar to the replication controller, but with an additional required field. selector: The matchLabels selector identifies which Pods are managed by the ReplicaSet. This field is important because it allows the ReplicaSet to adopt existing Pods that match the provided labels. Below is an example ReplicaSet definition: # replicaset-definition.yaml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: myapp-replicaset
  labels:
    app: myapp
    type: front-end
spec:
  replicas: 3
  selector:
    matchLabels:
      type: front-end
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
        - name: nginx-container
          image: nginx Create the ReplicaSet using: kubectl create -f replicaset-definition.yaml Verify its creation with: kubectl get replicaset
kubectl get pods Understanding Labels and Selectors Labels are key-value pairs assigned to Kubernetes objects that enable you to group and select resources. For example, if you deploy three instances of a front-end application as Pods, you can label them accordingly and create a ReplicaSet with a matching selector. This tells the ReplicaSet which Pods to monitor. Even if the Pods already exist, the ReplicaSet will only create new ones if an existing Pod dies, ensuring that the desired number is maintained. Below is a snippet showing matching labels in a ReplicaSet selector and the corresponding Pod metadata: # In the ReplicaSet definition
selector:
  matchLabels:
    tier: front-end # In the Pod metadata
metadata:
  name: myapp-pod
  labels:
    tier: front-end Note Ensure that the labels defined in the ReplicaSet's selector exactly match those in the Pod template. Mismatches can lead to unexpected behavior where the ReplicaSet fails to manage the intended Pods. Scaling a ReplicaSet If you need to scale your ReplicaSet from 3 to 6 replicas, there are two common approaches: Update the Definition File: Modify the replicas field in your replicaset-definition.yaml file to 6, then apply the change using: kubectl replace -f replicaset-definition.yaml Use the kubectl Scale Command: Scale directly from the command line with one of these commands: kubectl scale --replicas=6 -f replicaset-definition.yaml Or by specifying the ReplicaSet name: kubectl scale --replicas=6 replicaset/myapp-replicaset Note Remember that when you use the scale command, the change only affects the running ReplicaSet. The original definition file will still show the previous replica count until you update it. Command Review Below is a quick reference for essential Kubernetes commands used in this lesson: Command Description kubectl create -f <definition-file> Create an object from a file kubectl get replicationcontroller List all replication controllers kubectl get replicaset List all ReplicaSets kubectl get pods List all Pods kubectl delete replicaset <name> Delete a ReplicaSet by name kubectl replace -f <definition-file> Update an existing object using a definition kubectl scale --replicas=<number> Scale an object to the specified number This concludes our lesson on replication controllers and ReplicaSets. These controllers ensure that your applications remain highly available, efficiently scaled, and properly load balanced within your Kubernetes cluster. Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Recap Pods with YAML,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Core-Concepts/Recap-Pods-with-YAML,"Certified Kubernetes Application Developer - CKAD Core Concepts Recap Pods with YAML In this lesson, we explore how to create a Kubernetes Pod using a YAML configuration file. YAML files are fundamental to Kubernetes and are used to define various objects such as Pods, ReplicaSets, Deployments, and Services. Every Kubernetes definition file follows a standard structure with four top-level fields: apiVersion, kind, metadata, and spec. These properties are required for your configuration file to be valid. Key Components of a Kubernetes YAML File apiVersion This field specifies the version of the Kubernetes API that is used to create the object. For a Pod, you'll typically use ""v1"". Different objects may require other versions such as apps/v1beta or extensions/v1beta, but for our purpose, ""v1"" is used. kind The kind denotes the type of object you're creating. In this lesson, we're creating a Pod. Other valid values include ReplicaSets, Deployments, or Services. metadata Metadata provides essential information about the object, including its name, labels, and other identifying data. It is important that all properties under metadata (like name and labels) are indented consistently. This indentation ensures that these properties are recognized as siblings rather than nested incorrectly. Note Ensure that all sibling properties under metadata share the same indentation level to avoid YAML parsing errors. Below is an example YAML snippet demonstrating proper indentation: apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
spec: spec The spec section contains the detailed configuration for the object. In the case of a Pod, this is where you specify one or more containers and their properties. Although a Pod can run multiple containers, in this example, we define a single container. Complete YAML Example for a Pod Below is a complete YAML configuration for creating a Pod with one container that runs the nginx image: apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx After saving this configuration to a file (for example, pod-definition.yaml ), you can create the Pod in Kubernetes with the following command: kubectl create -f pod-definition.yaml Verifying Your Pod Once you have created the Pod, you can check its status using: kubectl get pods This command lists all the Pods currently running. A sample output might look like this: NAME        READY   STATUS    RESTARTS   AGE
myapp-pod   1/1     Running   0          20s To view detailed information about the Pod—including node placement, container details, and recent events—use the following command: kubectl describe pod myapp-pod An example excerpt from the detailed output is shown below: Name:              myapp-pod
Namespace:         default
Node:              minikube/192.168.99.100
Start Time:        Sat, 03 Mar 2018 14:26:14 +0800
Labels:            app=myapp
Annotations:       <none>
Status:            Running
IP:                172.17.0.24
Containers:
  nginx-container:
    Container ID:   docker://830bb56c8c42a860b4b70e9c1488fae1bc38663e4918b6c2f5a78e768b8c9d
    Image:          nginx
    Image ID:       docker-pullable://nginx@sha256:4771d09578c7ca65299e110b3ee1c0a2592f5ea2618d32e4ffe7a4cab1c5de
    Port:           <none>
    State:          Running
      Started:      Sat, 03 Mar 2018 14:26:21 +0800
    Ready:          True
    Restart Count:  0
    Environment:    <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-x95w7 (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  PodScheduled      True
Events:
  Type    Reason                  Age   From                  Message
  Normal  Scheduled               27s   default-scheduler     Successfully assigned myapp-pod to minikube
  Normal  SuccessfulMountVolume   27s   minikube              MountVolume.SetUp succeeded for volume ""default-token-x95w7""
  Normal  Pulling                 27s   minikube              pulling image ""nginx""
  Normal  Pulled                  27s   minikube              Successfully pulled image ""nginx""
  Normal  Created                 27s   minikube              Created container
  Normal  Started                 27s   minikube              Started container This detailed output provides crucial insights into the Pod’s specifications, such as its running state, the container image details, and the events that occurred—from scheduling to container initiation. Tip Review the YAML configuration structure and verify command outputs to ensure your Kubernetes objects are correctly defined and running as expected. Happy coding and enjoy exploring Kubernetes with YAML! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solution Pods optional,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Core-Concepts/Solution-Pods-optional,"Certified Kubernetes Application Developer - CKAD Core Concepts Solution Pods optional In this lab, you will learn how to work with Pods in Kubernetes using both imperative commands and declarative YAML configurations. The lab covers checking existing Pods, creating new Pods with different images, inspecting their details, troubleshooting common errors, and updating configurations to fix issues. Listing Existing Pods Start by listing the Pods in your default namespace. This step helps you verify that no unexpected Pods are running before you begin: kubectl get pods If no Pods are running, the output will look similar to: controlplane ~ ⟫ kubectl get pods
No resources found in default namespace.
controlplane ~ ⟫ Creating a New Pod with the nginx Image Next, create a new Pod that uses the nginx image by executing: kubectl run nginx --image=nginx The output should confirm the creation of the Pod: kubectl run nginx --image=nginx
pod/nginx created Verify the creation by listing the Pods again: kubectl get pods You may see multiple Pods, especially if you have done previous exercises. For example: controlplane ~ ➜ kubectl get pods
NAME            READY   STATUS              RESTARTS   AGE
nginx           0/1     ContainerCreating   0          17s
newpods-llstt   0/1     ContainerCreating   0          11s
newpods-pnnx8   0/1     ContainerCreating   0          11s
newpods-k87fx   0/1     ContainerCreating   0          11s Note Some Pods might still display ""ContainerCreating"" as they are in the process of being set up. Inspecting Pod Details and Verifying the Image To inspect the details of a specific Pod and verify which image is used, run: kubectl describe pod <pod-name> For example, the detailed output may include: Node:         controlplane/172.25.0.47
Start Time:   Fri, 15 Apr 2022 18:12:04 +0000
Labels:       tier=busybox
...
Containers:
  busybox:
    Container ID:   containerd://b05cd692af1f3b433883f9a8ece19ec2e8c4fcf861aa97ae6a82857ed6037a6d
    Image:          busybox
    Image ID:       docker.io/library/busybox@sha256:d2b533584f580310186df7a2055ce3ff83cc0df6caacf1e3489bf
    Command:
      sleep
      1000
    State:           Running
... In this excerpt, the image used is busybox . You can also use the wide output option to display node information: kubectl get pods -o wide The output might be: NAME              READY   STATUS    RESTARTS   AGE     IP            NODE         NOMINATED NODE   READY
newpods-pnnx8    1/1     Running   0          2m3s    10.42.0.10   controlplane  <none>           <none>
newpods-llstt    1/1     Running   0          2m3s    10.42.0.12   controlplane  <none>           <none>
newpods-k87fx    1/1     Running   0          2m3s    10.42.0.11   controlplane  <none>           <none>
nginx            1/1     Running   0          2m9s    10.42.0.9    controlplane  <none>           <none> Working with a Multi-Container Pod Consider a Pod named webapp that contains two containers. To view the details of this multi-container Pod, run: kubectl describe pod webapp A sample excerpt from the description might show: Priority: 0
Node: controlplane/172.25.0.47
Start Time: Fri, 15 Apr 2022 18:14:22 +0000
...
Containers:
  nginx:
    Container ID: containerd://42fc9932fb07a3047ea1902a6255e75139e981a37426e0d31ea5ec7a833481d
    Image: nginx
    State: Running
    Ready: True
    Restart Count: 0
    ...
  agentx:
    Container ID: agentx
    Image: agentx
    State: Waiting
    Reason: ErrImagePull
    Ready: False
    Restart Count: 0
    ... In this Pod, the nginx container is running normally, while the agentx container is in a waiting state due to an image pull error (ErrImagePull). When you list the Pods, observe that the READY column indicates the number of ready containers versus the total containers: kubectl get pods
NAME            READY   STATUS             RESTARTS   AGE
newpods-pnnx8   1/1     Running            0          2m33s
newpods-llstt   1/1     Running            0          2m33s
newpods-k87fx   1/1     Running            0          2m33s
nginx           1/1     Running            0          2m39s
webapp          0/1     ImagePullBackOff   0          15s Warning The ErrImagePull error indicates that Kubernetes failed to pull the image for the agentx container, likely because the image name is incorrect or the image does not exist on Docker Hub. Deleting the Erroneous Web App Pod Since the webapp Pod contains an image pull error, remove it by running: kubectl delete pod webapp You should see a deletion confirmation: pod ""webapp"" deleted Creating and Correcting a Redis Pod Now, create a new Pod named redis using an intentionally incorrect image name ( redis123 ). This exercise demonstrates how to use declarative YAML and fix common errors. Step 1: Create the YAML File Generate the Pod definition using a dry run: kubectl run redis --image=redis123 --dry-run=client -o yaml Redirect the output to a file named redis.yaml . The file should resemble: apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: redis
  name: redis
spec:
  containers:
  - image: redis123
    name: redis
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {} Step 2: Create the Redis Pod Apply the YAML file to create the Pod: kubectl create -f redis.yaml The expected output is: pod/redis created Verify the Pod status: kubectl get pods The output may look like: NAME            READY   STATUS         RESTARTS   AGE
newpods-pnnx8   1/1     Running        0          8m16s
newpods-llstt   1/1     Running        0          8m16s
newpods-k87fx   1/1     Running        0          8m16s
nginx           1/1     Running        0          8m22s
redis           0/1     ErrImagePull   0          10s Since the image name redis123 is invalid, the Pod enters an ErrImagePull state. Step 3: Update the YAML File with the Correct Image Edit the redis.yaml file to update the image name from redis123 to redis . The updated file should be: apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: redis
  name: redis
spec:
  containers:
  - image: redis
    name: redis
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {} Apply the changes with: kubectl apply -f redis.yaml You might see a warning indicating that the Pod is missing an annotation necessary for future apply operations. After updating, verify the status again: kubectl get pods An example output is: NAME            READY   STATUS    RESTARTS   AGE
newpods-pnnx8   1/1     Running   0          9m38s
newpods-llstt   1/1     Running   0          9m38s
newpods-k87fx   1/1     Running   0          9m44s
nginx           1/1     Running   0          9m44s
redis           1/1     Running   0          92s The redis Pod is now running correctly thanks to the valid Docker image. Conclusion In this lab, you learned how to: List existing Pods in a Kubernetes cluster. Create new Pods using imperative commands with different images. Inspect detailed information about Pods to verify configurations. Troubleshoot common issues like image pull errors. Use declarative YAML configurations to manage and update Pods. Happy clustering and keep exploring more Kubernetes features! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Recap Deployments,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Core-Concepts/Recap-Deployments,"Certified Kubernetes Application Developer - CKAD Core Concepts Recap Deployments Hello and welcome. My name is Mumshad Mannambeth. In this article, we’ll explore Kubernetes Deployments, a vital tool for managing applications in production environments. Kubernetes Deployments simplify rolling updates, rollbacks, and coordinated changes, ensuring your applications remain highly available and robust. Imagine you need to deploy a production web server. Instead of running a single instance, you’ll need multiple instances for load balancing and high availability. When new builds are available in your Docker registry, updating all instances simultaneously isn’t ideal—this might impact users. Instead, a Rolling Update allows you to upgrade instances one by one, and if an error occurs, you can quickly roll back to a stable version. Additionally, if you need to update various aspects of your environment—like upgrading web server versions, scaling resources, or adjusting resource allocations—it’s best to pause, apply all changes together, and then resume operations. Kubernetes Deployments provide these capabilities. Pods represent a single instance of your application, and they are managed by ReplicaSets (or replication controllers). The Deployment object orchestrates these Pods, enabling seamless updates and coordinated changes. Creating a Deployment To create a Deployment, you begin by writing a Deployment definition file. The primary difference between a ReplicaSet and a Deployment definition is that the kind is set to Deployment. Below is an example of a valid Deployment YAML definition: apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
    type: front-end
spec:
  replicas: 3
  selector:
    matchLabels:
      type: front-end
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx This YAML file includes the following key sections: API Version: Uses apps/v1 for the Deployment. Metadata: Provides a unique name ( myapp-deployment ) and labels to identify the Deployment. Specification (spec): Replicas: Defines the desired number of pod copies. Selector: Matches the defined labels. Template: Describes the pod configuration, including container specifications. Note Make sure to save your YAML file, for example, as deployment-definition.yml before proceeding with deployment creation. Once your file is ready, create the Deployment using the following command: kubectl create -f deployment-definition.yml You should see confirmation similar to: deployment ""myapp-deployment"" created Verifying the Deployment After creating the Deployment, verify that it is running as expected by executing: kubectl get deployments The output should resemble: NAME               DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
myapp-deployment   3         3         3            3           21s When a Deployment is created, Kubernetes automatically generates a corresponding ReplicaSet. To view this ReplicaSet, run: kubectl get replicasets And to see the Pods managed by the ReplicaSet, execute: kubectl get pods To display all related Kubernetes objects simultaneously, use: kubectl get all An example output might be: NAME                              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
deploy/myapp-deployment           3         3         3            3           9h

NAME                              DESIRED   CURRENT   READY   AGE
rs/myapp-deployment-6795844b58     3         3         3       9h

NAME                                       READY   STATUS    RESTARTS   AGE
po/myapp-deployment-6795844b58-5rbj1         1/1     Running   0          9h
po/myapp-deployment-6795844b58-h4w55          1/1     Running   0          9h
po/myapp-deployment-6795844b58-1fjhv          1/1     Running   0          9h This output confirms that your Deployment, the associated ReplicaSet, and all Pods are operating as intended. Key Takeaway Kubernetes Deployments not only simplify the management of your application’s lifecycle but also provide powerful features such as automated scaling, rolling updates, and instant rollback capabilities. That concludes this article on Kubernetes Deployments. Stay tuned for upcoming lessons where we will dive deeper into advanced features like rolling updates, rollbacks, and coordinated changes to further enhance your application's deployment strategy. For additional details and tutorials, consider checking out: Kubernetes Documentation Kubernetes Basics Docker Hub Happy deploying! Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Define build and modify container images,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Define-build-and-modify-container-images,"Certified Kubernetes Application Developer - CKAD Configuration Define build and modify container images Welcome to this comprehensive guide on Docker images. In this tutorial, you’ll learn how to create your own Docker image by containerizing a simple web application built with the Python Flask framework. Containerizing an application is essential for creating customized components that may not be available on Docker Hub, as well as streamlining deployment processes. Overview Before containerizing, consider the manual deployment steps: starting with a base operating system (like Ubuntu), updating package repositories using APT, installing required system and Python packages, copying the source code to a designated directory (e.g., /opt ), and finally running the web server with Flask. With these steps in mind, you can automate the process using a Dockerfile. Below is an example Dockerfile for our Flask application: FROM ubuntu

RUN apt-get update && apt-get -y install python python-setuptools python-dev
RUN pip install flask flask-mysql

COPY . /opt/source-code

ENTRYPOINT FLASK_APP=/opt/source-code/app.py flask run --host=0.0.0.0 Dockerfile Breakdown FROM : Sets the base image as Ubuntu. Every Docker image starts with a base layer from either an operating system or a prebuilt image available on Docker Hub. RUN : Executes commands during image build. The first RUN updates the system and installs Python, while the second installs Flask and MySQL support via pip . COPY : Transfers your application's source code from your local repository to the container directory /opt/source-code . ENTRYPOINT : Specifies the command to execute when the container launches, setting the FLASK_APP environment variable and starting the Flask server. Docker processes these instructions sequentially, creating layers that represent the changes made with each step. If you update any instruction, Docker reuses the cached layers for all preceding steps, speeding up the build process significantly. Viewing Docker Image Layers To inspect the image layers and their sizes, run the following command: docker history <your-image-name> Building the Docker Image After finalizing your Dockerfile, build your image with the following command: docker build -f Dockerfile -t mmumshad/my-custom-app . During the build, Docker outputs details for each step. Here’s an example of what the build output might include: root@osboxes:/root/simple-webapp-docker # docker build .
Sending build context to Docker daemon  3.072kB
Step 1/5 : FROM ubuntu
 ---> ccc7a11d65b1
Step 2/5 : RUN apt-get update && apt-get install -y python python-setuptools python-dev
 ---> Running in a7840bfad17
Get:1 http://archive.ubuntu.com/ubuntu xenial InRelease [247 kB]
Get:2 http://security.ubuntu.com/ubuntu xenial-security InRelease [102 kB]
Get:3 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [102 kB]
Get:4 http://security.ubuntu.com/ubuntu xenial-security/universe Sources [46.3 kB]
Get:5 http://archive.ubuntu.com/ubuntu xenial-backports InRelease [102 kB]
Get:6 http://security.ubuntu.com/ubuntu xenial-security/main amd64 Packages [440 kB]
Step 3/5 : RUN pip install flask flask-mysql
 ---> Running in a4a6c9190ba3
Collecting flask
Downloading Flask-0.12.2-py2.py3-none-any.whl (83kB)
Collecting flask-mysql
Downloading Flask-MySQL-1.4.0-py2.py3-none-any.whl
Removing intermediate container a4a6c9190ba3
Step 4/5 : COPY app.py /opt/
 ---> 7cda1b1782
Removing intermediate container faaa63c512
Step 5/5 : ENTRYPOINT FLASK_APP=/opt/app.py flask run --host=0.0.0.0
 ---> Running in d452c574a8b8
 ---> 9f27c36920bc
Removing intermediate container d452c574a8b8
Successfully built 9f27c36920bc Build Caching If a build step fails, Docker caches the successful layers up to the failure point. After correcting the error, re-running the build command leverages the cached layers, drastically reducing rebuild times. Containerizing a Diverse Range of Applications Docker is not limited to web applications. You can containerize databases, development tools, web browsers, and utilities like curl, Spotify, or Skype. The versatility of containerization is reshaping how applications are deployed and managed. In the future, instead of installing software directly onto your operating system, you'll run it in a containerized environment. This approach ensures that applications can be removed cleanly without leaving residual files or configurations on your system. Happy containerizing! Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Solution Imperative Commands optional,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Core-Concepts/Solution-Imperative-Commands-optional,"Certified Kubernetes Application Developer - CKAD Core Concepts Solution Imperative Commands optional In this lesson, we walk you through a series of hands-on tasks using imperative commands in Kubernetes. These exercises reinforce concepts and help you prepare for your exam by showcasing how to create and manage Kubernetes objects on the fly using the kubectl command-line tool. Deploying an Nginx Pod Start by deploying a pod named ""nginx-pod"" using the nginx Alpine image with the following command: kubectl run nginx-pod --image=nginx:alpine If successful, your terminal will output something similar to: pod/nginx-pod created Deploying a Redis Pod with Labels Next, create a Redis pod using the Redis Alpine image and assign it a label ( tier=db ) to facilitate later operations like service selection: kubectl run redis --image=redis:alpine --labels=tier=db This command creates the Redis pod with the specified label. Creating a Redis Service Expose the Redis pod within the cluster by creating a service called ""redis-service"" on port 6379. We recommend using the kubectl expose command as it automatically detects pod labels to configure selectors. Run the following command to expose the pod: kubectl expose pod redis --port=6379 --name=redis-service This command creates a ClusterIP service that routes traffic to your Redis pod. To verify the service, execute: kubectl get svc Creating a Deployment for a Web Application Now, create a deployment named ""webapp"" with a specified image and scale it to three replicas using the following command: kubectl create deployment webapp --image=<your-image> --replicas=3 After deploying, verify that all replicas are running correctly by checking the deployment status. Note Ensure you use the correct number of dashes when specifying replica count to avoid syntax errors. Creating a Custom Nginx Pod with a Specific Port Create a pod named ""custom-nginx"" with the nginx image and set up the container to listen on port 8080: kubectl run custom-nginx --image=nginx --port=8080 This command configures the container’s port to 8080. Check the pod’s configuration to confirm it is running with the correct port settings. Creating a New Namespace To isolate your resources, create a new namespace called ""dev-ns"": kubectl create namespace dev-ns You should see a confirmation message similar to: namespace/dev-ns created Creating a Redis Deployment in the ""dev-ns"" Namespace Within the ""dev-ns"" namespace, deploy a Redis application with two replicas by running: kubectl create deployment redis-deploy --image=redis --replicas=2 -n dev-ns Verify the deployment with: kubectl get deployment -n dev-ns Expected output: NAME           READY   UP-TO-DATE   AVAILABLE   AGE
redis-deploy   2/2     2            2           12s Creating a Pod and Exposing It as a Service in a Single Command For the final task, create a pod named ""httpd"" using the httpd Alpine image and simultaneously expose it as a ClusterIP service on port 80. You can accomplish this in a single step: kubectl run httpd --image=httpd:alpine --port=80 --expose This command does the following: Creates a pod named ""httpd"". Exposes the pod by creating a corresponding ClusterIP service on port 80. Verify both the pod and service with: kubectl get pod
kubectl get svc For a detailed view of the service configuration, run: kubectl describe svc httpd The output will include details such as the selector (e.g., ""run=httpd"") and the endpoint configurations, ensuring that the service is properly set up to expose the pod on port 80. Verification Commands After completing the above tasks, use the following commands to check the status of your pods and services: To list all pods: kubectl get pod An example output might look like: NAME                       READY   STATUS    RESTARTS   AGE
nginx-pod                  1/1     Running   0          12m
redis                      1/1     Running   0          10m
webapp-7b59bf687d-n7xxp    1/1     Running   0          5m4s
webapp-7b59bf687d-rds95    1/1     Running   0          5m4s
webapp-7b59bf687d-4gqmt    1/1     Running   0          5m4s
custom-nginx               1/1     Running   0          3m41s
httpd                      1/1     Running   0          8s To list all services: kubectl get svc Sample output: NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)       AGE
kubernetes      ClusterIP   10.43.0.1       <none>        443/TCP       20m
redis-service   ClusterIP   10.43.56.187    <none>        6379/TCP      6m35s
httpd           ClusterIP   10.43.112.233   <none>        80/TCP        15s To describe the httpd service in detail: kubectl describe svc httpd You should see output detailing the service configuration, such as: Name:              httpd
Namespace:         default
Labels:            <none>
Annotations:       <none>
Selector:          run=httpd
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.43.112.233
IPs:               10.43.112.233
Port:              <unset>  80/TCP
TargetPort:       80/TCP
Endpoints:        10.0.2.17:80
Session Affinity:  None
Events:           <none> This confirms that the ""httpd"" service is correctly exposing the corresponding pod on port 80. This concludes the lab on imperative commands for deploying and exposing Kubernetes objects. Each command illustrates a practical approach to managing pods, deployments, and services, providing valuable hands-on experience for your journey toward becoming a certified Kubernetes professional. Happy practicing! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Commands and Arguments in Docker,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Commands-and-Arguments-in-Docker,"Certified Kubernetes Application Developer - CKAD Configuration Commands and Arguments in Docker Welcome to this detailed lesson on Docker commands and arguments. My name is Mumshad Mannambeth, and in this article we will explore how commands work in containers and Docker. Although this topic may not be explicitly featured in certification curriculums, understanding it is crucial since it is often overlooked. We will begin by reviewing container commands with Docker and then translate these concepts to Pods in Kubernetes. Running a Container with an Ubuntu Image Imagine running a Docker container using an Ubuntu image. Executing the commands below creates a container instance that immediately exits: docker run ubuntu
docker ps
docker ps -a The container does not appear in the list of running containers because, unlike virtual machines, containers are designed for specific tasks (such as hosting a web server, application server, or performing computations) and terminate once these tasks complete. Essentially, a container’s life cycle is tied to its main process. For instance, if the web service inside the container stops or crashes, the container will exit. Note Containers are ideal for running single processes because they are lightweight and are not intended to persist beyond the execution of their primary task. Understanding the CMD Instruction in Dockerfiles The behavior of a container is defined by its Dockerfile. Many popular Docker images, such as nginx or MySQL, use the CMD instruction to specify the default command that runs when the container starts. For example, the nginx image is configured to launch the nginx process, whereas the MySQL image starts the MySQL server process. Consider the following Dockerfile excerpt that installs and configures nginx as well as MySQL: # Install Nginx.
RUN \
    add-apt-repository -y ppa:nginx/stable && \
    apt-get update && \
    apt-get install -y nginx && \
    rm -rf /var/lib/apt/lists/* && \
    echo ""\ndaemon off;"" >> /etc/nginx/nginx.conf && \
    chown -R www-data:www-data /var/lib/nginx

# Define mountable directories.
VOLUME [""/etc/nginx/sites-enabled"", ""/etc/nginx/certs""]

# Define working directory.
WORKDIR /etc/nginx

# Define default command.
CMD [""nginx""]

# Install server
RUN rpmmkeys --import https://repo.mysql.com/RPM-GPG-KEY-mysql \
    && yum install -y $MYSQL_SERVER_PACKAGE_URL $MYSQL_SHELL_PACKAGE_URL libpqquality \
    && yum clean all \
    && mkdir /docker-entrypoint-initdb.d

VOLUME /var/lib/mysql

COPY docker-entrypoint.sh /entrypoint.sh
COPY healthcheck.sh /healthcheck.sh
ENTRYPOINT [""/entrypoint.sh""]
HEALTHCHECK CMD /healthcheck.sh
EXPOSE 3306 33060
CMD [""mysqld""] Another common example is a Dockerfile for an Ubuntu image that sets bash as the default command: # Pull base image.
FROM ubuntu:14.04

# Install.
RUN \
    sed -i 's/#\(.*multiverse\)$/\1/' /etc/apt/sources.list && \
    apt-get update && \
    apt-get -y upgrade && \
    apt-get install -y build-essential && \
    apt-get install -y software-properties-common && \
    apt-get install -y byobu curl git htop man unzip vim wget && \
    rm -rf /var/lib/apt/lists/*

# Add files.
ADD root/.bashrc /root/.bashrc
ADD root/.gitconfig /root/.gitconfig
ADD root/.scripts /root/.scripts

# Set environment variables.
ENV HOME /root

# Define working directory.
WORKDIR /root

# Define default command.
CMD [""bash""] Because bash is a shell that waits for terminal input, if Docker does not attach a terminal at runtime, the container exits immediately when no input is provided. Overriding the Default Command You can override the default command defined in the Docker image by appending a different command to the Docker run command. For instance, if you want the container to run a sleep command for five seconds instead of starting bash, execute: docker run ubuntu sleep 5 This approach temporarily replaces the default CMD from the image. To make a permanent change, create a new Docker image based on the Ubuntu image with a custom CMD. For example: FROM ubuntu
CMD [""sleep"", ""5""] Build and run your new image with the following commands: docker build -t ubuntu-sleeper .
docker run ubuntu-sleeper The container will now always sleep for five seconds before exiting. Using ENTRYPOINT to Combine Commands and Arguments What if you want to pass a variable argument, such as the number of seconds, when running the container without specifying the command every time? This is where the ENTRYPOINT instruction proves useful. ENTRYPOINT sets the default executable that runs when the container starts. Any command-line arguments provided at runtime are appended to the ENTRYPOINT. Consider this Dockerfile: FROM ubuntu
ENTRYPOINT [""sleep""]
CMD [""5""] With this configuration: Running the container without additional arguments: docker run ubuntu-sleeper Executes the command: Command at Startup: sleep 5 Running the container with an extra argument: docker run ubuntu-sleeper 10 Executes the command: Command at Startup: sleep 10 Warning If the necessary argument is missing (for example, if the ENTRYPOINT command expects an argument and none is provided), the container will fail to run and display an error. To temporarily override the ENTRYPOINT and run a different command (for example, switching from sleep to sleep2.0), you can use the --entrypoint flag: docker run --entrypoint sleep2.0 ubuntu-sleeper 10 This command will execute: Command at Startup: sleep2.0 10 This example clearly illustrates the difference between CMD and ENTRYPOINT. CMD's parameters can be completely overridden by command-line arguments, whereas ENTRYPOINT ensures that its predefined executable is always run, appending any supplied arguments. That concludes our lesson on managing commands and arguments in Docker. By mastering these concepts, you can create more flexible and powerful containerized applications. Happy containerizing! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Recap Namespaces,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Core-Concepts/Recap-Namespaces,"Certified Kubernetes Application Developer - CKAD Core Concepts Recap Namespaces Welcome to this detailed overview of Kubernetes namespaces. In this lesson, we'll explore how namespaces help organize resources within your cluster using a simple analogy and practical examples. This guide is designed to improve your understanding and management of namespaces in various environments. Imagine two boys named Mark. To differentiate between them, their last names—Smith and Williams—are used. Within their respective houses, family members refer to them by their first names. However, when addressing someone from a different house, the full name is used. Similarly, in Kubernetes, namespaces act like houses, each with its own set of rules and resources. Up until now, objects such as Pods, Deployments, and Services have been created in a single namespace—the default namespace—which is automatically generated when the cluster is set up. Kubernetes also creates several internal namespaces: The kube-system namespace contains system components (e.g., networking and DNS services) and keeps them isolated from user modifications. The kube-public namespace is designed for resources that should be accessible to all users. For small environments, learning purposes, or experimental setups, you might work exclusively in the default namespace. However, in enterprise or production settings, creating additional namespaces helps isolate resources. For instance, you can use separate namespaces for development and production to prevent accidental modifications and enforce specific policies and resource quotas. Each namespace can enforce its own set of policies and quotas. This ensures that every namespace receives a guaranteed portion of resources and remains within its limits. Namespace Isolation Benefits Using namespaces improves security and resource management by isolating workloads. It also simplifies policy enforcement and resource tracking across different environments. Within a namespace, resources can simply refer to one another by name—just as family members within a house use only first names. For example, a web application pod in the default namespace can access a database service using the service's hostname. When a pod needs to access a service in another namespace, append the namespace to the service name using the following format: python
mysql.connect(""db-service.namespace.svc.cluster.local"") For example, if a pod in the default namespace needs to connect to a database in the ""dev"" namespace: python
mysql.connect(""db-service.dev.svc.cluster.local"") This works because Kubernetes automatically creates a DNS entry for each service using the format: service-name.namespace.svc.cluster.local Here, ""cluster.local"" is the default domain name of the cluster, and ""svc"" is the subdomain for services. Operational Aspects with kubectl Commands Below are some examples of how to work with namespaces using kubectl commands: bash
> kubectl get pods
> kubectl get pods --namespace=kube-system
> kubectl get pods --namespace=prod Creating and Managing Pods Consider a pod definition file that creates a pod in the default namespace: apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx To create the pod in a different namespace (e.g., ""dev""), run: bash
> kubectl create -f pod-definition.yml --namespace=dev If you want to ensure the pod is always created in the ""dev"" namespace, add the namespace within the pod definition file under the metadata section. Creating Namespaces Creating a new namespace is as straightforward as creating other Kubernetes objects. Below is an example YAML file for a namespace: apiVersion: v1
kind: Namespace
metadata:
  name: dev Once prepared, create the namespace using: bash
> kubectl create -f namespace-definition.yml Alternatively, you can create a namespace via the command line: bash
> kubectl create namespace dev Setting the Default Namespace If you work with multiple namespaces, setting a default namespace in your current context can streamline your workflow: bash
kubectl config set-context $(kubectl config current-context) --namespace=dev After running this, simple commands like kubectl get pods will operate within the ""dev"" namespace. To list pods across all namespaces, use: bash
> kubectl get pods --all-namespaces Managing Resource Quotas Resource quotas help control resource usage within a namespace. Use a ResourceQuota definition file to limit resources. For example, the following YAML sets resource limits for the ""dev"" namespace: apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-quota
  namespace: dev
spec:
  hard:
    pods: ""10""
    requests.cpu: ""4""
    requests.memory: 5Gi
    limits.cpu: ""10""
    limits.memory: 10Gi Deploy this resource quota with: bash
> kubectl create -f compute-quota.yaml Resource Quotas Implementing resource quotas helps ensure that no single namespace overconsumes cluster resources, thereby maintaining overall cluster stability and performance. Practice these namespace techniques to efficiently manage your Kubernetes clusters. For more details on Kubernetes concepts and best practices, visit the Kubernetes Documentation . Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Solution Deployments optional,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Core-Concepts/Solution-Deployments-optional,"Certified Kubernetes Application Developer - CKAD Core Concepts Solution Deployments optional In this lab article, you will be introduced to Kubernetes deployments. We will walk through each step of the process while ensuring that your cluster is correctly set up and that deployments function as expected. Step 1: Check the Initial Cluster State Before creating any deployments, verify that your cluster has no existing pods, ReplicaSets, or deployments. Start by checking for pods: controlplane ~ ➜ kubectl get pods
No resources found in default namespace. Then, check for ReplicaSets: controlplane ~ ➜ kubectl get rs
No resources found in default namespace. Finally, check for any deployments: controlplane ~ ➜ kubectl get deployments
No resources found in default namespace. At this point, your environment is clean with zero pods, zero ReplicaSets, and zero deployments. Step 2: Observe Changes After a Deployment is Created After applying some changes, check the deployments again. You should now see that one deployment exists: controlplane ~ ➜ kubectl get deployments
NAME                  READY   UP-TO-DATE   AVAILABLE   AGE
frontend-deployment   0/4     4            0           10s Examine the ReplicaSet created by that deployment: controlplane ~ ➜ kubectl get rs
NAME                                   DESIRED   CURRENT   READY   AGE
frontend-deployment-7f8dcd-b696        4         4         0       35s And then inspect the pods: controlplane ~ ➜ kubectl get pods
NAME                                             READY   STATUS             RESTARTS   AGE
frontend-deployment-7f8dcd-b696-stmbx            0/1     ImagePullBackOff   0          59s
frontend-deployment-7f8dcd-b696-zc6x             0/1     ErrImagePull       0          59s
frontend-deployment-7f8dcd-b696-jgcbx            0/1     ErrImagePull       0          59s
frontend-deployment-7f8dcd-b696-jbr44            0/1     ErrImagePull       0          59s Because none of the four pods are in a ready state, you need to identify which image they are trying to pull. Run the following command to display detailed information for one pod: kubectl describe pod frontend-deployment-7fd8cdb696-stmbx From the output, notice that the specified image is busybox:888 . Since this image does not exist, the pods are unable to reach a ready state. Note The image name must be valid and available in your container registry. Verify the image tag before deployment to avoid issues like ImagePullBackOff. Step 3: Create a New Deployment Using a YAML Definition Verify Your Current Directory and Files Ensure you are in the correct working directory and check the available files: controlplane ~ ➜ pwd
/root

controlplane ~ ➜ ls
deployment-definition-1.yaml  sample.yaml Attempt to Create the Deployment Run the following command to create the deployment from the YAML file: controlplane ~ ➜ kubectl create -f deployment-definition-1.yaml If you encounter an error such as: Error from server (BadRequest): error when creating ""deployment-definition-1.yaml"": deployment in version ""v1"" cannot be handled as a Deployment: no kind ""deployment"" is registered for version ""apps/v1"" in scheme ""k8s.io/apimachinery/v1.23.3-k3s1/pkg/runtime/scheme.go:100"" Open the file to correct the issue: controlplane ~ ➜ vi deployment-definition-1.yaml Update the Deployment YAML The error is caused by incorrect casing in the kind field. The resource kind should be capitalized. Use the corrected YAML below: apiVersion: apps/v1
kind: Deployment
metadata:
  name: deployment-1
spec:
  replicas: 2
  selector:
    matchLabels:
      name: busybox-pod
  template:
    metadata:
      labels:
        name: busybox-pod
    spec:
      containers:
      - name: busybox-container
        image: busybox888
        command:
        - sh
        - ""-c""
        - echo Hello Kubernetes! && sleep 3600 After saving the changes, create the deployment again: controlplane ~ ➜ kubectl create -f deployment-definition-1.yaml Step 4: Create a Deployment Using Command-Line Parameters You can also create a deployment by specifying parameters directly in the command line. First, review the help for deployment creation: kubectl create deployment --help For example, to create a deployment named http-frontend using a specified image with three replicas, run: kubectl create deployment http-frontend --image=<your-image> --replicas=3 After executing the command, verify that the deployment and pods are running as expected: kubectl get deployments Make sure that the http-frontend deployment reflects the desired number of replicas in the ready state. Final Validation To ensure all deployments are functioning, validate by checking that all created deployments are running and the pods are in a ready state. This confirms that your environment is correctly configured and operational. This concludes the lab article on Kubernetes deployments. In upcoming labs, you’ll explore additional Kubernetes functionalities to further enhance your skills. Links and References Kubernetes Documentation Kubernetes Basics Docker Hub Terraform Registry Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,A Quick Reminder,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Core-Concepts/A-Quick-Reminder,"Certified Kubernetes Application Developer - CKAD Core Concepts A Quick Reminder Welcome to our course at KodeKloud! This article is a reminder to stay focused on the hands-on labs and detailed lessons provided in this training. Although you may be eager to experiment with your own local environment, we recommend following the structured labs and video tutorials until the designated point in the course. Stay on Track By concentrating on the provided course materials, you eliminate distractions and follow a clear, step-by-step learning path. This approach ensures you develop the essential skills required for success. Below is an example command to display all available Docker images on your system: $ docker images
REPOSITORY                       TAG       SIZE
redis                            latest    105MB
ubuntu                           latest    72.7MB
mysql                            latest    556MB
nginx                            latest    22.6MB
alpine                           latest    5.61MB
nginx                            alpine    133MB
postgres                         latest    314MB
kodekloud/simple-webapp-mysql    latest    96.6MB
kodekloud/simple-webapp          latest    84.8MB Once you are comfortable with the foundational content, you will receive resources to help set up and customize your local development environment. For now, focus on following along with the labs. For example, run the Redis container from our labs using the following command: $ docker run redis Thank you for choosing KodeKloud for your learning journey. We are excited to support you as you build your skills and progress towards your goals. For additional documentation and resources, consider exploring: Docker Documentation Kubernetes Basics Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solution Namespaces optional,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Core-Concepts/Solution-Namespaces-optional,"Certified Kubernetes Application Developer - CKAD Core Concepts Solution Namespaces optional In this article, we present a step-by-step guide to complete the Kubernetes lab exercise on namespaces. Follow along as we cover tasks ranging from identifying namespaces to configuring inter-namespace communication for applications and services. Identifying the Number of Namespaces To begin, determine how many namespaces exist on the system. Run the following command to list all namespaces: kubectl get namespaces Alternatively, you can use the shorthand version: kubectl get ns For example, you might see an output similar to: NAME                     STATUS    AGE
default                  Active    6m50s
kube-system              Active    6m49s
kube-public              Active    6m49s
kube-node-lease          Active    6m49s
finance                  Active    27s
marketing                Active    27s
dev                      Active    27s
prod                     Active    27s
manufacturing            Active    27s
research                 Active    27s This output indicates that there are 10 namespaces available. Listing Pods in a Specific Namespace Next, check the number of pods running within the research namespace. Use one of the following commands: kubectl get pods --namespace=research Or with the shorthand option: kubectl get pods -n research The expected outcome should display that there are two pods in the research namespace. Creating a Pod in the Finance Namespace To create a pod in the finance namespace, you can use the kubectl run command with the appropriate namespace flag. For instance, running the following command without specifying a namespace creates the pod in the default namespace: kubectl run redis --image=redis To instead create the pod in the finance namespace, include the -n flag as shown below: kubectl run redis --image=redis -n finance After executing this command, verify the pod creation by listing pods in the finance namespace: kubectl get pods -n finance A sample output might resemble: NAME      READY   STATUS             RESTARTS   AGE
payroll   1/1     Running            0          2m20s
redis     0/1     ContainerCreating  0          8s Determining the Namespace of the Blue Pod To identify which namespace contains the blue pod, you can either inspect pods in each namespace individually or use the --all-namespaces flag to list every pod cluster-wide. First, inspect a specific namespace using: kubectl get pods -n finance And list all namespaces with: kubectl get ns For a more efficient search, list pods across all namespaces: kubectl get pods --all-namespaces A sample output might look like this: NAMESPACE      NAME                                                       READY   STATUS              RESTARTS   AGE
kube-system    local-path-provisioner-6c79684f77-j5qx                      1/1     Running             0          9m5s
kube-system    corends-5788995cd-bkj56                                      1/1     Running             0          9m5s
kube-system    helm-install-traefik-crd-d2f67                               0/1     Completed           0          9m7s
kube-system    metrics-server-7cd5fc6b67-kdgzb                                1/1     Running             0          9m5s
kube-system    helm-install-traefik-c987s                                   1/1     Running             0          9m5s
kube-system    svcvlc-traefik-jptsh                                          2/2     Running             0          8m30s
kube-system    traefik-6bb96f9fbd8-w9zxr                                     1/1     Running             0          3m3s
marketing      redis-db                                                   1/1     Running             0          3m3s
dev            red-app                                                    1/1     Running             0          3m3s
finance        payroll                                                    1/1     Running             0          3m3s
marketing      blue                                                       0/1     CrashLoopBackOff    4 (80s ago) 3m3s
research       dna-1                                                      1/1     Running             0          3m3s
research       dna-2                                                      0/1     CrashLoopBackOff    4 (70s ago) 3m3s
finance        redis                                                      1/1     Running             0          51s From this output, notice that the blue pod is located in the marketing namespace. Accessing the Blue Application and Its Database Service The blue application is a simple web-based tool used for connectivity tests. When launched from the terminal interface, it opens up a browser window that displays the application. The next step is to determine which DNS name the blue application should use to access the database (DB) service within its own namespace. To verify, first list the pods and services in the marketing namespace. List the pods: kubectl get pods -n marketing Sample output: NAME       READY   STATUS    RESTARTS   AGE
redis-db   1/1     Running   0          4m14s
blue       1/1     Running   0          4m14s Then, list the services in the same namespace: kubectl get svc -n marketing Sample output: NAME          TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)           AGE
blue-service  NodePort   10.43.82.162   <none>        8080:30082/TCP    4m27s
db-service    NodePort   10.43.134.33   <none>        6379:30758/TCP    4m27s Note: DNS Access When connecting to services within the same namespace, referencing the service by its name (e.g., ""db-service"") is sufficient due to Kubernetes' internal DNS resolution. After configuring the connection, verify that the connectivity test confirms the correct host name and port settings. Accessing the DB Service in a Different Namespace Now, consider how to access the DB service when it is located in a different namespace from the blue application. Assume that there is a service named ""db-service"" in both the marketing and dev namespaces. First, verify the services in these namespaces. In the marketing namespace: kubectl get svc -n marketing Output example: NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)         AGE
blue-service NodePort    10.43.82.162   <none>        8080:30082/TCP  4m27s
db-service   NodePort    10.43.134.33   <none>        6379:30758/TCP  4m27s In the dev namespace: kubectl get svc -n dev Output example: NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
db-service   ClusterIP   10.43.252.9    <none>        6379/TCP   5m22s To access the DB service in the dev namespace from an application situated in another namespace, use the fully qualified domain name (FQDN) that follows this format: db-service.dev.svc.cluster.local This format includes the service name, the namespace, and the cluster domain, ensuring accurate DNS resolution across namespaces. Tip for Inter-Namespace Communication: Using the FQDN is essential when accessing services from a different namespace. This ensures clear resolution, especially in clusters with multiple services sharing the same name. Testing the connection with this FQDN should confirm that the blue application can successfully communicate with the DB service in the dev namespace. At this point, you have successfully completed all the lab tasks related to Kubernetes namespaces. Right. That's the end of the lab. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Environment Variables,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Environment-Variables,"Certified Kubernetes Application Developer - CKAD Configuration Environment Variables In this article, we explain how to set environment variables in a Kubernetes Pod. Environment variables can be defined directly in your Pod specification or managed externally using ConfigMaps and Secrets, offering flexibility for different deployment scenarios. Directly Defining Environment Variables When creating a Pod, you can directly assign environment variables using the env property in the container specification. The env property is an array where each variable is defined with a name and value . For example, if you would run a Docker container with an environment variable like this: docker run -e APP_COLOR=pink simple-webapp-color you can define the same variable in your Kubernetes Pod manifest as follows: apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
    - containerPort: 8080
    env:
    - name: APP_COLOR
      value: pink In this YAML configuration, the environment variable APP_COLOR is directly set to pink . Note Direct assignment is a straightforward approach, ideal for simple scenarios or development environments. Managing Environment Variables with ConfigMaps and Secrets For more dynamic configuration management, you can externalize environment variable data using ConfigMaps or Secrets. Instead of hardcoding a value, you reference the value using the valueFrom field. This helps decouple configuration from application code and allows a more secure and manageable configuration. Consider the previous direct definition: env:
- name: APP_COLOR
  value: pink You can modify it to use a ConfigMap like this: env:
- name: APP_COLOR
  valueFrom:
    configMapKeyRef:
      name: my-config
      key: app_color In this configuration, Kubernetes retrieves the value for APP_COLOR from a ConfigMap named my-config instead of hardcoding it in the manifest. Tip Using ConfigMaps or Secrets is recommended in production environments to manage sensitive data and configuration changes without modifying the application code. Comparison of Environment Variable Methods Method Description Use Case Direct Assignment Environment variable is hardcoded in the Pod manifest. Simple setups or development. ConfigMap Reference Environment variable value is sourced from a ConfigMap. Dynamic configuration management. Secret Reference Environment variable value is sourced from a Secret. Managing sensitive data securely. Conclusion Managing environment variables effectively is essential for successful Kubernetes deployments. Whether you use direct assignments or manage them via ConfigMaps and Secrets, Kubernetes provides the flexibility to suit your application's needs. That's it for this article. For more information on managing configurations in Kubernetes, visit Kubernetes Documentation . Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Commands and Arguments in Kubernetes,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Commands-and-Arguments-in-Kubernetes,"Certified Kubernetes Application Developer - CKAD Configuration Commands and Arguments in Kubernetes Welcome to this lesson on handling commands and arguments within a Kubernetes pod. In this guide, you'll learn how to customize container behavior by overriding default settings defined in your Docker image. Previously, we built a simple Docker image called ""ubuntu-sleeper"" that pauses execution (sleeps) for a specified number of seconds. By default, running: docker run ubuntu-sleeper makes the container sleep for five seconds. However, you can change this behavior by providing a command-line argument. For example, running: docker run --name ubuntu-sleeper ubuntu-sleeper 10 will cause the container to sleep for 10 seconds. Overriding Default Arguments in a Pod Definition Kubernetes allows you to replicate the Docker behavior of passing command-line arguments by using the args field in a pod definition. When you specify additional arguments in a Kubernetes pod, they are supplied as an array. Consider this pod definition template. In the example below, the pod runs the ""ubuntu-sleeper"" container and overrides the default sleep duration by setting the args field to [""10""] : apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-pod
spec:
  containers:
    - name: ubuntu-sleeper
      image: ubuntu-sleeper
      args: [""10""] To create the pod, execute: kubectl create -f pod-definition.yml When the pod starts, it creates a container from the specified image. The args field in the Kubernetes definition effectively overrides the default CMD instruction defined in the Dockerfile. Note In Kubernetes, you can manipulate container behavior at startup by tweaking the pod specification. Always ensure your YAML syntax is valid to avoid deployment issues. Overriding the Entrypoint In our Dockerfile, we defined an entry point and a CMD instruction as follows: FROM ubuntu
ENTRYPOINT [""sleep""]
CMD [""5""] Typically, when you run the container, the entry point sleep is combined with the CMD default 5 . To override the entry point in Docker, you would use the --entrypoint flag, for instance: docker run --name ubuntu-sleeper --entrypoint sleep2.0 ubuntu-sleeper 10 In Kubernetes, you achieve the same result by using the command field in the pod definition. Specifically, the command field replaces the ENTRYPOINT from the Dockerfile, and the args field continues to override the CMD instruction. Below is an example demonstrating how to set both fields: apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-pod
spec:
  containers:
    - name: ubuntu-sleeper
      image: ubuntu-sleeper
      command: [""sleep2.0""]
      args: [""10""] To deploy this pod, run: kubectl create -f pod-definition.yml Note Using the command and args fields in tandem gives you full control over the container's startup process, allowing you to override both the ENTRYPOINT and CMD as needed. Summary There are two primary fields in a Kubernetes pod definition that correspond to your Dockerfile settings: Field Dockerfile Equivalent Purpose command ENTRYPOINT Overrides the default entry point of the image args CMD Replaces the default arguments passed to the entry point By correctly configuring these fields, you can modify the startup parameters of your container dynamically. This capability is particularly useful for customizing application behavior in different environments. Review the provided exercises to practice configuring and troubleshooting commands and arguments in Kubernetes, and enhance your deployment strategies effectively. For additional resources, refer to Kubernetes Documentation and Kubernetes Basics . Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solution Commands and Arguments Optional,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Solution-Commands-and-Arguments-Optional,"Certified Kubernetes Application Developer - CKAD Configuration Solution Commands and Arguments Optional In this lesson, we will explore several Kubernetes tasks related to pods, commands, and arguments while working with Dockerfiles. The content follows step-by-step lab instructions, and each technical diagram is kept with its associated explanation. 1. Examining a Pod's Command and Arguments Begin by checking how many pods are currently running. In our example, there is a single pod. Next, inspect the pod named ubuntu-sleeper to reveal the command it is executing. According to its definition, the container uses the Ubuntu image and runs the command: sleep 4800 To display these details, run: kubectl describe pod ubuntu-sleeper The output will include the following information: Command: sleep Args: 4800 This confirms that the pod executes sleep 4800 . 2. Creating a Pod That Sleeps for 5000 Seconds To modify an existing pod definition ( ubuntu-sleeper-2.yaml ), update it so that the container using the Ubuntu image sleeps for 5000 seconds. An initial basic file may look like this: apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-2
spec:
  containers:
    - name: ubuntu
      image: ubuntu You can instruct the container to sleep for 5000 seconds using one of two options. Both methods are valid, but in this guide we use the first option. Option 1: Single Command Array Specify both the command and its argument in one array: apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-2
spec:
  containers:
    - name: ubuntu
      image: ubuntu
      command: [""sleep"", ""5000""] Option 2: Separate Command and Args Alternatively, define the command and arguments separately: apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-2
spec:
  containers:
    - name: ubuntu
      image: ubuntu
      command: [""sleep""]
      args: [""5000""] After creating this pod, verify that it is running and that the command and argument have been set to 5000. 3. Troubleshooting an Incorrect Pod Manifest Consider a third file, ubuntu-sleeper-3.yaml , intended to create a pod that runs sleep 1200 . The file is as follows: apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper-3
spec:
  containers:
    - name: ubuntu
      image: ubuntu
      command:
        - ""sleep""
        - ""1200"" Even though the file appears correctly formatted, Kubernetes may return an error like: Error ""cannot unmarshal number into the Go struct field Container.command of type string."" This error indicates that every element in the command array must be a string. Ensure both ""sleep"" and ""1200"" are enclosed in quotes. Use the below command to create the pod: kubectl create -f ubuntu-sleeper-3.yaml Any deviation from quoted strings will trigger an error, reinforcing the importance of proper YAML syntax. 4. Updating a Pod That Is Not Directly Editable Suppose you need to update the ubuntu-sleeper-3 pod, altering its sleep duration from 1200 seconds to 2000 seconds. Directly editing a running pod using a command like: kubectl edit pod ubuntu-sleeper-3 may trigger an error because Kubernetes restricts modifications of most pod fields once created. Best Practice The recommended approach is to update your local pod manifest file and then perform a forced replace. To force an update, use: kubectl replace --force -f /tmp/kubectl-edit-2693604347.yaml This command deletes the existing pod and creates a new one with the updated specifications. Note that termination may take some time as the container handles the kill signal. 5. Analyzing Dockerfiles and Container Startup Commands Understanding Dockerfiles is crucial. Below are two examples that illustrate how the ENTRYPOINT and CMD instructions interact. Dockerfile 1 Located at /root/webapp-color/Dockerfile , this file contains: FROM python:3.6-alpine
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT [""python"", ""app.py""] This Dockerfile instructs the container to run python app.py on startup. Dockerfile 2 The second file, /root/webapp-color/Dockerfile2 , uses a CMD instruction along with the ENTRYPOINT : FROM python:3.6-alpine
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT [""python"", ""app.py""]
CMD [""--color"", ""red""] When a container is created from this image, it executes the entrypoint ( python app.py ) followed by the CMD arguments ( --color red ), resulting in: python app.py --color red 6. Overriding Commands and Arguments in a Pod Manifest In the webapp-color-2 directory, there are two files: Dockerfile2: FROM python:3.6-alpine
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT [""python"", ""app.py""]
CMD [""--color"", ""red""] webapp-color-pod.yaml: apiVersion: v1
kind: Pod
metadata:
  name: webapp-green
  labels:
    name: webapp-green
spec:
  containers:
    - name: simple-webapp
      image: kodekloud/webapp-color
      command: [""--color"", ""green""] Despite the Dockerfile specifying an ENTRYPOINT and CMD , the pod manifest overrides these settings by providing a new command. As a consequence, the container starts with the arguments --color green and does not execute the original entrypoint command. Overriding ENTRYPOINT To override the ENTRYPOINT, you must use the --command flag or specify a new command array that includes the entire command. 7. Combining Command and Arguments in a Pod Manifest In the webapp-color-3 directory, similar files are used: Dockerfile2: FROM python:3.6-alpine
RUN pip install flask
COPY . /opt/
EXPOSE 8080
WORKDIR /opt
ENTRYPOINT [""python"", ""app.py""]
CMD [""--color"", ""red""] webapp-color-pod-2.yaml: apiVersion: v1
kind: Pod
metadata:
  name: webapp-green
  labels:
    name: webapp-green
spec:
  containers:
    - name: simple-webapp
      image: kodekcloud/webapp-color
      command: [""python"", ""app.py""]
      args: [""--color"", ""pink""] Here, the pod manifest explicitly sets both the command and the arguments. The actual command executed by the container will be: python app.py --color pink This demonstrates the flexibility Kubernetes offers when combining command and argument settings. 8. Creating a Pod with Overridden Application Arguments The final task involves creating a pod using the image kodekloud/webapp-color . By default, this image displays a blue background, but we want to override its startup arguments to display green. Running the pod without overrides uses the default CMD: kubectl run webapp-green --image=kodekloud/webapp-color To override the default arguments and pass --color=green , separate the options for kubectl from those passed to the container using a double dash ( -- ): kubectl run webapp-green --image=kodekloud/webapp-color -- --color=green This command instructs Kubernetes to use the image’s default entrypoint ( python app.py ) and pass the --color=green argument to the container. Verify that the pod runs with the updated settings, resulting in a green background. Conclusion This lesson has covered several important practices in Kubernetes: Inspecting pod commands and arguments. Defining pod manifests with command arrays. Understanding the distinctions between ENTRYPOINT and CMD in a Dockerfile. Overriding default commands and arguments at runtime using a pod manifest. Employing the double dash ( -- ) to separate kubectl options from container arguments. Happy learning and keep exploring Kubernetes! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Security Contexts,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Security-Contexts,"Certified Kubernetes Application Developer - CKAD Configuration Security Contexts Hello and welcome to this lesson on security contexts in Kubernetes. My name is Mumshad Mannambeth, and in this guide, I'll walk you through how Kubernetes manages security contexts. Previously, we explored Docker container security, where you can define user IDs and modify Linux capabilities for your containers. Kubernetes extends this capability, allowing you to configure similar security settings. Docker vs. Kubernetes Security Context In Docker, you may run containers with security options like these: docker run --user=1001 ubuntu sleep 3600
docker run --cap-add MAC_ADMIN ubuntu In Kubernetes, containers run within Pods. You have the flexibility to set security contexts either at the container level or at the Pod level. Settings defined at the Pod level affect all containers in that Pod. However, if the same security context options are specified for both the Pod and individual containers, the container-level settings override those at the Pod level. Security Hierarchy Security settings specified at the container level have a higher precedence than those set at the Pod level. Always verify your configuration to ensure the intended security policies are applied. Example Pod Definition Consider the following example of a Pod definition file. In this configuration, an Ubuntu container is started with the sleep command. The security context is defined within the container specification using the securityContext field. Here, the runAsUser parameter sets the user ID for the container, and the capabilities option adds specific Linux capabilities: apiVersion: v1
kind: Pod
metadata:
  name: web-pod
spec:
  containers:
    - name: ubuntu
      image: ubuntu
      command: [""sleep"", ""3600""]
      securityContext:
        runAsUser: 1000
        capabilities:
          add: [""MAC_ADMIN""] This example illustrates how to configure user permissions and capabilities in Kubernetes. Take some time to practice viewing, configuring, and troubleshooting security context issues using this configuration. Next Steps After you experiment with this configuration, explore how to integrate more advanced security policies across multiple Pods and clusters. Delving deeper into Kubernetes security will strengthen your operational best practices. That's it for now—I look forward to seeing you in the next lesson! Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Pre requisite Security in Docker,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Pre-requisite-Security-in-Docker,"Certified Kubernetes Application Developer - CKAD Configuration Pre requisite Security in Docker Welcome to this comprehensive guide on Docker security. In this article, we explore key Docker security concepts that form the basis for understanding security contexts in Kubernetes. If you are already comfortable with Docker security, feel free to move on to the next sections. Overview of Docker Process Isolation Imagine a host machine running Docker with multiple processes such as OS services, the Docker daemon, and an SSH server. When you launch an Ubuntu container that executes a sleep command for one hour, you run: docker run ubuntu sleep 3600 Containers are lightweight because, unlike virtual machines, they share the host’s kernel. They achieve isolation through Linux namespaces. Each container gets its own isolated namespace, so checking processes within the container might show the sleep process with a PID of 1. However, on the host, the process appears with a different PID. For example, running: ps aux might yield: USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.0  0.0   4528   828 ?        Ss   03:06   0:00 sleep 3600 Here, even though the sleep process is isolated within the container, it is visible on the host but with a different PID. A more detailed host process listing could look like: ps aux
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
project   3720  0.1  0.1  95500  4916 ?        R     06:06   0:00 sshd: project@pts/0
project   3725  0.0  0.1  95196  4132 ?        S     06:06   0:00 sshd: project@notty
project   3727  0.2  0.1  21352  5340 pts/0    Ss    06:06   0:00 -bash
root      3802  0.0  0.0  8924  3616 ?        Sl    06:06   0:00 docker-containerd-shim --namespace m
root      3816  1.0  0.0  4528  828 ?        Ss    06:06   0:00 sleep 3600 Note The difference in PID values between the container and the host is due to the isolation provided by Linux namespaces. User Context and Privilege Management By default, Docker runs container processes as the root user. On the host, both container and host processes might appear to run as root. For example: ps aux
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root          1  0.0  0.0   4528   828 ?        Ss    03:06   0:00 sleep 3600 If you prefer a non-root execution within a container, specify a user ID using the --user flag. To run as user ID 1000, execute: docker run --user=1000 ubuntu sleep 3600 After running this command, checking the process list on the host will show the container process running as user 1000: ps aux
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
1000          1  0.0  0.0  4528   828 ?        Ss    03:06   0:00 sleep 3600 Alternatively, you can set a default non-root user within your Docker image. For instance, create a Dockerfile for a custom Ubuntu image: FROM ubuntu
USER 1000 Build and run your custom image: docker build -t my-ubuntu-image .
docker run my-ubuntu-image sleep 3600 Verifying the process confirms that it runs under user 1000: ps aux
USER         PID %CPU %MEM     VSZ   RSS TTY      STAT START   TIME COMMAND
1000          1  0.0  0.0   4528    828 ?        Ss   03:06   0:00 sleep 3600 Tip Defining the default user in the Dockerfile removes the need to specify the user each time the container runs. Docker Root User and Linux Capabilities A common question concerns whether the root user inside a container has the same privileges as the host's root user. By design, Docker restricts the container's root privileges using Linux capabilities. On a standard Linux system, the root user can modify file permissions, control network ports, manage processes, reboot the system, and perform various critical tasks. In contrast, Docker limits these actions within containers. This means that a container's root user cannot, for example, reboot the host or impact other containers. The diagram below illustrates some of the key Linux capabilities (like CHOWN, KILL, and NET_ADMIN) available to a process: Docker provides runtime flags to customize these capabilities. You can add a capability with the --cap-add option: docker run --cap-add MAC_ADMIN ubuntu To remove specific capabilities, use --cap-drop. If you need to run a container with full privileges, the --privileged flag is available, though it should be used with caution. Warning Avoid using the --privileged flag in production environments unless absolutely necessary, as it elevates container privileges significantly. Conclusion This article reviewed essential Docker security concepts, including: Process isolation using Linux namespaces User context and privilege management Limiting root user actions through Linux capabilities These principles are crucial for understanding similar security mechanisms in Kubernetes. For further reading, explore Kubernetes Basics . Thank you for reading this guide on Docker security. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Secrets,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Secrets,"Certified Kubernetes Application Developer - CKAD Configuration Secrets Welcome to this comprehensive guide on managing Secrets in Kubernetes. In this article, we explain how to securely handle sensitive data for your applications. We begin by reviewing a Python web application that connects to a MySQL database. Upon a successful connection, the application displays a success message. However, the application currently contains hardcoded values for the hostname, username, and password. While moving configuration data to a ConfigMap is suitable for non-sensitive information, it is not recommended for handling passwords and other sensitive data. Below is an excerpt from the Python application: import os
from flask import Flask, render_template
import mysql.connector

app = Flask(__name__)

@app.route(""/"")
def main():
    mysql.connector.connect(host=""mysql"", database=""mysql"",
                              user=""root"", password=""paswrd"")
    return render_template('hello.html', color=fetchcolor())

if __name__ == ""__main__"":
    app.run(host=""0.0.0.0"", port=""8080"") Storing Sensitive Data: ConfigMaps vs Secrets While ConfigMaps are a great option for storing configuration data in plain text, they are not designed to keep passwords or keys secure. This is where Secrets come in. Kubernetes Secrets store sensitive information in an encoded format, making them more secure than using ConfigMaps for these purposes. Here is an example of a ConfigMap definition: apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data: Why Use Secrets? Storing passwords in plain text—even in a ConfigMap—exposes them to potential security risks. Using Secrets ensures that sensitive data is handled more securely. Working with Secrets Similar to ConfigMaps, handling Secrets typically involves two steps: Creating the Secret. Injecting the Secret into a Pod. Below are some sample encoded values corresponding to your application's configuration: DB_Host: bXlzcWw=
DB_User: cm9vdA==
DB_Password: cGFzd3Jk And here is the plain text corresponding to those encoded values: DB Host: mysql
DB User: root
DB Password: paswrd There are two primary methods for creating a Secret: the imperative approach and the declarative approach. Imperative Method The imperative method allows you to create a Secret directly from the command line without a definition file. For example: kubectl create secret generic <secret-name> --from-literal=<key>=<value> A practical example would be: kubectl create secret generic \
  app-secret --from-literal=DB_Host=mysql \
  --from-literal=DB_User=root \
  --from-literal=DB_Password=paswrd Alternatively, you can create a Secret from a file: kubectl create secret generic <secret-name> --from-file=<path-to-file> For instance: kubectl create secret generic \
  app-secret --from-file=app_secret.properties If you need to include multiple key-value pairs, use additional --from-literal options. For larger datasets, creating the Secret from a file might be more efficient. Declarative Method The declarative approach leverages a definition file to create a Secret, similar to how ConfigMaps are defined. A typical Secret definition includes the API version, kind, metadata, and data fields. Note that sensitive values should always be encoded in base64. Below is an example of a Secret definition file. Although the values appear in plain text here for demonstration, they must be encoded for production use: apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data:
  DB_Host: mysql
  DB_User: root
  DB_Password: paswrd You can then create the Secret using: kubectl create -f secret-data.yaml To encode data on a Linux host, you can use the base64 command. For example: echo -n 'mysql' | base64
# Output: bXlzcWw=
echo -n 'root' | base64
# Output: cm9vdA==
echo -n 'paswrd' | base64
# Output: cGFzd3Jk Viewing and Decoding Secrets To list all Secrets in your cluster, run: kubectl get secrets Example output: NAME          TYPE     DATA   AGE
app-secret    Opaque   3      10m For detailed information about a specific Secret, use: kubectl describe secrets This command displays the Secret's attributes without revealing the actual sensitive data. To inspect the encoded values, execute: kubectl get secret app-secret -o yaml To decode an encoded value, for example: echo -n 'bXlzcWw=' | base64 --decode
echo -n 'cm9vdA==' | base64 --decode
# Output: root Injecting Secrets into Pods Once you have created your Secret, you can inject its data into a Pod. There are two common methods for this: as environment variables or by mounting them as files in a volume. As Environment Variables Below is an example of a pod definition that imports the Secret as environment variables: apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
  labels:
    name: simple-webapp-color
spec:
  containers:
    - name: simple-webapp-color
      image: simple-webapp-color
      ports:
        - containerPort: 8080
      envFrom:
        - secretRef:
            name: app-secret And here is the corresponding Secret definition with properly encoded data: apiVersion: v1
kind: Secret
metadata:
  name: app-secret
data:
  DB_Host: bXlzcWw=
  DB_User: cm9vdA==
  DB_Password: cGFzd3Jk When the pod is created, the Secret's data will be available to the container as environment variables. As Mounted Volumes Another approach involves mounting Secrets as files within a Pod. When mounted, each key in the Secret becomes a file, and its content is the corresponding decoded value. For example: volumes:
  - name: app-secret-volume
    secret:
      secretName: app-secret Listing the mounted volume may reveal files like: ls /opt/app-secret-volumes
# Output: DB_Host  DB_Password  DB_User And to view the database password: cat /opt/app-secret-volumes/DB_Password
# Output: paswrd Important Considerations When Working with Secrets Secrets are encoded in base64, not encrypted. Anyone with access to the Secret object can decode the sensitive data. Avoid version controlling your secret definition files to prevent accidental exposure. By default, Secrets stored in etcd are not encrypted. Consider enabling encryption at rest in your Kubernetes cluster. Security Reminder Ensure that secrets in etcd are properly encrypted and that access is restricted using Role-Based Access Control (RBAC). Do not expose your secret files in public repositories. For example, here is an encryption configuration file that secures Secrets along with other resources: apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
      - secrets
    providers:
      - identity: {}
      - aesgcm:
          keys:
            - name: key1
              secret: C2VjcmVjT0glZzIHNLY3VyZQ==
            - name: key2
              secret: dGhpcpyBcyBwYXNzd29yZA==
      - aescbc:
          keys:
            - name: key1
              secret: C2VjcmVjT0glZzIHNLY3VyZQ==
            - name: key2
              secret: dGhpcpyBcyBwYXNzd29yZA==
      - secretbox:
          keys:
            - name: key1
              secret: YWjjZGVnZ2hpamtsbW5vcHyc3R1nd4eXoXMjM0NTY= You must pass this file to the Kubernetes API server. Here is an example of modifying the kube-apiserver pod configuration to make use of the encryption configuration: apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeddm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.10.30.4:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
    - command:
        - kube-apiserver
        # ...
        - --encryption-provider-config=/etc/kubernetes/enc/enc.yaml # <--- add this line
  volumeMounts:
    # ...
    - name: enc
      mountPath: /etc/kubernetes/enc
      readOnly: true # <--- add this line
  volumes:
    # ...
    - name: enc
      hostPath:
        path: /etc/kubernetes/enc
        type: DirectoryOrCreate # <--- add this line This setup ensures that Secrets stored in etcd are encrypted. Note that only users with the correct permissions can create pods or deployments that have access to these Secrets. It is important to enforce proper RBAC policies. Additionally, consider leveraging third-party secret management providers such as AWS, Azure, GCP, or HashiCorp Vault. These external providers help in storing secrets securely outside of etcd and enforce robust security measures. For further details on advanced secret management and external providers, check out the Certified Kubernetes Security Specialist (CKS) course. That concludes our guide on Kubernetes Secrets. Now, head over to the labs and practice managing Secrets to enhance your security skills in a Kubernetes environment. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Resource Requirements,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Resource-Requirements,"Certified Kubernetes Application Developer - CKAD Configuration Resource Requirements Understanding resource requirements is crucial for effectively managing a Kubernetes cluster. In this guide, we explore how resource allocation works in a three-node Kubernetes setup and demonstrate best practices for defining and enforcing CPU and memory constraints. Each node in the cluster has a fixed amount of CPU and memory, and every pod scheduled on those nodes consumes a portion of these resources. For instance, if a pod requires two CPUs and one memory unit, the scheduler evaluates each node’s available resources to determine the best fit. In our scenario, the scheduler selects node 2 because it meets the pod's resource demands. If none of the nodes have sufficient resources, the pod will remain in a pending state. To troubleshoot such cases, use: kubectl describe pod <pod-name> This command can reveal issues like insufficient CPU availability. Defining Resource Requests A resource request defines the minimum amount of CPU or memory a container requires to run. By specifying these requests in the pod configuration, you ensure that the node selected has the necessary resources. For example, the following YAML configuration sets a pod to request 4 Gi of memory and 2 CPUs: apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
  labels:
    name: simple-webapp-color
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
    - containerPort: 8080
    resources:
      requests:
        memory: ""4Gi""
        cpu: ""2"" This setup guarantees that the node must have at least the requested resources available before scheduling the pod. Note that CPU values can be fractional (e.g., 0.1 or 100m, where ""m"" stands for milli). One CPU represents one vCPU, making it equivalent to 1 AWS vCPU, 1 GCP core, 1 Azure core, or 1 hyper-thread on other platforms. Setting Resource Limits By default, containers aren’t restricted in the amount of resources they can consume. For example, a container might start with 1 CPU but later use all available CPU resources on the node. To avoid resource contention among containers or processes, you can set resource limits. Below is an example of a pod configuration that restricts a container’s CPU to 2 vCPUs and its memory to 2 Gi: apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
  labels:
    name: simple-webapp-color
spec:
  containers:
  - name: simple-webapp-color
    image: simple-webapp-color
    ports:
    - containerPort: 8080
    resources:
      requests:
        memory: ""1Gi""
        cpu: ""1""
      limits:
        memory: ""2Gi""
        cpu: ""2"" With these limits, the container is throttled if it attempts to use more than 2 vCPUs. Conversely, if a container consistently exceeds its memory limit, it will be terminated, often resulting in an Out Of Memory (OOM) error. Warning Exceeding memory limits can cause a pod to be terminated unexpectedly. Always ensure that resource limits are properly calibrated to avoid OOM errors. Remember that resource requests and limits are set for each container individually. If a pod runs multiple containers, each must have its own resource configuration. Managing Resource Allocation: Key Scenarios A few different scenarios can occur depending on how resource requests and limits are configured: Scenario Description Behavior No resource requests or limits The container has the potential to consume all available CPU or memory on a node. May starve other containers or applications. No requests but limits specified Kubernetes treats the specified limits as the default resource requests. Each container is guaranteed the defined limit, without the ability to exceed it. Both requests and limits set Containers receive the requested resources but can use up to the limit if additional resources are available. Ensures efficient allocation while allowing flexibility for burst usage. Only requests set (no limits) The container is guaranteed the requested resources, but can consume any additional available resources from the node. Risk of overconsumption may occur if no limits are enforced. CPU overconsumption can be mitigated through throttling, but memory usage cannot be throttled. Instead, containers that exceed their memory limits will be terminated. Memory management follows similar patterns: Without resource requests or limits, a container might use all available memory. Specifying only memory limits implies that Kubernetes will use these limits as the default minimum allocation. Setting both requests and limits guarantees that a container receives its requested memory while capping its maximum usage. Defining only resource requests ensures a guaranteed memory allocation, but excessive usage could lead to container termination, since memory cannot be reallocated on the fly. Enforcing Defaults with LimitRanges By default, Kubernetes does not automatically apply resource requests or limits unless explicitly specified in the pod definition. To enforce default settings, use LimitRange resources at the namespace level. Below is an example that applies a default CPU constraint: apiVersion: v1
kind: LimitRange
metadata:
  name: cpu-resource-constraint
spec:
  limits:
  - type: Container
    default: 
      cpu: ""500m""
    defaultRequest:
      cpu: ""500m""
    max:
      cpu: ""1""
    min:
      cpu: ""100m"" Similarly, you can enforce memory defaults with a LimitRange: # limit-range-memory.yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: memory-resource-constraint
spec:
  limits:
  - type: Container
    default:
      memory: ""1Gi""
    defaultRequest:
      memory: ""1Gi""
    max:
      memory: ""1Gi""
    min:
      memory: ""500Mi"" These LimitRanges ensure that any container created without specific resource settings receives the defined default values. Note that updates to LimitRange configurations impact new or updated pods only. Using ResourceQuotas To control the overall resource usage within a namespace, you can implement ResourceQuotas. A ResourceQuota object sets hard limits on the total CPU and memory consumption (both requested and actual usage) across all pods in the namespace. For example, a ResourceQuota might restrict the namespace to a total of 4 CPUs and 4 Gi of memory in requests while enforcing a maximum usage of 10 CPUs and 10 Gi of memory across all pods. This mechanism is essential for preventing uncontrolled resource consumption in multi-tenant environments. Conclusion In this guide, we detailed how Kubernetes manages resource requests and limits, ensuring proper resource allocation and preventing overuse. We covered pod scheduling based on resource availability, the importance of setting appropriate resource requests and limits, and how LimitRanges and ResourceQuotas help maintain overall cluster stability. For further insights on Kubernetes resource management, refer to the official Kubernetes documentation on managing CPU, memory, and API resources. Next Steps Continue exploring hands-on labs to reinforce these resource management fundamentals and optimize your cluster’s performance. Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Demo Encrypting Secret Data at Rest,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Demo-Encrypting-Secret-Data-at-Rest,"Certified Kubernetes Application Developer - CKAD Configuration Demo Encrypting Secret Data at Rest In this guide, you will learn how to encrypt secret data at rest in Kubernetes. Based on the official Kubernetes documentation , this tutorial walks you through the storage of secret objects, inspecting them in etcd, and configuring encryption at rest to secure sensitive data. In the beginning, launch a Kubernetes playground running a single-node cluster based on Kubernetes and ContainerD. Once the playground is up, open the terminal. Creating a Secret Object Kubernetes secrets help to store sensitive data such as passwords, tokens, or keys. There are multiple methods to create a secret object, including from files, literals, or environment variable files. Below are some examples: # Create a new secret named ""my-secret"" from files in folder ""bar""
kubectl create secret generic my-secret --from-file=path/to/bar

# Create a secret with specified keys from files rather than using disk filenames as keys
kubectl create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-file=ssh-publickey=path/to/id_rsa.pub

# Create a secret with literal key-value pairs
kubectl create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret

# Create a secret using a combination of a file and a literal value
kubectl create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-literal=passphrase=topsecret

# Create a secret from environment variable files
kubectl create secret generic my-secret --from-env-file=path/to/foo.env --from-env-file=path/to/bar.env Additional options include: --allow-missing-template-keys=true: Ignores errors in templates if fields or keys are missing. --append-hash=false: Appends a hash of the secret to its name. --dry-run: Specify ""none"", ""server"", or ""client"" to perform a dry run or see what object would be sent. For this demonstration, a secret is created with a literal value: kubectl create secret generic my-secret --from-literal=key1=supersecret The output confirms the creation: secret/my-secret created

# Listing the secret
kubectl get secret
NAME        TYPE      DATA   AGE
my-secret   Opaque    1      5s You can inspect the secret details with: kubectl describe secret my-secret To view the secret in YAML format, use: kubectl get secret my-secret -o yaml Example YAML output: apiVersion: v1
data:
  key1: c3VwZXJzZWNyZXQ=
kind: Secret
metadata:
  creationTimestamp: ""2022-10-24T05:34:13Z""
  name: my-secret
  namespace: default
  resourceVersion: ""2111""
  uid: dfe97c62-5aa1-46a8-b71c-ffa0cd4c08ec
type: Opaque If you decode the base64-encoded value, you will obtain the cleartext secret: echo ""c3VwZXJzZWNyZXQ="" | base64 --decode Output: supersecret Important Because secrets are stored as base64 encoded plaintext, anyone with access to etcd can decode and view them. Avoid storing secret definition files in public repositories without further protection. Inspecting Secret Data in etcd Next, examine how Kubernetes stores secrets in etcd, where the data is kept unencrypted by default. To inspect the stored secrets, use the etcdctl utility with API version 3. Start by verifying that etcd is running on your cluster: kubectl get pods -n kube-system You should see an etcd pod (for example, ""etcd-controlplane""). Confirm the existence of the certificate file: ls /etc/kubernetes/pki/etcd/ca.crt If etcdctl is not installed, install it using: apt-get install etcd-client Set the ETCDCTL_API to version 3 and check the etcdctl version: etcdctl Retrieve and inspect your secret stored in etcd. Adjust the key path to match your secret (e.g., ""my-secret""): ETCDCTL_API=3 etcdctl \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  get /registry/secrets/default/my-secret | hexdump -C The output will display a hex dump showing the secret fields, including the cleartext value (""supersecret""), confirming that etcd stores the data unencrypted. Configuring Encryption at Rest To secure secret data, enable encryption at rest in etcd. Begin by verifying whether encryption is already configured in your cluster. Check the Kube API server for the ""encryption-provider-config"" flag: ps -aux | grep kube-api | grep ""encryption-provider-config"" If no output is returned, encryption is not yet enabled. Inspect the API server manifest, typically located in: ls /etc/kubernetes/manifests/ Open the kube-apiserver manifest: vi /etc/kubernetes/manifests/kube-apiserver.yaml Since encryption is missing from the configuration, create an encryption configuration file and update the kube-apiserver manifest accordingly. Encryption Configuration File Create a YAML file (for example, enc.yaml ) with the following content. This configuration specifies that secret objects will be encrypted using the AESCBC provider: apiVersion: apiserver.config.k8s.io/v1
kind: EncryptionConfiguration
resources:
  - resources:
      - secrets
    providers:
      - aescbc:
          keys:
            - name: key1
              secret: INSERT_BASE64_ENCODED_32_BYTE_KEY_HERE
      - identity: {} Generate a 32-byte random key and encode it in base64 using: head -c 32 /dev/urandom | base64 Replace INSERT_BASE64_ENCODED_32_BYTE_KEY_HERE with the generated key. Move the enc.yaml file to your control plane node: mkdir -p /etc/kubernetes/enc
mv enc.yaml /etc/kubernetes/enc/ Updating the Kube API Server Manifest Edit the kube-apiserver manifest ( /etc/kubernetes/manifests/kube-apiserver.yaml ) to apply the encryption configuration: Append the following flag to reference the encryption configuration file: - --encryption-provider-config=/etc/kubernetes/enc/enc.yaml Under the volumeMounts section of the kube-apiserver container, add: - name: enc
  mountPath: /etc/kubernetes/enc
  readOnly: true Under the volumes section, add a hostPath volume: - name: enc
  hostPath:
    path: /etc/kubernetes/enc
    type: DirectoryOrCreate A simplified excerpt of the manifest changes: spec:
  containers:
    - name: kube-apiserver
      command:
        - kube-apiserver
        - --advertise-address=10.6.118.3
        # ... other flags ...
        - --encryption-provider-config=/etc/kubernetes/enc/enc.yaml
      volumeMounts:
        # ... existing volume mounts ...
        - name: enc
          mountPath: /etc/kubernetes/enc
          readOnly: true
  volumes:
    # ... existing volumes ...
    - name: enc
      hostPath:
        path: /etc/kubernetes/enc
        type: DirectoryOrCreate After saving your changes, the kube-apiserver will automatically restart and apply the new encryption settings. Verifying Encryption Once the API server has restarted with the new encryption configuration, create a new secret so it will be encrypted on write: kubectl create secret generic my-secret-2 --from-literal=key2=topsecret Verify that the secret is created: kubectl get secret
NAME          TYPE    DATA   AGE
my-secret     Opaque  1      16m
my-secret-2   Opaque  1      3s Now, inspect the new secret in etcd: ETCDCTL_API=3 etcdctl \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  get /registry/secrets/default/my-secret-2 | hexdump -C The output will confirm that the secret value (""topsecret"") is no longer plainly visible because it is now encrypted. Existing Secrets Note that secrets created before enabling encryption remain unencrypted until updated. To re-encrypt these, fetch and replace them without modifying the data: kubectl get secrets --all-namespaces -o json | kubectl replace -f - Summary This article demonstrated how to: Create and inspect Kubernetes secrets. Verify that secrets are stored in etcd as base64-encoded plaintext. Enable encryption at rest by creating an encryption configuration file. Update the kube-apiserver manifest to integrate the encryption config. Confirm that new secrets are encrypted and secure in etcd. Encrypting secret data at rest is essential for protecting sensitive information from unauthorized access. Remember that encryption applies only to future changes unless existing secrets are updated. Thank you for following this guide on encrypting Kubernetes secrets at rest! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solutions Security Contexts,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Solutions-Security-Contexts,"Certified Kubernetes Application Developer - CKAD Configuration Solutions Security Contexts In this article, we explore various Kubernetes security context scenarios and demonstrate how to update pod configurations to meet specific security requirements. You'll learn how to check the running user inside pods, update configurations to run processes as non-root or root with additional capabilities, and understand container-level versus pod-level security context inheritance. Checking the User of a Running Pod To determine which user is executing the sleep process inside an Ubuntu sleeper pod, follow these steps: List the Pods Identify the Ubuntu sleeper pod by listing all pods: kubectl get pods Access the Pod Use kubectl exec to access the pod's shell: kubectl exec -it ubuntu-sleeper -- bash Verify the User Inside the Container Once inside, run the command: whoami The expected output should be: root Below is a sample console output demonstrating these steps: kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
ubuntu-sleeper   1/1     Running   0          7m58s And, inside the container: whoami
root This confirms that the sleep process is running as the root user. Updating the Pod to Run as a Specific Non-Root User (UID 1010) To update the Ubuntu sleeper pod so that the process runs with the user ID 1010, follow these steps: Export the Current Pod Configuration Retrieve the current configuration and save it to a file: kubectl get pod ubuntu-sleeper -o yaml > ubuntu-sleeper.yaml Edit the Configuration File Open ubuntu-sleeper.yaml in your favorite text editor. Locate the pod's securityContext , which should currently look like: securityContext: {} Modify the Security Context Update the file by adding the runAsUser property with UID 1010: securityContext:
  runAsUser: 1010 Apply the Updated Configuration Save the changes, then delete the existing pod (forcing deletion if necessary) and reapply the configuration: kubectl delete pod ubuntu-sleeper --force
kubectl apply -f ubuntu-sleeper.yaml An excerpt of the modified configuration is as follows: apiVersion: v1
kind: Pod
metadata:
  name: ubuntu-sleeper
  namespace: default
spec:
  containers:
  - command:
    - sleep
    - ""4800""
    image: ubuntu
    imagePullPolicy: Always
    name: ubuntu
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-vwn5z
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: controlplane
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext:
    runAsUser: 1010
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists After applying these changes, you can verify that the pod is running: kubectl get pods Note At the container level, the securityContext settings override those defined at the pod level. Determining the User for Multiple Containers in a Pod When dealing with pods that contain multiple containers, it's important to understand how user contexts are inherited. Consider the following pod definition from the multi-pod.yaml file: apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
spec:
  securityContext:
    runAsUser: 1001
  containers:
    - image: ubuntu
      name: web
      command: [""sleep"", ""5000""]
      securityContext:
        runAsUser: 1002
    - image: ubuntu
      name: sidecar
      command: [""sleep"", ""5000""] Key observations: The pod-level security context sets runAsUser to 1001. The web container defines its own securityContext with runAsUser: 1002 . Since the container-level security context takes precedence, the processes in the web container run as user 1002 . The sidecar container, which lacks a container-specific security context, inherits the pod-level setting, so its processes run as user 1001 . The final configuration is provided below for reference: apiVersion: v1
kind: Pod
metadata:
  name: multi-pod
spec:
  securityContext:
    runAsUser: 1001
  containers:
  - image: ubuntu
    name: web
    command: [""sleep"", ""5000""]
    securityContext:
      runAsUser: 1002
  - image: ubuntu
    name: sidecar
    command: [""sleep"", ""5000""] Updating the Ubuntu Sleeper Pod to Run as Root with SYS_TIME Capability To modify the Ubuntu sleeper pod so that: The process runs as the default root user. The pod is granted the SYS_TIME capability, follow these steps: Edit the Pod Configuration Open the current configuration file (e.g., ubuntu-sleeper.yaml ), and remove any pod-level securityContext that enforces a non-root user. Add Capabilities at the Container Level Under the container specification responsible for running the sleep command, add a securityContext that includes the required capability: securityContext:
  capabilities:
    add: ['SYS_TIME'] A sample updated configuration is shown below: apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: ""2023-07-26T22:12:07Z""
  name: ubuntu-sleeper
  namespace: default
  resourceVersion: ""945""
  uid: 34c800d-d278-498b-b6be-f9e41086be9f
spec:
  containers:
  - command:
    - sleep
    - ""4800""
    image: ubuntu
    imagePullPolicy: Always
    name: ubuntu
    securityContext:
      capabilities:
        add: ['SYS_TIME']
    resources: {}
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-km279
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: controlplane
  preemptionPolicy: PreemptLowerPriority
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute After saving the configuration, delete the existing pod and apply the new configuration: kubectl delete pod ubuntu-sleeper --force
kubectl apply -f ubuntu-sleeper.yaml Verify that the pod is running as expected: kubectl get pods Adding the NET_ADMIN Capability To enhance the security context further by adding the NET_ADMIN capability in addition to SYS_TIME , follow these steps: Modify the Container’s Security Context Open the ubuntu-sleeper.yaml file and update the securityContext under the container section to include both capabilities: securityContext:
  capabilities:
    add: ['SYS_TIME', 'NET_ADMIN'] Apply the Changes Save the file and reapply the pod configuration by deleting the current pod and applying the updated configuration: kubectl delete pod ubuntu-sleeper --force
kubectl apply -f ubuntu-sleeper.yaml Verify the Update Optionally, check that the pod is running with the updated capabilities: kubectl get pods A sample console interaction may look like this: controlplane ~ ➜ vi ubuntu-sleeper.yaml
controlplane ~ ➜ kubectl delete pod ubuntu-sleeper --force
Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.
pod ""ubuntu-sleeper"" force deleted

controlplane ~ ➜ kubectl apply -f ubuntu-sleeper.yaml
pod/ubuntu-sleeper created

controlplane ~ ➜ kubectl get pods
NAME             READY   STATUS    RESTARTS   AGE
ubuntu-sleeper   1/1     Running   0          10s This completes the guide on updating Kubernetes pod configurations with various security contexts and capabilities. For more detailed information on Kubernetes security practices, see the Kubernetes Documentation . Happy securing! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solution Secrets Optional,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Solution-Secrets-Optional,"Certified Kubernetes Application Developer - CKAD Configuration Solution Secrets Optional In this guide, we walk you through a lab exercise that focuses on managing Kubernetes Secrets. We'll start by inspecting the default secret in the cluster, review its content, and then deploy an application that uses a custom secret for connecting to a MySQL database. Inspecting Default Secrets Begin by checking the number of secrets within the default namespace with the following command: kubectl get secrets You should see an output similar to this: NAME                      TYPE                                  DATA   AGE
default-token-cr4sr       kubernetes.io/service-account-token     3     7m50s Next, retrieve details about the default token secret: kubectl describe secret default-token-cr4sr The output will list several key items under the Data section: ca.crt (570 bytes) namespace (7 bytes) token (a long encoded token) This confirms there are three data items. Note that the secret type is kubernetes.io/service-account-token . Reviewing the Secret Data When you run the following command: kubectl get secrets the output will still be: NAME                    TYPE                                   DATA   AGE
default-token-cr4sr     kubernetes.io/service-account-token    3      7m50s Then, describing the secret with: kubectl describe secret default-token-cr4sr reveals the three keys: ca.crt namespace token Note Even though these keys represent secret data, the ""type"" field ( kubernetes.io/service-account-token ) is not classified as secret data. Reviewing the Application Architecture The next step involves inspecting the deployed application architecture. The necessary Pods and Services are already created. However, note that there is no deployment resource in the default namespace. Check deployments using: kubectl get deploy Output: No resources found in default namespace. Next, verify the pods: kubectl get pods Output: NAME         READY   STATUS    RESTARTS   AGE
webapp-pod   1/1     Running   0          26s
mysql        1/1     Running   0          26s Two pods are present: one for the web application and another for MySQL. To inspect services, run: kubectl get svc Output: NAME             TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)           AGE
kubernetes       ClusterIP  10.43.0.1       <none>        443/TCP           10m
webapp-service   NodePort   10.43.73.93     <none>        8080:30000/TCP    40s
sql01            ClusterIP  10.43.128.20    <none>        3306/TCP          40s This verifies the presence of two services: one managing the web application and another for the SQL database (named ""sql01""). The default token secret, which was reviewed earlier, is not used by the application. Instead, the web application needs a dedicated secret to connect to the MySQL database. Application Error and Creating the DB Secret Upon examining the application, you might notice an error indicating a failure to connect to MySQL. This happens because the necessary environment variables (database host, user, and password) are not set. The error message typically includes: Database host is not set. DB user is not set. DB password is not set. To fix the issue, create a new Kubernetes Secret named db-secret containing the required credentials. Execute the following command: kubectl create secret generic db-secret \
  --from-literal=DB_Host=sql01 \
  --from-literal=DB_User=root \
  --from-literal=DB_Password=password123 Confirm the secret was created by running: kubectl get secret Output: NAME                 TYPE                                  DATA   AGE
default-token-cr4sr  kubernetes.io/service-account-token     3      10m
db-secret            Opaque                                3      6s You can further inspect the secret with: kubectl describe secret db-secret and you'll see three keys: DB_Host , DB_User , and DB_Password . Configuring the Web Application Pod to Use the Secret Now, update your web application Pod so that it sources its environment variables from the newly created db-secret . This configuration allows the container to directly access the MySQL connection information via environment variables. Below is a sample pod specification illustrating how to include the secret using the envFrom field: apiVersion: v1
kind: Pod
metadata:
  name: webapp-pod
  namespace: default
  labels:
    name: webapp-pod
spec:
  containers:
    - name: webapp
      image: kodekloud/simple-webapp-mysql
      imagePullPolicy: Always
      envFrom:
        - secretRef:
            name: db-secret
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
  volumes:
    - name: kube-api-access-dxllf
      mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      readOnly: true
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  schedulerName: default-scheduler
  serviceAccount: default
  serviceAccountName: default Apply the updated configuration with: kubectl replace --force -f <filename>.yaml Allow the Pod to restart and then verify that it has been updated successfully: kubectl describe pod webapp-pod Within the container section, you should see a reference confirming that environment variables are loaded from db-secret . Following these changes, the application should successfully establish a connection with the MySQL database using the correct host, user, and password settings. Conclusion After updating the pod configuration, the web application now correctly reads the required environment variables from db-secret and connects to the MySQL database. This lab exercise emphasizes how Kubernetes Secrets can be used to securely externalize sensitive data—like database credentials—and integrate them seamlessly into application Pods. This completes the lab exercise. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solutions Resource Requirements,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Solutions-Resource-Requirements,"Certified Kubernetes Application Developer - CKAD Configuration Solutions Resource Requirements In this lesson, we will walk through the solutions for the resource limits lab. Follow the steps below to verify CPU requirements for the ""rabbit"" pod, troubleshoot the ""elephant"" pod, and update its memory configuration. Question 1: Verifying CPU Requirements for the ""rabbit"" Pod A pod named rabbit is deployed, and our task is to determine its CPU settings. Run the following command to describe the pod: kubectl describe pod rabbit Scroll through the output until you find the resources section. You should see details such as: Exit Code: 128
Started: Thu, 01 Jan 1970 00:00:00 +0000
Finished: Tue, 02 Aug 2022 17:24:15 +0000
Ready: False
Restart Count: 5
Limits:
  cpu: 2
Requests:
  cpu: 1
Environment: <none>
Mounts:
  /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-27zq6 (ro)
Conditions:
  Type                Status
  Initialized        True
  Ready              False
  ContainersReady    False
  PodScheduled       True
Volumes:
  kube-api-access-27zq6:
    Type: Projected (a volume that contains injected data from multiple sources)
    TokenExpirationSeconds: 3607
    ConfigMapName: kube-root-ca.crt
    ConfigMapOptional: <nil>
    DownwardAPI: true
QoS Class: Burstable
Node-Selectors: <none>
Tolerations: node.kubernetes.io/not-ready:NoExecute op=Exists for 300s From the above information, notice that the CPU request is set to 1 while the limit is 2 . This indicates that the pod is configured to run with one CPU. After verification, delete the rabbit pod with the command: controlplane ~ ➜ kubectl delete pod rabbit
pod ""rabbit"" deleted
controlplane ~ ➜ [] Question 2: Troubleshooting the ""elephant"" Pod The elephant pod, deployed in the default namespace, is not reaching a running state. To diagnose the issue, describe the pod using: kubectl describe pod elephant In the output, locate the Last State section. You might observe output similar to the following: Image: polinux/stress
Image ID: docker.io/polinux/stress@sha256:b6144f84f9c15dac80deb48d3a646b55c7043ab1d83ea0a697c09097aaad21aa
Port: <none>
Host Port: <none>
Command:
  stress
Args:
  --vm
  1
  --vm-bytes
  15M
  --vm-hang
  1
State: Waiting
Reason: CrashLoopBackOff
Last State: Terminated
Reason: OOMKilled
Exit Code: 1
Started: Tue, 02 Aug 2022 17:25:47 +0000
Finished: Tue, 02 Aug 2022 17:25:47 +0000
Ready: False
Restart Count: 1
Limits:
  memory: 10Mi
Requests:
  memory: 5Mi
Environment: <none>
Mounts:
  /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-h841s (ro)
Conditions: Note The OOMKilled status indicates the pod was terminated because it ran out of memory. The pod has a memory limit set at 10Mi , but its process needs 15 MB . Question 3: Increasing the Memory Limit for the ""elephant"" Pod To resolve the OOMKilled issue, follow these steps to increase the memory limit from 10Mi to 20Mi : Export the Current Pod Configuration Export the current pod configuration to a YAML file by running: controlplane ~ ➜ kubectl get pod -o yaml > elephant.yaml Delete the Existing Pod Remove the current elephant pod with: controlplane ~ ➜ kubectl delete pod elephant
pod ""elephant"" deleted
controlplane ~ ➜ Edit the YAML Configuration Open the elephant.yaml file using your favorite text editor (e.g., vi ): controlplane ~ ➜ vi elephant.yaml Locate the section specifying the memory limits and update it from 10Mi to 20Mi . Apply the Updated Configuration Recreate the pod by applying the modified YAML file: controlplane ~ ➜ kubectl apply -f elephant.yaml
pod/elephant created Verify the Pod Status Check to ensure the pod is running: controlplane ~ ➜ kubectl get pod
NAME      READY   STATUS    RESTARTS   AGE
elephant  1/1     Running   0          13s
controlplane ~ ➜ [] Final Deletion Once verified, delete the elephant pod as the lab instructions specify: controlplane ~ ➜ kubectl delete pod elephant After completing these steps and confirming that the pod eventually reaches a running state, you have successfully completed the resource limits lab. This concludes the Resource Limits Lab Solutions lesson. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solutions Service Account,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Solutions-Service-Account,"Certified Kubernetes Application Developer - CKAD Configuration Solutions Service Account In this lesson, we will explore how to inspect and modify service account configurations in Kubernetes, focusing on the Kubernetes dashboard application's interaction with the Kubernetes API. ────────────────────────────── 1. Identifying Service Accounts in the Default Namespace To begin, check the number of service accounts in the default namespace by running: kubectl get sa The output might look like this: controlplane ~ → kubectl get sa
NAME       SECRETS   AGE
default    0         20m
dev        0         35s

controlplane ~ → This output shows two service accounts: default and dev . ────────────────────────────── 2. Checking the Secret Token for the Default Service Account Next, verify whether the default service account has an associated secret token. Run the following command: kubectl describe serviceaccount default Inspect the ""Tokens"" section in the output. In this case, it indicates that no token is set. ────────────────────────────── 3. Inspecting the Dashboard Deployment After deploying the dashboard application, inspect its deployment configuration and container image details. List Deployments: kubectl get deployments Describe the Dashboard Deployment: kubectl describe deployment web-dashboard Focus on the ""Pod Template"" section under ""Containers."" An example of the output is: Name:                   web-dashboard
Namespace:              default
CreationTimestamp:      Wed, 26 Jul 2023 22:41:47 +0000
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               name=web-dashboard
Replicas:               1 desired | 1 updated | 1 total | 1 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=web-dashboard
  Containers:
   web-dashboard:
    Image:          gcr.io/kodekloud/customimage/my-kubernetes-dashboard
    Port:           8080/TCP
    Host Port:      0/TCP
    Environment:
      PYTHONUNBUFFERED:  1
    Mounts:        <none>
Volumes:             <none>
Conditions:
  Type             Status  Reason
  ----             ------  ------
  Available        True    MinimumReplicasAvailable
  Progressing      True    NewReplicaSetAvailable
OldReplicaSets:     <none>
NewReplicaSet:      web-dashboard-97c9c59f6 (1 replicas created)
Events:
  Type    Reason                  Age   From                   Message At one point, an error occurs: pods is forbidden: User ""system:serviceaccount:default:default"" cannot list resource ""pods"" in API group """" in the namespace ""default"" This error indicates that the dashboard application is using the default service account, which lacks the required permissions. ────────────────────────────── 4. Determining the Service Account Used by the Dashboard Application The logs and error messages confirm that the default service account is being used. To verify which service account is mounted on the dashboard pod, execute: kubectl get pod
kubectl describe pod <pod-name> Within the pod description, locate the Service Account section, which should indicate it is set to ""default."" Additionally, note that the credentials are mounted from: /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-swjvh (ro) This directory is where the dashboard pod accesses its service account tokens and related secrets. ────────────────────────────── 5. Creating a New Service Account for the Dashboard Since the default service account has limited permissions, create a new service account (named dashboard-sa ) with enhanced rights. Create the New Service Account: kubectl create serviceaccount dashboard-sa You should see a confirmation message: serviceaccount/dashboard-sa created Review RBAC Configurations: Check the RBAC configuration files (e.g., dashboard-sa-role-binding.yaml and pod-reader-role.yaml ) located in the /var/rbac/ directory. These files contain role and role-binding settings that grant additional permissions. For more details on RBAC, review the relevant documentation. Generate an Access Token: To authenticate with the dashboard application, generate an access token for dashboard-sa : kubectl create token dashboard-sa The output might be similar to: eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiJhdmF0YXJzaW1lLmFwcGxpbWF0aW9uIiwibmFtZSI6ImRhc2hib2FyZC1zYSIsImlhdCI6MTY5MjkzMDM4OCwiZXhwIjoxNzA4Mzk2Mzg4LCJhdWQiOiJodHRwczovL2FkbWluLmRhc2hib2FyZC5jb20iLCJpc3MiOiJodHRwczovL3dlYi16YWhhYW5pdGUuY29tIn0.ksP0DhTDxueD9... (truncated) Copy the token and paste it into the dashboard UI. Once authenticated, you should see all the pods running within your cluster. Dashboard Authentication After generating the token for dashboard-sa , enter it into the dashboard UI to access the cluster's detailed information. ────────────────────────────── 6. Updating the Deployment to Use the New Service Account To eliminate the need for manual token retrieval, update the dashboard deployment so it automatically uses dashboard-sa . Export the Current Deployment Configuration: kubectl get deployment web-dashboard -o yaml > dashboard.yaml Update the Deployment YAML: Open dashboard.yaml in your preferred editor and locate the pod specification within the spec section. Add the serviceAccountName field with the value ""dashboard-sa"" under the pod spec. The updated portion should resemble: generation: 1
name: web-dashboard
namespace: default
resourceVersion: ""1024""
uid: daaa0628-d1bd-4624-b5e3-9475f1958464
spec:
  progressDeadlineSeconds: 600
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: web-dashboard
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    type: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: web-dashboard
    spec:
      serviceAccountName: dashboard-sa
      containers:
      - env:
        - name: PYTHONUNBUFFERED
          value: ""1""
        image: gcr.io/kodekloud/customimage/my-kubernetes-dashboard
        imagePullPolicy: Always
        name: web-dashboard
        ports:
        - containerPort: 8080
          protocol: TCP
        resources: {} Apply the Updated Configuration: kubectl apply -f dashboard.yaml You might see a warning about a missing annotation due to changes in resource management history; this warning is harmless as the configuration will be patched automatically. Verify the Deployment: Check the deployment status by running: kubectl get deployment After applying these changes, refresh your dashboard application. The pod will now automatically mount the credentials for dashboard-sa , eliminating the need for manual token entry. ────────────────────────────── Conclusion By following these steps, you have accomplished the following: Identified service accounts in the default namespace. Inspected the token associated with the default service account. Noted that the dashboard application was using a default account with insufficient permissions. Created a new service account ( dashboard-sa ) with enhanced permissions using RBAC. Updated the dashboard deployment to automatically use the new service account, streamlining the authentication process with the Kubernetes API. Final Notes Managing service accounts and RBAC configurations properly is vital for maintaining security and operational efficiency in your Kubernetes environment. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,ConfigMaps,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/ConfigMaps,"Certified Kubernetes Application Developer - CKAD Configuration ConfigMaps In this guide, you'll learn how to manage application configuration data in Kubernetes using ConfigMaps. ConfigMaps help decouple configuration artifacts from container images, streamlining the management of environment-specific data across multiple Pod definitions. Overview Traditionally, environment variables were defined directly within Pod definition files. For example, a basic Pod configuration might look like this: apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
spec:
  containers:
    - name: simple-webapp-color
      image: simple-webapp-color
      ports:
        - containerPort: 80
      env:
        - name: APP_COLOR
          value: blue
        - name: APP_MODE
          value: prod When you need to manage configuration data across many Pods, hardcoding environment variables quickly becomes unwieldy. By extracting these settings into a ConfigMap, you centralize the configuration, making it easier to maintain and update. Key Concept ConfigMaps store configuration data as key-value pairs. When a Pod is created, this data can be injected either as environment variables or as mounted files, giving your application easy access to the configuration. There are two main steps when working with ConfigMaps: Creating the ConfigMap. Injecting the ConfigMap into the Pod. Creating a ConfigMap There are two approaches to create ConfigMaps: imperative (command line) and declarative (YAML file). Imperative Approach The imperative method allows you to create a ConfigMap directly from the command line. For instance, to create a ConfigMap named app-config with key-value pairs for APP_COLOR and APP_MODE , run: kubectl create configmap app-config --from-literal=APP_COLOR=blue --from-literal=APP_MODE=prod If you have configuration data stored in a file (e.g., app_config.properties ), you can create a ConfigMap from that file: kubectl create configmap app-config --from-file=app_config.properties Declarative Approach For the declarative method, define a ConfigMap in a YAML file and apply it using kubectl create -f . Below is an example YAML file ( config-map.yaml ): apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
data:
  APP_COLOR: blue
  APP_MODE: prod Deploy the ConfigMap with the following command: kubectl create -f config-map.yaml Best Practice When creating multiple ConfigMaps (such as for your application, MySQL, or Redis), use descriptive names. This ensures easier reference within Pod definitions. Viewing ConfigMaps To list all available ConfigMaps, you can run: kubectl get configmaps The output looks similar to: NAME         DATA   AGE
app-config   2      3s For a detailed view of a specific ConfigMap, use: kubectl describe configmaps app-config This command displays comprehensive details, including the stored key-value pairs: Name:         app-config
Namespace:    default
Labels:       <none>
Annotations:  <none>

Data
====
APP_COLOR:
----
blue
APP_MODE:
----
prod
Events: <none> Injecting a ConfigMap into a Pod After creating a ConfigMap, you can inject the configuration data into a Pod. A common method is using the envFrom property within the Pod’s container specification, as shown below: apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp-color
  labels:
    name: simple-webapp-color
spec:
  containers:
    - name: simple-webapp-color
      image: simple-webapp-color
      ports:
        - containerPort: 8080
      envFrom:
        - configMapRef:
            name: app-config In this configuration, all key-value pairs from the app-config ConfigMap are injected as environment variables into the container. Alternative Methods for Configuration Injection There are additional ways to inject configuration data into your Pods: Method Description Example Code Snippet Individual Environment Variables Reference specific keys from the ConfigMap using env and valueFrom . yaml<br>env:<br>  - name: APP_COLOR<br>    valueFrom:<br>      configMapKeyRef:<br>        name: app-config<br>        key: APP_COLOR<br> Mounting as a Volume Mount the ConfigMap as a volume to provide configuration as files. yaml<br>volumes:<br>  - name: app-config-volume<br>    configMap:<br>      name: app-config<br> Choose the method that best fits your application's requirements. Next Steps Now that you understand how to create and inject ConfigMaps, try experimenting with live Kubernetes clusters to practice configuring, viewing, and troubleshooting environment variables. This guide provides a structured approach to managing configuration data in Kubernetes, empowering you to build more maintainable and scalable applications. For further reading, please visit the Kubernetes Documentation . Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Service Account,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Service-Account,"Certified Kubernetes Application Developer - CKAD Configuration Service Account Welcome to this lesson on Kubernetes service accounts. In this article, we explore how service accounts serve as the backbone for authenticating and authorizing machine-to-cluster interactions. While closely related to authentication, authorization, and role-based access control topics, this guide specifically focuses on creating and managing service accounts as part of our Kubernetes curriculum. There are two primary account types in Kubernetes: User Accounts: For human users such as administrators and developers. Service Accounts: For machine users, such as monitoring tools (e.g., Prometheus) or CI/CD systems like Jenkins . Example: Kubernetes Dashboard Application Consider a simple dashboard application, “my Kubernetes dashboard,” built in Python. When deployed, it queries the Kubernetes API to retrieve the list of Pods and displays them on a web page. For secure communication with the Kubernetes API, the application uses a service account to authenticate. Creating a Service Account To create a service account for your application, run the following command. In this example, we create a service account named dashboard-sa : kubectl create serviceaccount dashboard-sa After creation, list all service accounts with: kubectl get serviceaccount The output might look similar to: NAME          SECRETS   AGE
default       1         218d
dashboard-sa  1         4d Inspect the newly created service account along with its token by executing: kubectl describe serviceaccount dashboard-sa This command will show details including the associated token (stored as a secret), such as: Name:                dashboard-sa
Namespace:           default
Labels:              <none>
Annotations:         <none>
Image pull secrets:  <none>
Mountable secrets:   dashboard-sa-token-kbbdm
Tokens:              dashboard-sa-token-kbbdm
Events:              <none> The token is stored in a Secret object. To view the token, describe the corresponding secret: kubectl describe secret dashboard-sa-token-kbbdm Use the token as a bearer authentication token when accessing the Kubernetes API. For example: curl https://192.168.56.70:6443/api --insecure --header ""Authorization: Bearer eyJhbgG…"" Automatic Token Mounting in Pods When a third-party application is deployed on a Kubernetes cluster, the service account token is automatically mounted as a volume inside the Pod. This allows your application to read the token from a well-known location rather than providing it manually. By default, each namespace in Kubernetes includes a service account named default . When a Pod is created without specifying a service account, Kubernetes mounts the default service account’s token into the Pod. Consider the following minimal Pod definition: apiVersion: v1
kind: Pod
metadata:
  name: my-kubernetes-dashboard
spec:
  containers:
    - name: my-kubernetes-dashboard
      image: my-kubernetes-dashboard To inspect the Pod, run: kubectl describe pod my-kubernetes-dashboard A typical output displays that a secret (e.g., default-token-j4hkv ) is mounted at /var/run/secrets/kubernetes.io/serviceaccount : Name:         my-kubernetes-dashboard
Namespace:    default
Status:       Running
IP:           10.244.0.15
Containers:
  my-kubernetes-dashboard:
    Image:      my-kubernetes-dashboard
Mounts:
  /var/run/secrets/kubernetes.io/serviceaccount from default-token-j4hkv (ro)
Volumes:
  default-token-j4hkv:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  default-token-j4hkv
    Optional:    false Inside the Pod, you can list the mounted secret files: kubectl exec -it my-kubernetes-dashboard -- ls /var/run/secrets/kubernetes.io/serviceaccount And to view the token: kubectl exec -it my-kubernetes-dashboard -- cat /var/run/secrets/kubernetes.io/serviceaccount/token The file named token contains the bearer token used for accessing the Kubernetes API. Keep in mind that the default service account has limited permissions geared towards basic API queries. Using a Custom Service Account in Pods To use a service account other than the default (for example, dashboard-sa ), update your Pod definition by specifying the serviceAccountName field: apiVersion: v1
kind: Pod
metadata:
  name: my-kubernetes-dashboard
spec:
  containers:
    - name: my-kubernetes-dashboard
      image: my-kubernetes-dashboard
  serviceAccountName: dashboard-sa After recreating the Pod (since you cannot change the service account for an existing Pod), inspect it with: kubectl describe pod my-kubernetes-dashboard You should notice that the volume now mounts the token belonging to dashboard-sa : Mounts:
  /var/run/secrets/kubernetes.io/serviceaccount from dashboard-sa-token-kbbdm (ro) If you prefer to disable the automatic mounting of the service account token, set the automountServiceAccountToken field to false : apiVersion: v1
kind: Pod
metadata:
  name: my-kubernetes-dashboard
spec:
  automountServiceAccountToken: false
  containers:
    - name: my-kubernetes-dashboard
      image: my-kubernetes-dashboard Changes in Kubernetes Versions 1.22 and 1.24 Starting with Kubernetes 1.22, enhancements were made to how service account tokens are handled. Previously, creating a service account automatically produced a Secret with a non-expiring token. Now, tokens are generated via the token request API and mounted as projected volumes with a defined lifetime. The following is an example Pod manifest that reflects the new token provisioning method: apiVersion: v1
kind: Pod
metadata:
  name: nginx
  namespace: default
spec:
  containers:
    - image: nginx
      name: nginx
      volumeMounts:
        - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
          name: kube-api-access-6mtg8
          readOnly: true
  volumes:
    - name: kube-api-access-6mtg8
      projected:
        defaultMode: 420
        sources:
          - serviceAccountToken:
              expirationSeconds: 3607
              path: token
          - configMap:
              name: kube-root-ca.crt
              items:
                - key: ca.crt
                  path: ca.crt
          - downwardAPI:
              items:
                - fieldRef:
                    apiVersion: v1
                    fieldPath: metadata.namespace In this configuration, the token is automatically requested with a specified validity (e.g., 3607 seconds) and is mounted through a projected volume rather than a pre-created secret. With Kubernetes version 1.24, further improvements (as detailed in Kubernetes Enhancement Proposal 2799) have minimized the use of secret-based service account tokens. Now, when a service account is created, a token is not automatically generated. Instead, explicitly generate a token using this command if needed: kubectl create token dashboard-sa This command produces a token that is both audience-bound and time-limited. You can adjust the expiration using additional options. If you prefer the legacy behavior of creating non-expiring tokens stored in Secrets (though this method is less secure), manually create a Secret object with the proper annotation: apiVersion: v1
kind: Secret
type: kubernetes.io/service-account-token
metadata:
  name: mysecretname
  annotations:
    kubernetes.io/service-account.name: dashboard-sa Ensure that the service account ( dashboard-sa ) already exists before creating this Secret. Tip Decoding the token using tools like jq or services such as jwt.io can help verify token details. Note that earlier tokens did not have an expiry date, but the new token request API provides tokens with a bounded lifetime for enhanced security. Summary In this lesson, you learned how to: Create a service account and retrieve its token (either stored as a Secret or generated via the token request API). Automatically mount the service account token into Pods for in-cluster applications. Customize Pod definitions to use a specific service account or disable automatic token mounting. Understand the improvements in Kubernetes versions 1.22 and 1.24 regarding bound, time-limited tokens using the token request API. For further reading, refer to the official Kubernetes documentation and relevant enhancement proposals. Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Solution ConfigMaps Optional,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Solution-ConfigMaps-Optional,"Certified Kubernetes Application Developer - CKAD Configuration Solution ConfigMaps Optional Hi, I'm Sanjeev, one of the co-instructors for this course. In this lesson, we will walk through the solution for the ConfigMap section of the assignment in a step-by-step manner. ───────────────────────────── 1. Determining the Number of Pods Begin by checking how many pods are currently running in your system. Execute the following command: controlplane ~ ➜ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
webapp-color    1/1     Running   0          54s
controlplane ~ ➜ This confirms that there is one active pod, named webapp-color . ───────────────────────────── 2. Identifying the Environment Variable in the Pod Next, determine which environment variable is configured in the container by describing the pod: kubectl describe pod webapp-color Search for the Environment section in the output. An example excerpt might look like: IP:
  10.42.0.9
Containers:
  webapp-color:
    Container ID:   containerd://750b751f5351c7edbaaaaf33e77d8f9709ed85ea4862914bbe4d79c1254b54fb7
    Image:          kodekloud/webapp-color
    Image ID:       docker.io/kodekloud/webapp-color@sha256:99c3821ea49b89c7a22d3eebab5c2e1ec651452e7675af2434850
    Port:           <none>
    Host Port:      <none>
    State:          Running
    Started:        Wed, 27 Jul 2022 05:21:49 +0000
    Ready:          True
    Restart Count:  0
    Environment:
      APP_COLOR: pink
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rs7nh (ro) From the Environment section, note that the variable APP_COLOR is set (ensure you select the correct one if multiple similar names are present). ───────────────────────────── 3. Verifying the Environment Variable Value Verify the value of the APP_COLOR variable. The output above indicates that its value is pink . ───────────────────────────── 4. Viewing the Web Application Click on the web app color tab to open the application in your browser. When the page loads, you should see the application displaying the color corresponding to the APP_COLOR value (in this case, pink). Once confirmed, click OK to proceed. ───────────────────────────── 5. Updating the Environment Variable for a Green Background The next task is to update the pod configuration so that the background changes to green. This involves deleting and recreating the pod with the necessary modification while keeping the pod name unchanged. 5.1 Exporting the Current Pod YAML First, verify the pod again: controlplane ~ ➜ kubectl get pods
NAME            READY   STATUS    RESTARTS   AGE
webapp-color    1/1     Running   0          8m40s Then, export the pod configuration to a file: kubectl get pod webapp-color -o yaml > pod.yaml 5.2 Deleting the Current Pod Remove the running pod to make way for the updated configuration: kubectl delete pod webapp-color
pod ""webapp-color"" deleted 5.3 Modifying and Applying the Pod Configuration Open the exported pod.yaml file in your favorite text editor, such as using vi : vi pod.yaml Locate the env section under the container definition and change the value of APP_COLOR from pink to green . Save your modifications. Apply the updated configuration: kubectl apply -f pod.yaml Finally, verify that the pod is running and that the background has changed to green. ───────────────────────────── 6. Listing ConfigMaps in the Default Namespace To determine how many ConfigMaps are available in the default namespace, run: kubectl get cm The expected output is: NAME               DATA   AGE
kube-root-ca.crt   1      20m
db-config          3      50s This output shows there are two ConfigMaps. ───────────────────────────── 7. Identifying the Database Host in the ConfigMap Next, identify the database host by inspecting the ConfigMap named db-config : kubectl describe cm db-config Within the Data section of the output, you might find: Data
====
DB_PORT:   3306
DB_HOST:   SQL01.example.com
DB_NAME:   SQL01 Here, DB_HOST is set to SQL01.example.com . ───────────────────────────── 8. Creating a New ConfigMap for the Webapp-Color Pod Create a new ConfigMap named webapp-config-map that defines the APP_COLOR variable with a value of darkblue by executing: kubectl create cm webapp-config-map --from-literal=APP_COLOR=darkblue Verify that the new ConfigMap has been created successfully. ───────────────────────────── 9. Updating the Pod to Use the New ConfigMap The next step is to update the webapp-color pod so that it sources the APP_COLOR environment variable from the new ConfigMap. 9.1 Deleting the Existing Pod Delete the current pod with this command: kubectl delete pod webapp-color 9.2 Modifying the Pod YAML Update the pod.yaml file to reference the ConfigMap rather than using a hard-coded environment variable value. Modify the container specification under the env section as follows: apiVersion: v1
kind: Pod
metadata:
  name: webapp-color
  labels:
    name: webapp-color
    namespace: default
spec:
  containers:
  - name: webapp-color
    image: kodekloud/webapp-color
    imagePullPolicy: Always
    env:
    - name: APP_COLOR
      valueFrom:
        configMapKeyRef:
          name: webapp-config-map
          key: APP_COLOR
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-4bwn
      readOnly: true
  # Other pod specifications remain unchanged. Save your changes and apply the updated file: kubectl apply -f pod.yaml After applying the configuration, verify that the pod is running and displays a background color of darkblue . ───────────────────────────── 10. Verifying the Updated Web Application Finally, open the web app color interface once more and refresh the page. Confirm that the background has successfully changed to darkblue . When this change is verified, click OK to complete the exercise. ───────────────────────────── This concludes the assignment for the ConfigMap lab session. Tip Ensure that you carefully check the environment variable sections in both the pod description and the YAML file when making updates to avoid configuration errors. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Taints and Tolerations,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Taints-and-Tolerations,"Certified Kubernetes Application Developer - CKAD Configuration Taints and Tolerations Hello and welcome to this lesson on taints and tolerations in Kubernetes. I'm Mumshad Manambeth, and in this article, we'll explore how pods are scheduled on nodes while using taints and tolerations to control pod placement. In this lesson, our focus is on the pod-to-node relationship and how to restrict pod placement on specific nodes. While taints and tolerations may initially seem complex, an everyday analogy can help clarify the concept: Imagine a bug approaching a person. To deter the bug, you spray the person with a repellent. Here, the repellent represents a taint applied on a node. Most bugs (pods) are repelled and cannot land on the person (node). However, some bugs might be resistant to the repellent; these represent pods that can tolerate the taint. The ability of a pod to tolerate a taint decides whether it can be scheduled on a node that has been ""tainted."" In Kubernetes, the node is analogous to the person while the pods are like bugs. Taints are applied to nodes solely to restrict which pods can be scheduled on them; they do not serve as a security measure. Consider a simple cluster with three worker nodes—named one, two, and three—and four pods labeled A, B, C, and D. Without any restrictions, the Kubernetes scheduler evenly distributes these pods among the nodes. Now, assume you want to dedicate node one for a specific application and have only the related pods scheduled there. You first prevent all pods from landing on node one by applying a taint (for example, with the key ""app"" and value ""blue""). Since pods do not come with tolerations by default, they will be excluded from node one. To schedule only certain pods on node one, add a toleration to the pod’s definition. For instance, if you want only pod D to be scheduled on node one, you would add a toleration corresponding to the taint (blue) into its specification. With taints and tolerations configured appropriately, the scheduler will avoid node one for pods A, B, and C while allowing pod D to run on it. You can apply a taint to a node using the following command: kubectl taint nodes node-name key=value:taint-effect For example, to taint node1 with the key-value pair ""app=blue"" and an effect of NoSchedule, run: kubectl taint nodes node1 app=blue:NoSchedule The taint effect determines the behavior for pods that do not tolerate the taint. There are three available effects: NoSchedule: Pods without the required toleration are not scheduled on the node. PreferNoSchedule: The scheduler avoids placing a pod on the node if possible, but it is not strictly enforced. NoExecute: New pods that do not tolerate the taint are not scheduled, and existing pods without the toleration are evicted. To add a toleration to a pod, update its pod definition file by including a tolerations section under spec. For example, to allow a pod to tolerate the taint with key ""app"" and value ""blue"" with a NoSchedule effect, include the following in the pod's YAML file: apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
    - name: nginx-container
      image: nginx
  tolerations:
    - key: ""app""
      operator: ""Equal""
      value: ""blue""
      effect: ""NoSchedule"" Ensure that each value is correctly enclosed in double quotes as shown. Once pods are created or updated with the necessary tolerations, they will be scheduled or evicted based on the taint's effect and the pod’s toleration settings. Let's examine the NoExecute effect in more detail. Imagine a cluster with three nodes running workloads without any taints. Later, when node one is dedicated to a special application, you taint node one using the NoExecute effect and add a toleration only to the designated pod (pod D). In this scenario, the NoExecute taint ensures that any pod on node one that does not tolerate the taint (for example, pod C) is evicted, while pod D remains since it has the necessary toleration. Important Note Taints and tolerations control which pods a node accepts. They do not force pods to be scheduled on a particular node. Even if a pod tolerates a node’s taint, without additional constraints such as node affinity, it may still be scheduled on another available node. An interesting aspect of taints and tolerations concerns master nodes. Although master nodes are technically capable of hosting pods, the Kubernetes scheduler does not place regular application pods on them by default. When you set up a Kubernetes cluster, a taint is automatically applied to the master node to prevent scheduling application workloads. You can verify this taint by running: kubectl describe node kube-master Examine the taint section to see that the master node is configured to not accept pods. While this behavior can be modified, it is best practice to keep application workloads separate from master nodes. That concludes this lesson on taints and tolerations. Now, you can reinforce your understanding by practicing these concepts through hands-on coding exercises. For further reading, check out the following resources: Kubernetes Basics Kubernetes Documentation Docker Hub Terraform Registry Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Node Selectors Logging,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Node-Selectors-Logging,"Certified Kubernetes Application Developer - CKAD Configuration Node Selectors Logging Welcome to this article on Kubernetes node selectors. In this guide, we will explore how node selectors help schedule pods onto the most suitable nodes in your cluster. Understanding Node Selectors Imagine you have a three-node cluster where two nodes are smaller with limited hardware resources and one node is larger with higher resources. Your cluster runs various workloads, but you want to ensure that resource-intensive data processing pods always run on the larger node. By default, Kubernetes can schedule any pod on any available node, which might result in a resource-heavy pod running on a smaller node. To overcome this, you can restrict pods to run on specific nodes by using node selectors. A node selector allows you to define a key-value pair in your pod definition that corresponds to labels assigned to nodes. Pod Definition Using Node Selectors Below is an example of a pod definition in YAML format that uses a node selector to ensure the pod runs exclusively on the larger node: apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
    - name: data-processor
      image: data-processor
  nodeSelector:
    size: Large In this example, the nodeSelector property is added under the spec section, and it specifies the key-value pair size: Large . These labels must be assigned to the corresponding nodes. Labeling Your Nodes Before creating a pod with a node selector, ensure your nodes are labeled appropriately. You can label a node using the following command: kubectl label nodes <node-name> <label-key>=<label-value>
kubectl label nodes node-1 size=Large After labeling the node, create the pod with the node selector. When the pod is created, it will be scheduled on the node labeled size=Large (in this example, node-1). To create the pod, run: kubectl create -f pod-definition.yaml Limitations of Node Selectors While node selectors are effective for basic node restrictions, they have limitations when handling more complex scheduling requirements. For instance, if you need to schedule a pod on a node labeled either ""Large"" or ""Medium"" while excluding nodes labeled ""Small,"" node selectors cannot express this conditional logic. Note For advanced scheduling requirements, consider using Kubernetes node affinity and anti-affinity. These features offer greater flexibility in defining complex scheduling rules beyond simple node selectors. Advanced Scheduling: Node Affinity and Anti-affinity Kubernetes introduced node affinity and anti-affinity to address more complex scheduling needs. These advanced features enable you to define rules that span multiple conditions, ensuring that pods are scheduled on nodes that better meet your workload requirements. For more detailed information on advanced scheduling in Kubernetes, check out the Kubernetes Documentation . By understanding both node selectors and node affinity, you can optimize your pod scheduling strategy and ensure that your workloads run efficiently on the most appropriate nodes. Happy scheduling! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Readiness Probes,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Observability/Readiness-Probes,"Certified Kubernetes Application Developer - CKAD Observability Readiness Probes Welcome to this comprehensive guide on observability in Kubernetes. In this guide, we explore readiness and liveness probes, logging, and monitoring concepts to help you build reliable applications. Below is a brief recap of the pod lifecycle stages, which is essential for understanding how readiness probes function: When a pod is first created, it enters a Pending state while the scheduler selects an appropriate node for placement. If no node is immediately available, the pod stays in the Pending state. Use kubectl describe pod to investigate any delays. After scheduling, the pod transitions to the ContainerCreating state as images are pulled and containers begin their startup process. Once all containers launch successfully, the pod moves into the Running state and remains there until the application completes execution or is terminated. To view the current status of your pods, run the following command: osboxes@kubemaster:~$ kubectl get pods
NAME                        READY   STATUS    RESTARTS   AGE
jenkins-566f687bf-c7nzf     1/1     Running   0          12m
nginx-65899c769f-9lzh       1/1     Running   0          6h
redis-b48685f8b-fbnmx       1/1     Running   0          6h For more detailed insights, inspect the pod’s conditions. Initially, when a pod is scheduled, the ""PodScheduled"" condition becomes true. Following initialization, the ""Initialized"" condition is set to true, and finally, once all containers are ready, the ""ContainersReady"" condition is affirmed. When all these conditions are true, the pod is officially considered ready. Examine these conditions by running: osboxes@kubemaster:~$ kubectl describe pod <pod-name> Additionally, the kubectl get pods command output reflects the ready state, confirming whether the application's components are fully operational: osboxes@kubemaster:~$ kubectl get pods
NAME                                     READY   STATUS    RESTARTS   AGE
jenkins-566f687bf-c7nzf                  1/1     Running   0          12m
nginx-65899c769f-9lwzh                  1/1     Running   0          6h
redis-b48685f8b-fbnmx                    1/1     Running   0          6h Note The ready condition signifies that the application inside the pod can receive traffic. However, this status might not reflect the actual readiness of your application, as different applications require varying warm-up times. For example: A minimal script might be ready within milliseconds. A database service may take several seconds to become responsive. Complex web servers could require minutes to fully initialize. A Jenkins server, for instance, might take 10 to 15 seconds to initialize followed by a brief warm-up period. Without suitable readiness checks, traffic might be erroneously routed to an unready pod: osboxes@kubemaster:~$ kubectl get all
NAME          READY   STATUS    RESTARTS   AGE
pod/jenkins   1/1     Running   0          11s Kubernetes relies on the pod’s ready condition to determine if it should receive traffic. By default, it assumes that container creation equates to readiness. This is where readiness probes are critically important—they help signal the true readiness of the application within the container. As an application developer, you can define tests (probes) that confirm the application has reached a ready state. Examples include: An HTTP GET request to check API responsiveness for a web application. A TCP port check for database services. Executing a custom command that confirms the application is ready. To configure a readiness probe, include the readinessProbe field in your pod specification. For an HTTP-based readiness probe, specify the endpoint path and port. For example: apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
  labels:
    name: simple-webapp
spec:
  containers:
  - name: simple-webapp
    image: simple-webapp
    ports:
    - containerPort: 8080
    readinessProbe:
      httpGet:
        path: /api/ready
        port: 8080
      initialDelaySeconds: 10
      periodSeconds: 5
      failureThreshold: 8 There are three primary types of readiness probes you can use: HTTP GET Probe: readinessProbe:
  httpGet:
    path: /api/ready
    port: 8080 TCP Socket Probe: readinessProbe:
  tcpSocket:
    port: 3306 Exec Command Probe: readinessProbe:
  exec:
    command:
      - cat
      - /app/is_ready In addition to the probe type, you can also configure parameters such as: initialDelaySeconds : The delay before the first probe. periodSeconds : How frequently the probe is executed. failureThreshold : The number of consecutive failures that trigger the pod to be marked as not ready. Now, consider a multi-pod setup under a ReplicaSet or Deployment. When scaling up, a new pod might take a minute or more to warm up. Without properly configured readiness probes, traffic could be routed to a pod that isn't fully ready, leading to potential service disruptions. Note A correctly configured readiness probe ensures that only pods that pass the readiness checks will receive traffic. This maintains a smooth user experience even during pod updates or scaling events. That concludes our discussion on readiness probes. To further reinforce these concepts, we encourage you to practice with the available labs and exercises. For more details on Kubernetes observability, refer to the Kubernetes Documentation . Happy probing! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Node Affinity,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Node-Affinity,"Certified Kubernetes Application Developer - CKAD Configuration Node Affinity Welcome to this detailed guide on node affinity in Kubernetes. In this lesson, we explore how node affinity can help control where Pods are scheduled by ensuring they run on specific nodes. This feature is especially useful when you need a large data processing Pod to run on a designated node. Previously, developers used node selectors to achieve this control. However, node selectors lack the flexibility to handle advanced query expressions, such as logical ""or"" or ""not."" Node affinity overcomes these limitations by providing advanced scheduling capabilities. Using a Node Selector Below is an example of a simple node selector to schedule a Pod on a node labeled as Large: apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
    - name: data-processor
      image: data-processor
  nodeSelector:
    size: Large Transition to Node Affinity With node affinity, you can achieve the same scheduling requirements with more flexibility. The following YAML configuration demonstrates how to specify the node affinity equivalent: apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
    - name: data-processor
      image: data-processor
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: size
                operator: In
                values:
                  - Large Explanation of the Configuration Under spec , the affinity field is introduced with a child field named nodeAffinity . The property requiredDuringSchedulingIgnoredDuringExecution ensures that the scheduler places the Pod on a node that complies with the specified rules. Once the Pod runs, modifications to node labels are ignored. Inside nodeSelectorTerms (an array), you define one or more match expressions. Each match expression includes: A key (e.g., ""size"") An operator (e.g., In ) that specifies how the label's value is evaluated using the provided values list. In this case, the Pod will be placed on nodes where the label ""size"" is set to ""Large"". Tip If your deployment requires extra flexibility—say, allowing execution on either a large or medium node—simply add both values to the values list. Excluding Nodes Using Node Affinity To exclude nodes with a specific label value, you can use the NotIn operator. For instance: apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
    - name: data-processor
      image: data-processor
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: size
                operator: NotIn
                values:
                  - Small In this configuration, the Pod is scheduled on nodes where the ""size"" label is not ""Small."" In cases where the label is absent, this rule further limits the scheduler's candidate nodes. Using the Exists Operator When you simply need to check for the presence of a label, regardless of its value, the Exists operator is ideal: apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
spec:
  containers:
    - name: data-processor
      image: data-processor
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        nodeSelectorTerms:
          - matchExpressions:
              - key: size
                operator: Exists The Exists operator does not require a values array since it only verifies the existence of the specified label. Node Affinity Types After defining affinity rules, the Kubernetes scheduler evaluates these rules when placing Pods on nodes. Here’s how different affinity types work: Required During Scheduling, Ignored During Execution: During Scheduling: The Pod must run on a node that meets the affinity rules; if there's no match, the Pod is not scheduled. During Execution: Once running, changes to node labels do not affect the Pod. Preferred During Scheduling, Ignored During Execution: During Scheduling: The scheduler prefers nodes that meet the affinity rules but will schedule the Pod on any available node if no perfect match exists. During Execution: Similar to the required type, changes to node labels do not impact the running Pod. Note ""Required During Scheduling, Ignored During Execution"" mandates that a Pod only starts on a node meeting the rules, whereas ""Preferred During Scheduling, Ignored During Execution"" allows flexibility during scheduling by placing the Pod on an alternative node if necessary. Future Enhancements There is ongoing discussion about a new category called ""required during execution."" Unlike current settings, this model would evict a running Pod if its node's labels change, making it non-compliant with the affinity rules. For instance, if a node loses its ""size=Large"" label while hosting a Pod configured with this future type, the Pod would be terminated. Currently, with the ""ignored during execution"" setting, the Pod continues to run despite changes. Diagram Insight The diagram illustrates the three node affinity types, highlighting that while current models ignore label changes post-scheduling, a ""required during execution"" model would enforce rules dynamically by evicting Pods when necessary. Conclusion This guide has covered the fundamentals of node affinity: from basic node selectors to advanced expressions using In , NotIn , and Exists operators. Understanding these configurations will help you tailor your Kubernetes deployments for optimized scheduling. Proceed to practice these rules with coding exercises, and stay tuned for upcoming sections comparing taints and tolerations with node affinity. For further details, you may also refer to the following resources: Kubernetes Documentation Kubernetes Basics Docker Hub Terraform Registry Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Solution Taints and Tolerations Optional,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Solution-Taints-and-Tolerations-Optional,"Certified Kubernetes Application Developer - CKAD Configuration Solution Taints and Tolerations Optional In this lesson, you will learn how to work with taints and tolerations in Kubernetes. The walkthrough covers checking the cluster's nodes, examining and modifying taints on a node, deploying pods with and without tolerations, and finally, removing a taint from the control plane node. 1. Counting the Nodes First, determine the total number of nodes (including the control plane) in your cluster. Run the following command to view the nodes: root@controlplane:~# kubectl get nodes
NAME           STATUS   ROLES                  AGE    VERSION
controlplane   Ready    control_plane,master   17m    v1.20.0
node01         Ready    <none>                16m    v1.20.0
root@controlplane:~# As shown, the cluster contains two nodes: one control plane node and one worker node (node01). 2. Checking for Taints on node01 To verify if there are any taints on node01, list the nodes again: root@controlplane:~# kubectl get nodes
NAME          STATUS   ROLES                     AGE   VERSION
controlplane  Ready    control_plane,master      17m   v1.20.0
node01        Ready    <none>                    16m   v1.20.0
root@controlplane:~# Next, inspect node01 in detail: root@controlplane:~# kubectl describe node node01
Name:               node01
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=node01
                    kubernetes.io/os=linux
Annotations:        flannel.alpha.coreos.com/backend-data: {""VNI"":1,""VtepMAC"":""8e:62:74:26:35:47""}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 10.58.27.11
                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    node.alpha.kubernetes.io/ttl: 0
CreationTimestamp:  Fri, 15 Apr 2022 22:57:19 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:   node01
  AcquireTime:      <unset>
  RenewTime:        Fri, 15 Apr 2022 23:14:20 +0000
Conditions:
  Type                  Status  LastHeartbeatTime                  LastTransitionTime                Reason
  ----                  ------  -----------------                  ---------------------            ------
  NetworkUnavailable    False   Fri, 15 Apr 2022 22:57:25 +0000  Fri, 15 Apr 2022 22:57:25 +0000  Flannel
  IsUp                  True    Flannel is running on this node     Fri, 15 Apr 2022 22:57:19 +0000  Kubelet
  MemoryPressure        False   Fri, 15 Apr 2022 23:12:35 +0000  Kubelet has sufficient memory available
  DiskPressure          False   Fri, 15 Apr 2022 22:57:57 +0000  Kubelet has no disk pressure Note Since the ""Taints"" field shows <none> , node01 currently does not have any taints. 3. Adding a Taint to node01 Now, add a taint to node01 that prevents pods without the necessary toleration from being scheduled there. Use the following parameters: Key: spray Value: moreteam Effect: NoSchedule Execute this command: root@controlplane:~# kubectl taint node node01 spray=moreteam:NoSchedule After running this command, node01 now has the taint. Pods that do not tolerate this taint will not be scheduled on node01. 4. Creating the ""mosquito"" Pod (Without Tolerations) Next, create a pod named ""mosquito"" that uses the nginx image. Since this pod lacks a toleration for the taint on node01, it will remain in a pending state: root@controlplane:~# kubectl run mosquito --image=nginx
pod/mosquito created
root@controlplane:~# Verify the pod's status: root@controlplane:~# kubectl get pods
NAME        READY   STATUS    RESTARTS   AGE
mosquito    0/1     Pending   0          3m37s
root@controlplane:~# Inspect the pod details to confirm the scheduling issue: root@controlplane:~# kubectl describe pod mosquito
Name:           mosquito
Namespace:      default
Priority:       0
Node:           <none>
Labels:         run=mosquito
Annotations:    <none>
Status:         Pending
IP:             <none>
Containers:
  mosquito:
    Image:      nginx
    Port:       <none>
    Host Port:  <none>
    Environment: <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-f6lxf (ro)
Conditions:
  Type           Status
  PodScheduled   False
Volumes:
  default-token-f6lxf:
    Type:       Secret (a volume populated by a Secret)
    SecretName: default-token-f6lxf
    Optional:   false
QoS Class:      BestEffort
Tolerations:    node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type     Reason             Age               From                Message
  ----     ------             ----              ----                -------
  Warning  FailedScheduling   45s (x2 over 45s) default-scheduler   0/2 nodes are available: 1 node(s) had taint {spray: moreteam:NoSchedule}, that the pod didn't tolerate, 1 node(s) had taint {node-role.kubernetes.io/master:NoSchedule}, that the pod didn't tolerate.
root@controlplane:~# The error indicates that the pod did not tolerate the {spray: moreteam:NoSchedule} taint on node01. 5. Creating the ""bee"" Pod with a Toleration To schedule a pod with a toleration for the taint on node01, create a new pod called ""bee"" using the nginx image. Since you cannot specify tolerations directly with the kubectl run command, generate a YAML manifest with a dry run and edit it accordingly. Generate the YAML manifest: root@controlplane:~# kubectl run bee --image=nginx --dry-run=client -o yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: bee
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {} Redirect the output to a file: root@controlplane:~# kubectl run bee --image=nginx --dry-run=client -o yaml > bee.yaml Open the generated file (bee.yaml) and add the following ""tolerations"" section under the spec: apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: bee
  name: bee
spec:
  containers:
  - image: nginx
    name: bee
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
  tolerations:
  - key: ""spray""
    operator: ""Equal""
    value: ""moreteam""
    effect: ""NoSchedule""
status: {} Save the changes and apply the manifest: root@controlplane:~# kubectl apply -f bee.yaml
pod/bee created Monitor the creation process: root@controlplane:~# kubectl get pods --watch
NAME        READY   STATUS              RESTARTS   AGE
bee         0/1     ContainerCreating   0          10s
mosquito    0/1     Pending             0          3m40s
...
bee         1/1     Running             0          14s Finally, confirm that the ""bee"" pod is running on node01: root@controlplane:~# kubectl get pods -o wide
NAME        READY   STATUS    RESTARTS   AGE     IP            NODE    NOMINATED NODE   READINESS GATES
bee         1/1     Running   0          36s     10.244.1.2    node01  <none>           <none>
mosquito    0/1     Pending   0          4m6s    <none>        <none>  <none>           <none>
root@controlplane:~# 6. Removing the Control Plane Taint The control plane node initially had a taint that prevented regular pods from being scheduled on it. To allow the ""mosquito"" pod to run, remove this taint. First, check the taint on the control plane: root@controlplane:~# kubectl describe node controlplane
...
Taints:
  node-role.kubernetes.io/master:NoSchedule
... Remove the taint with the following command: root@controlplane:~# kubectl taint node controlplane node-role.kubernetes.io/master:NoSchedule- Verify that the taint has been removed: root@controlplane:~# kubectl describe node controlplane
...
Taints:            <none>
... Once the taint is removed, the ""mosquito"" pod, which was pending earlier, is automatically scheduled on the control plane node. Confirm its status: root@controlplane:~# kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
bee        1/1     Running   0          2m37s
mosquito   1/1     Running   0          6m7s
root@controlplane:~# 7. Summary This lesson demonstrated the following steps: Step Description Counting the Nodes Verified that the cluster consists of two nodes: one control plane and one worker node. Checking for Taints on node01 Inspected node01 to confirm that it did not have any taints before applying changes. Adding a Taint Applied a taint ( spray=moreteam:NoSchedule ) to node01 to prevent pods without the appropriate toleration from being scheduled. Creating the ""mosquito"" Pod (Without Toleration) Deployed a pod that remained pending because it did not tolerate the taint on node01. Creating the ""bee"" Pod with a Toleration Generated and edited a YAML manifest to include a toleration, ensuring the pod was scheduled successfully on node01. Removing the Control Plane Taint Removed the NoSchedule taint from the control plane node, allowing the pending pod to be scheduled. This concludes the lesson on taints and tolerations in Kubernetes. For more information, please refer to the Kubernetes Documentation . Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Taints Tolerations vs Node Affinity,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Taints-Tolerations-vs-Node-Affinity,"Certified Kubernetes Application Developer - CKAD Configuration Taints Tolerations vs Node Affinity Hello, and welcome to this comprehensive lesson. In this guide, we will explore how to combine taints and tolerations with node affinity in Kubernetes to ensure that specific pods run exclusively on designated nodes. In our example, we consider a Kubernetes cluster with three nodes and three pods, each associated with one of three colors: blue, red, and green. The objective is to schedule the blue pod on the blue node, the red pod on the red node, and the green pod on the green node, even when the cluster is shared with other teams running different pods on various nodes. Problem Overview In a shared cluster environment, we want to ensure that: Our dedicated nodes do not run pods from other teams. Our pods are not placed on nodes that are primarily serving other workloads. To solve this, we explore two approaches: First Approach: Taints and Tolerations In this method, each node is tainted with its respective color (blue, red, or green). Correspondingly, each pod is given a toleration that matches the node’s taint. This ensures that: Only pods with the appropriate toleration can be scheduled on the tainted node. For example, the green pod will only be scheduled on the green node, and the blue pod on the blue node. However, while taints and tolerations can prevent pods lacking the correct toleration from being scheduled on these nodes, they do not guarantee that the pods will preferentially use the dedicated nodes. Consequently, there might be cases where the red pod is scheduled on a node without the corresponding taint and toleration, resulting in undesired pod placement. Second Approach: Node Affinity Node affinity offers another strategy by allowing you to: Label nodes with their respective colors (blue, red, and green). Define node affinity rules or selectors on pods to match these labels. This method directs pods to the correct nodes based on labels. Although node affinity helps achieve the desired pod distribution, it does not prevent other pods from being scheduled on the same nodes, which could lead to conflicts with other teams’ deployments. Combining Taints, Tolerations, and Node Affinity To fully dedicate nodes exclusively for specific pods, you can harness the strengths of both approaches by combining them: Step 1: Taints First, apply taints on your nodes. This ensures that pods without the matching toleration are not scheduled on these nodes. Step 2: Node Affinity Next, implement node affinity rules on your pods. This guarantees that the pods are scheduled only on the nodes with the corresponding color labels. By using both strategies together, you achieve strict node allocation. This method ensures that only your dedicated pods run on their targeted nodes, thereby preventing any unintended pod placements from other teams. Final Thoughts This lesson has demonstrated the effective combination of taints, tolerations, and node affinity to achieve precise pod placement in a shared Kubernetes cluster. You are encouraged to experiment with these configurations in your own cluster environment to deepen your understanding of Kubernetes scheduling policies and improve your cluster management practices. Happy Kubernetes configuring! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solution Multi Container Pods Optional,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Multi-Container-Pods/Solution-Multi-Container-Pods-Optional,"Certified Kubernetes Application Developer - CKAD Multi Container Pods Solution Multi Container Pods Optional In this article, we review a lab exercise on multi-container pods. The lab demonstrates how to identify container counts in pods, create multi-container pods using specified images, and add a sidecar container to forward logs to Elasticsearch. This step-by-step guide will help you understand these concepts and implement them in your Kubernetes environment. Identifying Containers in the Red Pod To determine the number of containers in the red pod, you have two options: Check the ""READY"" column in the pod listing—the numbers indicate total containers versus how many are ready. Use the describe command to inspect the pod details. In the output, look for the ""Containers:"" section. For example, the following YAML snippet shows three container entries named apple , wine , and searle : Containers:
  apple:
    Container ID: busybox
    Image: <none>
    Image ID: <none>
    Port:
    Host Port: <none>
    Command:
      sleep
      4500
    State: Waiting
    Reason: ContainerCreating
    Ready: False
    Restart Count: 0
    Environment: <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7sggv (ro)
  wine:
    Container ID: busybox
    Image: <none>
    Image ID: <none>
    Port:
    Host Port: <none>
    Command:
      sleep
      4500
    State: Waiting
    Reason: ContainerCreating
    Ready: False
    Restart Count: 0
    Environment: <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-7sggv (ro)
  searle:
    Container ID: busybox
    Image: <none>
    Image ID: <none>
    Port:
    Host Port: <none>
    Command:
      <none> Both methods confirm that the red pod contains three containers. Identifying Containers in the Blue Pod Next, determine the container names in the blue pod. Examining its details reveals two containers, named teal and navy . Below is an excerpt of the pod details: Node:                          controlplane/10.40.119.6
Start Time:                    Sun, 17 Apr 2022 18:16:47 +0000
Labels:                        <none>
Annotations:                   <none>
Status:                        Pending
IP:                            <none>
IPs:                           <none>
Containers:
  teal:
    Container ID:              
    Image:                     busybox
    Image ID:                  <none>
    Port:                      <none>
    Host Port:                 <none>
    Command:
    - sleep
    - '4500'
    State:                      Waiting
    Reason:                     ContainerCreating
    Ready:                      False
    Restart Count:              0
    Environment:                <none>
    Mounts:                    /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6qm72 (ro)
  navy:
    Container ID:              
    Image:                     busybox
    Image ID:                  <none>
    Port:                      <none>
    Host Port:                 <none>
    Command:
    - sleep
    - '4500'
    State:                      Waiting
    Reason:                     ContainerCreating
    Ready:                      False
    Restart Count:              0
    Environment:                <none>
    Mounts:                    /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-6qm72 (ro) This confirms that the blue pod runs two containers: teal and navy . Creating a Multi-Container Pod (Yellow Pod) The next task is to create a multi-container pod named yellow that includes two containers. Follow these specifications: Container 1 : Named lemon , uses the busybox image with a sleep command ( sleep 1000 ) to prevent a CrashLoopBackOff state. Container 2 : Named gold , uses the redis image. An image from the lab instructions illustrates these details: Step 1: Generate the Base Pod Definition Run the following command using dry run to generate a YAML definition for the pod: kubectl run yellow --image=busybox --dry-run=client -o yaml The command produces a YAML definition similar to this: apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: yellow
  name: yellow
spec:
  containers:
  - image: busybox
    name: yellow
    resources: {}
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {} Step 2: Update the YAML Rename the first container from yellow to lemon and add the sleep command. Then, include a second container named gold with the redis image. The updated YAML should look like this: apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    run: yellow
  name: yellow
spec:
  containers:
  - image: busybox
    name: lemon
    resources: {}
    command: [""sleep"", ""1000""]
  - image: redis
    name: gold
  dnsPolicy: ClusterFirst
  restartPolicy: Always
status: {} Save this file (e.g., as yellow.yaml ) and apply the configuration: kubectl apply -f yellow.yaml Verifying the Yellow Pod After the pod is created, verify the status with: kubectl get pods You should see the yellow pod with two containers: lemon (busybox) running with sleep 1000 and gold (redis). Exploring the Elastic Stack Logging Setup This section covers the deployment of a simple application and a logging stack within the elastic-stack namespace. The deployment includes: app pod (running the event simulator) elastic-search pod kibana pod Wait until all pods are in the running state: kubectl get pods -n elastic-stack Expected output: root@controlplane ~ # kubectl get pods -n elastic-stack
NAME              READY   STATUS    RESTARTS   AGE
app               1/1     Running   0          5m31s
elastic-search    1/1     Running   0          5m31s
kibana            1/1     Running   0          5m30s Inspecting the Kibana UI Once the pods are up and running, open the Kibana UI using the link provided above your terminal. Kibana serves as the dashboard for viewing the logs gathered by Elasticsearch. Elasticsearch collects log data—such as metrics and application logs—and Kibana visualizes them. To view logs from Kibana, run: kubectl -n elastic-stack logs kibana Investigating the Application Pod Next, examine the app pod to verify its container configuration and image details. The app pod runs a single container with an event simulator that sends log messages to /log/app.log . An excerpt from its description is shown below: Priority: 0
Node: controlplane/10.40.119.6
Start Time: Sun, 17 Apr 2022 18:14:50 +0000
Labels:
  name: app
Annotations: <none>
Status: Running
IPs:
  IP: 10.244.0.4
Containers:
  Container ID: docker://2770f0362307539e774d768fea3a27327257b7be7b460aad978a91c26bfb7c
  Image: kodekloud/event-simulator
  Image ID: docker-pullable://kodekloud/event-simulator@sha256:1e3e9c72136bbc76c96dd98f29c04f298c3ae241c7d44e2bf70bcc209b030bf9
  Port: <none>
  Host Port: <none>
  State: Running
  Started: Sun, 17 Apr 2022 18:15:06 +0000
  Ready: True
  Restart Count: 0
  Environment: <none>
  Mounts:
    /log from log-volume (rw)
    /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wnzb6 (ro)
Conditions:
  Type               Status
  Initialized        True
  Ready              True
  ContainersReady    True
  PodScheduled       True
Volumes:
  log-volume:
    Type:     HostPath (bare host directory volume)
    Path:     /var/log/webapp
    HostPathType: DirectoryOrCreate
  kube-api-access-wnzb6:
    Type:     Projected (a volume that contains injected data from multiple sources) The event simulator writes its logs to /log/app.log . To view the log entries directly within the pod, execute: kubectl -n elastic-stack exec -it app -- cat /log/app.log Inspect the log entries to identify any user issues. For example, the logs indicate that USER5 experienced login problems due to the account being locked after multiple failed attempts: [2022-04-17 18:21:57,696] INFO in event-simulator: USER4 is viewing page3
[2022-04-17 18:21:58,698] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
... Thus, the problematic user is USER5 . Adding a Sidecar Container for Log Shipping to Elasticsearch To forward logs from the app pod to Elasticsearch, a sidecar container must be added. This container, named sidecar , utilizes a custom Fluent Bit image (configured similarly to Filebeat) to read logs from a shared volume and forward them to Elasticsearch. Editing the App Pod Definition The current definition of the app pod includes only the event simulator container. To implement log forwarding, modify the pod definition to include both the event simulator and the sidecar container, ensuring that they share the same volume. Below is an example YAML snippet with the updated configuration: apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: ""2022-04-17T18:14:50Z""
  labels:
    name: app
  name: app
  namespace: elastic-stack
  resourceVersion: ""1217""
  uid: 1d0aca45-2fd7-4c79-a6e3-e0c744e2dd3f
spec:
  containers:
    - image: kodekloud/filebeat-configured
      name: sidecar
      volumeMounts:
        - mountPath: /var/log/event-simulator/
          name: log-volume
    - image: kodekloud/event-simulator
      name: app
      imagePullPolicy: Always
      resources: {}
      terminationMessagePath: /dev/termination-log
      terminationMessagePolicy: File
      volumeMounts:
        - mountPath: /log
          name: log-volume
  dnsPolicy: ClusterFirst
  enableServiceLinks: true
  nodeName: controlplane
  preemptionPolicy: PreemptLowerPriority
  priority: 0 In this configuration: The sidecar container mounts log-volume at /var/log/event-simulator/ (where Filebeat expects the logs). The app container writes logs to /log . Updating the Pod Since Kubernetes does not permit the modification of container sets on a live pod, you must force replace the existing pod with the updated YAML. If you try to edit the pod interactively, you might see an error similar to: Warning Editing an existing pod to add or remove containers is not allowed. Instead, update the configuration file and force replace the pod. To apply the changes, use the force replace command: kubectl replace --force -f /tmp/kubectl-edit-3922970489.yaml The output should confirm the replacement: pod ""app"" deleted
pod/app replaced Finally, verify the pod status: kubectl get pods -n elastic-stack Verifying Logs in Kibana After updating the app pod with the sidecar container, open the Kibana UI to confirm that logs are flowing into the Discover section. Follow these steps: Create an index pattern in Kibana (for example, filebeat-* ). Configure the time filter as needed. Navigate to the Discover section to view the log entries. The following image shows the Kibana interface for creating an index pattern: Once the index pattern is established, view the logs in the Discover section: This confirms that logs from the app pod are successfully forwarded to Elasticsearch and visualized in Kibana. This concludes the guide on setting up multi-container pods and integrating a sidecar container for log shipping to Elasticsearch. Happy learning! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Logging,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Observability/Logging,"Certified Kubernetes Application Developer - CKAD Observability Logging Welcome to this comprehensive guide on logging mechanisms in containerized environments. In this article, we explore logging in Docker before transitioning to Kubernetes logging practices. This guide is ideal for developers and system administrators looking to efficiently manage logs and troubleshoot containerized applications. Logging in Docker Imagine a Docker container named ""event simulator"" that produces random events to mimic a web server. This container sends these event logs to its standard output. For instance, executing the container in the foreground using the command below: docker run kodekloud/event-simulator may yield output similar to: docker run kodekloud/event-simulator
2018-10-06 15:57:15,937 - root - INFO - USER1 logged in
2018-10-06 15:57:16,943 - root - INFO - USER2 logged out
2018-10-06 15:57:17,944 - root - INFO - USER3 is viewing page3
2018-10-06 15:57:18,951 - root - INFO - USER4 is viewing page1
2018-10-06 15:57:19,954 - root - INFO - USER2 logged out
2018-10-06 15:57:20,955 - root - INFO - USER1 logged in
2018-10-06 15:57:21,956 - root - INFO - USER3 is viewing page2
2018-10-06 15:57:22,957 - root - INFO - USER4 is viewing page2
2018-10-06 15:57:23,959 - root - INFO - USER1 logged out
2018-10-06 15:57:24,963 - root - INFO - USER2 is viewing page2
2018-10-06 15:57:25,943 - root - INFO - USER4 is viewing page3
2018-10-06 15:57:26,965 - root - INFO - USER3 logged out
2018-10-06 15:57:27,965 - root - INFO - USER1 is viewing page2
2018-10-06 15:57:28,961 - root - INFO - USER4 is viewing page3
2018-10-06 15:57:29,967 - root - INFO - USER3 is viewing page1
2018-10-06 15:57:30,970 - root - INFO - USER4 logged in
2018-10-06 15:57:31,973 - root - INFO - USER1 is viewing page3 Detached Mode Tip If you run the Docker container in detached mode using the -d option, the logs won't display on the console immediately. Instead, you can inspect the logs later by using the docker logs command along with the container ID. For example, starting the container in detached mode: docker run -d kodekloud/event-simulator You can then view the logs with: docker logs <container_id> Logging in Kubernetes Transitioning to Kubernetes, the process is similar but integrated into a managed cluster environment. You can create a pod using the same Docker image defined in a pod specification file. Once the pod is running, you can stream its logs with the kubectl logs command using the -f flag. Start by creating the pod: kubectl create -f event-simulator.yaml Then, follow the logs using: kubectl logs -f event-simulator-pod Below is the example pod definition file ( event-simulator.yaml ): apiVersion: v1
kind: Pod
metadata:
  name: event-simulator-pod
spec:
  containers:
    - name: event-simulator
      image: kodekloud/event-simulator Executing the above commands will display log messages such as: 2018-10-06 15:57:15,937 - root - INFO - USER1 logged in
2018-10-06 15:57:16,943 - root - INFO - USER2 logged out
2018-10-06 15:57:17,944 - root - INFO - USER2 is viewing page2
2018-10-06 15:57:18,951 - root - INFO - USER3 is viewing page3
2018-10-06 15:57:19,954 - root - INFO - USER4 is viewing page1
2018-10-06 15:57:20,955 - root - INFO - USER2 logged out
2018-10-06 15:57:21,956 - root - INFO - USER1 logged in Multiple Containers in a Pod Kubernetes allows you to create pods with multiple containers. Suppose you update your pod specification to include an additional container called ""image-processor"" as follows: apiVersion: v1
kind: Pod
metadata:
  name: event-simulator-pod
spec:
  containers:
    - name: event-simulator
      image: kodekloud/event-simulator
    - name: image-processor
      image: some-image-processor Specify Container Name When a pod contains more than one container, running the command: kubectl logs -f event-simulator-pod results in an error because Kubernetes cannot determine which container's logs to display. To view logs for a specific container, explicitly specify the container name. For example, to view logs for the ""event-simulator"" container: kubectl logs -f event-simulator-pod event-simulator This command will then output logs similar to: 2018-10-06 15:57:15,937 - root - INFO - USER1 logged in
2018-10-06 15:57:16,943 - root - INFO - USER2 logged out
2018-10-06 15:57:17,944 - root - INFO - USER2 is viewing page2 Conclusion This guide has walked you through managing logs in both Docker and Kubernetes environments. Understanding these logging practices is critical for debugging and monitoring your applications. With these insights, you are now ready to move on to coding exercises that further reinforce your knowledge of logging in containerized environments. Thank you for reading, and happy logging! Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Solution Logging optional,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Observability/Solution-Logging-optional,"Certified Kubernetes Application Developer - CKAD Observability Solution Logging optional In this lesson, we walk through a comprehensive lab that focuses on managing application logs using Kubernetes. The lab covers two important scenarios: one diagnosing user login issues and the other addressing issues during an item purchase within a multi-container web application. By leveraging Kubernetes commands and log inspection, you will learn how to quickly identify and resolve problems. Scenario 1: Diagnosing User Login Issues Initially, we deployed a Pod hosting the application. To verify that the Pod is running, execute the following command: kubectl get pods The expected output should resemble: NAME       READY   STATUS    RESTARTS   AGE
webapp-1   1/1     Running   0          110s When USER5 reported login issues, we inspected the application logs to diagnose the problem. The logs revealed repeated failed login attempts by USER5, which resulted in the account being locked. Below are the log entries outlining the sequence of events: [2022-04-16 19:55:55,919] INFO in event-simulator: USER4 is viewing page3
[2022-04-16 19:55:56,920] INFO in event-simulator: USER1 is viewing page3
[2022-04-16 19:55:57,921] INFO in event-simulator: USER2 is viewing page2
[2022-04-16 19:55:58,923] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
[2022-04-16 19:55:58,923] INFO in event-simulator: USER7 Order failed as the item is OUT OF STOCK.
[2022-04-16 19:56:01,944] INFO in event-simulator: USER2 logged in
[2022-04-16 19:56:02,941] INFO in event-simulator: USER4 logged in
[2022-04-16 19:56:03,950] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
[2022-04-16 19:56:04,950] INFO in event-simulator: USER3 logged out
[2022-04-16 19:56:06,953] INFO in event-simulator: USER2 is viewing page2
[2022-04-16 19:56:08,957] INFO in event-simulator: USER4 logged in
[2022-04-16 19:56:10,958] INFO in event-simulator: USER3 is viewing page2
[2022-04-16 19:56:12,961] WARNING in event-simulator: USER7 Order failed as the item is OUT OF STOCK.
[2022-04-16 19:56:13,963] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
[2022-04-16 19:56:16,966] INFO in event-simulator: USER1 is viewing page3
[2022-04-16 19:56:18,968] INFO in event-simulator: USER3 logged out
[2022-04-16 19:56:20,969] INFO in event-simulator: USER2 is viewing page1 Note The logs make it clear that multiple failed login attempts by USER5 have caused an automatic account lock. This is an essential diagnostic detail when troubleshooting login issues. Scenario 2: Handling Logs in a Multi-Container Pod A second application was deployed involving two Pods. To confirm the deployment and status of these Pods, run: kubectl get pods You should now see two Pods listed: NAME       READY   STATUS    RESTARTS   AGE
webapp-1   1/1     Running   0          110s
webapp-2   2/2     Running   0          9s When inspecting the logs for the second Pod ( webapp-2 ), an error occurs because the Pod contains two containers. Attempting to view the logs without specifying a container name results in this error: kubectl logs webapp-2 error: a container name must be specified for pod webapp-2, choose one of: [simple-webapp db] This message indicates that you must specify the container name—either ""simple-webapp"" or ""db""—to view the relevant logs. For examining the web application behavior, you should use the container that manages the web service (typically ""simple-webapp""). Next, to diagnose an item purchase failure, we analyze the logs for clues. A user reported an issue during the purchase process. The log entries below help pinpoint the problem: [2022-04-16 19:57:24,773] INFO in event-simulator: USER2 logged in
[2022-04-16 19:57:25,774] INFO in event-simulator: USER1 logged in
[2022-04-16 19:57:26,775] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
[2022-04-16 19:57:26,776] INFO in event-simulator: USER4 is viewing page1
[2022-04-16 19:57:27,781] INFO in event-simulator: USER1 is viewing page1
[2022-04-16 19:57:28,781] INFO in event-simulator: USER3 is viewing page3
[2022-04-16 19:57:31,783] INFO in event-simulator: USER2 logged in
[2022-04-16 19:57:31,784] INFO in event-simulator: USER4 is viewing page1
[2022-04-16 19:57:31,785] WARNING in event-simulator: USER30 Order failed as the item is OUT OF STOCK.
[2022-04-16 19:57:34,786] INFO in event-simulator: USER1 logged in
[2022-04-16 19:57:36,788] WARNING in event-simulator: USER5 Failed to Login as the account is locked due to MANY FAILED ATTEMPTS.
[2022-04-16 19:57:36,789] WARNING in event-simulator: USER30 Order failed as the item is OUT OF STOCK.
[2022-04-16 19:57:40,796] INFO in event-simulator: USER3 is viewing page1
[2022-04-16 19:57:46,791] INFO in event-simulator: USER4 is viewing page1
[2022-04-16 19:57:46,793] INFO in event-simulator: USER3 logged out
[2022-04-16 19:57:46,794] INFO in event-simulator: USER1 is viewing page2 Note From the above logs, it is evident that USER30 encountered an issue—the order failed because the item was out of stock. This warning, together with the login failure for USER5, highlights two separate issues within the application. Summary By inspecting the logs and using the appropriate Kubernetes commands, we successfully identified the key issues: In the first scenario, USER5 experienced login failures due to repeated failed attempts that resulted in an account lock. In the second scenario, USER30 was unable to complete an order because the item was out of stock. Additionally, remember to specify a container name when retrieving logs from Pods with multiple containers. Happy logging and troubleshooting! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solution Init Containers Optional,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Multi-Container-Pods/Solution-Init-Containers-Optional,"Certified Kubernetes Application Developer - CKAD Multi Container Pods Solution Init Containers Optional In this lesson, we address several tasks related to init containers. We demonstrate how to identify pods with init containers, update a pod configuration to use a new init container, and fix an issue causing a CrashLoopBackOff in one pod. Follow the steps below for a comprehensive guide. Identifying the Pod with an Init Container First, list all the pods with: kubectl get pods Example output: NAME    READY   STATUS    RESTARTS   AGE
red     1/1     Running   0          54s
green   2/2     Running   0          54s
blue    1/1     Running   0          54s We have three pods: red, green, and blue. To determine which pod includes an init container, execute: kubectl describe pod <pod-name> Red Pod Analysis When you inspect the red pod, you see an init container section alongside the main container. Here’s a snippet from the red pod's description: Annotations: <none>
Status: Running
IP: 10.42.0.11
IPs:
  IP: 10.42.0.11
Init Containers:
  init-myservice:
    Container ID: containerd://06b2eef4bb35af6bbd7d436785fa8988bbe43617def32fb320bb1cc0f9d88708
    Image: busybox
    Image ID: docker.io/library/busybox@sha256:ef320ff10026a50cf5f0213d35537ce0041ac1d96e9b78
    Port: <none>
    Host Port: <none>
    Command:
      sh
      -c
      sleep 5
    State: Terminated
    Reason: Completed
    Exit Code: 0
    Started: Tue, 02 Aug 2022 18:16:26 +0000
    Finished: Tue, 02 Aug 2022 18:16:31 +0000
    Ready: True
    Restart Count: 0
    Environment: <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-wqxmr (ro)
Containers:
  green-container-1:
    Container ID: containerd://0124061f1d72bf48f8b8692e2a68c39962c53a43e639eb4f27cabcfb4637b51
    Image: busybox:1.28 Although an init container is shown in the red pod, further analysis of other pods is required. Green Pod Analysis The green pod does not include an init container. Its configuration is similar to the following: IP:
  IP: 10.42.0.9
Containers:
  red-container:
    Container ID: containerd://73700f570a1c0df8222bd892b67bf9c9b37f5747a800a0bd9623e8db3fbfb
    Image: busybox:1.28
    Image ID: docker.io/library/busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff03b6d416dea4f41046e0f37d47
    Port: <none>
    Host Port: <none>
    Command:
      - sh
      - -c
      - echo The app is running! && sleep 3600
    State: Running
    Started: Tue, 02 Aug 2022 18:16:25 +0000
    Ready: True
    Restart Count: 0
    Environment: <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vslgg (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  kube-api-access-vslgg:
    Type: Projected (a volume that contains injected data from multiple sources) Since it lacks an init container, proceed to the next pod. Blue Pod Analysis The blue pod includes a well-defined init container. Its description shows: Status: Running
IP:
  IP: 10.42.0.10
Containers:
  green-container-1:
    Container ID: containerd://2c091a018044c1d3a06dfae0056a7715c7469cc353d2314651b967661ff8086a
    Image: busybox:1.28
    Image ID: docker.io/library/busybox@sha256:141c253bc4c3fd0a201d32dc1f493bcf3fff003b6d6f416dea4f410460e0f37d47
    Port: <none>
    Host Port: <none>
    Command:
      sh
      -c
      echo The app is running! && sleep 3600
    State: Running
    Started: Tue, 02 Aug 2022 18:16:25 +0000
    Ready: True
    Restart Count: 0
    Environment: <none>
    Mounts:
      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2kr6f (ro)
Init Containers:
  init-myservice:
    Container ID: containerd://06b2eef4bb35a6fbbd7d436785fa8988bbe43617def32fb320bb1cc0f9d88708
    Image: busybox
    Image ID: docker.io/library/busybox@sha256:ef320ff10026a50cf5f021d35537ce0041ac1d96e9b79800bafd8c9eff6c693
    Port: <none>
    Host Port: <none>
    Command:
      - sh
      - -c
      - sleep 5
    State: Terminated
    Reason: Completed Thus, the blue pod is the one with an init container using the ""busybox"" image. The container ran a ""sleep 5"" command, and its state is confirmed as Terminated (Reason: Completed). Question 1 Recap Pod with an init container: blue Init container's image: busybox Init container's state: Terminated (Reason: Completed) Analyzing the Purple Pod A new application named purple was deployed. Its configuration includes multiple init containers. To examine it, run: kubectl describe pod purple This outputs a snippet similar to: Name:           purple
Namespace:      default
Priority:       0
Node:           controlplane/172.25.0.95
Start Time:     Tue, 02 Aug 2022 18:18:43 +0000
Status:         Pending
IP:             10.42.0.12
IPs:
  IP:           10.42.0.12
Init Containers:
  warm-up-1:
    Container ID:   containerd://86f76be333d14dcd7ff161cab46d35fddb2e22fcd4867b8f25848f386beabb10
    Image:          busybox:1.28
    Command:
      sh
      -c
      sleep 600
    State:          Running
      Started:      Tue, 02 Aug 2022 18:18:44 +0000
    Ready:          False
    Restart Count:  0
  warm-up-2:
    Container ID: <none>
    Image:        busybox:1.28
    Command:
      sh
      -c
      sleep 1200
    State:        Waiting
    Reason:       PodInitializing
    Ready:        False
    Restart Count:  0
Containers:
  purple-container:
    Container ID: <none>
    Image:        busybox:1.28 The purple pod has two init containers : warm-up-1: sleeps for 600 seconds (10 minutes) warm-up-2: sleeps for 1200 seconds (20 minutes) They run sequentially, so the main container only starts after both complete. The total wait time before availability is 1800 seconds (30 minutes). Updating a Pod to Use an Init Container The next task is to update the ""red"" pod to incorporate an init container. The new init container uses the busybox image to sleep for 20 seconds instead of a longer duration. Steps to Update the ""red"" Pod Export the current configuration: kubectl get pod red -o yaml > red.yaml Delete the existing red pod: kubectl delete pod red Edit red.yaml to add the init container. Below is the updated configuration: apiVersion: v1
kind: Pod
metadata:
  name: red
  namespace: default
spec:
  initContainers:
  - name: red-initcontainer
    image: busybox
    command:
    - ""sleep""
    - ""20""
  containers:
  - name: red-container
    image: busybox:1.28
    imagePullPolicy: IfNotPresent
    command:
    - sh
    - -c
    - echo The app is running! && sleep 3600
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-vslgg
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true Apply the new configuration: kubectl apply -f red.yaml After applying, verify that the red pod is recreated and the new init container executes the ""sleep 20"" command before starting the main container. Fixing the Issue with the Orange Pod The orange pod is encountering an issue with its init container, as it is stuck in a crash loop. Listing pods shows: kubectl get pods Output example: NAME      READY   STATUS                     RESTARTS   AGE
green     2/2     Running                    0          7m2s
blue      1/1     Running                    0          7m2s
purple    0/1     Init:0/2                   0          4m40s
orange    0/1     Init:CrashLoopBackOff      1 (12s ago) 16s
red       1/1     Running                    0          29s Describing the orange pod reveals an error in the init container command: Command:
  sh
  -c
  sleeep 2; The typo ""sleeep"" (with an extra ""e"") causes the container to fail (Exit Code: 127) and triggers a CrashLoopBackOff. Warning Ensure that command spelling is correct, as errors like this prevent the container from starting and lead to repeated restarts. Steps to Fix the Orange Pod Export the orange pod configuration: kubectl get pod orange -o yaml > orange.yaml Delete the existing orange pod: kubectl delete pod orange Edit orange.yaml to correct the command in the init container. The updated configuration should be: spec:
  initContainers:
  - name: init-myservice
    image: busybox
    imagePullPolicy: Always
    command:
    - sh
    - -c
    - sleep 2
  containers:
  - name: orange-container
    image: busybox:1.28
    imagePullPolicy: IfNotPresent
    command:
    - sh
    - -c
    - echo The app is running! && sleep 3600
    volumeMounts:
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: kube-api-access-dsgpf
      readOnly: true
  dnsPolicy: ClusterFirst
  enableServiceLinks: true Apply the corrected configuration: kubectl apply -f orange.yaml Once updated, the orange pod should operate normally without entering a crash loop. Conclusion This guide provided a step-by-step walkthrough of: Identifying pods with init containers (examining red, green, and blue pods) Reviewing key details such as image names, commands, and the state of init containers Updating a pod's configuration (red pod) to replace a lengthy sleep command in its init container with a shorter one Diagnosing and fixing a typo in the orange pod that caused a CrashLoopBackOff Following these steps will help you efficiently manage and troubleshoot init containers in your Kubernetes environment. Additional Resources Kubernetes Documentation Kubernetes Basics Docker Hub Terraform Registry Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solution Readiness and Liveness Probes,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Observability/Solution-Readiness-and-Liveness-Probes,"Certified Kubernetes Application Developer - CKAD Observability Solution Readiness and Liveness Probes This lesson explains how to implement and validate readiness and liveness probes on your Kubernetes web application. Follow along as we inspect your deployment, test with HTTP requests, scale your application, and configure probes to ensure only healthy pods receive traffic and that failing pods are automatically recovered. 1. Inspecting the Initial Deployment To begin, verify that your application is running by inspecting the pods and services in your cluster. Execute the following commands: root@controlplane ~ ➜ kubectl get pod
NAME               READY   STATUS     RESTARTS   AGE
simple-webapp-1    1/1     Running    0          2m28s

root@controlplane ~ ➜ kubectl get service
NAME               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kubernetes         ClusterIP   10.96.0.1       <none>        443/TCP         6m39s
webapp-service     NodePort    10.96.70.224    <none>        8080:30080/TCP   2m34s Opening the web portal via the Kubernetes dashboard confirms that the application is accessible. 2. Testing the Application with a Script A provided test script ( curl-test.sh ) sends HTTP requests to verify the web server’s response. Run the script as shown below: root@controlplane ~ ➜ ./curl-test.sh
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
^C
root@controlplane ~ ✗ At this stage, only one pod (simple-webapp-1) is handling the traffic. 3. Scaling Up: Adding a Second Pod To improve scaling, a second pod (simple-webapp-2) is introduced. Run the test script again to observe the behavior: root@controlplane ~ ➜ ./curl-test.sh
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Failed
Failed
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Failed
Failed
Failed
Failed Here, although simple-webapp-1 responds correctly, some requests to simple-webapp-2 fail because it is receiving traffic even though it’s not yet ready. Note The readiness probe will ensure that a pod only receives traffic when it is ready, preventing failed requests. 4. Configuring a Readiness Probe for Pod Two To stop the failed requests, configure a readiness probe for simple-webapp-2. This probe sends an HTTP GET request to the /ready endpoint on port 8080. Export the current configuration of simple-webapp-2: root@controlplane ~ ➜ kubectl get pod simple-webapp-2 -o yaml > webapp2.yaml Delete the existing pod to prepare for the updated configuration: root@controlplane ~ ➜ kubectl delete pod simple-webapp-2 Edit the webapp2.yaml file to include the readiness probe under the container configuration. Insert the following snippet: readinessProbe:
  httpGet:
    path: /ready
    port: 8080 Below is an example configuration for simple-webapp-2: apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: ""2022-08-03T15:13:44Z""
  labels:
    name: simple-webapp
  name: simple-webapp-2
  namespace: default
  resourceVersion: ""1058""
  uid: ce7bcf8f-dfd4-42e0-a16f-54061fbeb85d
spec:
  containers:
  - env:
    - name: APP_START_DELAY
      value: ""80""
    image: kodekloud/webapp-delayed-start
    imagePullPolicy: Always
    name: simple-webapp
    ports:
    - containerPort: 8080
      protocol: TCP
    readinessProbe:
      httpGet:
        path: /ready
        port: 8080
  resources: {}
  terminationMessagePath: /dev/termination-log
  terminationMessagePolicy: File
  volumeMounts:
  - mountPath: /var/run/secrets/kubernetes.io/serviceaccount Re-deploy the updated pod configuration: root@controlplane ~ ➜ kubectl apply -f webapp2.yaml Verify the effectiveness of the readiness probe by watching the status of your pods. 5. Verifying Load Balancing After Applying the Readiness Probe Re-run the test script to ensure that traffic is now only directed to pods that are ready. Initially, traffic might still be sent to simple-webapp-1, but once simple-webapp-2 becomes ready, load balancing will occur: root@controlplane ~ ➜ kubectl apply -f webapp2.yaml
pod/simple-webapp-2 created
root@controlplane ~ ➜ ./curl-test.sh
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-1 : I am ready! OK
Message from simple-webapp-2 : I am ready! OK
^C At this point, traffic is successfully balanced between the two pods. 6. Simulating a Pod Crash To understand the behavior when a pod fails, a crash script ( crash-app.sh ) is provided to simulate a failure in simple-webapp-2: root@controlplane ~ ➜ ./crash-app.sh
Message from simple-webapp-2 : Mayday! Mayday! Going to crash!
root@controlplane ~ ➜ After running the crash script, check the pods using: root@controlplane ~ ➜ kubectl get pod The test script ( curl-test.sh ) will temporarily show that requests are routed only to simple-webapp-1 until Kubernetes restarts the crashed pod automatically. Warning When a pod crashes, Kubernetes automatically restarts it; however, during the downtime, ensure that your application can handle the temporary loss of a pod. 7. Implementing a Liveness Probe to Handle Freezing In this final section, you will configure a liveness probe to automatically recover pods that become unresponsive (frozen). Update the Configuration for Both Pods Export the current configuration for your pods: root@controlplane ~ ➜ kubectl get pod -o yaml > webapp.yaml Delete all existing pods to redeploy them with the new liveness probe configuration: root@controlplane ~ ➜ kubectl delete pod --all Open the webapp.yaml file and insert the liveness probe under the container specification for each pod. For example, for simple-webapp-1, add: livenessProbe:
  httpGet:
    path: /live
    port: 8080
  periodSeconds: 1
  initialDelaySeconds: 80 The full configuration for simple-webapp-1 might look like this: apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: ""2022-08-03T15:10:30Z""
  labels:
    name: simple-webapp
  name: simple-webapp-1
  namespace: default
  resourceVersion: ""825""
  uid: 694b5225-1ad4-422c-b514-4f139c84ecf2
spec:
  containers:
  - image: kodekloud/webapp-delayed-start
    imagePullPolicy: Always
    name: simple-webapp
    ports:
    - containerPort: 8080
      protocol: TCP
    livenessProbe:
      httpGet:
        path: /live
        port: 8080
      periodSeconds: 1
      initialDelaySeconds: 80
  resources: {}
  terminationMessagePath: /dev/termination-log
  terminationMessagePolicy: File
  volumeMounts:
  - mountPath: /var/run/secrets/kubernetes.io/serviceaccount Similarly, update the configuration for simple-webapp-2 as follows: apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: ""2022-08-03T15:17:03Z""
  labels:
    name: simple-webapp
  name: simple-webapp-2
  namespace: default
  resourceVersion: ""1858""
  uid: 82ec848b-b098-45f0-9975-513ef5d21b1d
spec:
  containers:
  - env:
    - name: APP_START_DELAY
      value: ""30""
    image: kodekloud/webapp-delayed-start
    imagePullPolicy: Always
    name: simple-webapp
    ports:
    - containerPort: 8080
      protocol: TCP
    readinessProbe:
      failureThreshold: 3
      httpGet:
        path: /ready
        port: 8080
        scheme: HTTP
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 1
    livenessProbe:
      httpGet:
        path: /live
        port: 8080
      periodSeconds: 1
      initialDelaySeconds: 80
  resources: {}
  terminationMessagePath: /dev/termination-log
  terminationMessagePolicy: File Redeploy the updated configuration: root@controlplane ~ ➜ kubectl apply -f webapp.yaml Test the Liveness Probe Simulate a pod freeze by executing the freeze script: root@controlplane ~ ➜ ./freeze-app.sh Then, use the test script ( curl-test.sh ) to observe how traffic is affected. Finally, check the pod statuses: root@controlplane ~ ➜ kubectl get pod You should notice that Kubernetes restarts the frozen pod automatically, ensuring continuous service availability. Summary The readiness probe ensures that only pods that are ready receive traffic. The liveness probe automatically restarts pods that become unresponsive or frozen. By following these configurations, you enhance the overall availability and reliability of your Kubernetes-deployed applications. Happy deploying! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Liveness Probes,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Observability/Liveness-Probes,"Certified Kubernetes Application Developer - CKAD Observability Liveness Probes Hello, and welcome to this comprehensive guide on liveness probes in Kubernetes. My name is Mumshad Mannambeth, and in this article, we will explore how liveness probes help maintain container health, ensuring that your applications remain resilient and available. Understanding the Basics When you run an Nginx container using Docker: docker run nginx the container starts serving users immediately. However, if the Nginx process crashes, the container will exit. You can inspect the container status with: docker ps -a which might produce output similar to: CONTAINER ID        IMAGE               CREATED             STATUS                     PORTS
45aacca36850        nginx               43 seconds ago      Exited (1) 41 seconds ago Since Docker is not designed for orchestration, the container remains stopped until you manually restart it. In contrast, running the same web application in Kubernetes provides automated resilience. If the application crashes, Kubernetes will detect the issue and automatically restart the container. For example, you can start the container with: kubectl run nginx --image=nginx Check the pod status and observe the restart count by executing: kubectl get pods A sample output may look like this: NAME       READY   STATUS      RESTARTS   AGE
nginx-pod  0/1     Completed   1          1d If the application crashes repeatedly, the restart count increases: kubectl get pods Output: NAME       READY   STATUS      RESTARTS   AGE
nginx-pod  0/1     Completed   2          1d Note Even if a container appears ""up"", the application inside might not function correctly—for example, if it gets stuck in an infinite loop due to a bug. In such cases, Kubernetes would not automatically remedy the issue without further configuration. The Role of Liveness Probes This is where liveness probes become essential. A liveness probe periodically checks the health of your application running inside the container. If the probe’s test fails, Kubernetes considers the container unhealthy and recreates it to restore service. As a developer, you define what “healthy” means for your specific application. For a web service, it might mean that the API is responsive; for a database, ensuring the TCP socket is open could be the test; or you may choose to execute a custom command for more complex scenarios. Liveness probes are defined within your pod's container specification in a manner similar to readiness probes but in a distinct configuration section. The available methods include: HTTP GET: Perform an HTTP request to check an API endpoint. TCP Socket: Validate that a specific TCP port is accepting connections. Exec Command: Run a command inside the container to verify application health. Additionally, you can customize the behavior of the probe with parameters such as the initial delay, probe frequency, and failure thresholds. Configuring a Liveness Probe Below is an example of a complete pod definition that uses an HTTP GET liveness probe for a simple web application: apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
  labels:
    name: simple-webapp
spec:
  containers:
    - name: simple-webapp
      image: simple-webapp
      ports:
        - containerPort: 8080
      livenessProbe:
        httpGet:
          path: /api/healthy
          port: 8080
        initialDelaySeconds: 10
        periodSeconds: 5
        failureThreshold: 8 Below are additional configuration examples for liveness probes: Using a TCP Socket livenessProbe:
  tcpSocket:
    port: 3306 Using an Exec Command livenessProbe:
  exec:
    command:
      - cat
      - /app/is_healthy Important Ensure that your liveness probe configuration accurately reflects your application's health criteria. Misconfigured probes can lead to unnecessary restarts and disruptions. Summary This article has detailed the concept of liveness probes and demonstrated how to configure them to manage container health automatically in Kubernetes. Experiment with these configurations in your development setup to better understand how liveness probes can optimize application availability and reliability. For more detailed Kubernetes guides and examples, continue exploring our resources and practical coding exercises. See you in the next article! Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Solution Node Affinity Optional,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Configuration/Solution-Node-Affinity-Optional,"Certified Kubernetes Application Developer - CKAD Configuration Solution Node Affinity Optional In this lesson, we demonstrate a practical test for node affinity configuration in Kubernetes. You will learn how to identify node labels, assign a new label to a node, and configure deployments with node affinity rules that restrict pod scheduling to specific nodes. ────────────────────────────── Step 1: Identify the Labels on node01 Begin by examining the labels on node01 . Run the following command to obtain detailed node information: root@controlplane:~# kubectl describe node node01
Name:               node01
Roles:              <none>
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/os=linux
Annotations:        flannel.alpha.coreos.com/backend-data: {""VNI"":1,""VtepMAC"":""5a:c6:da:f2:19:f9""}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 10.35.54.12
                    kubeadm.alpha.kubernetes.io/cri-socket: /var/run/dockershim.sock
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 16 Apr 2022 17:27:31 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:   node01
  AcquireTime:      <unset>
  RenewTime:        Sat, 16 Apr 2022 17:39:41 +0000
Conditions:
  Type                Status  LastHeartbeatTime                 LastTransitionTime             Reason
  ----                ------  ------------------                 ------------------             ------
  NetworkUnavailable  False   Sat, 16 Apr 2022 17:27:38 +0000      Sat, 16 Apr 2022 17:27:38 +0000   Flannel
  MemoryPressure      False   Sat, 16 Apr 2022 17:37:46 +0000      Sat, 16 Apr 2022 17:27:31 +0000   Kubelet has sufficient memory available
  DiskPressure        False   Sat, 16 Apr 2022 17:37:46 +0000      Sat, 16 Apr 2022 17:27:31 +0000   Kubelet has no disk pressure
  PIDPressure         False   Sat, 16 Apr 2022 17:37:46 +0000      Sat, 16 Apr 2022 17:27:31 +0000   Kubelet has sufficient PID available
  Ready               True    Sat, 16 Apr 2022 17:37:46 +0000      Sat, 16 Apr 2022 17:27:42 +0000   Kubelet is posting ready status
Addresses:
  InternalIP:       10.35.54.12 From the output, note the label beta.kubernetes.io/arch with the value amd64 . ────────────────────────────── Step 2: Apply a New Label on node01 Next, assign a new label color=blue to node01 . Execute the following command: root@controlplane:~# kubectl label node node01 color=blue Verify that the label has been applied by inspecting the node details: root@controlplane:~# kubectl describe node node01
...
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/hostname=node01
                    color=blue
... Tip Labeling nodes strategically can help in managing pod placement and workload isolation across your Kubernetes cluster. ────────────────────────────── Step 3: Create the ""blue"" Deployment with Node Affinity for node01 Create a deployment named blue utilizing the nginx image with three replicas: root@controlplane:~# kubectl create deployment blue --image=nginx --replicas=3
deployment.apps/blue created Before adding node affinity, confirm that no taints are present that could interfere with pod scheduling. Check for taints on node01: root@controlplane:~# kubectl describe node node01 | grep Taints
Taints: <none> Also, inspect the node status: root@controlplane:~# kubectl get nodes
NAME           STATUS   ROLES                     AGE   VERSION
controlplane   Ready    control-plane,master      15m   v1.20.0
node01         Ready    <none>                    14m   v1.20.0 Since there are no taints on the nodes, pods can be scheduled on any node by default. Now, update the blue deployment to enforce node affinity. Edit the deployment's pod specification to include a node affinity rule that restricts pods to nodes labeled with color=blue . Integrate the following YAML snippet under the spec.template.spec section: apiVersion: apps/v1
kind: Deployment
metadata:
  name: blue
  labels:
    app: blue
spec:
  replicas: 3
  selector:
    matchLabels:
      app: blue
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
  template:
    metadata:
      labels:
        app: blue
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: color
                    operator: In
                    values:
                      - blue
      containers:
        - name: nginx
          image: nginx
          imagePullPolicy: Always
          resources: {}
          terminationMessagePath: /dev/termination-log
          terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always Save your changes. To verify that the pods are scheduled on node01 , execute: root@controlplane:~# kubectl get pods -o wide
NAME                         READY   STATUS      RESTARTS   AGE   IP           NODE    NOMINATED NODE   READINESS GATES
blue-566c768bd6-f8xzm        1/1     Running     0          16s   10.244.1.5   node01  <none>           <none>
blue-566c768bd6-jsz95        1/1     Running     0          9s    10.244.1.7   node01  <none>           <none>
blue-566c768bd6-sf9dk        1/1     Running     0          13s   10.244.1.6   node01  <none>           <none> All blue deployment pods are now correctly placed on node01 as dictated by the node affinity rule. ────────────────────────────── Step 4: Create the ""red"" Deployment with Node Affinity for the Control Plane In the next step, we create a deployment named red using the nginx image with two replicas. The deployment is configured to run its pods exclusively on the control plane node by leveraging the node-role.kubernetes.io/master label. First, generate the deployment YAML file using a dry run: root@controlplane:~# kubectl create deployment red --image=nginx --replicas=2 --dry-run=client -o yaml > red.yaml Edit the generated red.yaml file to add the node affinity rule under the spec.template.spec section. Update the file to resemble the following: apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: null
  labels:
    app: red
  name: red
spec:
  replicas: 2
  selector:
    matchLabels:
      app: red
  strategy: {}
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: red
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
              - matchExpressions:
                  - key: node-role.kubernetes.io/master
                    operator: Exists
      containers:
        - image: nginx
          name: nginx
          resources: {}
status: {} Save the changes and create the deployment by running: root@controlplane:~# kubectl create -f red.yaml
deployment.apps/red created Verify that the pods of the red deployment are scheduled on the control plane by checking their node assignments: root@controlplane:~# kubectl get pods -o wide Deployment Best Practice Using a dry run to generate deployment YAML files allows you to safely modify pod specifications—such as adding node affinity—before applying the changes to your cluster. ────────────────────────────── Additional Diagram Reference The image below illustrates the process for creating the red deployment with the nginx image, two replicas, and node affinity targeting the control plane node by checking for the label node-role.kubernetes.io/master : ────────────────────────────── Conclusion In this lesson, you learned how to: Identify node labels with the kubectl describe node command. Apply custom labels to nodes using the kubectl label command. Configure node affinity in a deployment to restrict pod scheduling based on node labels. Generate and modify deployment YAML files using a dry run to enforce node affinity settings for both worker nodes and control plane nodes. This completes the lab on node affinity configuration in Kubernetes. For further reading and more advanced scenarios, you may refer to Kubernetes Documentation . Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solution Rolling Updates and Rollbacks optional,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/POD-Design/Solution-Rolling-Updates-and-Rollbacks-optional,"Certified Kubernetes Application Developer - CKAD POD Design Solution Rolling Updates and Rollbacks optional In this lesson, we explore rolling updates and rollbacks, with a focus on rolling updates. We will demonstrate how to verify your environment and update your application while ensuring minimal downtime. For simplicity, we use an alias (k) for kubectl. The alias is confirmed to be set with the following output: kubectl [flags] [options]

Use ""kubectl <command> --help"" for more information about a given command.
Use ""kubectl options"" for a list of global command-line options (applies to all commands).

controlplane ~ ⬢ alias
alias k='kubectl'
alias kubectl='k3s kubectl'
alias vi='vim'

controlplane ~ ⬢ cl Checking the Application Deployment We have deployed a simple web application. First, inspect the Pods: controlplane ~ ⟩ k get pod
NAME                            READY   STATUS    RESTARTS   AGE
frontend-5c74c57d95-mkjgh      1/1     Running   0          48s
frontend-5c74c57d95-dkbjj      1/1     Running   0          48s
frontend-5c74c57d95-gk6xp      1/1     Running   0          48s
frontend-5c74c57d95-xpwbt      1/1     Running   0          48s Then, check the Deployment details: controlplane ~ ⟩ k get deploy
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
frontend   4/4     4            4           55s

controlplane ~ ⟩ Wait for the application to be fully deployed. Once it is ready, open the provided web app portal in your browser. The application displays the message “hello, front end moment.” Q1: What Is the Current Color of the Web Application? Recheck the Pods and Deployment information: controlplane ~ ↗ k get pod
NAME                                     READY   STATUS    RESTARTS   AGE
frontend-5c74c57d95-nkgjh                1/1     Running   0          48s
frontend-5c74c57d95-dkbj1                 1/1     Running   0          48s
frontend-5c74c57d95-gk6xp                1/1     Running   0          48s
frontend-5c74c57d95-xpwbt                1/1     Running   0          48s

controlplane ~ ↗ k get deploy
NAME      READY   UP-TO-DATE   AVAILABLE   AGE
frontend  4/4     4            4           55s After running a test script that sends multiple requests, the output confirms the application color is blue: Hello, Application Version: v1 ; Color: blue OK
Hello, Application Version: v1 ; Color: blue OK
... This script simulates multiple users accessing the web application, confirming that all responses return the blue color. Additionally, the deployment details confirm that four Pods have been created as expected. Q2: What Container Image Is Used to Deploy the Application? To inspect the container image, first get the deployment details: controlplane ~ ⟩ k get deploy
NAME      READY   UP-TO-DATE   AVAILABLE   AGE
frontend  4/4     4            4           2m41s

controlplane ~ ⟩ [] Then, describe the deployment to view the pod template: controlplane ~ ⟩ k describe deploy frontend
Name:                   frontend
Namespace:              default
CreationTimestamp:      Sat, 16 Apr 2022 20:35:48 +0000
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               name=webapp
Replicas:               4 desired | 4 updated | 4 total | 4 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        20
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:    name=webapp
  Containers:
   simple-webapp:
      Image: kodekloud/web-app-color:v1
      Port: 8080/TCP
      Host Port: 0/TCP
      Environment: <none>
      Mounts: <none>
Conditions:
  Type              Status  Reason
  Available         True    MinimumReplicasAvailable
  Progressing       True    NewReplicaSetAvailable
OldReplicaSet:      <none>
NewReplicaSet:      frontend-5c74c57d95 (4/4 replicas created)
Events:
  Type    Reason            Age   From                      Message
  Normal  ScalingReplicaSet  3m4s  deployment-controller     Scaled up replica set frontend-5c74c57d95 to 4
controlplane ~ ⟩ The container image used is ""kodekloud/web-app-color:v1"". Q3: What Is the Current Deployment Strategy? The deployment details show that a rolling update strategy is in use. With this strategy, Pods are replaced one at a time, ensuring continuous availability. Specifically, only 25% of Pods are allowed to be unavailable at any given moment, preventing complete downtime. Note A rolling update strategy allows gradual replacement of Pods to ensure that some instances are always available during an upgrade. Upgrading the Application to Version 2 To upgrade the application, update the image in the deployment to version 2 using the ""kubectl set image"" command. Our container name is ""simple-webapp"". Run the following command: kubectl set image deploy frontend simple-webapp=kodekloud/webapp-color:v2 After executing the command, verify that the image has been updated by describing the deployment again. You should now see version v2 in the container image field. Run the curl test (or your test script) again: ./curl-test.sh The output may initially show a mix of blue and green responses: Hello, Application Version: v1 ; Color: blue OK
Hello, Application Version: v2 ; Color: green OK
... This indicates a gradual rollout where some Pods still run the older version (blue) and others have been updated (green). Eventually, all responses will reflect the new version. Scaling Considerations During a Rollout With the current rolling update settings (25% max unavailable) and a deployment of 4 replicas, only one Pod is taken down at a time. The following command confirms these updates and settings: controlplane ~ ✗ k set image deploy frontend simple-webapp=kodekloud/webapp-color:v2
deployment.apps/frontend image updated

controlplane ~ ✗ k describe deploy frontend
Name:                   frontend
Namespace:              default
CreationTimestamp:      Sat, 16 Apr 2022 20:35:48 +0000
...
Replicas:               4 desired | 2 updated | 5 total | 3 available | 2 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        20
RollingUpdateStrategy:  25% max unavailable, 25% max surge
... Changing the Deployment Strategy to Recreate If you prefer to minimize complexities related to gradual updates, you can change the deployment strategy to ""Recreate"". This strategy terminates all running Pods before creating new ones. To update the deployment strategy, use kubectl edit on the deployment and modify the strategy section as shown below: apiVersion: apps/v1
kind: Deployment
metadata:
  annotations:
    deployment.kubernetes.io/revision: ""2""
  creationTimestamp: ""2022-04-16T20:35:48Z""
  generation: 2
  name: frontend
  namespace: default
  resourceVersion: ""1053""
  uid: a8b0a6e3-1c54-47e6-9e4a-b037be079fab
spec:
  minReadySeconds: 20
  progressDeadlineSeconds: 600
  replicas: 4
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      name: webapp
  strategy:
    type: Recreate
  template:
    metadata:
      creationTimestamp: null
      labels:
        name: webapp
    spec:
      containers:
        - image: kodekloud/webapp-color:v2
          imagePullPolicy: IfNotPresent
          name: simple-webapp
      ports:
        - /tmp/kubectl-edit-2730099630.yaml 68L, 181B After saving the changes, verify that the strategy has been updated to ""Recreate"". Note that the recreate strategy does not support rolling update parameters. Warning Switching to the ""Recreate"" strategy may cause temporary downtime since all Pods are terminated before new ones are created. Upgrading the Application to Version 3 Finally, upgrade the application by setting the deployment image to version 3: kubectl set image deploy frontend simple-webapp=kodekloud/webapp-color:v3 Then, run your test script once again: ./curl-test.sh Since the deployment strategy is now set to “Recreate”, you might experience a brief period of downtime with failed requests (e.g., ""Failed"" messages or ""bad gateway"" errors) during the Pod replacement process. Once the new Pods are running, the output should stabilize to: Hello, Application Version: v3 ; Color: red OK
Hello, Application Version: v3 ; Color: red OK
... This confirms that the new version (v3) displaying red is fully deployed. That concludes the lesson on rolling updates and rollbacks. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Multi Container Pods,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Multi-Container-Pods/Multi-Container-Pods,"Certified Kubernetes Application Developer - CKAD Multi Container Pods Multi Container Pods Hello and welcome to this comprehensive lesson on multi-container pods in Kubernetes. My name is Mumshad Mannambeth, and in this guide, we will explore the design, functionality, and benefits of using multi-container pods. Multi-container pods in Kubernetes allow you to run two or more containers together in a single pod. This approach is essential when containerized services must work closely together, such as pairing a web server with a logging agent. In such cases, the containers share the same lifecycle, network namespace, and storage volumes, ensuring smooth communication and synchronized behavior. Microservices vs. Tightly Coupled Services Modern application development increasingly favors microservices, where large monolithic applications are broken down into smaller, independent sub-components. This design paradigm allows developers to build, deploy, and scale individual services independently, streamlining both development and maintenance. However, there are specific situations when two services must operate side by side. For example, a web server might need a dedicated logging agent running alongside it to capture detailed logs without incorporating supplementary code into the web server container. This setup facilitates independent deployment and scaling while keeping the application code modular. Advantages of Multi-Container Pods Multi-container pods offer several significant advantages: Shared Lifecycle: All containers in the pod start and stop simultaneously. Unified Network Namespace: Containers communicate via localhost without extra configuration. Shared Storage Volumes: Persistent and shared storage is available between containers. Note To create a multi-container pod, simply list multiple container definitions under the containers array in your pod specification file. Creating a Multi-Container Pod Below is an example of a basic pod definition with a single container: apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
  labels:
    name: simple-webapp
spec:
  containers:
  - name: simple-webapp
    image: simple-webapp
    ports:
    - containerPort: 8080 To transform this into a multi-container pod, add another container entry to include, for instance, a log agent container alongside the web server: apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
  labels:
    name: simple-webapp
spec:
  containers:
  - name: simple-webapp
    image: simple-webapp
    ports:
    - containerPort: 8080
  - name: log-agent
    image: log-agent Multi-Container Pod Design Patterns There are three common patterns for designing multi-container pods: Sidecar Pattern Adapter Pattern Ambassador Pattern Sidecar Pattern The sidecar pattern involves deploying a supplemental container, such as a logging agent, alongside your primary container. This design pattern enables services to extend or enhance the capabilities of the main application without altering its code. It is particularly useful when the application produces logs in various formats across different services. Adapter Pattern The adapter pattern is useful when you need to standardize data formats. For example, when logs from multiple sources need to be unified before they are processed by a central logging service. Log messages might vary as follows: 12-JULY-2018 16:05:49 ""GET /index1.html"" 200
12/JUL/2018:16:05:49 -0800 ""GET /index2.html"" 200
GET 1531411549 ""/index3.html"" 200 An adapter container can normalize these formats, ensuring consistency before the logs reach the centralized system. Ambassador Pattern The ambassador pattern is applied when an application needs to communicate with different database environments. For example, the application might require a local database for development, another for testing, and a production database in live deployment. Instead of incorporating logic to handle multiple environments in the application code, an ambassador container acts as a proxy. The application always sends requests to localhost, while the ambassador routes traffic to the appropriate database backend. Each of these patterns leverages the flexible design of multi-container pods by including multiple containers within a single pod specification. Next Steps Head over to the coding exercises section to practice configuring multi-container pods. Real-world practice will solidify your understanding of these patterns and their implementation in Kubernetes. That concludes this lesson on multi-container pods in Kubernetes. Thank you for reading, and see you in the next lesson! Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Labels Selectors Annotations,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/POD-Design/Labels-Selectors-Annotations,"Certified Kubernetes Application Developer - CKAD POD Design Labels Selectors Annotations Hello and welcome! I'm Mumshad Mannambeth, and in this article we explore how Kubernetes leverages labels, selectors, and annotations to efficiently organize and manage objects within a cluster. Understanding these concepts is crucial for maintaining large-scale, dynamic environments. Labels and selectors offer a systematic approach for grouping and filtering objects. Think of them like product filters in an online store or YouTube video tags; you attach specific properties (labels) to each item and then use selectors to filter items based on those properties. This helps when you want to narrow down a collection—for instance, filtering for green animals or green birds from a larger set. In Kubernetes, objects such as Pods, Services, ReplicaSets, and Deployments are often created simultaneously. Over time, as clusters grow to include hundreds or thousands of these objects, using labels and selectors to group and locate them by attributes like type, application, or functionality is essential. For each Kubernetes object, you can attach labels (e.g., app, function, etc.). When selecting objects, you define a condition—such as app equals ""App1""—to filter exactly the objects you need. Consider the following diagram that demonstrates how you can label objects with attributes like ""Front-end,"" ""Auth,"" and ""DB"" and then filter them accordingly: Specifying Labels in Kubernetes Labels are specified in a Pod definition file under the metadata section as key-value pairs. For example: apiVersion: v1
kind: Pod
metadata:
  name: simple-webapp
  labels:
    app: App1
    function: Front-end
spec:
  containers:
  - name: simple-webapp
    image: simple-webapp
    ports:
    - containerPort: 8080 Once the Pod is created, use the following command to select it based on its label: kubectl get pods --selector app=App1 The output might look similar to: NAME            READY   STATUS      RESTARTS   AGE
simple-webapp   0/1     Completed   0          1d Using Labels and Selectors with ReplicaSets Kubernetes objects use labels and selectors to form relationships. For instance, when deploying a ReplicaSet that manages multiple Pods, each Pod's definition includes labels that the ReplicaSet uses to identify and manage them. See the example below of a ReplicaSet definition: apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: simple-webapp
  labels:
    app: App1
    function: Front-end
  annotations:
    buildversion: ""1.34""
spec:
  replicas: 3
  selector:
    matchLabels:
      app: App1
  template:
    metadata:
      labels:
        app: App1
        function: Front-end
    spec:
      containers:
      - name: simple-webapp
        image: simple-webapp Note In the ReplicaSet configuration above: The labels in the ReplicaSet metadata describe its properties. The selector matches the Pods based on labels. The labels in the Pod template ( metadata.labels ) specify which Pods the ReplicaSet should manage. Annotations, like buildversion: ""1.34"" , are used for storing metadata that does not affect selection. Beginners often confuse the labels on the ReplicaSet with those on the Pods. Remember, the ReplicaSet’s selector is key—it connects the ReplicaSet to the corresponding Pods based on matching labels. Using a single common label (e.g., app: App1 ) will cause the ReplicaSet to manage only those Pods. For finer control, additional labels can be added to avoid unintentional matches. Whenever other objects (such as Services) need to identify these Pods, they use selectors that match the labels assigned directly to the Pods. Understanding Annotations While labels and selectors are used for grouping and selection, annotations are designed for storing non-identifying metadata. This metadata might include tool information, version numbers, build details, or contact information. Annotations provide additional context for integrations, debugging, and information sharing without affecting how objects are grouped or selected. In our ReplicaSet example, the annotation buildversion: ""1.34"" demonstrates how version-specific metadata can be coupled with object definitions. Additional Information Annotations are especially useful for external systems or processes that require insight into the object's metadata without impacting the operational logic in Kubernetes. That concludes our comprehensive review of labels, selectors, and annotations. Practice these concepts to enhance your expertise on how Kubernetes organizes and manages its objects. For further reading, visit the following resources: Kubernetes Documentation Kubernetes Basics Docker Hub Terraform Registry Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Solution Monitoring optional,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Observability/Solution-Monitoring-optional,"Certified Kubernetes Application Developer - CKAD Observability Solution Monitoring optional In this lesson, you will learn how to monitor your Kubernetes cluster by inspecting deployed Pods and deploying a metrics server to track resource usage. Inspecting Deployed Pods Begin by verifying the running Pods in your cluster. Execute the following command: kubectl get pods You should see output similar to the following: NAME       READY   STATUS              RESTARTS   AGE
elephant   1/1     Running             0          20s
lion       1/1     Running             0          20s
rabbit     0/1     ContainerCreating   0          20s At this point, the ""elephant"" and ""lion"" Pods are running, while ""rabbit"" is still initializing. Deploying the Metrics Server To monitor resource usage, you'll deploy the metrics server. This lab uses a preconfigured Git repository containing all the necessary configurations. Note Do not use these configurations in a production environment. Always refer to the official documentation before deploying to production. Step 1: Clone the Repository Clone the repository by running: git clone https://github.com/kodekloudhub/kubernetes-metrics-server.git After cloning, you should see output similar to this: Cloning into 'kubernetes-metrics-server'...
remote: Enumerating objects: 24, done.
remote: Counting objects: 100% (12/12), done.
remote: Compressing objects: 100% (12/12), done.
remote: Total 24 (delta 4), reused 0 (delta 0), pack-reused 12
Unpacking objects: 100% (24/24), done. Step 2: Review the Repository Contents First, check the directory structure: ls You should see a directory named kubernetes-metrics-server and possibly a file like sample.yaml . Next, navigate into the repository directory and list its files: cd kubernetes-metrics-server/
ls You will find key configuration files such as: README.md aggregated-metrics-reader.yaml auth-delegator.yaml metrics-server-deployment.yaml metrics-server-service.yaml resource-reader.yaml (if present) Step 3: Deploy the Metrics Server Deploy the metrics server by creating all required objects with the following command: kubectl create -f . You should see output indicating that roles, role bindings, deployments, and services have been created successfully: clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrole.rbac.authorization.k8s.io/system:metrics-server:system:auth-delegator created
clusterrole.rbac.authorization.k8s.io/system:metrics-server:auth-reader created
clusterrole.apiresources.k8s.io created
deployment.apps/metrics-server created
service/metrics-server created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created It may take a few minutes for the metrics server to start gathering and reporting data. Verify this by listing the Pods again: kubectl get pods Verifying Metrics Collection After deployment, you can view resource usage by using the kubectl top commands. Check Node Resource Utilization Run the following command: kubectl top node Expected output: NAME         CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
controlplane 470m         1%     1252Mi          0%
node01       57m          0%     349Mi           0% Check Pod Resource Utilization To view the resource allocation for each Pod, execute: kubectl top pod Expected output: NAME       CPU(cores)   MEMORY(bytes)
elephant   20m          32Mi
lion       1m           18Mi
rabbit     131m         252Mi Analyzing Resource Consumption Use the collected metrics to identify resource usage by both nodes and Pods. Node Resource Analysis Identifying the Node with the Highest CPU Consumption From the kubectl top node output, the ""controlplane"" node shows a CPU usage of approximately 470 milli cores, which is significantly higher than node01's 57 milli cores. Answer: The controlplane node consumes the most CPU. Identifying the Node with the Highest Memory Consumption The controlplane node also consumes 1252Mi of memory compared to node01's 349Mi. Answer: The controlplane node consumes the most memory. Pod Resource Analysis Identifying the Pod with the Highest Memory Consumption The ""rabbit"" Pod consumes 252Mi of memory, which is higher than the other two Pods. Answer: The ""rabbit"" Pod consumes the most memory. Identifying the Pod with the Lowest CPU Consumption The ""lion"" Pod uses only 1m of CPU, making it the least resource-intensive among the Pods. Answer: The ""lion"" Pod consumes the least CPU. Recap In this lesson, you performed the following steps: Inspected running Pods using the kubectl get pods command. Cloned and explored a preconfigured metrics server repository. Deployed the metrics server using kubectl create -f . . Verified node and Pod resource usage utilizing the kubectl top commands. Analyzed resource consumption to identify which nodes and Pods were consuming the most or least CPU and memory. Final Verification Run the following commands one more time to confirm metrics collection: kubectl top node Expected output: NAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
controlplane   470m         1%     1252Mi          0%
node01         57m          0%     349Mi           0% kubectl top pod Expected output: NAME        CPU(cores)   MEMORY(bytes)
elephant    20m          32Mi
lion        1m           18Mi
rabbit      131m         252Mi This concludes the lesson on monitoring cluster components using the metrics server. For additional Kubernetes monitoring resources, consider visiting the Kubernetes Documentation . Happy monitoring! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solution Labels and Selectors optional,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/POD-Design/Solution-Labels-and-Selectors-optional,"Certified Kubernetes Application Developer - CKAD POD Design Solution Labels and Selectors optional In this lesson, we explore practical techniques for using labels and selectors to manage Kubernetes objects. You will learn how to filter Pods, count resources, and troubleshoot configuration issues using real-world examples. Step 1: Count Pods in the Dev Environment Begin by listing all the Pods: kubectl get pods To filter Pods running in the dev environment, assume the environment label key is env and the value is ""dev"" : kubectl get pods --selector env=dev For a manual count, you might simply observe the output if there are few Pods. However, for larger sets, use the following command to count Pods, excluding the header line by adding the --no-headers option: kubectl get pods --selector env=dev --no-headers | wc -l Tip The command above returns the number of Pods in the dev environment. In our example, the output shows there are seven Pods. Step 2: Count Pods in the Finance Business Unit To count the Pods that belong to the finance business unit, assume the business unit label key is bu and filter by the value ""finance"" . Run: kubectl get pods --selector bu=finance --no-headers | wc -l The result indicates that there are six Pods associated with the finance business unit. Step 3: Count All Objects in the Prod Environment In this step, you'll count all Kubernetes objects (including Pods, Services, ReplicaSets, etc.) in the prod environment. Replace the Pod-specific command with kubectl get all : kubectl get all --selector env=prod --no-headers | wc -l This command returns the total number of objects in the prod environment. In our case, the output is seven. Step 4: Identify a Specific Pod in the Prod Environment Next, identify the Pod that meets all of the following criteria: It is in the prod environment. It belongs to the finance business unit. It is part of the frontend tier. Combine multiple labels by separating them with commas: kubectl get all --selector env=prod,bu=finance,tier=frontend This command returns the specific Pod. In our example, the Pod name begins with ""ZZ XDF…"". Step 5: Fixing a ReplicaSet Definition The final task involves troubleshooting an issue with a ReplicaSet definition. Below is the original YAML file: apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replicaset-1
spec:
  replicas: 2
  selector:
    matchLabels:
      tier: front-end
  template:
    metadata:
      labels:
        tier: nginx
    spec:
      containers:
        - name: nginx
          image: nginx When attempting to create this ReplicaSet: kubectl create -f replicaset-definition-1.yaml You receive the following error: The ReplicaSet ""replicaset-1"" is invalid: spec.template.metadata.labels: Invalid value: map[string]string{""tier"":""nginx""}: selector does not match template labels Configuration Mismatch The error occurs because the matchLabels in the selector ( tier: front-end ) do not match the label defined in the Pod template ( tier: nginx ). Ensure that the selector and the template labels are identical. To resolve the error, update the template labels to match the selector: apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: replicaset-1
spec:
  replicas: 2
  selector:
    matchLabels:
      tier: front-end
  template:
    metadata:
      labels:
        tier: front-end
    spec:
      containers:
      - name: nginx
        image: nginx After updating the YAML file, create the ReplicaSet again: kubectl create -f replicaset-definition-1.yaml The ReplicaSet should now be created successfully. Verify its creation by listing the ReplicaSets. That concludes this lesson on using labels and selectors with Kubernetes. For more advanced topics and troubleshooting, refer to the Kubernetes Documentation . Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Monitoring,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Observability/Monitoring,"Certified Kubernetes Application Developer - CKAD Observability Monitoring Welcome to this lesson on monitoring Kubernetes clusters. In this article, you will learn how to track resource consumption and performance in your Kubernetes environment. Why Monitor Kubernetes? Monitoring in Kubernetes is essential for maintaining your cluster’s health and performance. Key metrics to watch include: Node-Level Metrics: Total nodes, their health status, and overall performance. Performance Metrics: CPU, memory, network, and disk usage. Pod-Level Metrics: Number of pods and individual pod performance (e.g., CPU and memory consumption). Implementing a monitoring solution enables you to collect, aggregate, and analyze these metrics efficiently. Although Kubernetes does not include a full-featured built-in monitoring system, several open-source options—such as Metrics Server, Prometheus, and Elastic Stack—as well as proprietary solutions like Datadog and Dynatrace are available. In this lesson, we'll focus on the Metrics Server. For more advanced monitoring techniques, see the Kubernetes Administration: Package Management with Glasskube course. Key Information The Metrics Server is a lightweight, in-memory monitoring solution, ideal for real-time metrics collection. However, it does not persist historical data. For long-term data analysis, consider advanced monitoring systems. Historical Perspective Originally, the Hipster project provided monitoring and analysis features for Kubernetes. Even though you might encounter references to Hipster in online resources, it has been deprecated in favor of its streamlined counterpart, the Metrics Server. Remember, you can deploy only one Metrics Server per cluster. How Metrics are Collected Each node in your Kubernetes cluster runs a kubelet, which manages pods and communicates with the Kubernetes API master server. A subcomponent of the kubelet, known as cAdvisor (container advisor), collects performance data from pods and makes it available to the Metrics Server via the kubelet API. Deploying the Metrics Server For Minikube Users If you’re running a local cluster with Minikube, you can enable the Metrics Server with this simple command: minikube addons enable metrics-server For Other Environments To deploy the Metrics Server in other environments, first clone the repository, then apply the deployment files as shown below: git clone https://github.com/kubernetes-incubator/metrics-server.git kubectl create -f deploy/1.8+/ After running these commands, you should see output similar to the following: clusterrolebinding ""metrics-server:system:auth-delegator"" created
rolebinding ""metrics-server-auth-reader"" created
apiservice ""v1beta1.metrics.k8s.io"" created
serviceaccount ""metrics-server"" created
deployment ""metrics-server"" created
service ""metrics-server"" created
clusterrole ""system:metrics-server"" created
clusterrolebinding ""system:metrics-server"" created These steps deploy the necessary pods, services, and roles for the Metrics Server to gather real-time performance metrics from your cluster nodes. Allow a few moments for the Metrics Server to start collecting data. Viewing Cluster Metrics After deployment, you can view your cluster's performance metrics using the following commands: Node Metrics To see CPU and memory usage for each node, run: kubectl top node Example output: NAME          CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
kubemaster    166          8%     1337Mi          70%
kubnode1      36           1%     1046Mi          55%
kubnode2      39           1%     1048Mi          55% Pod Metrics To display pod-level metrics, execute: kubectl top pod Example output: NAME    CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
nginx   166          8%     1337Mi          70%
redis   36           1%     1046Mi          55% Conclusion This article provided an overview of monitoring Kubernetes clusters using the Metrics Server. By deploying this lightweight solution, you can efficiently monitor node and pod performance, ensuring your cluster runs smoothly. For further practice, explore the provided coding exercises to reinforce your monitoring skills in Kubernetes. Happy monitoring! Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Solution Deployment strategies,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/POD-Design/Solution-Deployment-strategies,"Certified Kubernetes Application Developer - CKAD POD Design Solution Deployment strategies In this lesson, we will review the deployment strategies used in our lab exercise. This guide details how to identify the deployment strategy, verify service configurations, and gradually shift traffic between application versions for a safe rollout. Question 1: Identify the Deployment Strategy Begin by identifying the deployment strategy used for the deployment in the default namespace. List all deployments by running: root@controlplane ~ ➜ kubectl get deployments
NAME      READY   UP-TO-DATE   AVAILABLE   AGE
frontend  5/5     5            5           30s Next, inspect the details of the ""frontend"" deployment to check its strategy: root@controlplane ~ ➜ kubectl describe deployment frontend
Name:                   frontend
Namespace:              default
CreationTimestamp:      Thu, 04 Aug 2022 23:30:33 +0000
Labels:                 app=myapp
                        tier=frontend
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=frontend
Replicas:               5 desired | 5 updated | 5 total | 5 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:        app=frontend
                 version=v1
  Containers:
   webapp-color:
     Image:          kodekloud/webapp-color:v1
     Port:           <none>
     Host Port:      <none>
     Environment:    <none>
     Mounts:         <none>
  Volumes:        <none>
Conditions:
  Type             Status  Reason From the output, notice the deployment strategy is RollingUpdate . Question 2: Identify the Correct Service The ""frontend"" deployment is connected to a NodePort service. Follow these steps to verify the service: List the services: root@controlplane ~ ➜ kubectl get service
NAME                TYPE           CLUSTER-IP     EXTERNAL-IP   PORT(S)            AGE
frontend-service    NodePort       10.97.89.251   <none>        8080:30080/TCP    65s
kubernetes          ClusterIP      10.96.0.1      <none>        443/TCP           16m Describe the service to inspect its configuration: root@controlplane ~ ➜ kubectl describe service frontend-service
Name:                     frontend-service
Namespace:                default
Labels:                   app=myapp
Annotations:              <none>
Selector:                 app=frontend
Type:                     NodePort
IP Family Policy:         SingleStack
IP Families:              IPv4 Since the service selector ( app=frontend ) matches the pod labels in the ""frontend"" deployment, the correct service is frontend-service . Question 3: Accessing the Web Application To confirm that the web application is running, click the web app button provided in the interface. This step verifies that the application is accessible and functioning as expected. Testing Tip Reload the web application multiple times to observe consistency in responses from the running version. Question 4: Traffic Splitting with a New Deployment A new deployment, frontend-v2 , has been created in the default namespace with an updated image. The goal is to divert less than 20% of traffic to this new version, while keeping the total number of pods unchanged. Verify both deployments: root@controlplane ~ ➜ kubectl get deployments
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
frontend       5/5     5            5           2m43s
frontend-v2    2/2     2            2           35s With the same selector ( app=frontend ), traffic is balanced across 7 pods (5 for frontend and 2 for frontend-v2 ), meaning frontend-v2 receives approximately 28% of the traffic. To reduce the traffic to about 16.7%, scale down frontend-v2 to 1 replica: root@controlplane ~ ➜ kubectl scale deployment --replicas=1 frontend-v2
deployment.apps/frontend-v2 scaled Confirm the change: root@controlplane ~ ➜ kubectl get deployments
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
frontend       5/5     5            5           4m52s
frontend-v2    1/1     1            1           2m44s With a total of 6 pods now running, frontend-v2 receives roughly 16.7% of the traffic. Refresh the application in your browser several times to observe any version changes when a request is routed to frontend-v2 . Question 5: Redirect All Traffic to the New Version (v2) Once you have confirmed that the new version functions correctly, safely redirect all traffic to frontend-v2 by following these steps: Scale down the original frontend deployment: root@controlplane ~ ➜ kubectl scale deployment --replicas=0 frontend
deployment.apps/frontend scaled Scale up the frontend-v2 deployment to handle all traffic: root@controlplane ~ ➜ kubectl scale deployment --replicas=5 frontend-v2
deployment.apps/frontend-v2 scaled Verify the updates: root@controlplane ~ ➜ kubectl get deployments
NAME           READY   UP-TO-DATE   AVAILABLE   AGE
frontend       0/0     0            0           4m52s
frontend-v2    5/5     5            5           2m44s Finally, remove the old frontend deployment entirely: root@controlplane ~ ➜ kubectl delete deployment frontend After reloading the application tab, only version 2 of the application should be active. Important Ensure that your testing confirms the new deployment is stable before fully decommissioning the original version. This successful adjustment in the deployment strategy confirms that all user traffic is now routed to version 2 of the web application. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Deployment Strategy Canary,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/POD-Design/Deployment-Strategy-Canary,"Certified Kubernetes Application Developer - CKAD POD Design Deployment Strategy Canary In this guide, we explore how to implement canary deployments in Kubernetes to safely introduce new changes. A canary deployment enables you to deploy a new version of your application alongside the current version while routing only a small percentage of traffic to the latest version. This approach minimizes risk while testing new features in production. Below you will find a detailed explanation of how to achieve this strategy using Kubernetes Deployments and Services. Overview of the Canary Deployment Process Begin by deploying the primary version of your application. In this configuration, the primary deployment runs five pods, and a Kubernetes Service routes traffic to these pods. A label, for example, version: v1 , is assigned to the pods to facilitate proper routing. Next, deploy a secondary deployment for the canary version. Initially, all traffic is directed to version v1. The canary deployment tests the new version (version v2) by ensuring that only a small portion of traffic is routed to it. If the new version meets expectations, you can later update the primary deployment and retire the canary. The key steps include: Using a single Service to route traffic to both deployments by leveraging a common label (e.g., app: front-end ). Setting a lower replica count for the canary deployment (e.g., replicas: 1 ) so that a limited percentage of traffic reaches version v2. Once testing is successful, upgrading the primary deployment to the new version and removing the canary deployment. Consider the following diagram that illustrates the traffic distribution, with roughly 83% of traffic directed to version v1 and 17% to version v2: Implementation Details Primary Deployment and Service The primary version of the application is deployed using a Kubernetes Deployment. A corresponding Service directs traffic to the pods, which are marked with the label app: front-end to ensure correct routing. Canary Deployment A separate deployment is created for the canary version, which uses a new image (e.g., version 2.0) and labels the pods with version: v2 . Although both deployments share the common label app: front-end , setting the canary deployment’s replica count to 1 ensures that only a minor portion of traffic is routed to the new version. Note Keep in mind that the percentage of traffic routed to each version is directly influenced by the number of pod replicas. The inherent traffic distribution mechanism in Kubernetes spreads traffic evenly across all pods. Limitation One limitation of this Kubernetes-only setup is that traffic distribution is solely determined by the number of pods in each deployment. For example, if you want an exact percentage split (e.g., precisely 1% to the canary), Kubernetes alone may not suffice unless you have a very high number of pods. For more granular control, consider using a service mesh like Istio Service Mesh , which allows for precise, percentage-based traffic routing regardless of pod count. Code Example Below are examples of Kubernetes configuration files to define your primary and canary deployments alongside a Service. # myapp-primary.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-primary
  labels:
    app: myapp
    type: front-end
spec:
  replicas: 5
  selector:
    matchLabels:
      app: front-end
  template:
    metadata:
      labels:
        version: v1
        app: front-end
    spec:
      containers:
        - name: app-container
          image: myapp-image:1.0
---
# service-definition.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: front-end
---
# myapp-canary.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-canary
  labels:
    app: myapp
    type: front-end
spec:
  replicas: 1
  selector:
    matchLabels:
      app: front-end
  template:
    metadata:
      labels:
        version: v2
        app: front-end
    spec:
      containers:
        - name: app-container
          image: myapp-image:2.0 In this setup: The primary deployment ( myapp-primary.yml ) operates with five replicas of version v1. The Service ( service-definition.yaml ) targets pods with the common label app: front-end . The canary deployment ( myapp-canary.yml ) runs a single replica of version v2, allowing only a fraction of traffic to test the new version. Conclusion By employing this canary deployment strategy, approximately 83% of traffic is directed to the stable primary deployment (v1), while about 17% is routed to the new canary deployment (v2). Once thorough testing shows that the new version is stable, you can update the primary deployment accordingly and remove the canary configuration. For more practical experience, try implementing this deployment strategy in your Kubernetes environment and observe how it helps in safely rolling out new application versions. Additional Resources Kubernetes Basics Kubernetes Documentation Istio Service Mesh Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Rolling Updates Rollbacks,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/POD-Design/Rolling-Updates-Rollbacks,"Certified Kubernetes Application Developer - CKAD POD Design Rolling Updates Rollbacks Welcome to this comprehensive guide on how updates and rollbacks work within a Kubernetes deployment. In this article, we will explore the concepts of rollouts, versioning, and deployment strategies, empowering you to manage application upgrades with minimal downtime. When you create a deployment, Kubernetes triggers an initial rollout, creating the first deployment revision (revision one). Later, when you update your application—for example, by changing the container image version—a new rollout occurs and a new deployment revision (revision two) is generated. This mechanism enables tracking changes and facilitates rolling back to a previous stable version if needed. Tip: Use the following command to view the status of your rollout: kubectl rollout status deployment/myapp-deployment The command output will be similar to: Waiting for rollout to finish: 0 of 10 updated replicas are available...
Waiting for rollout to finish: 1 of 10 updated replicas are available...
Waiting for rollout to finish: 2 of 10 updated replicas are available...
Waiting for rollout to finish: 3 of 10 updated replicas are available...
Waiting for rollout to finish: 4 of 10 updated replicas are available...
Waiting for rollout to finish: 5 of 10 updated replicas are available...
Waiting for rollout to finish: 6 of 10 updated replicas are available...
Waiting for rollout to finish: 7 of 10 updated replicas are available...
Waiting for rollout to finish: 8 of 10 updated replicas are available...
Waiting for rollout to finish: 9 of 10 updated replicas are available...
deployment ""myapp-deployment"" successfully rolled out To review the revision history of your deployment, run: kubectl rollout history deployment/myapp-deployment Deployment Strategies Kubernetes supports two primary deployment strategies: Recreate Strategy With the recreate strategy, if you have, for example, five replicas, all replicas are terminated and then replaced with new ones running the updated version. The main drawback is potential downtime, as the application is temporarily unavailable. Rolling Update Strategy The rolling update strategy gradually replaces the old version with the new one by updating replicas one at a time. This ensures continuous application availability during the upgrade. Note that Kubernetes uses the rolling update strategy by default when no specific strategy is configured. The diagram below visually compares the two strategies: Updating a Deployment There are multiple methods to update your deployment. You can update the container image version, modify labels, or adjust the number of replicas. If you are using a deployment definition file, simply modify the file and apply the update: kubectl apply -f deployment-definition.yml This triggers a new rollout and creates a new deployment revision. Alternatively, update just the container image with: kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1 Caution: Updating with kubectl set image modifies the running configuration, which may differ from the file-based configuration. Remember to update your deployment definition file accordingly. Below is an example deployment definition file after updating the container image version: apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
  labels:
    app: myapp
    type: front-end
spec:
  replicas: 3
  selector:
    matchLabels:
      type: front-end
  template:
    metadata:
      name: myapp-pod
      labels:
        app: myapp
        type: front-end
    spec:
      containers:
      - name: nginx-container
        image: nginx:1.7.1 Examining Deployment Details Describing your deployment provides insights into the underlying processes and differences between deployment strategies. When using the recreate strategy, the old ReplicaSet scales down to zero before the new one scales up. Run: kubectl describe deployment myapp-deployment You might see output similar to: Name:                   myapp-deployment
Namespace:              default
CreationTimestamp:      Sat, 03 Mar 2018 17:01:55 +0000
Labels:                 app=myapp
Annotations:            deployment.kubernetes.io/revision=2
                        kubectl.kubernetes.io/change-cause=kubectl apply --filename=.../Kubernetes/Demo
Selector:               5 updated, 5 total / 5 available / 0 unavailable
StrategyType:          Recreate
MinReadySeconds:       0
Pod Template:
  Labels:    app=myapp
             type=front-end
  Containers:
   nginx-container:
     Image:        nginx:1.7.1
     Port:         <none>
     Environment:  <none>
     Mounts:       <none>
  Volumes:     <none>
Conditions:
  Type         Status  Reason
  Available    True    MinimumReplicasAvailable
  Progressing  True    NewReplicaSetAvailable
OldReplicaSets:   <none>
NewReplicaSet:    myapp-deployment-54c76dccc (5/5 replicas created)
Events:
  Type    Reason             Age   From                  Message
  Normal  ScalingReplicaSet  11m   deployment-controller Scaled up replica set myapp-deployment-6795844b58 to 5
  Normal  ScalingReplicaSet  1m    deployment-controller Scaled down replica set myapp-deployment-6795844b58 to 0
  Normal  ScalingReplicaSet  56s   deployment-controller Scaled up replica set myapp-deployment-54c76dccc to 5 For the rolling update strategy, the output indicates that the old ReplicaSet reduces gradually as the new one scales up: kubectl describe deployment myapp-deployment This produces an output similar to: Name:                   myapp-deployment
Namespace:              default
CreationTimestamp:      Sat, 03 Mar 2018 17:16:53 +0800
Labels:                 app=myapp
Annotations:            deployment.kubernetes.io/revision=2
                        kubectl.kubernetes.io/change-cause=kubectl apply --filename=.../Kubernetes/Demo
Selector:               5 desired | 5 updated | 6 total | 4 available | 2 unavailable
StrategyType:          RollingUpdate
MinReadySeconds:       0
RollingUpdateStrategy: 25% max unavailable, 25% max surge
Pod Template:
  Labels:    app=myapp
             type=front-end
  Containers:
   nginx-container:
     Image:        nginx
     Port:         <none>
     Environment:  <none>
     Mounts:       <none>
  Volumes:     <none>
Conditions:
  Type         Status  Reason
  Available    True    MinimumReplicasAvailable
  Progressing  True    ReplicaSetUpdated
OldReplicaSets:    myapp-deployment-67c749c58c (1/1 replicas created)
NewReplicaSet:     myapp-deployment-75d78bd8d (5/5 replicas created)
Events:
  Type    Reason             Age   From                  Message
  Normal  ScalingReplicaSet  1m    deployment-controller Scaled up replica set myapp-deployment-67c749c58c to 5
  Normal  ScalingReplicaSet  15s   deployment-controller Scaled down replica set myapp-deployment-67c749c58c to 4
  Normal  ScalingReplicaSet  0s    deployment-controller Scaled up replica set myapp-deployment-75d78bd8d to 3
  Normal  ScalingReplicaSet  0s    deployment-controller Scaled down replica set myapp-deployment-67c749c58c to 0 The rolling update strategy preserves application availability by gradually transitioning from the old to the new ReplicaSet. Understanding the Underlying Process When you create a deployment (for example, with five replicas), Kubernetes automatically creates a ReplicaSet that manages the corresponding pods. During an upgrade, a new ReplicaSet is created for the updated configuration while the old ReplicaSet gradually terminates its pods. You can monitor these changes by running: kubectl get replicasets This command displays both the old ReplicaSet (with zero pods) and the new ReplicaSet with the desired number of pods. If issues arise with the new release, Kubernetes allows you to roll back to the previous revision. To perform a rollback, use: kubectl rollout undo deployment/myapp-deployment This command terminates the pods in the new ReplicaSet and restores the previous stable version. Compare the ReplicaSet status before and after the rollback: Before rollback: kubectl get replicasets
NAME                        DESIRED   CURRENT   READY   AGE
myapp-deployment-67c749c58c  0         0         0       22m
myapp-deployment-7d57dbd8d   5         5         5       20m After rollback: kubectl get replicasets
NAME                        DESIRED   CURRENT   READY   AGE
myapp-deployment-67c749c58c  5         5         5       22m
myapp-deployment-7d57dbd8d   0         0         0       20m When you run the undo command, you should see: kubectl rollout undo deployment/myapp-deployment deployment ""myapp-deployment"" rolled back Quick Reference Summary Below is a summary of the key commands used to create, update, monitor, and roll back your deployments: Command Description kubectl create -f deployment-definition.yml Create a new deployment kubectl get deployments List current deployments kubectl apply -f deployment-definition.yml Apply changes from the deployment definition file kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1 Update a deployment's container image kubectl rollout status deployment/myapp-deployment Check the status of the deployment rollout kubectl rollout undo deployment/myapp-deployment Roll back to the previous deployment revision By mastering both the recreate and rolling update strategies along with the associated commands, you can effectively manage your Kubernetes deployments and minimize downtime during application updates. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Deployment Strategy Blue Green,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/POD-Design/Deployment-Strategy-Blue-Green,"Certified Kubernetes Application Developer - CKAD POD Design Deployment Strategy Blue Green In this article, we explore the Blue-Green deployment strategy—a method that complements the previously discussed Recreate and Rolling Update strategies. Earlier, we reviewed two common deployment methods: Recreate Strategy: The old version of the application is completely shut down before deploying the new version. This results in temporary unavailability for users during the transition. Rolling Update Strategy: Instances of the old version are gradually replaced with newer ones one at a time. This approach minimizes downtime and facilitates a smoother upgrade process. Many systems default to this strategy. Beyond these approaches, Kubernetes also allows you to implement deployment patterns using its native primitives, such as blue-green and canary deployments. In this guide, we focus on blue-green deployments. Blue-Green Deployment Strategy Blue-green deployment leverages two production environments: Blue Environment: Runs the current or stable version of the application. Green Environment: Hosts the new version that is being prepared and tested. Initially, 100% of user traffic is directed to the blue environment. The green environment is then set up with the new version, tested, and validated. Once testing confirms that the green version meets quality standards, traffic is switched over to it with a single update. This strategy can be implemented using service meshes like Istio Service Mesh for more granular traffic control. Alternatively, you can achieve blue-green deployment using native Kubernetes components such as Deployments and Services. Key Insight Blue-green deployment minimizes risk by allowing testing of a new release in a live but isolated environment before directing all user traffic to it. How Blue-Green Deployment Works with Kubernetes Deploy the Blue Version (Current Production): Deploy the original application version as the blue deployment with a label (e.g., version: v1 ). A Kubernetes Service is created with a selector that matches this label, ensuring that all user traffic is routed to the blue Pods. Deploy the Green Version (New Release): Deploy a separate green deployment that includes the new application version. Label these Pods with version: v2 . Although both blue and green deployments run concurrently, the Service continues routing traffic based on the label selector, initially favoring the blue version. Switch Traffic After Testing: After thorough testing confirms that the green version is stable, update the Service’s label selector to point from version: v1 to version: v2 . This update directs all incoming traffic to the green Pods, effectively switching the live environment. Blue Deployment Example The following YAML file defines the Service that initially directs traffic to the blue version: # service-definition.yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    version: v1 Here is the deployment definition for the blue version: # myapp-blue.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-blue
  labels:
    app: myapp
    type: front-end
spec:
  replicas: 5
  selector:
    matchLabels:
      version: v1
  template:
    metadata:
      labels:
        version: v1
    spec:
      containers:
        - name: app-container
          image: myapp-image:1.0 After the blue version is deployed and receiving all user traffic, you can deploy the green version by creating a similar deployment with the updated image (for example, myapp-image:2.0 ) and labeling it as version: v2 . Once the green version has been rigorously tested, update the Service to switch traffic to it: # Updated service-definition.yaml to route traffic to the green version
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    version: v2 This change ensures that all incoming traffic will now be routed to the green deployment, completing the blue-green transition. Conclusion By utilizing blue-green deployments, you can reduce downtime and mitigate risks during application updates by ensuring the new version is fully tested before it takes over production traffic. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Cron Jobs,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/POD-Design/Cron-Jobs,"Certified Kubernetes Application Developer - CKAD POD Design Cron Jobs Welcome to this comprehensive guide on CronJobs in Kubernetes. In this article, you'll learn how to schedule and execute recurring jobs in your Kubernetes cluster, similar to using a Cron tab in Linux. A CronJob allows you to run tasks on a recurring schedule. For example, you might have a job that generates reports and sends emails. Rather than triggering the job immediately with the ""kubectl create"" command, you can schedule it with a CronJob to occur periodically. Converting a Basic Job to a CronJob Consider the following basic Job template: apiVersion: batch/v1
kind: Job
metadata:
  name: reporting-job
spec:
  completions: 3
  parallelism: 3
  template:
    spec:
      containers:
      - name: reporting-tool
        image: reporting-tool
      restartPolicy: Never To convert this into a CronJob, you need to update the API version to ""batch/v1beta1"" and change the kind to ""CronJob"". Then, add a schedule field with a Cron-formatted string, and embed the original Job template within the CronJob definition under the jobTemplate section. Note The schedule field uses a standard Cron format. In this example, ""*/1 * * * *"" schedules the job to run every minute. Below is the complete YAML definition for a CronJob that runs every minute: apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: reporting-cron-job
spec:
  schedule: ""*/1 * * * *""
  jobTemplate:
    spec:
      completions: 3
      parallelism: 3
      template:
        spec:
          containers:
          - name: reporting-tool
            image: reporting-tool
          restartPolicy: Never Creating and Verifying Your CronJob After saving your CronJob definition (for example, as cron-job-definition.yaml ), you can create the CronJob in your Kubernetes cluster with the following command: kubectl create -f cron-job-definition.yaml To verify that your CronJob has been created correctly, run: kubectl get cronjob A typical output might resemble: NAME                  SCHEDULE      SUSPEND   ACTIVE
reporting-cron-job    */1 * * * *   False     0 Tip Ensure your YAML file is properly formatted and saved before executing the kubectl create command to avoid any deployment issues. Conclusion With this guide, you now understand how to transform a standard Kubernetes Job into a CronJob, enabling you to automate and schedule recurring tasks. Practice with different scheduling configurations to get the most out of CronJobs in your Kubernetes environment. For further reading, explore additional resources: Kubernetes Basics Kubernetes Documentation Docker Hub Terraform Registry Happy scheduling and see you in the next article! Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Solution Services optional,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Services-Networking/Solution-Services-optional,"Certified Kubernetes Application Developer - CKAD Services Networking Solution Services optional In this article, we walk through a practical lab solution for managing Kubernetes services. We cover listing services, inspecting service details, reviewing deployments, and exposing a web application using a NodePort service. This guide is ideal for Kubernetes administrators and developers looking to understand service management in-depth. Listing Kubernetes Services To determine how many services exist on the system, run the following commands: kubectl get service You can also use the shorthand: kubectl get svc The output is similar to: kubectl get service
NAME         TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE
kubernetes   ClusterIP   10.43.0.1     <none>        443/TCP    10m This output confirms that only one service exists—the default Kubernetes service. You can verify this with both commands: controlplane ~ ⟩ kubectl get service
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
kubernetes   ClusterIP   10.43.0.1     <none>       443/TCP    10m

controlplane ~ ⟩ kubectl get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
kubernetes   ClusterIP   10.43.0.1     <none>       443/TCP    10m

controlplane ~ ⟩ Note The default service is automatically created by Kubernetes and represents the API server. Checking the Service Type From the output, you can observe that the service type is ClusterIP : controlplane ~ ¬ kubectl get service
NAME        TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
kubernetes  ClusterIP   10.43.0.1     <none>        443/TCP    10m

controlplane ~ ¬ kubectl get svc
NAME        TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
kubernetes  ClusterIP   10.43.0.1     <none>        443/TCP    10m

controlplane ~ ¬ Later, we will explore the role of the API server in more detail. For now, think of this service like any other Kubernetes service you create. Determining the Target Port Next, we determine the target port configured on the Kubernetes service by using the kubectl describe command: controlplane ~ ⚡ kubectl get service
NAME        TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE
kubernetes  ClusterIP  10.43.0.1    <none>        443/TCP    10m

controlplane ~ ⚡ kubectl get svc
NAME        TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)    AGE
kubernetes  ClusterIP  10.43.0.1    <none>        443/TCP    10m

controlplane ~ ⚡ kubectl describe svc kubernetes
Name:              kubernetes
Namespace:         default
Labels:            component=apiserver
                   provider=kubernetes
Annotations:       <none>
Selector:          <none>
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.43.0.1
IPs:               10.43.0.1
Port:              <unset>
TargetPort:        6443/TCP
Endpoints:         10.53.180.9:6443
Session Affinity:  None
Events:            <none>

controlplane ~  ↣ This output shows that the service receives traffic on port 443 and forwards it to the target port 6443 . Examining Service Labels Review the labels configured on the Kubernetes service by examining its description. You will see two labels: component: apiserver provider: kubernetes These labels confirm that this service is associated with the Kubernetes API server. For more detailed insights, especially in preparation for the Certified Kubernetes Application Developer (CKAD) exam, further examination of the API server is recommended. Inspecting Service Endpoints Endpoints represent the Pod IPs where the service directs traffic. To inspect them, run: controlplane ~ ⚡ kubectl get service
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
kubernetes   ClusterIP   10.43.0.1     <none>        443/TCP    10m

controlplane ~ ⚡ kubectl get svc
NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)    AGE
kubernetes   ClusterIP   10.43.0.1     <none>        443/TCP    10m

controlplane ~ ⚡ kubectl describe svc kubernetes
Name:                     kubernetes
Namespace:                default
Labels:                   component=apiserver
                          provider=kubernetes
Annotations:              <none>
Selector:                 <none>
Type:                     ClusterIP
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.43.0.1
IPs:                      10.43.0.1
Port:                     https 443/TCP
TargetPort:               6443/TCP
Endpoints:                10.53.180.9:6443
Session Affinity:         None
Events:                   <none>

controlplane ~ ✗ The output confirms there is one endpoint: 10.53.180.9:6443. Understanding Endpoints Endpoints are dynamically determined based on the labels and selectors defined in a service specification. When a service is created, it monitors Pods that match its specified selector. If there is a misconfiguration, the service could unintentionally attach extra endpoints, or conversely, none at all if labels are mismatched. In our example, the service directs traffic to a single endpoint. Exploring Deployments Next, let's examine the deployments in the default namespace. Counting Deployments List the deployments with: kubectl get deploy The output shows that there is one deployment deployed. Checking the Container Image To verify the container image used within the deployment, inspect the deployment details: controlplane ~ ➜ kubectl get deploy
NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
simple-webapp-deployment   0/4     4            0           15s

controlplane ~ ➜ kubectl describe deploy simple-webapp-deployment
Name:                   simple-webapp-deployment
Namespace:              default
CreationTimestamp:      Fri, 15 Apr 2022 20:35:47 +0000
Labels:                 <none>
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               name=simple-webapp
Replicas:               4 desired | 4 updated | 4 total | 4 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  name=simple-webapp
  Containers:
    simple-webapp:
      Image: kodekloud/simple-webapp:red
      Port: 8080/TCP
      Host Port: 0/TCP
      Environment: <none>
      Mounts: <none>
      Volumes: <none>
Conditions:
  Type               Status  Reason
  ----               ------  ------
  Available          True    MinimumReplicasAvailable
  Progressing        True    NewReplicaSetAvailable
OldReplicaSet:      <none>
NewReplicaSet:      simple-webapp-deployment-7b59598d59 (4/4 replicas created)
Events:
  Type    Reason              Age   From                     Message
  ----    ------              ----  ----                     ------
  Normal  ScalingReplicaSet   63s   deployment-controller    Scaled up replica set simple-webapp-deployment-7b59598d59 to 4 The container image used is kodekloud/simple-webapp:red . Accessing the Web Application UI Attempting to access the web application UI may result in a ""bad gateway"" error. This occurs because there is no service defined to expose the web application. Warning Without a proper service configuration, your web application will remain inaccessible externally. To resolve this, you need to create a new service using a service definition file. Creating a Service for the Web Application Below is a template for a service definition: apiVersion: v1
kind: Service
metadata:
  name: 
spec:
  type: 
  ports:
    - targetPort: 
      port: 
      nodePort: 
  selector:
    name: For reference, the Kubernetes documentation offers sample YAML definitions. An example is: apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: MyApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9376 For our web application, update the service definition with the following parameters: Name: webapp-service Type: NodePort Port: 8080 TargetPort: 8080 NodePort: 30080 Selector: name: simple-webapp The complete YAML definition is: apiVersion: v1
kind: Service
metadata:
  name: webapp-service
spec:
  type: NodePort
  ports:
    - targetPort: 8080
      port: 8080
      nodePort: 30080
  selector:
    name: simple-webapp After saving this definition (for example, as service-definition-1.yaml ), verify the file content: controlplane ~ → ls
service-definition-1.yaml

controlplane ~ → cat service-definition-1.yaml
---
apiVersion: v1
kind: Service
metadata:
  name:
spec:
  type:
  ports:
    - targetPort:
      port:
      nodePort:
  selector:
    name: Open the file with your preferred editor: controlplane ~ → vi service-definition-1.yaml Create the service with: controlplane ~ → kubectl create -f service-definition-1.yaml
service/webapp-service created

controlplane ~ → Now, you can access the web application using the newly created service. In future lessons, we will also explore imperative commands to create services. For example, try the following commands: kubectl expose pod redis --port=6379 --name=redis-service --dry-run=client -o yaml kubectl create service clustip redis --tcp=6379:6379 --dry-run=client -o yaml kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml Visualizing Service Endpoints For a better understanding of how services direct traffic to Pods, consider the following hand-drawn diagram. It illustrates a service directing traffic to three distinct Pods labeled ""app: FE"" and ""app: FG"": This visual representation reinforces how the service uses endpoints to direct incoming traffic to the correct Pods. That concludes this lab. By following these steps, you now know how to list services, inspect service configurations and endpoints, review deployments, and expose a web application via a NodePort service in Kubernetes. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Services Cluster IP,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Services-Networking/Services-Cluster-IP,"Certified Kubernetes Application Developer - CKAD Services Networking Services Cluster IP Hello, and welcome to this in-depth lesson on the Kubernetes ClusterIP service. In modern full-stack web applications, different Pods are responsible for managing various components of the application. For instance, you might have multiple Pods managing the front-end web servers, a set handling back-end servers, another group dedicated to a key-value store like Redis, and additional Pods operating persistent databases such as MySQL. In this architecture, the web front-end servers need to communicate seamlessly with the back-end servers, and the back-end servers, in turn, must interact with both the database and the Redis service. Because Pods receive dynamic IP addresses that can change over time, relying on these IP addresses for internal communication is not reliable. Instead, Kubernetes services are used to logically group Pods under a single, stable interface. This design ensures that when a front-end Pod connects to the back-end service, the request is forwarded to one of the available back-end Pods, enabling effective load distribution. Additionally, by setting up separate services (for example, for Redis), backend Pods can communicate using persistent endpoints without having to accommodate IP address changes. Note The ClusterIP service type is the default configuration in Kubernetes. If the type is omitted from a service definition, Kubernetes automatically assumes it to be a ClusterIP service. Creating a ClusterIP Service When creating a ClusterIP service, you define it in a YAML file. Below is an example of a service definition for the backend: apiVersion: v1
kind: Service
metadata:
  name: back-end
spec:
  type: ClusterIP
  ports:
    - targetPort: 80
      port: 80
  selector:
    app: myapp
    type: back-end In this example: API Version: v1 Resource Kind: Service Service Name: ""back-end"" Service Type: ClusterIP (default) Ports: The service exposes port 80 and forwards traffic to the target port 80 on the backend Pods. Selector: Matches Pods labeled with app: myapp and type: back-end . Below is the corresponding Pod definition with the matching labels: apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: back-end
spec:
  containers:
    - name: nginx-container
      image: nginx Deploying the Service After defining these resources in separate files (e.g., service-definition.yml and pod-definition.yml ), follow these steps to create and verify your service: Create the service using the following command: kubectl create -f service-definition.yml Verify the service status by listing all services: kubectl get services You should see an output similar to the following: > kubectl create -f service-definition.yml
service ""back-end"" created

> kubectl get services
NAME        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)       AGE
kubernetes  ClusterIP   10.96.0.1        <none>        443/TCP      16d
back-end    ClusterIP   10.106.127.123   <none>        80/TCP       2m Best Practice Using DNS names provided by Kubernetes for communication between Pods offers greater stability and scalability compared to direct IP addressing. How It Works In this configuration, other Pods within the cluster utilize the ClusterIP or DNS name of the backend service to communicate reliably despite underlying changes when Pods scale or redeploy. This ensures stable interaction between different application components. For further reading, check out the following resources: Kubernetes Documentation Introduction to Kubernetes Services That concludes this detailed discussion on Kubernetes ClusterIP services. A practical walkthrough of these concepts is provided in the accompanying demonstration. Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Jobs,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/POD-Design/Jobs,"Certified Kubernetes Application Developer - CKAD POD Design Jobs Welcome to this article on Kubernetes Jobs. In this guide, we’ll explore Jobs in Kubernetes and learn how they differ from long-running workloads. By the end, you’ll understand how to implement both simple operations and more complex batch processing tasks using Kubernetes Jobs. Understanding Container Workloads Containerized workloads are typically classified into two categories: Long-running workloads: For example, web servers and database applications that continue to run until they are manually stopped. Batch processing tasks: These execute a specific operation—such as computation, image processing, data analysis, or report generation—and then terminate. We'll start by examining how a simple workload behaves in Docker, and afterward, we’ll translate the concept into Kubernetes. Simple Workload Example in Docker When you run a Docker container tasked with a basic math operation—like adding two numbers—the container starts, performs the calculation, prints the output, and then exits. For example: docker run ubuntu expr 3 + 2
5
docker ps -a
CONTAINER ID        IMAGE               CREATED             STATUS                      PORTS
45aacca36850        ubuntu              43 seconds ago      Exited (0) 41 seconds ago In this case, the task completed successfully as indicated by a return code of zero. Replicating the Task in Kubernetes To replicate the math addition in Kubernetes, you first need to create a pod definition file. When this pod is created, it launches a container that performs the computation and exits. Here is the pod definition: apiVersion: v1
kind: Pod
metadata:
  name: math-pod
spec:
  containers:
  - name: math-add
    image: ubuntu
    command: ['expr', '3', '+', '2'] Create the pod using: kubectl create -f pod-definition.yaml Check the pod status with: kubectl get pods
NAME      READY   STATUS    RESTARTS   AGE
math-pod  0/1     Running   0          1d Even though the container finishes its task, Kubernetes will attempt to keep the pod running by default. After a short period, you will notice: kubectl get pods
NAME       READY   STATUS      RESTARTS   AGE
math-pod   0/1     Completed   3          1d This is due to the default restart policy being set to Always. To prevent Kubernetes from restarting the container after the job finishes, override the restart policy by setting it to Never (or OnFailure, as needed): apiVersion: v1
kind: Pod
metadata:
  name: math-pod
spec:
  containers:
  - name: math-add
    image: ubuntu
    command: ['expr', '3', '+', '2']
  restartPolicy: Never Note Setting the restart policy to Never ensures that the pod does not restart automatically after the command has completed, which is ideal for one-off tasks. Introducing Jobs for Batch Processing For batch processing or large-scale data tasks, you might need multiple pods working together concurrently. Unlike ReplicaSets, which ensure a certain number of pods remain running, Kubernetes Jobs are designed to run pods until the specified task is completed successfully. To create a Job, start with a job definition file that uses the API version batch/v1 and kind Job . In the job specification, a template holds the pod definition. Here’s an example job definition that performs our math addition: apiVersion: batch/v1
kind: Job
metadata:
  name: math-add-job
spec:
  template:
    spec:
      containers:
      - name: math-add
        image: ubuntu
        command: ['expr', '3', '+', '2']
      restartPolicy: Never Create the job with: kubectl create -f job-definition.yaml Verify the job creation and its completion status: kubectl get jobs
NAME          DESIRED   SUCCESSFUL   AGE
math-add-job  1         1            38s Fetch the pod created by the job: kubectl get pods
NAME                     READY   STATUS      RESTARTS   AGE
math-add-job-<suffix>    0/1     Completed   0          38s The job remains in a completed state with zero restarts, indicating that Kubernetes did not attempt to restart the pod after a successful completion. You can view the output of the command inside the container using the kubectl logs command with the pod name. To delete the job along with its associated pods, run: kubectl delete job math-add-job Running Multiple Pods with Job Completions In many real-world scenarios, you might require a job to run multiple pods simultaneously to process data in parallel or handle retries for failed operations. To run three pods for a single job, set the completions field to 3: apiVersion: batch/v1
kind: Job
metadata:
  name: math-add-job
spec:
  completions: 3
  template:
    spec:
      containers:
      - name: math-add
        image: ubuntu
        command: ['expr', '3', '+', '2']
      restartPolicy: Never After creating the job: kubectl create -f job-definition.yaml
kubectl get jobs
NAME            DESIRED   SUCCESSFUL   AGE
math-add-job    3         1            38s And checking the pod status: kubectl get pods
NAME                     READY   STATUS      RESTARTS   AGE
math-add-job-25j9p       0/1     Completed   0          2m
math-add-job-87g4m       0/1     Completed   0          2m
math-add-job-ds295       0/1     Completed   0          2m By default, pods for a job are created sequentially—each pod starts only after the previous one completes. Handling Failures with Jobs Consider a scenario where you use an image like kodekloud/random-error that randomly either completes successfully or fails. In such cases, if one pod fails, Kubernetes will create another pod until it achieves the specified number of successful completions. Here is an example job definition to handle this scenario: apiVersion: batch/v1
kind: Job
metadata:
  name: random-error-job
spec:
  completions: 3
  template:
    spec:
      containers:
      - name: random-error
        image: kodekloud/random-error
      restartPolicy: Never Create the job: kubectl create -f job-definition.yaml Check the job status: kubectl get jobs
NAME               DESIRED   SUCCESSFUL   AGE
random-error-job   3         2            38s And review the pods: kubectl get pods
NAME                     READY   STATUS     RESTARTS
random-error-job-ktmtt   0/1     Completed  0
random-error-job-sdsrf   0/1     Error      0
random-error-job-wwqbn   0/1     Completed  0 Kubernetes continues to create new pods until three pods complete successfully. Running Pods in Parallel with Jobs For scenarios where you want the pods to run concurrently, you can set the parallelism property. This allows multiple pods to be created simultaneously. For example, to run up to three pods in parallel: apiVersion: batch/v1
kind: Job
metadata:
  name: random-error-job
spec:
  completions: 3
  parallelism: 3
  template:
    spec:
      containers:
      - name: random-error
        image: kodekloud/random-error
      restartPolicy: Never Create the job with: kubectl create -f job-definition.yaml Check the job status: kubectl get jobs
NAME                DESIRED   SUCCESSFUL   AGE
random-error-job    3         2            38s And inspect the pods: kubectl get pods
NAME                       READY   STATUS      RESTARTS
random-error-job-ktmtt     0/1     Completed   0
random-error-job-sdsrf     0/1     Error       0
random-error-job-wwqbn     0/1     Completed   0 Kubernetes will intelligently create new pods as necessary until all three completions are successful. Summary Kubernetes Jobs are ideal for managing batch tasks where tasks must complete successfully before termination. They allow for sequential or parallel pod execution and include robust failure handling. This concludes our article on Kubernetes Jobs. Try these examples yourself to reinforce your understanding, and explore further to see how Jobs can help you manage batch processing in Kubernetes effectively. Happy learning! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Network Policies,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Services-Networking/Network-Policies,"Certified Kubernetes Application Developer - CKAD Services Networking Network Policies Welcome to this comprehensive lesson on network policies. My name is Mumshad Mannambeth, and in this guide, we will explore fundamental networking and security concepts through practical examples. We’ll illustrate how traffic flows between a web server, an API server, and a database server and explain how Kubernetes network policies help enforce security. Imagine a scenario where: A web server serves the front-end to users, An API server handles backend logic and communication, and A database server stores persistent data. The traffic flow works as follows: A user sends a request to the web server on port 80. The web server forwards the request to the API server on port 5000. The API server retrieves the required data from the database server on port 3306 and sends the response back to the user. There are two key types of traffic in this scenario: Ingress Traffic: Incoming traffic to a server (e.g., user requests reaching the web server). Egress Traffic: Outgoing traffic from a server (e.g., the web server forwarding a request to the API server). Note that when referring to ingress or egress traffic, only the direction in which the traffic originates is considered. Response traffic (often illustrated as dotted lines) is not directly controlled by these rules. For our setup, consider these traffic rules: Component Traffic Type Port Description Web Server Ingress 80 Accepts HTTP requests from users. Web Server Egress 5000 Forwards requests to the API server. API Server Ingress 5000 Receives traffic from the web server. API Server Egress 3306 Sends requests to the database server. Database Server Ingress 3306 Accepts traffic from the API server. Network Security in Kubernetes In a Kubernetes cluster, nodes host Pods and services, with each node, Pod, and service possessing its own IP address. A core principle of Kubernetes networking is that Pods can communicate with one another without additional routing configurations. Typically, all Pods reside on a virtual private network, enabling direct communication using IP addresses, Pod names, or service definitions. By default, Kubernetes allows all intra-cluster communication with an ""all-allow"" rule. In our application deployment within Kubernetes: The web server, API server, and database server each run in their own Pod. Services facilitate communication between these Pods and provide external access. Without additional restrictions, all Pods can freely communicate with one another. However, there may be security or compliance requirements that necessitate restricting direct communication (for example, preventing the web server from directly accessing the database server). This is where network policies come into play. What Are Network Policies? A network policy is a Kubernetes object that controls the traffic flow to and from Pods. Using labels and selectors, you can bind a network policy to one or more Pods, thereby restricting access to them. For example, you can set up a policy that only permits ingress traffic to the database Pod from the API Pod on port 3306. Once a network policy is applied, any traffic not matching the defined rules is blocked. If a policy only allows API Pod traffic to the database Pod on port 3306, then any traffic from a different source will be denied. Creating a Network Policy To enforce a network policy, you use labels and selectors, much like linking ReplicaSets or Services to Pods. In our example, we want to allow only ingress traffic to the database Pod from the API Pod on port 3306. Here is the complete YAML definition for our network policy: apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          name: api-pod
    ports:
    - protocol: TCP
      port: 3306 In this configuration: The podSelector targets Pods labeled with role: db . The policyTypes declaration specifies that this network policy controls ingress traffic. The ingress rule restricts access, allowing traffic only from Pods labeled with name: api-pod on TCP port 3306. Note To apply this network policy, run the appropriate kubectl create command. Remember that network policies are enforced by the underlying network solution in your Kubernetes cluster. Not all network solutions support network policies. For instance, as of the time of this lesson, Flannel does not support them. Supported solutions include KubeRouter, Calico, Romana, and Weave Net. Always review your network solution's documentation to confirm support for network policies. Warning Even if your cluster’s network solution does not support network policies, you can define them. However, these policies will not be enforced unless supported. Conclusion This lesson provided an in-depth understanding of network policies in Kubernetes, covering: Basic traffic flow between services. How ingress and egress rules are defined. The application of network policies to restrict communication between Pods. For more practice, review the official documentation and try out related coding challenges to solidify your understanding of Kubernetes network policies. Learn more from the Kubernetes Documentation and explore additional resources on network security to further enhance your skills. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solution Jobs and Cronjobs optional,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/POD-Design/Solution-Jobs-and-Cronjobs-optional,"Certified Kubernetes Application Developer - CKAD POD Design Solution Jobs and Cronjobs optional In this lesson, we will walk through a series of steps to deploy a pod, create and update a Job, run jobs in parallel, and set up a CronJob in Kubernetes. Follow along to see how each component is implemented. ───────────────────────────── 1. Deploying the Pod We begin with a pod definition in the file throw-dice-pod.yaml . This pod runs a container using the kodekloud/throw-dice image. The container randomly generates a number between one and six. Here, a six indicates success while any other number signals failure. Our goal is to deploy the pod and review its logs to inspect the generated number. First, list the file and preview its contents: root@controlplane ~ ➜ ls
throw-dice-pod.yaml

root@controlplane ~ ➜ cat throw-dice-pod.yaml The contents of throw-dice-pod.yaml are as follows: apiVersion: v1
kind: Pod
metadata:
  name: throw-dice-pod
spec:
  containers:
    - image: kodekloud/throw-dice
      name: throw-dice
  restartPolicy: Never Deploy the pod using: kubectl apply -f throw-dice-pod.yaml Then, check the pod's status: kubectl get pod If the pod status shows Error , it means the container generated a number other than six. Verify the result by inspecting the logs: kubectl logs throw-dice-pod For example, an output like: 4 indicates the container generated a four. ───────────────────────────── 2. Creating a Job to Measure Attempts Next, we will create a Kubernetes Job named throw-dice-job . This Job uses the same pod definition (without extra commands) to determine how many attempts it takes to roll a six. For further details, refer to the official Kubernetes Jobs documentation here . Create a file named throw-dice-job.yaml with the following content: apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  template:
    spec:
      containers:
      - name: throw-dice-job
        image: kodekloud/throw-dice
      restartPolicy: Never
  backoffLimit: 25 Deploy the Job: kubectl apply -f throw-dice-job.yaml Check its status with: kubectl get job You might see an output like: NAME              COMPLETIONS   DURATION   AGE
throw-dice-job    0/1           9s         9s To review detailed pod events and status, describe the Job: kubectl describe job throw-dice-job A sample output might display: Labels: 
  controller-uid: bbe00c05-eb94-4fed-a8b3-d064a2fd1dc5
  job-name: throw-dice-job
Annotations: 
  batch.kubernetes.io/job-tracking: 
Parallelism: 1
Completions: 1
Completion Mode: NonIndexed
Start Time: Wed, 03 Aug 2022 20:07:21 +0000
Completed At: Wed, 03 Aug 2022 20:07:39 +0000
Duration: 18s
Pods Statuses: 0 Active / 1 Succeeded / 3 Failed
... Note In this example, the Job ran a total of 4 attempts (1 succeeded and 3 failed) until a pod succeeded in generating a six. ───────────────────────────── 3. Updating the Job for Multiple Completions To modify the Job such that it continues running until it achieves three successful completions, follow these steps: Delete the current Job: kubectl delete -f throw-dice-job.yaml Update throw-dice-job.yaml to include the completions property. You can also increase the backoffLimit if more attempts are expected: apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  template:
    spec:
      containers:
      - name: throw-dice-job
        image: kodekloud/throw-dice
      restartPolicy: Never
  backoffLimit: 25
  completions: 3 Apply the updated Job: kubectl apply -f throw-dice-job.yaml Monitor the Job’s status using: kubectl describe job throw-dice-job A sample output might be: Labels:
  controller-uid: 5b26aa1c-d92b-4634-85d3-ca1031f2ab0c
  job-name: throw-dice-job
Parallelism: 1
Completions: 3
Start Time: Wed, 03 Aug 2022 20:10:23 +0000
Completed At: Wed, 03 Aug 2022 20:10:37 +0000
Duration: 14s
Pods Statuses: 0 Active / 3 Succeeded / 1 Failed
... This output indicates that the Job took 4 attempts (three successful and one failed) to complete. ───────────────────────────── 4. Running Jobs in Parallel To run the Job in parallel instead of sequentially, add the parallelism property to the definition. If necessary, delete the existing Job and update throw-dice-job.yaml as follows: apiVersion: batch/v1
kind: Job
metadata:
  name: throw-dice-job
spec:
  parallelism: 3
  completions: 3
  template:
    spec:
      containers:
      - name: throw-dice-job
        image: kodekloud/throw-dice
      restartPolicy: Never
  backoffLimit: 35 Deploy the updated Job: kubectl apply -f throw-dice-job.yaml You can describe the Job to verify that pods are now running in parallel and that you achieve three successful completions more quickly. ───────────────────────────── 5. Creating a CronJob The final task is to create a CronJob that executes the job daily at 21:30. Create a file named throw-dice-cronjob.yaml with the following definition: apiVersion: batch/v1
kind: CronJob
metadata:
  name: throw-dice-cron-job
spec:
  schedule: ""30 21 * * *""
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: throw-dice
            image: kodekloud/throw-dice
          restartPolicy: Never Warning Ensure that the restart policy is specified as Never (with a capital “N”). Using ""never"" (all lowercase) will result in an error. Deploy the CronJob: kubectl apply -f throw-dice-cronjob.yaml A successful deployment might return: cronjob.batch/throw-dice-cron-job created ───────────────────────────── Conclusion In this lesson, we covered how to: Deploy a pod that performs a dice-rolling operation. Create a Job to measure the number of attempts required for a successful roll. Update the Job to require multiple completions. Configure the Job to run in parallel. Schedule the Job using a CronJob to run daily at a specified time. These steps provide a comprehensive look at managing Kubernetes Jobs and CronJobs. Enjoy experimenting with these configurations in your Kubernetes environment! For more detailed information on Kubernetes components, visit the following resources: Kubernetes Documentation Kubernetes Basics Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Services,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Services-Networking/Services,"Certified Kubernetes Application Developer - CKAD Services Networking Services Hello and welcome to this lesson! I’m Mumshad Mannambeth, and today we’re diving into Kubernetes services for beginners. Kubernetes services are essential as they enable seamless communication between various application components, both internally and externally. They make it possible for different parts of your application—for instance, front-end pods, back-end processors, and external data connectors—to interact without being directly coupled. Understanding the Need for Services Imagine you've deployed a pod running a web application. While pods can communicate internally over the cluster network, external access presents a challenge. Consider this scenario: The Kubernetes node’s IP address is 192.168.1.2 . Your laptop’s IP address is 192.168.1.10 (on the same network). The internal pod network falls under the 10.244.0.0/24 range, and the pod’s IP is 10.244.0.2 . Since your laptop is on a different network than the pod, directly pinging or accessing 10.244.0.2 won’t work. One workaround is to SSH into the Kubernetes node and then access the pod using a command like: curl http://10.244.0.2 This command, for example, might return: Hello World! Warning Using SSH to access node-hosted pods is not ideal for production environments due to security and management challenges. Introducing the Kubernetes Service Rather than relying on SSH, Kubernetes services provide a robust solution. A service acts as an intermediary by listening on a designated port on the node and forwarding requests to the respective pod. For instance, if you run: curl http://192.168.1.2:30008 This will fetch the web page by mapping the node's port (30008) to the pod's port, and the service performing this task is known as a NodePort service. Types of Kubernetes Services Kubernetes supports multiple service types: Service Type Description Example Use Case NodePort Exposes a pod on a port on each node. External access to a web server. ClusterIP Creates a virtual IP inside the cluster to facilitate pod-to-pod communication. Inter-service communication between front-end and back-end. LoadBalancer Provisions a load balancer from supported cloud providers for distributing traffic. Production environments requiring high availability. In this lesson, we'll focus on NodePort services. How NodePort Works A NodePort service maps three key ports: Target Port: The port on the pod where the web server is running (e.g., port 80). Service Port: The port defined on the service object (typically also set to 80). Node Port: The port on the Kubernetes node used for external access (e.g., 30008). Note Node ports must be within the valid range of 30000 to 32767. Using a NodePort service, external users can access the application by hitting the node's IP address combined with the node port, simplifying external access. Creating a NodePort Service To create a NodePort service, define its configuration in a YAML file. This file should include key components such as: API version Kind Metadata Spec (including service type, ports, and selectors) Below are examples of how you can define a NodePort service and its associated pod. Service Definition (service-definition.yml) apiVersion: v1
kind: Service
metadata:
  name: myapp-service
spec:
  type: NodePort
  ports:
    - targetPort: 80
      port: 80
      nodePort: 30008
  selector:
    app: myapp
    type: front-end Pod Definition (pod-definition.yml) apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: myapp
    type: front-end
spec:
  containers:
    - name: nginx-container
      image: nginx In these YAML files, the service is linked to the pod by matching the labels ( app: myapp and type: front-end ). Deploying and Verifying the Service After preparing the YAML files, deploy the service with the following command: kubectl create -f service-definition.yml To verify its creation, execute: kubectl get services You should see output similar to: NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
kubernetes      ClusterIP   10.96.0.1       <none>        443/TCP          16d
myapp-service   NodePort    10.106.127.123  <none>        80:30008/TCP     5m Now, you can access your web application externally using: curl http://192.168.1.2:30008 This command returns the HTML content of the web page hosted by your pod. Scaling and Production Considerations In production, you might run multiple instances of your web application to ensure high availability and load distribution. When pods with the same labels (e.g., app: myapp ) are running across several nodes, the NodePort service will distribute requests randomly among them. Kubernetes automatically updates the service endpoints as pods are added or removed. Moreover, when pods are deployed across multiple nodes, the same node port is accessible on all nodes. This ensures that you can use any node’s IP address with the defined port to reach your application. Key Takeaway Whether you have a single pod on one node or multiple pods spread across several nodes, the service definition remains the same. This flexibility is at the heart of Kubernetes’ design, enabling dynamic scaling and simplified load balancing. Next Steps This concludes our detailed lesson on Kubernetes services with a focus on NodePort. Now, let's move on to the demonstration where you'll see these concepts in action. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solution Ingress Networking 2,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Services-Networking/Solution-Ingress-Networking-2,"Certified Kubernetes Application Developer - CKAD Services Networking Solution Ingress Networking 2 In this lesson, we explore Kubernetes Ingress Networking by deploying an Ingress Controller and configuring it to route traffic to two applications located in different namespaces. We will deploy a ""video"" app and a ""wear"" app within the app-space namespace while managing the Ingress Controller in its own namespace. Step 1: Create the Ingress Namespace Begin by creating a dedicated namespace for the Ingress Controller. Open your terminal and list all pods across all namespaces to confirm your setup: root@controlplane:~# k get pod -A
NAMESPACE     NAME                                        READY   STATUS              RESTARTS   AGE
app-space     default-backend-5cf9fb9d-jqcp2               1/1     Running             0          50s
app-space     webapp-video-84f8655bd8-qpb5n                1/1     Running             0          50s
app-space     webapp-wear-6f19449555-mmtqh                 1/1     Running             0          49s
kube-system   coredns-74ff55c5b-4zg8j                      1/1     Running             0          20m
kube-system   coredns-74ff55c5b-gffzq                      1/1     Running             0          20m
kube-system   etcd-controlplane                            1/1     Running             0          20m
kube-system   kube-apiserver-controlplane                  1/1     Running             0          20m
kube-system   kube-controller-manager-controlplane         1/1     Running             0          20m
kube-system   kube-flannel-ds-ks7d7                        1/1     Running             0          20m
kube-system   kube-proxy-4929b                             1/1     Running             0          20m
kube-system   kube-scheduler-controlplane                  1/1     Running             0          20m
root@controlplane:~# k create namespace ingress-space
namespace/ingress-space created
root@controlplane:~# Next, within the ingress-space namespace, create a ConfigMap (which may contain Nginx configuration data) and a Service Account. The related Roles and RoleBindings for the service account ingress-serviceaccount are pre-configured. Verify the roles and role bindings with: root@controlplane:~# k get roles -n ingress-space
NAME           CREATED AT
ingress-role   2022-04-19T21:05:42Z

root@controlplane:~# k get rolebindings -n ingress-space
NAME                  ROLE              AGE
ingress-role-binding  Role/ingress-role  20s

root@controlplane:~# k describe role ingress-role -n ingress-space
Name:         ingress-role
Labels:       app.kubernetes.io/name=ingress-nginx
              app.kubernetes.io/part-of=ingress-nginx
Annotations:  <none>
PolicyRule:
  Resources                 Non-Resource URLs  Resource Names                         Verbs
  ---------                 ------------------  --------------                         -----
  configmaps                []                []
  configmaps                []                [ingress-controller-leader-nginx]     
  endpoints                 []                []
  namespaces                []                []
  pods                      []                []
  secrets                   []                [] Step 2: Deploy the Ingress Controller Deploy the Ingress Controller using the following manifest. This YAML configuration ensures that the controller runs within the ingress-space namespace and uses the pre-created service account. Note the proper indentation, namespace specification, and container arguments: apiVersion: apps/v1
kind: Deployment
metadata:
  name: ingress-controller
  namespace: ingress-space
spec:
  replicas: 1
  selector:
    matchLabels:
      name: nginx-ingress
  template:
    metadata:
      labels:
        name: nginx-ingress
    spec:
      serviceAccountName: ingress-serviceaccount
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
          args:
            - /nginx-ingress-controller
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
            - --default-backend-service=app-space/default-http-backend
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
      ports:
        - name: http
          containerPort: 80
        - name: https
          containerPort: 443 Deploy the Ingress Controller by running: root@controlplane:~# k create -f ingress-controller.yaml
deployment.apps/ingress-controller created Monitor the pod status (optionally using --watch ), and wait for the pod to transition from ContainerCreating to Running: root@controlplane:~# k get pods -n ingress-space
NAME                                  READY   STATUS              RESTARTS   AGE
ingress-controller-5857685bf-qd8jz      0/1     ContainerCreating   0          10s Note After a short wait, the status should update to Running. This confirms that the Ingress Controller has been successfully deployed. Step 3: Expose the Ingress Controller To allow external access to your Ingress Controller, expose it using a Service of type NodePort. Execute the following command to expose the deployment: root@controlplane:~# k expose deploy ingress-controller -n ingress-space --name ingress --port=80 --target-port=80 --type NodePort
service/ingress exposed Verify the details of the newly created service: root@controlplane:~# k get svc -n ingress-space
NAME      TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE
ingress   NodePort   10.109.33.190  <none>        80:32741/TCP     9s If you prefer a specific NodePort (for example, port 30080), edit the service accordingly: root@controlplane:~# k edit svc ingress -n ingress-space Change the nodePort value to 30080 and save the changes. Step 4: Create the Ingress Resource Now, create an Ingress resource to route traffic to the applications deployed in the app-space namespace. The Ingress rules will direct: Requests to /wear to the wear-service on port 8080. Requests to /watch to the video-service on port 8080. Run the following command to create the Ingress: root@controlplane:~# k create ingress ingress-wear-watch -n app-space --rule=""/wear=wear-service:8080"" --rule=""/watch=video-service:8080""
ingress.networking.k8s.io/ingress-wear-watch created Verify the Ingress resource with: root@controlplane:~# k get ingress -n app-space
NAME                CLASS   HOSTS   ADDRESS   PORTS   AGE
ingress-wear-watch  <none>  *       <none>    80      8s Step 5: Debugging and Resolving Redirect Issues If you observe that requests to the /watch path are not reaching the intended video service, and the logs remain inactive, review the Ingress Controller logs. You might see repeated HTTP 308 redirects indicating SSL redirection is enforced: 121.6.144.181 - - [19/Apr/2022:21:15:38 +0000] ""GET /watch HTTP/1.1"" 308 171 ""-"" ""Mozilla/5.0 ..."" ... [app-space-video-service-8080] ... To resolve this, disable SSL redirection for this Ingress resource by adding the appropriate annotations. Edit the Ingress manifest to include: apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
  namespace: app-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
spec:
  rules:
    - http:
        paths:
          - path: /wear
            pathType: Exact
            backend:
              service:
                name: wear-service
                port:
                  number: 8080
          - path: /watch
            pathType: Exact
            backend:
              service:
                name: video-service
                port:
                  number: 8080
status:
  loadBalancer:
    ingress: [] Apply the changes by editing the existing Ingress: root@controlplane:~# k edit ingress ingress-wear-watch -n app-space Note After saving these changes, the SSL redirect issue should be resolved, ensuring proper routing of traffic to both applications. Final Verification Perform a final check of the service and Ingress statuses to confirm that everything is functioning as expected: root@controlplane:~# k get svc -n app-space
NAME                  TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
default-http-backend  ClusterIP   10.109.67.66    <none>        80/TCP    10m
video-service         ClusterIP   10.99.96.249    <none>        8080/TCP  10m
wear-service          ClusterIP   10.105.104.69   <none>        8080/TCP  10m

root@controlplane:~# k get ingress -n app-space
NAME                CLASS   HOSTS   ADDRESS   PORTS   AGE
ingress-wear-watch  <none>  *       80      3m24s This confirms that the Ingress Controller is properly deployed, exposed, and routing traffic correctly with SSL redirection disabled. That concludes the lab on Kubernetes Ingress Networking. For further reading on Ingress configurations and best practices, consider exploring the following resources: Kubernetes Ingress Documentation NGINX Ingress Controller GitHub Repository Kubernetes Networking Concepts Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Storage in Docker,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/State-Persistence/Storage-in-Docker,"Certified Kubernetes Application Developer - CKAD State Persistence Storage in Docker Welcome to this lesson on advanced Docker concepts. In this guide, we will explore Docker storage drivers, file systems, and how Docker manages local filesystem data for images, containers, and volumes. Docker Data Storage on the Host When Docker is installed, it organizes data within the /var/lib/docker directory. This folder contains several subdirectories such as aufs , containers , images , and volumes . Each subdirectory serves a specific role in Docker’s architecture: containers: Stores all files related to running containers. images: Contains stored images. volumes: Holds data for persistent storage created by containers. Layered Architecture in Docker Images Docker images use a layered architecture where each instruction in a Dockerfile creates a new layer capturing only the changes from the previous one. Consider the following example Dockerfile: FROM Ubuntu

RUN apt-get update && apt-get -y install python

RUN pip install flask flask-mysql

COPY . /opt/source-code

ENTRYPOINT FLASK_APP=/opt/source-code/app.py flask run You can build this image with the following command: docker build -t mmumshad/my-custom-app . In this example: The base layer is the Ubuntu operating system. Subsequent layers add APT packages, Python packages, application source code, and finally, the entry point configuration. Each layer only stores the differences from its predecessor. For example, although the base Ubuntu image might be around 120 MB and the APT updates add an additional 300 MB, the remaining layers are much smaller. This strategy optimizes build times and minimizes disk space usage. Reusing Layers for Similar Applications Tip If subsequent applications share many common layers, Docker will reuse the unchanged layers from its cache, significantly speeding up builds. Consider a second application similar to the first, with the same base image and dependencies but a different source file and entry point: FROM Ubuntu

RUN apt-get update && apt-get -y install python

RUN pip install flask flask-mysql

COPY app2.py /opt/source-code

ENTRYPOINT FLASK_APP=/opt/source-code/app2.py flask run Build this image using: docker build -t mmumshad/my-custom-app-2 . Because the first three layers are identical across both applications, Docker reuses the cached layers and only builds the new layers corresponding to the changes. Understanding Image and Container Layers A Docker image consists of several read-only layers: Base Layer: The Ubuntu operating system. Packages Layer: APT packages installed on top of Ubuntu. Dependencies Layer: Python packages such as Flask. Source Code Layer: Your application code included in the image. Entry Point Layer: The layer that sets the container’s entry point. When building the image: docker build -t mmumshad/my-custom-app Dockerfile the resulting layers remain read-only. Running a container from this image creates a new writable layer on top, which stores changes such as logs, temporary files, or modifications made during runtime. This mechanism is known as copy-on-write. Even if you modify a read-only file from the image, Docker creates a separate copy in the writable layer before applying the changes. docker run mmumshad/my-custom-app Important Remember, when you remove a container, its writable layer and any associated changes will be lost. The original image remains unchanged unless it is rebuilt. Persisting Data with Volumes For data persistence outside the container’s ephemeral writable layer—such as database storage—use Docker volumes. Creating and Using Volumes First, create a volume: docker volume create data_volume Then, mount the created volume when launching a container. For example, to store MySQL data in the volume, run: docker run -v data_volume:/var/lib/mysql mysql If a specified volume does not exist, Docker automatically creates it. To inspect the volumes, you can list the contents of /var/lib/docker/volumes . Bind Mounts Alternatively, if you prefer using external storage (for instance, storing database files in /data/mysql on your host), you can use bind mounts: docker run -v /data/mysql:/var/lib/mysql mysql This technique maps a directory on the host to the container, enabling direct access to the host’s filesystem. Using the --mount Option The newer --mount flag provides a clearer and more explicit syntax. The equivalent bind mount example using --mount is: docker run \
  --mount type=bind,source=/data/mysql,target=/var/lib/mysql \
  mysql This syntax explicitly defines each parameter (type, source, target) and is recommended for its clarity. Docker Storage Drivers Docker uses storage drivers to manage layered filesystems, the creation of writable layers, and copy-on-write operations. Common storage drivers include: AUFS ZFS BTRFS Device Mapper Overlay Overlay2 The default storage driver varies by operating system: for example, Ubuntu typically uses AUFS, whereas Fedora or CentOS may use Device Mapper if AUFS is unavailable. Each driver offers unique performance and stability characteristics, so choose one based on your application’s requirements. For more detailed information on these storage drivers, please refer to the documentation provided in the relevant links. Conclusion This lesson on Docker’s storage architecture covered the fundamentals of how Docker organizes data on the host, utilizes a layered image architecture, and manages persistent data with volumes and bind mounts. Understanding these concepts is crucial for optimizing Docker builds, ensuring efficient disk usage, and managing data persistence. Thank you for reading, and we look forward to sharing more advanced Docker topics in our next lesson. Further Reading Docker Documentation Kubernetes Basics Docker Hub Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Volumes in Kubernetes,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/State-Persistence/Volumes-in-Kubernetes,"Certified Kubernetes Application Developer - CKAD State Persistence Volumes in Kubernetes Hello, and welcome to this lesson on persistent volumes in Kubernetes. I’m Mumshad Mannambeth, and today we’ll explore key concepts from the Certified Kubernetes Application Developer course. Understanding Volumes Before diving into persistent volumes, let's revisit the concept of volumes, starting with Docker. Docker containers are inherently ephemeral—they exist solely to process data and are removed once their task is done, taking with them any data stored exclusively inside the container. To ensure data persistence, a volume is attached when the container is created. The container writes data to the volume, and even if the container is removed later, the data remains intact. Similarly, in Kubernetes, Pods are transient. When a Pod is created to process data and subsequently deleted, any data stored within it is lost unless it is saved externally. By attaching a volume to the Pod, you can ensure that the data written to the volume remains persistent even after the Pod is terminated. A Simple Example with a Single-Node Cluster Consider a simple implementation where a Pod generates a random number between 1 and 100 and writes it to /opt/number.out . Note that without a persistent volume, deleting the Pod also removes the generated number. apiVersion: v1
kind: Pod
metadata:
  name: random-number-generator
spec:
  containers:
    - image: alpine
      name: alpine
      command: [""/bin/sh"",""-c""]
      args: [""shuf -i 0-100 -n 1 >> /opt/number.out;""] To retain the generated number, we attach a volume that leverages host storage. In this example, the host directory /data is used as the storage backend. When the volume is mounted inside the container at /opt , any file written there will be persisted on the host, even after the Pod is removed. Below is the updated Pod specification with the volume properly configured: apiVersion: v1
kind: Pod
metadata:
  name: random-number-generator
spec:
  containers:
    - image: alpine
      name: alpine
      command: [""/bin/sh"", ""-c""]
      args: [""shuf -i 0-100 -n 1 >> /opt/number.out;""]
      volumeMounts:
        - mountPath: /opt
          name: data-volume
  volumes:
    - name: data-volume
      hostPath:
        path: /data
        type: Directory In this configuration, the file /opt/number.out is stored on the host's /data directory. As a result, the data persists independently of the Pod’s lifecycle. Important Remember: Using the hostPath option is effective for single-node clusters, but it is not ideal for multi-node clusters, where consistent storage across nodes is required. Storage Options for Volumes The previous example utilized the hostPath option, which directly uses a directory on the host for persistent storage. While this works well in a single-node implementation, multi-node clusters typically require a more robust solution because the same directory structure is not guaranteed across different nodes. Kubernetes supports various storage solutions beyond hostPath, including network-based options like NFS, GlusterFS, and Flocker, as well as block storage types such as Fibre Channel, CephFS, and ScaleIO. Moreover, public cloud providers offer native persistent storage options, for example: AWS Elastic Block Store (EBS) Azure Disk Google Persistent Disk For instance, to configure an AWS EBS volume instead of using hostPath, the Pod specification is modified as follows: volumes:
  - name: data-volume
    awsElasticBlockStore:
      volumeID: <volume-id>
      fsType: ext4 With this configuration, the Pod utilizes AWS EBS for persistent storage, enabling reliable data management through the cloud provider’s block storage services. Conclusion This lesson reviewed how volumes in Kubernetes work and how to persist data using hostPath and other storage solutions. In the next segment, we will delve into persistent volumes and explore how Kubernetes manages long-term data persistence in various environments. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solution Ingress Networking 1,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Services-Networking/Solution-Ingress-Networking-1,"Certified Kubernetes Application Developer - CKAD Services Networking Solution Ingress Networking 1 In this lab, we will walk through configuring an Ingress Controller, examining deployed resources, and updating Ingress paths to route traffic appropriately across multiple applications and namespaces. Environment Overview Begin by reviewing your cluster environment. Verify the nodes, namespaces, deployments, and pods. In this setup, there is one node with various namespaces hosting different components. For instance, run the following command to list all pods across all namespaces: root@controlplane ~ ➜ k get pods -A
NAMESPACE      NAME                                       READY   STATUS      RESTARTS   AGE
app-space      default-backend-7f8f4c484-zw8kb           1/1     Running     0          103s
app-space      webapp-video-d54b764b6-d5rnd              1/1     Running     0          104s
app-space      webapp-wear-5b84c4f565-djh5m               1/1     Running     0          101s
ingress-nginx  ingress-nginx-admission-create-wbssr      0/1     Completed   0          101s
ingress-nginx  ingress-nginx-admission-patch-2pmxg       0/1     Completed   0          101s
kube-system    corends-648979b5d-mwkh8                    1/1     Running     0          11m
kube-system    etcd-controlplane                          1/1     Running     0          12m
kube-system    kube-apiserver-controlplane                1/1     Running     0          12m
kube-system    kube-controller-manager-controlplane       1/1     Running     0          12m
kube-system    kube-flannel-ds-xtsxv                      1/1     Running     0          11m
kube-system    kube-proxy-m26kb                           1/1     Running     0          11m
kube-system    kube-scheduler-controlplane                1/1     Running     0          12m
root@controlplane ~ ➜ From the output, you can see pods running in several namespaces (such as app-space , ingress-nginx , and kube-system ). The Ingress Controller is specifically deployed in the ingress-nginx namespace. Ingress Controller Details To verify the Ingress Controller deployment, execute the command below: root@controlplane ~ ⟩ k get pods -A
NAMESPACE         NAME                                                READY   STATUS    RESTARTS   AGE
app-space         default-backend-7f8f4c484-zw8kb                    1/1     Running   0          103s
app-space         webapp-video-d54b764b6-d5rn5                        1/1     Running   0          104s
app-space         webapp-wear-5b44cf565-5hjmj                         1/1     Running   0          101s
ingress-nginx     ingress-nginx-admission-create-wbssr                0/1     Completed 0          101s
ingress-nginx     ingress-nginx-admission-patch-2pmxg                 0/1     Completed 0          101s
ingress-nginx     ingress-nginx-controller-546d8cf744-4hz72            1/1     Running   0          101s
kube-system       coredns-64897985d-mwkh8                            1/1     Running   0          11m
kube-system       etcd-controlplane                                   1/1     Running   0          12m
kube-system       kube-apiserver-controlplane                          1/1     Running   0          12m
kube-system       kube-controller-manager-controlplane                 1/1     Running   0          12m
kube-system       kube-flannel-ds-xtsxv                               1/1     Running   0          11m
kube-system       kube-proxy-m26kb                                    1/1     Running   0          11m
kube-system       kube-scheduler-controlplane                          1/1     Running   0          12m
root@controlplane ~ ⟩ Notice that the Ingress Controller resource is named ingress-nginx-controller and operates within the ingress-nginx namespace. Application and Ingress Resource Applications are deployed within the app-space namespace. In our scenario, three application pods are running: A default backend A web application for video streaming A web application for wear services To view the Ingress resource, run: k get ingress -A The output will display an Ingress resource from the app-space namespace: NAMESPACE   NAME                   CLASS    HOSTS   ADDRESS       PORTS   AGE
app-space   ingress-wear-watch    <none>   *       10.96.152.118 80      3m1s To gather more details, describe the Ingress resource: k describe ingress ingress-wear-watch -n app-space The description reveals: Two paths: • /wear routes to wear-service on port 8080. • /watch routes to video-service on port 8080. A default backend ( default-http-backend ) is configured to handle unmatched requests. The host is set to * , meaning the rules apply across all hosts. A request made to the Ingress without a matching path results in a 404 error as the default backend is invoked. For example: Accessing .../wear opens the wear application. Accessing .../watch (which will later be changed to .../stream ) serves the video streaming application. Updating the Ingress Resource Redirecting Video Streaming to ""/stream"" To expose the video streaming application under the new URL path /stream : Edit the Ingress resource for the app-space namespace. Change the path from /watch to /stream . Below is the updated Ingress YAML specification: apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
  namespace: app-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
spec:
  rules:
    - http:
        paths:
          - path: /wear
            pathType: Prefix
            backend:
              service:
                name: wear-service
                port:
                  number: 8080
          - path: /stream
            pathType: Prefix
            backend:
              service:
                name: video-service
                port:
                  number: 8080
status:
  loadBalancer:
    ingress:
      - ip: 10.96.152.118 After applying these modifications: Navigating to /watch now results in a 404 error. Accessing /stream correctly displays the video streaming application. Testing Changes After updating the Ingress resource, always verify the configuration using: k get ingress -A k describe ingress ingress-wear-watch -n app-space This ensures the new path registrations are active. Adding a Path for the Food Delivery Application The business has expanded by incorporating a food delivery service, now deployed in the app-space namespace. First, verify the deployments: k get deploy -n app-space Example output: NAME                READY   UP-TO-DATE   AVAILABLE   AGE
default-backend     1/1     1            1           7m53s
webapp-food         1/1     1            1           20s
webapp-video        1/1     1            1           7m54s
webapp-wear         1/1     1            1           7m54s Then, check the services: k get svc -n app-space Example output: NAME                    TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)     AGE
default-http-backend    ClusterIP   10.102.190.18  <none>        80/TCP     8m22s
food-service            ClusterIP   10.108.9.190   <none>        8080/TCP   49s
video-service           ClusterIP   10.107.118.120 <none>        8080/TCP   8m22s
wear-service            ClusterIP   10.110.195.79  <none>        8080/TCP   8m23s To expose the food delivery application, update the Ingress in the app-space namespace by adding an /eat path. For example: apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-wear-watch
  namespace: app-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
    nginx.ingress.kubernetes.io/ssl-redirect: ""false""
spec:
  rules:
    - http:
        paths:
          - path: /wear
            pathType: Prefix
            backend:
              service:
                name: wear-service
                port:
                  number: 8080
          - path: /stream
            pathType: Prefix
            backend:
              service:
                name: video-service
                port:
                  number: 8080
          - path: /eat
            pathType: Prefix
            backend:
              service:
                name: food-service
                port:
                  number: 8080
status:
  loadBalancer:
    ingress:
      - ip: 10.96.152.118 Once applied: Accessing .../eat displays the food delivery application. All paths will be correctly routed to their respective services. Integrating a New Payment Service in a Separate Namespace A new critical payment service is deployed in its own namespace, critical-space . To verify the payment pods, run: k get pods -A A sample output should include: NAMESPACE          NAME                                           READY   STATUS      RESTARTS   AGE
app-space          webapp-food-...                                1/1     Running     0          ...
critical-space     webapp-pay-67888454d4b-wl8ks                     1/1     Running     0          13s Now, check the payment deployment: k get deploy -n critical-space Example output: NAME          READY   UP-TO-DATE   AVAILABLE   AGE
webapp-pay    1/1     1            1           22s Then, inspect the payment service details: k get svc -n critical-space Example output: NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)     AGE
pay-service     ClusterIP   10.107.31.180   <none>        8282/TCP   119s Following best practices, each namespace should manage its own Ingress. Create a new Ingress resource in critical-space to expose the payment service at the /pay path. Use the imperative command: kubectl create ingress ingress-pay -n critical-space --rule=""/pay=pay-service:8282"" Verify the new Ingress: k get ingress -n critical-space Expected output: NAME         CLASS   HOSTS   ADDRESS   PORTS   AGE
ingress-pay  <none>  *       <none>    80      10s Describing the Ingress provides further details: k describe ingress ingress-pay -n critical-space Output shows: The rule routes /pay to pay-service on port 8282. A default backend is present. Path Rewrite Consideration By default, the Ingress does not modify the URL path. If the payment application expects requests at / rather than /pay , add a rewrite annotation. To add the path rewrite, update the payment Ingress with the following YAML: apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-pay
  namespace: critical-space
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
    - http:
        paths:
          - path: /pay
            pathType: Exact
            backend:
              service:
                name: pay-service
                port:
                  number: 8282
status:
  loadBalancer:
    ingress:
      - ip: 10.96.152.118 After applying this change, requests to /pay will be rewritten to / before reaching the payment service, ensuring proper application functionality. Conclusion This lab demonstrated how to configure and update an Ingress Controller across multiple namespaces and applications. We: Examined the cluster environment. Verified and detailed the Ingress Controller deployment. Updated the Ingress resource to change a URL path. Added a new path for a food delivery application. Created a separate Ingress for a critical payment service with proper path rewrite. Each modification was verified by inspecting the Ingress resources and testing the endpoints to ensure a smooth transition. Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Persistent Volumes,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/State-Persistence/Persistent-Volumes,"Certified Kubernetes Application Developer - CKAD State Persistence Persistent Volumes Welcome to this comprehensive guide on Persistent Volumes. My name is Mumshad Mannambeth, and in this article, we walk through the concept and practical implementation of persistent volumes in Kubernetes. In previous discussions, we explored the concept of volumes where storage was configured directly within a pod definition file. For example: volumes:
- name: data-volume
  awsElasticBlockStore:
    volumeID: <volume-id>
    fsType: ext4 In larger environments with numerous users and multiple pod deployments, manually configuring storage for each pod becomes both tedious and error-prone. Every time a storage configuration change is required, it would need to be updated on every pod individually. Persistent volumes solve this problem by centralizing storage management. Administrators can create a large pool of storage, and users can request specific portions from that pool using Persistent Volume Claims (PVCs). Users select the storage they need from this pool by using Persistent Volume Claims, simplifying the management and scalability of storage in Kubernetes environments. Creating a Persistent Volume To get started, let’s create a Persistent Volume using a base template. The following example demonstrates updating the API version, setting the kind to PersistentVolume, and giving it a name (here, PV-01). Under the spec section, the access modes—which define how a volume is to be mounted (e.g., read-only or read-write)—are specified. Kubernetes offers several modes such as ReadOnlyMany , ReadWriteOnce , and ReadWriteMany . Additionally, you define the capacity to reserve the necessary storage, set to 1Gi in this example. Initially, we use the hostPath option to utilize storage from the node's local directory. Warning Using hostPath is intended for demonstration or testing purposes only and is not recommended for production environments. apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-voll
spec:
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 1Gi
  hostPath:
    path: /tmp/d To create the persistent volume, run the following command: kubectl create -f pv-definition.yaml After creation, list the persistent volumes with: kubectl get persistentvolume The expected output should appear similar to: NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS       CLAIM   STORAGECLASS   REASON   AGE
pv-voll   1Gi       RWO            Retain            Available                             3m Using a Cloud Storage Backend For production environments, it is advisable to use a cloud storage backend rather than hostPath . For instance, you can configure AWS Elastic Block Store as illustrated below, where you specify both the volumeID and the fsType : apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-voll
spec:
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 1Gi
  awsElasticBlockStore:
    volumeID: <volume-id>
    fsType: ext4 Apply the updated configuration with: kubectl create -f pv-definition.yaml Then verify its status using: kubectl get persistentvolume You should see an output similar to: NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
pv-voll   1Gi        RWO            Retain            Available           <none>                 3m Conclusion This article provided an overview of how persistent volumes in Kubernetes simplify storage management by centralizing configuration. In the next section, we will delve into Persistent Volume Claims and explore how they seamlessly allocate storage from the available persistent volumes. For more information, explore the following resources: Kubernetes Documentation Kubernetes Basics AWS Elastic Block Store Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Volume Driver Plugins in Docker,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/State-Persistence/Volume-Driver-Plugins-in-Docker,"Certified Kubernetes Application Developer - CKAD State Persistence Volume Driver Plugins in Docker Docker leverages storage drivers to manage images and containers, while volume driver plugins handle data persistence. Unlike storage drivers, volumes need to be explicitly created and are managed by these dedicated plugins. Note The default volume driver plugin, ""local,"" creates volumes on the Docker host and stores data at /var/lib/docker/volumes . Several third-party volume driver plugins further expand Docker's storage capabilities by enabling volume creation on external storage solutions. Popular plugins include: Azure File Storage Convoy DigitalOcean Block Storage Blocker Google Compute Persistent Disks ClusterFS NetApp RexRay Portworx VMware vSphere storage Note Certain volume drivers support multiple storage providers. For instance, the RexRay storage driver can provision storage on various platforms such as AWS EBS, S3, EMC storage arrays like Isilon and ScaleIO, Google Persistent Disk, and OpenStack Cinder. When running a Docker container, you can specify a volume driver—such as RexRay for AWS EBS—to provision a cloud-based volume. This approach ensures that your data remains safe even after the container exits. Below is an example command that demonstrates how to run a Docker container with a specified volume driver: docker run -it \
  --name mysql \
  --volume-driver rexray/ebs \
  --mount src=ebs-vol,target=/var/lib/mysql \
  mysql This command creates a container named ""mysql"" and attaches a volume provisioned from Amazon EBS, ensuring persistent data storage in the cloud. While this article focuses on Docker volume driver plugins, remember that Kubernetes also offers robust solutions for managing persistent storage through its volume mechanisms. For more details on persistent storage in Kubernetes, refer to Kubernetes Basics . Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Ingress Networking,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Services-Networking/Ingress-Networking,"Certified Kubernetes Application Developer - CKAD Services Networking Ingress Networking Welcome to this lesson on Ingress networking in Kubernetes. In this guide, you will learn how Ingress works, how it differs from traditional service exposure, and when to use each approach. We will start by reviewing services and then show how Ingress offers a consolidated, efficient way to manage external access to your applications. Traditional Service Exposure Imagine deploying an online store application accessible via ""myonlinestore.com"". Your application is containerized in Docker and deployed as a Pod within a Deployment on your Kubernetes cluster. Additionally, your architecture includes a MySQL database deployed as a Pod and exposed internally using a ClusterIP service for secure communication with your application. To allow external access, you initially create a service of type NodePort. For instance, assigning port 38080 to your application enables users to access it by visiting: http://[Node-IP]:38080 As traffic increases, you simply scale up by increasing the replica count, and the service load-balances requests among the Pods. For improved user experience, you would typically update your DNS to map to your node IPs, allowing access via myonlinestore.com:38080. However, requiring users to remember a port number is not ideal. To simplify this, you can introduce a proxy server between your DNS and cluster that forwards HTTP traffic on port 80 to port 38080. With DNS pointing to the proxy server, users can then simply access your application using myonlinestore.com. If you are leveraging public cloud platforms like Google Cloud Platform (GCP), you can opt for a LoadBalancer service instead of NodePort. When a LoadBalancer service is created, Kubernetes provisions a high port and triggers GCP to set up a network load balancer, assigning an external IP for DNS configuration. Users can then seamlessly access your application via myonlinestore.com. Scaling with Multiple Services As your business expands, you may introduce additional services. For example, suppose you add a video streaming service accessible via myonlinestore.com/watch, while keeping your original application available at myonlinestore.com/wear. In this case, you might deploy them as separate Deployments in the same cluster. A service named ""video-service"" of type LoadBalancer could be created, and Kubernetes would provision a separate external port (such as 38082) along with a dedicated GCP load balancer for that service. However, managing multiple load balancers increases both costs and administrative overhead, often requiring separate SSL configurations, firewall rules, and more. Enter Ingress Ingress provides a streamlined solution by offering a single externally accessible URL that can handle SSL, authentication, and URL-based routing. Think of Ingress as a layer-seven load balancer running within your Kubernetes cluster, and it is configured with native Kubernetes objects. Although you still require an external exposure mechanism (via a NodePort or cloud LoadBalancer), all advanced load balancing and routing rules are managed centrally by the Ingress controller. Without Ingress, you would have to manually configure reverse proxies or load balancers such as NGINX, HAProxy, or Traefik. Ingress automates these tasks by monitoring Kubernetes for new or updated Ingress resources. Ingress Controllers Ingress requires two key components to function effectively: Ingress Controller: A specialized implementation (for example, NGINX, HAProxy, or Traefik) that actively manages the underlying load balancing. Ingress Resources: Kubernetes YAML definitions that specify routing rules for directing incoming traffic. Note Remember that a Kubernetes cluster does not include an Ingress controller by default – you must deploy one. In this lesson, we use NGINX as our example. Deploying an NGINX Ingress Controller Below is an example of a Deployment configuration for the NGINX Ingress controller. This configuration creates a single replica of the NGINX controller with appropriate labels: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ingress-controller
spec:
  replicas: 1
  selector:
    matchLabels:
      name: nginx-ingress
  template:
    metadata:
      labels:
        name: nginx-ingress
    spec:
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.21.0
          args:
            - /nginx-ingress-controller Next, create a ConfigMap to manage NGINX configurations. An initially empty ConfigMap allows you to modify settings later without the need to change configuration files on the container image: apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-configuration A Service of type NodePort is then used to expose the Ingress controller to external traffic: apiVersion: v1
kind: Service
metadata:
  name: nginx-ingress
spec:
  type: NodePort
  ports:
    - port: 80
      targetPort: 80
      protocol: TCP
      name: http
    - port: 443
      targetPort: 443
      protocol: TCP
      name: https
  selector:
    name: nginx-ingress The following Deployment configuration further enhances the setup by including environment variables and explicit port definitions: apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: nginx-ingress-controller
spec:
  replicas: 1
  selector:
    matchLabels:
      name: nginx-ingress
  template:
    metadata:
      labels:
        name: nginx-ingress
    spec:
      containers:
        - name: nginx-ingress-controller
          image: quay.io/kubernetes-ingress-nginx/controller
          args:
            - /nginx-ingress-controller
            - --configmap=$(POD_NAMESPACE)/nginx-configuration
          env:
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
          ports:
            - name: http
              containerPort: 80
            - name: https
              containerPort: 443 Finally, create a ServiceAccount for the Ingress controller to manage access to the Kubernetes API: apiVersion: v1
kind: ServiceAccount
metadata:
  name: nginx-ingress-serviceaccount Summary of Ingress Controller Deployment Deploying an Ingress controller involves: A Deployment for the NGINX Ingress controller. A Service to expose it externally using NodePort. A ConfigMap to manage controller configurations. A ServiceAccount to authenticate and authorize the controller’s operations. Ingress Resources An Ingress resource defines the rules that instruct the Ingress controller on routing incoming requests. There are three common scenarios: Default Backend: Routes all traffic to a single backend service. Path-Based Routing: Directs traffic based on URL path segments. Host-Based Routing: Routes traffic according to the domain name in the request. Default Backend Example The following Ingress resource routes all incoming traffic to the ""wear-service"" on port 80: apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-wear
spec:
  backend:
    serviceName: wear-service
    servicePort: 80 After applying this configuration, you can create and verify it using: kubectl create -f Ingress-wear.yaml
ingress.extensions/ingress-wear created

kubectl get ingress
NAME           HOSTS    ADDRESS    PORTS   AGE
ingress-wear   *        <none>     80      2s Path-Based Routing Example For services accessible via specific URL paths (e.g., ""/wear"" and ""/watch""), define an Ingress resource with multiple path rules: apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-wear-watch
spec:
  rules:
  - http:
      paths:
      - path: /wear
        backend:
          serviceName: wear-service
          servicePort: 80
      - path: /watch
        backend:
          serviceName: watch-service
          servicePort: 80 Verify the configuration with the command below: kubectl describe ingress ingress-wear-watch Host-Based Routing Example For routing based on domain names, specify the host field in your Ingress resource. The following example directs traffic for ""wear.my-online-store.com"" and ""watch.my-online-store.com"" to separate backend services: apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ingress-host-based
spec:
  rules:
  - host: wear.my-online-store.com
    http:
      paths:
      - backend:
          serviceName: wear-service
          servicePort: 80
  - host: watch.my-online-store.com
    http:
      paths:
      - backend:
          serviceName: watch-service
          servicePort: 80 If the host field is omitted, the rule will match traffic from any host. Handling Unmatched Traffic For requests that do not match any defined rules (for example, if a user navigates to myonlinestore.com/listen), it is recommended to set up a default backend. This backend can serve a custom 404 Not Found page or any other appropriate response. Summary Services (ClusterIP, NodePort, LoadBalancer) provide various ways to expose your applications. Ingress consolidates external access through a single URL, simplifying SSL termination, load balancing, and routing. Deploy an Ingress Controller (e.g., NGINX) to continuously monitor and update configurations based on Ingress resources. Define Ingress Resources with specific routing rules to direct incoming traffic to the correct backend services. In this lesson, we have explored different approaches to expose your application in Kubernetes and demonstrated how Ingress simplifies the management of external access. By using Ingress, you benefit from centralized load balancing, SSL termination, and efficient URL-based routing. Next Steps In the practice test section, you will engage with two types of labs: An environment where an Ingress controller, corresponding resources, and applications are already deployed. In this lab, you will explore existing configurations, gather important data, and answer relevant questions. A more challenging lab where you will deploy an Ingress controller and configure Ingress resources from scratch. Good luck, and enjoy your hands-on learning experience! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solution Persistent Volumes and Persistent Volume Claims optional,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/State-Persistence/Solution-Persistent-Volumes-and-Persistent-Volume-Claims-optional,"Certified Kubernetes Application Developer - CKAD State Persistence Solution Persistent Volumes and Persistent Volume Claims optional In this lab exercise, you will learn how to configure persistent storage for your Kubernetes workloads by deploying a Pod that writes log entries to a file, configuring a hostPath volume to persist logs, creating a Persistent Volume (PV) and Persistent Volume Claim (PVC), resolving binding issues due to access mode mismatches, updating the Pod to use the PVC, and finally exploring the reclaim policy behavior upon deletion. Viewing Application Logs Inside the Pod After deploying the web application Pod, you can check its status and view the logs generated by the application. The application writes its log entries to /log/app.log within the container. To view these logs, run the following command: kubectl exec webapp -- cat /log/app.log This command displays the log entries generated by your application. If the Pod were deleted, the logs stored within the container would be lost since they reside on the default ephemeral volume provided by Kubernetes. To verify the Pod configuration, use: kubectl describe pod webapp Notice that there is no additional volume configured to persist the logs. Configuring a HostPath Volume for Log Storage Currently, the /log/app.log file exists only within the container’s file system. To ensure log persistence even when the Pod is recreated, you need to configure a hostPath volume. This involves mounting a directory from the host system (for example, /var/log/webapp ) into the container. Verify the Host Directory First, ensure that the host directory exists and is empty: ls /var/log/webapp Update the Pod Configuration Next, edit the Pod configuration to add a volume named log-volume of type hostPath: apiVersion: v1
kind: Pod
metadata:
  name: webapp
  namespace: default
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    imagePullPolicy: Always
    env:
      - name: LOG_HANDLERS
        value: file
    volumeMounts:
      - name: kube-api-access-lmntb
        mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        readOnly: true
      - name: log-volume
        mountPath: /log
  dnsPolicy: ClusterFirst
  nodeName: controlplane
  restartPolicy: Always
  volumes:
  - name: kube-api-access-lmntb
    projected:
      defaultMode: 420
      sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
  - name: log-volume
    hostPath:
      path: /var/log/webapp Apply the Configuration Save your changes. In some cases, direct edits to a running Pod may be rejected. If so, force the update by saving the configuration to a temporary file and executing: kubectl replace --force -f /tmp/kubectl-edit-XXXX.yaml After the Pod is recreated with the new volume, verify that the logs are now being written to the host’s /var/log/webapp/app.log file. Creating a Persistent Volume (PV) To leverage persistent storage, create a Persistent Volume. Here is an example PV specification using a hostPath: apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-log
spec:
  capacity:
    storage: 100Mi
  accessModes:
    - ReadWriteMany
  persistentVolumeReclaimPolicy: Retain
  hostPath:
    path: /pv/log Save the file as pv.yaml . Create the PV using: kubectl create -f pv.yaml Verify that the PV is available: kubectl get pv Creating a Persistent Volume Claim (PVC) Now, create a Persistent Volume Claim to request storage from the configured PV. Initially, your PVC might look like this: apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: claim-log-1
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Mi Save the file as pvc.yaml . Apply it with: kubectl create -f pvc.yaml Check the state of the PVC: kubectl get pvc At this point, you might notice that the PVC is still in a Pending state due to an access mode mismatch. Although the PV has a capacity of 100Mi, the PVC requests ReadWriteOnce while the PV supports ReadWriteMany . Resolving Access Mode Mismatch To resolve the pending state, update the PVC to request ReadWriteMany access mode, ensuring that it correctly binds to the PV. Edit the pvc.yaml file accordingly and execute: kubectl replace --force -f pvc.yaml Verify that the PVC and PV are now correctly bound: kubectl get pv
kubectl get pvc You should see output similar to: NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                     STORAGECLASS   AGE
pv-log    100Mi      RWX            Retain           Bound    default/claim-log-1       <none>         5m5s And: NAME          STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
claim-log-1   Bound    pv-log   100Mi      RWX            <none>         26s Updating the Pod to Use the PVC Update the webapp Pod configuration to mount the storage from the PVC instead of relying on the hostPath directly. Here is the updated volume configuration snippet for the Pod: apiVersion: v1
kind: Pod
metadata:
  name: webapp
spec:
  containers:
  - name: event-simulator
    image: kodekloud/event-simulator
    imagePullPolicy: Always
    env:
      - name: LOG_HANDLERS
        value: file
    volumeMounts:
      - name: kube-api-access-lmntb
        mountPath: /var/run/secrets/kubernetes.io/serviceaccount
        readOnly: true
      - name: log-volume
        mountPath: /log
  dnsPolicy: ClusterFirst
  nodeName: controlplane
  restartPolicy: Always
  volumes:
  - name: kube-api-access-lmntb
    projected:
      defaultMode: 420
      sources:
        - serviceAccountToken:
            expirationSeconds: 3607
            path: token
  - name: log-volume
    persistentVolumeClaim:
      claimName: claim-log-1 Replace the previous Pod definition by forcing a replacement if necessary: kubectl replace --force -f <your-updated-pod-file.yaml> After the update, verify that logs are now available on the host through the PV by checking /pv/log/app.log . Examining the Reclaim Policy and Cleanup Behavior The persistent volume pv-log is configured with a reclaim policy of Retain . This means that if the PVC is deleted, the PV is not automatically removed; it remains in the cluster with a status of Released . Check its status with: kubectl get pv pv-log Expected output: NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS     CLAIM                   STORAGECLASS   AGE
pv-log    100Mi      RWX            Retain           Bound      default/claim-log-1     <none>         9m47s Deleting Resources If you attempt to delete the PVC while it is still mounted by an active Pod, it may remain in a terminating state until the Pod is deleted. To fully remove the PVC and allow the PV to transition to a Released state, delete the associated Pod first. For example: Delete the PVC: kubectl delete pvc claim-log-1 If the PVC appears stuck, delete the Pod: kubectl delete pod webapp Finally, check the PV status to confirm it has transitioned to Released : kubectl get pv pv-log This lab exercise has guided you through configuring a hostPath volume for log persistence, creating and binding a PV and PVC, updating a Pod to utilize the PVC, and understanding the effects of the reclaim policy on cleanup behavior. For more detailed Kubernetes concepts and examples, consider exploring the official Kubernetes Documentation . Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Persistent Volume Claims,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/State-Persistence/Persistent-Volume-Claims,"Certified Kubernetes Application Developer - CKAD State Persistence Persistent Volume Claims Welcome to this lesson on Persistent Volume Claims (PVCs) in Kubernetes. In this article, we will walk through the process of creating a PVC, explain how Kubernetes binds PVCs to Persistent Volumes (PVs), and discuss reclaim policies for managing storage efficiently. Before you create a PVC, ensure that one or more persistent volumes are available in your cluster. Remember, persistent volumes and persistent volume claims are separate Kubernetes objects. Typically, an administrator creates a pool of persistent volumes, while a user creates PVCs to request and access that storage. When a PVC is created, Kubernetes searches for an appropriate PV that meets the requested capacity and matching properties such as access modes, volume modes, and storage class. Each PVC is bound to a single PV based on a best-match algorithm. The diagram below illustrates the relationship between PVCs and PVs: If multiple persistent volumes could satisfy a claim’s requirements, labels and selectors can be used to bind a claim to a specific volume. For example, consider the following configuration snippet that uses a selector with a label match: selector:
  matchLabels:
    name: my-p On the PV side, you might define: labels:
  name: my-pv Note Even if the PVC requests a smaller amount of storage, it may bind to a larger PV if all other criteria match and no better option is available. Once a PV is bound to a PVC, its remaining capacity cannot be used for other claims. If no suitable volumes are available, the PVC remains in a pending state. When new PVs are added that meet the claim’s requirements, Kubernetes binds the PVC automatically. Creating a Persistent Volume Claim Let's create a persistent volume claim using a YAML template. In this example, the API version is set to v1 and the kind is PersistentVolumeClaim. The claim is named ""myclaim"". Under the specification, we set the access mode to ReadWriteOnce and request 500Mi of storage. Below is the YAML definition for the PVC: apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi To create the claim, run the following command: kubectl create -f pvc-definition.yaml Once applied, you can view the status of your PVC with: kubectl get persistentvolumeclaim The output might initially be similar to: NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES
myclaim   Pending At this point, Kubernetes is evaluating the available persistent volumes. Suppose you have already configured a PV as shown below: apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-voll
spec:
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 1Gi
  awsElasticBlockStore:
    volumeID: <volume-id>
    fsType: ext4 Since the PV's access mode matches and its capacity (1Gi) exceeds the PVC's request (500Mi), Kubernetes binds the PVC to this PV if no other closer match exists. You can verify the binding by executing: kubectl get persistentvolumeclaim The PVC status should now show as ""Bound"". Deleting a Persistent Volume Claim and Reclaim Policies To delete a PVC, use the following command: kubectl delete persistentvolumeclaim myclaim After deletion, it's important to understand the fate of the underlying PV based on its reclaim policy. The reclaim policy determines what happens to a PV when its associated PVC is deleted. Reclaim Policy Options The default reclaim policy is set to Retain, which means the PV remains available until manually deleted by an administrator: persistentVolumeReclaimPolicy: Retain If you want Kubernetes to automatically delete the PV when the PVC is removed—thus freeing up the storage—you can change the reclaim policy accordingly. Alternatively, you might choose the Recycling policy, where the data on the volume is scrubbed before the volume is made available for reuse: persistentVolumeReclaimPolicy: Recycling Tip To practice configuring and troubleshooting persistent volumes and claims in Kubernetes, review the above configurations and experiment with these commands on your cluster. Summary In this lesson, we covered: PVC and PV Concepts: How PVCs bind to suitable PVs based on capacity and properties. Configuration Example: Creating a PVC using a YAML file. PVC Creation Process: Binding status and troubleshooting. Reclaim Policies: How PVs behave after their associated PVC is deleted. For more detailed Kubernetes information, refer to the following resources: Kubernetes Basics Kubernetes Documentation Docker Hub Happy learning! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Developing network policies,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Services-Networking/Developing-network-policies,"Certified Kubernetes Application Developer - CKAD Services Networking Developing network policies In this lesson, we will explore Kubernetes network policies in detail using our familiar web API and database pods. Our objective is to protect the database pod by allowing only the API pod to access it on port 3306, while all other pods remain unrestricted. By default, Kubernetes allows all traffic between pods. Therefore, the first step is to block all traffic to and from the database pod by creating a network policy—named ""db-policy""—and associating it with the database pod via labels. In our case, the database pod is labeled with role: db. Below is the network policy that initially blocks all ingress traffic to the database pod: apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress Allowing Specific Ingress Traffic To allow the API pod to connect to the database on port 3306, we add an ingress rule. This rule specifies a pod selector for the API pod (labeled as name: api-pod) and restricts the allowed port to TCP 3306. Note Once the incoming traffic is allowed, the corresponding response traffic is automatically permitted without needing an additional rule. apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          name: api-pod
    ports:
    - protocol: TCP
      port: 3306 It is important to note that this ingress rule only governs traffic coming into the database pod; it does not allow the database pod to initiate connections. For example, if the database pod tries to communicate with the API pod, that outbound (egress) traffic would be blocked unless you define an explicit egress rule. Restricting Traffic by Namespace Consider a scenario with multiple API pods across different namespaces (e.g., dev, test, and prod). If these pods share the same labels, the above policy would allow all API pods to access the database. To restrict access only to the API pod in the prod namespace, we can add a namespace selector to the ingress rule. This additional selector ensures that only pods matching both the label and the specific namespace label (name: prod) are allowed. apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: db-policy
spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          name: api-pod
      namespaceSelector:
        matchLabels:
          name: prod
  ports:
  - protocol: TCP
    port: 3306 Keep in mind: if you include only the namespace selector without a pod selector, every pod within the specified namespace would gain access, potentially permitting unwanted traffic from pods like the web pod. Allowing Traffic from External Sources In some scenarios, external systems like a backup server (outside your Kubernetes cluster) might need to connect to the database. Since the external server isn’t managed by Kubernetes, pod and namespace selectors do not apply. Instead, use an IP block to allow traffic from a specific external IP address. For example, if the backup server’s IP is 192.168.5.10, you can configure the ingress rule as follows: spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          name: api-pod
      namespaceSelector:
        matchLabels:
          name: prod
    - ipBlock:
        cidr: 192.168.5.10/32
  ports:
  - protocol: TCP
    port: 3306 In this configuration, there are two elements under the ""from"" section: The first element requires that the traffic come from a pod labeled api-pod residing in the prod namespace. The second element allows traffic directly from the specified external IP address. Separating selectors into distinct blocks changes the logic. It is crucial to structure these rules carefully based on your security and traffic requirements. Configuring Egress Policies While the previous rules focused on controlling incoming (ingress) traffic, there are cases when the database pod needs to initiate outbound communications. For example, if an agent on the database pod is pushing backups to an external server, you must define an egress rule. Below is an example rule that permits outbound traffic to an external server (with IP 192.168.5.10) on port 80: spec:
  podSelector:
    matchLabels:
      role: db
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          name: api-pod
    ports:
    - protocol: TCP
      port: 3306
  egress:
  - to:
    - ipBlock:
        cidr: 192.168.5.10/32
    ports:
    - protocol: TCP
      port: 80 This configuration maintains the ingress restrictions while adding an egress rule that specifically permits outbound traffic to the external backup server. Summary In this lesson, you learned how to configure Kubernetes network policies to manage both ingress and egress traffic. Key topics covered include: Blocking all traffic by default and protecting a specific pod (database). Allowing traffic from a specific pod and namespace. Permitting access from external sources using IP blocks. Defining egress rules for outbound communications. Practice Makes Perfect We recommend practicing with these network policies in your Kubernetes environment to reinforce these concepts and tailor them to your specific use cases. For further reading on Kubernetes networking and network policies, check out Kubernetes Documentation and other related resources. Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Stateful Sets Introduction,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/State-Persistence/Stateful-Sets-Introduction,"Certified Kubernetes Application Developer - CKAD State Persistence Stateful Sets Introduction In this lesson, we explore StatefulSets in Kubernetes and explain when to use them instead of Deployments. StatefulSets are ideal for applications that require: Ordered startup and shutdown Stable network identities Consistent storage provisioning When your application instances need to start in a specific order or require persistent identities between restarts, opting for a StatefulSet is the right choice. Key Difference from Deployments A StatefulSet is similar to a Deployment in that you define it via a YAML file with a Pod template. The main differences are: Change the kind from Deployment to StatefulSet (note the uppercase “S”). Include the additional field 'serviceName' to specify a headless service. Converting a Deployment to a StatefulSet Consider the following example of a MySQL Deployment: apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql To convert this Deployment into a StatefulSet, modify the YAML file by updating the kind and adding the serviceName field: apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  serviceName: mysql-h
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql When you create a StatefulSet using this file, Kubernetes will: Deploy Pods one at a time in an ordered, graceful manner. Assign a stable, unique DNS record to each Pod, allowing other applications to refer to them reliably. Scale Pods sequentially, where each new Pod starts only after the previous one is ready. This ordered behavior is particularly beneficial for applications like MySQL databases, where preserving state and order is critical. Creating and Managing a StatefulSet You can use standard Kubernetes commands to create and scale your StatefulSet. For example: kubectl create -f statefulset-definition.yml
kubectl scale statefulset mysql --replicas=5
# Output: statefulset.apps/mysql scaled When scaling down, Kubernetes terminates the Pods in reverse order: the last Pod is removed first, followed by earlier ones. Likewise, when deleting the StatefulSet, Pods are terminated in reverse order. Customizing Pod Deployment Strategy By default, StatefulSets follow an ordered approach for both deployment and termination. However, you can override this behavior by setting the podManagementPolicy to Parallel . This instructs Kubernetes to deploy and terminate all Pods simultaneously while still providing them with stable network identities. Below is an example StatefulSet configuration that uses the Parallel pod management policy: apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  serviceName: mysql-h
  podManagementPolicy: Parallel
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
      - name: mysql
        image: mysql You can manage this StatefulSet with the following commands: kubectl create -f statefulset-definition.yml
kubectl scale statefulset mysql --replicas=5
kubectl scale statefulset mysql --replicas=3
kubectl delete statefulset mysql
# Output: statefulset.apps/mysql deleted This configuration allows Pods to be launched or terminated in parallel, which can be advantageous when the order of operations is not a strict requirement for your application. Remember While the Parallel management policy offers faster scaling, it does so at the cost of ordered deployment. Choose the appropriate policy based on your application's initialization and shutdown needs. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Introduction to Docker Storage,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/State-Persistence/Introduction-to-Docker-Storage,"Certified Kubernetes Application Developer - CKAD State Persistence Introduction to Docker Storage In this lesson, we explore how storage works in Docker and how these foundational concepts apply to Kubernetes storage management. Understanding Docker storage is essential for grasping the more advanced storage mechanisms in Kubernetes. When working with Docker, two core storage concepts are essential to understand: Storage Drivers Volume Driver Plugins In the upcoming section, we focus on storage drivers—a topic thoroughly covered in the Docker Training Course for the Absolute Beginner . If you are already familiar with this material, feel free to skip ahead or use this section as a refresher. Note After discussing storage drivers, we will explore volume drivers to complete your understanding of Docker storage. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Why Stateful Sets,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/State-Persistence/Why-Stateful-Sets,"Certified Kubernetes Application Developer - CKAD State Persistence Why Stateful Sets In this lesson, we will explore StatefulSets in Kubernetes, emphasizing their necessity compared to Deployments. To understand their importance, let's begin with a real-world scenario outside of Kubernetes. Imagine you have a physical server running a database server, such as MySQL. After installing MySQL and creating the initial database, your database is up and running, accessible to client applications. To ensure high availability, you deploy additional servers with MySQL installed. However, these new servers start with empty databases, and you must replicate data from the original master database to them. For illustration, we use MySQL replication. Although the details of MySQL replication are simplified here, the focus is on the high-level procedure for setting up dynamic replicas. In a common single-master multi-slave topology, all write operations are directed to the master server, while read operations can be served by any server. The process involves the following steps: Deploy the master server and bring it online. Clone the database from the master to the first slave. Enable continuous replication from the master to the first slave. Once the first slave is ready, clone its data to a second slave. Enable continuous replication on the second slave, configuring it to replicate from the master. A critical aspect of this replication setup is configuring the slave servers with the master's hostname or address. This static reference is essential because if the master is replaced (for instance, due to a crash), its IP address might change if assigned dynamically. Therefore, a stable, static hostname is required. In typical deployments, pods are created with random names, which makes maintaining a static reference challenging. For example, if a master pod crashes and is recreated with a new name, the replication linkage is broken. Transitioning to Kubernetes In Kubernetes, you might consider deploying each instance in your MySQL cluster (both master and slaves) as pods within a Deployment. Deployments are excellent for scaling and managing pods; however, they do not guarantee the startup order of the pods. In our replication setup, the master must be established before starting the slaves, and this sequential order is imperative. Consider the following configuration snippet: MASTER_HOST=mysql-master
MASTER_HOST=mysql-master In this scenario, the master must start before any of the slave nodes. For example, slave one must initialize and clone data from the master, and later, slave two should clone data from slave one. Unfortunately, Deployments create all pods concurrently, so ensuring the necessary startup order becomes problematic. Additionally, during the cloning and continuous replication phases, it is critical to clearly distinguish between the master and slave pods. The master requires a constant and static hostname instead of an IP address—which might change when a pod is recreated. With Deployments generating dynamically named pods, this requirement is not met. If the designated master pod crashes and is replaced by a new pod with a different name, your replication configuration will break. Key Point A stable and predictable pod naming convention is crucial for proper database replication in a stateful system. Introducing StatefulSets StatefulSets address these challenges with features that resemble Deployments—such as pod templating, scaling, rolling updates, and rollbacks—but with a critical advantage: they enforce sequential pod creation. This ensures that the first pod is deployed, running, and ready before the next pod is initialized. This behavior directly caters to our requirement where the master must always start first, followed by the slaves. StatefulSets assign a unique ordinal index to each pod, starting at zero, and each pod’s name is derived from the StatefulSet name combined with its ordinal index. For example, given a StatefulSet named mySQL: The first pod is named mySQL-0. The second pod is named mySQL-1. The third pod is named mySQL-2. This predictable naming allows you to reliably designate mySQL-0 as the master pod. Slave pods can then be explicitly configured—for instance, mySQL-1 and mySQL-2 can be set up to clone data from the master, or further cascade replication among themselves. Even if the master pod fails and is recreated, it retains its stable name due to the sticky identity provided by StatefulSets. Conclusion StatefulSets offer significant advantages when deploying systems like a MySQL cluster on Kubernetes. They guarantee that pods are launched in a specific sequence and maintain a stable network identity essential for configuring master-slave replication. Next Steps This discussion sets the stage for a deeper exploration into Kubernetes topics such as creating StatefulSets, headless services, and configuring Persistent Volumes. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Headless Services,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/State-Persistence/Headless-Services,"Certified Kubernetes Application Developer - CKAD State Persistence Headless Services In this article, we explore headless services in Kubernetes and how they solve routing challenges in a master-slave architecture, such as a MySQL cluster. When using a StatefulSet, each pod is deployed one at a time with a unique ordinal index (for example, mysql-0, mysql-1, mysql-2). This approach provides stable, unique names for each pod, making it easier to direct connections to specific nodes—an essential requirement for database clusters where write operations must be handled by a single master. The Problem with Traditional Services Under typical conditions, Kubernetes services function as load balancers. They provide a Cluster IP and a DNS name (like mysql.default.svc.cluster.local) that routes traffic evenly to all matching pods. This behavior is acceptable when the application only performs read operations in a MySQL cluster but becomes problematic for write operations. Write queries must be directed solely to the master pod, and load balancing across all pods could lead to data inconsistency or conflicts. How Headless Services Address the Issue Headless services eliminate the load balancing behavior by not assigning a Cluster IP. Instead, they generate DNS records for each individual pod in the following format: pod-name.headless-service-name.namespace.svc.cluster.local For example, if you create a headless service named ""mysql-h"", the master pod can be accessed via: mysql-0.mysql-h.default.svc.cluster.local This DNS entry consistently resolves to the master pod in the MySQL deployment. Note In a headless service, setting clusterIP to ""None"" is the only key difference from a standard service definition. Headless Service Definition Example Below is an example of how to define a headless service: apiVersion: v1
kind: Service
metadata:
  name: mysql-h
spec:
  ports:
    - port: 3306
  selector:
    app: mysql
  clusterIP: None Configuring Pods to Use Headless Services When deploying a pod under a headless service, include the optional fields subdomain and hostname in the pod specification. The subdomain must match the name of your headless service, ensuring that Kubernetes creates the correct DNS record. For example, to create a pod with only the subdomain specified: apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: mysql
spec:
  containers:
    - name: mysql
      image: mysql
  subdomain: mysql-h However, to generate a fully qualified DNS record that includes the pod name, you must set the hostname field as well: apiVersion: v1
kind: Pod
metadata:
  name: myapp-pod
  labels:
    app: mysql
spec:
  containers:
    - name: mysql
      image: mysql
  subdomain: mysql-h
  hostname: mysql-pod Visual Overview of a MySQL Cluster Setup The diagram below illustrates the setup of a MySQL database cluster with one master and two replicas. It highlights the network configuration, including IP addresses and hostnames: Deployments vs. StatefulSets When deploying pods using a Deployment, the pod template generally does not specify the hostname or subdomain fields. Consequently, the headless service is unable to create unique A records for each pod. If these fields are manually added to the pod template, every pod receives the same DNS record, resulting in an address like mysql-pod.mysql-h.default.svc.cluster.local, which is unsuitable for distinct addressing. Deployment Example with Identical DNS Records Below is an example where a Deployment assigns the same hostname and subdomain to all pods: apiVersion: v1
kind: Service
metadata:
  name: mysql-h
spec:
  ports:
    - port: 3306
  selector:
    app: mysql
  clusterIP: None apiVersion: apps/v1
kind: Deployment
metadata:
  name: mysql-deployment
  labels:
    app: mysql
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
        - name: mysql
          image: mysql
      subdomain: mysql-h
      hostname: mysql-pod In this configuration, all pods share the same DNS record because they have identical hostnames and subdomains. Leveraging StatefulSets for Unique DNS Records StatefulSets are designed to overcome this limitation by automatically generating unique hostnames for each pod. You do this by simply referencing the headless service name in the StatefulSet specification using the serviceName field. Kubernetes then assigns a unique DNS record to each pod based on its ordinal index. StatefulSet Example with Headless Service Below is an example of a StatefulSet that utilizes a headless service: apiVersion: v1
kind: Service
metadata:
  name: mysql-h
spec:
  ports:
    - port: 3306
  selector:
    app: mysql
  clusterIP: None apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql-deployment
  labels:
    app: mysql
spec:
  serviceName: mysql-h
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
        - name: mysql
          image: mysql In this configuration, each pod is assigned a unique DNS record, for example: mysql-0.mysql-h.default.svc.cluster.local mysql-1.mysql-h.default.svc.cluster.local mysql-2.mysql-h.default.svc.cluster.local This setup meets the requirement for distinct addressing, ensuring that write requests intended for the master pod are routed correctly. Key Takeaway StatefulSets combined with headless services provide a robust solution for applications requiring individual pod addressing without load balancing interference. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Storage Classes,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/State-Persistence/Storage-Classes,"Certified Kubernetes Application Developer - CKAD State Persistence Storage Classes In this guide, we'll dive into storage classes in Kubernetes, a vital concept for managing dynamic storage provisioning. Earlier, we covered the creation of Persistent Volumes (PVs), Persistent Volume Claims (PVCs), and their integration within pod definitions. With static provisioning, you manually create and manage disks along with their corresponding PV definitions before deploying an application. Below is an example of static provisioning using Google Cloud Persistent Disk: # pv-definition.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-vol1
spec:
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 500Mi
  gcePersistentDisk:
    pdName: pd-disk
    fsType: ext4 # pvc-definition.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi # pod-definition.yaml
apiVersion: v1
kind: Pod
metadata:
  name: random-number-generator
spec:
  containers:
    - image: alpine
      name: alpine
      command: [""/bin/sh"", ""-c""]
      args: [""shuf -i 0-100 -n 1 >> /opt/number.out;""]
      volumeMounts:
        - mountPath: /opt
          name: data-volume
  volumes:
    - name: data-volume
      persistentVolumeClaim:
        claimName: myclaim Before creating the corresponding PV, you must provision the disk on Google Cloud manually. For example, you would use the following command to create a disk: gcloud beta compute disks create \
  --size 1GB \
  --region us-east1 \
  pd-disk Static Provisioning Static provisioning involves manually creating and managing the storage disks and their PV definitions. This can become cumbersome for dynamic applications. Dynamic Provisioning with Storage Classes Storage classes simplify storage management by allowing you to automatically create and configure storage resources when a PVC is created. They define a provisioner (such as Google Cloud Persistent Disk) that automatically creates a new disk, dynamically provisions a PV, and binds it to a PVC based on the storage class specified. To implement dynamic provisioning, create a StorageClass object with the API version set to storage.k8s.io/v1. For Google Cloud, set the provisioner to kubernetes.io/gce-pd. Here is an example: # sc-definition.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: google-storage
provisioner: kubernetes.io/gce-pd When a PVC references a storage class by name, Kubernetes automatically creates and attaches the required storage. For instance: # pvc-definition.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: google-storage
  resources:
    requests:
      storage: 500Mi # pod-definition.yaml
apiVersion: v1
kind: Pod
metadata:
  name: random-number-generator
spec:
  containers:
    - image: alpine
      name: alpine
      command: [""/bin/sh"", ""-c""]
      args: [""shuf -i 0-100 -n 1 >> /opt/number.out;""]
      volumeMounts:
        - mountPath: /opt
          name: data-volume
  volumes:
    - name: data-volume
      persistentVolumeClaim:
        claimName: myclaim In this setup, when the PVC is created, Kubernetes uses the specified StorageClass to automatically provision a new disk, create a PV, and bind it to the PVC. This eliminates the need for manual disk provisioning. Advanced Storage Classes Storage classes can be further customized with parameters specific to the underlying provisioner. For instance, with Google Cloud Persistent Disk, you can define the disk type and replication mode. Consider this example: # google-storage with parameters
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: google-storage
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard  # Options: pd-standard or pd-ssd
  replication-type: none  # Options: none or regional-pd This customization allows enterprises to define different classes of service based on performance and availability requirements. Below are examples of multiple storage classes: Storage Class Disk Type Replication Mode Silver pd-standard None Gold pd-ssd None Platinum pd-ssd Regional (High Availability) Examples for each: # silver storage class
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: silver
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard
  replication-type: none # gold storage class
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: gold
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
  replication-type: none # platinum storage class
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: platinum
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-ssd
  replication-type: regional-pd By utilizing these tailored storage classes in your PVC definitions, you ensure that the storage provisioned aligns precisely with your application’s performance requirements and budget considerations. Dynamic Provisioning Benefits Dynamic provisioning streamlines the process of deploying applications in Kubernetes by eliminating manual storage configuration. This leads to improved efficiency, reduced errors, and enhanced scalability. In Summary Storage classes in Kubernetes offer a powerful mechanism to manage storage dynamically. By abstracting the complexities of physical disk configurations, storage classes enable you to create, manage, and bind storage resources automatically as needed. Whether you opt for static provisioning or embrace the flexibility of dynamic provisioning, storage classes are integral to ensuring your applications have the right storage infrastructure. For more detailed information, consider visiting the official Kubernetes Documentation . Happy provisioning! Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,API Groups,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Security/API-Groups,"Certified Kubernetes Application Developer - CKAD Security API Groups Before diving into authorization, it's essential to understand how API groups are organized in Kubernetes. Understanding the Kubernetes API The Kubernetes API serves as the central interface for interacting with the cluster. Whether you use the kubectl command-line tool or make direct REST calls, every operation in the cluster is handled by the API server. For example, to check the cluster version, you can query the API server running on the master node (default port 6443) by appending the API version to the URL. Similarly, to list all the pods, you would send a request to /api/v1/pods . Below is an example command that retrieves the Kubernetes version: curl https://kube-master:6443/version The expected output is: {
  ""major"": ""1"",
  ""minor"": ""13"",
  ""gitVersion"": ""v1.13.0"",
  ""gitCommit"": ""ddd47ac13c1a9483ea035a79cd7c1005ff21a6d"",
  ""gitTreeState"": ""clean"",
  ""buildDate"": ""2018-12-03T20:56:12Z"",
  ""goVersion"": ""go1.11.2"",
  ""compiler"": ""gc"",
  ""platform"": ""linux/amd64""
} API Groups Overview Kubernetes organizes its API functionalities into various groups, making it easier to manage and scale operations. These groups include endpoints for version information, metrics, health checks, logs, and more. For instance, while the /version API reveals the cluster version, the /metrics and /healthz endpoints help monitor your cluster's health. The /logs endpoint can be integrated with third-party logging tools. Kubernetes APIs can be broadly divided into two categories: Core API Group : Contains fundamental components such as namespaces, pods, replication controllers, events, endpoints, nodes, bindings, persistent volumes, persistent volume claims, config maps, secrets, and services. Named API Groups : Organizes newer features into groups like apps, extensions, networking, storage, authentication, and authorization. For example, the ""apps"" group includes Deployments, ReplicaSets, and StatefulSets, while the ""networking"" group covers Network Policies. Other functionalities such as Certificate Signing Requests belong to different named groups. Each resource in these API groups supports a collection of actions—referred to as ""verbs""—that allow you to perform operations like list, get, create, delete, update, and watch. For a detailed overview of each API object, including its associated group and version, check out the Kubernetes API reference. For example, ""v1 core"" indicates the core API group at version v1. You can also explore your cluster’s API groups by accessing the API server on port 6443 without providing any specific path: curl http://localhost:6443 -k The response may resemble: {
  ""paths"": [
    ""/api"",
    ""/api/v1"",
    ""/apis"",
    ""/apis/"",
    ""/healthz"",
    ""/logs"",
    ""/metrics"",
    ""/openapi/v2"",
    ""/swagger-2.0.0.json""
  ]
} Accessing the Kubernetes API When making direct requests to the Kubernetes API using curl, you might face authentication restrictions. For example, executing the following command without proper certificates might lead to a Forbidden error: curl http://localhost:6443 -k Output: {
  ""kind"": ""Status"",
  ""apiVersion"": ""v1"",
  ""metadata"": {},
  ""status"": ""Failure"",
  ""message"": ""Forbidden: User \""system:anonymous\"" cannot get path \""/\"""",
  ""reason"": ""Forbidden"",
  ""details"": {},
  ""code"": 403
} Authentication Tip To access the API securely, include your certificate files in the curl command as follows: curl http://localhost:6443 -k --key admin.key --cert admin.crt --cacert <ca.crt> Alternatively, you can use the kubectl proxy command. This command starts a local proxy service on port 8001, utilizing the credentials from your kubeconfig file to authenticate your requests—eliminating the need to explicitly specify certificates. Start the proxy by running: kubectl proxy This command outputs: Starting to serve on 127.0.0.1:8001 Then, access the API via the proxy: curl http://localhost:8001 -k Expected response: {
  ""paths"": [
    ""/api"",
    ""/api/v1"",
    ""/apis"",
    ""/apis/"",
    ""/healthz"",
    ""/logs"",
    ""/metrics"",
    ""/openapi/v2"",
    ""/swagger-2.0.0.json""
  ]
} Important Distinction Do not confuse kubectl proxy with kube-proxy . The kube-proxy manages networking and connectivity between pods across nodes, while kubectl proxy forwards API requests using your kubeconfig credentials. Summary Kubernetes organizes its resources within various API groups, divided into the core API group and named API groups. Each group contains specific resources, and these resources support multiple actions (verbs) such as list, get, create, delete, update, and watch. In the next lesson on authorization, you will learn how these verbs control access within the cluster. That concludes this lesson. See you in the next one! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Authentication,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Security/Authentication,"Certified Kubernetes Application Developer - CKAD Security Authentication Welcome to this comprehensive lesson on authentication within a Kubernetes cluster. In a typical Kubernetes environment, the cluster consists of multiple nodes—whether physical or virtual—and a range of components that work together seamlessly. The cluster is accessed by various users, including administrators managing cluster operations, developers deploying applications, end users interacting with those applications, and even third-party applications for integrations. In this lesson, we focus on securing the cluster by ensuring secure communication between internal components and by managing administrative access with robust authentication and authorization mechanisms. Our primary focus is on authentication mechanisms that secure access for users (such as administrators and developers) and service accounts (robots or processes with cluster access). Note that while end user authentication for applications is generally managed by the application itself, Kubernetes relies on external sources for handling human user accounts. Kubernetes itself does not manage user accounts natively. Instead, it integrates with external sources like CSV files, certificate authorities, or third-party identity services (e.g., LDAP). Although service accounts can be created and managed via the Kubernetes API, user management for non-service accounts must be handled externally. Every request—whether it comes via the kubectl tool or the API—is first authenticated by the kube-apiserver. There are several authentication mechanisms available in Kubernetes: Using a static password file (a CSV file containing usernames and passwords) Using a static token file for token-based authentication Using certificates for user authentication Integrating with third-party authentication protocols such as LDAP or Kerberos Below is another visual representation that categorizes accounts into ""User"" (Admins, Developers) and ""Service Accounts"" (Bots). All authentication methods funnel through the kube-apiserver which validates each request before further processing. Static Password File Authentication The simplest authentication method uses a CSV file that lists users along with their passwords, usernames, and user IDs. Optionally, a fourth column can indicate group memberships. This file is made available to the kube-apiserver using the --basic-auth-file flag. Consider this example CSV file that stores user details: password123,user1,u0001
password123,user2,u0002
password123,user3,u0003
password123,user4,u0004
password123,user5,u0005 To configure the kube-apiserver to use this file, update its service configuration. For clusters set up with tools like kubeadm, modify the kube-apiserver manifest (typically located at /etc/kubernetes/manifests/kube-apiserver.yaml ) as shown below: kube-apiserver.service
ExecStart=/usr/local/bin/kube-apiserver \
    --advertise-address=${INTERNAL_IP} \
    --allow-privileged=true \
    --apiserver-count=3 \
    --authorization-mode=Node,RBAC \
    --bind-address=0.0.0.0 \
    --enable-swagger-ui=true \
    --etcd-servers=https://127.0.0.1:2379 \
    --event-ttl=1h \
    --runtime-config=api/all \
    --service-cluster-ip-range=10.32.0.0/24 \
    --service-node-port-range=30000-32767 \
    --v=2 \
    --basic-auth-file=user-details.csv Below is an excerpt from a modified kube-apiserver pod manifest for a kubeadm-based cluster: /etc/kubernetes/manifests/kube-apiserver.yaml
apiVersion: v1
kind: Pod
metadata:
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - name: kube-apiserver
    image: k8s.gcr.io/kube-apiserver-amd64:v1.11.3
    command:
    - kube-apiserver
    - --authorization-mode=Node,RBAC
    - --advertise-address=172.17.0.107
    - --allow-privileged=true
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true After updating the manifest, the kube-apiserver will restart automatically (in a kubeadm environment) to apply the new authentication configuration. Static Token File Authentication For token-based authentication, you can create a CSV file similar to the static password file, but with a token replacing the password. This file is provided to the kube-apiserver using the --token-auth-file flag. Here’s an example CSV file with tokens and group memberships: KpjCvB7rcFAHYPKByTIZRb7gulcUc4B,user10,u0010,group1
rJJncHmvtxHc6M1WQddhtvNyhgTdxSC,user11,u0011,group1
mjpOFTEiFOKL9toikaRNTt59ePtczZSq,user12,u0012,group2
PG411Xhs7QjqWKmBkvgGT9glOyUqZij,user13,u0013,group2 When starting the kube-apiserver, include the token file as follows: --token-auth-file=user-token-details.csv To authenticate a request with a token, pass the token as an authorization bearer token. For example: curl -v -k https://master-node-ip:6443/api/v1/pods --header ""Authorization: Bearer KpjCvB7rcFAHYPKByTIZRb7gulcUc4B"" Important Considerations Warning Using static password or token files is not recommended for production environments since they store sensitive information (usernames, passwords, tokens) in clear text. Consider using more secure methods such as client certificates or integrative external identity providers. If you are testing these authentication methods in a kubeadm cluster, ensure that you configure volume mounts appropriately to pass the authentication file into the kube-apiserver pod. Additionally, configure robust authorization mechanisms for the new users. We will explore authorization strategies later in this course. This concludes our detailed examination of Kubernetes authentication mechanisms. For further details and best practices, consult the Kubernetes Documentation . Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Storage in StatefulSets,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/State-Persistence/Storage-in-StatefulSets,"Certified Kubernetes Application Developer - CKAD State Persistence Storage in StatefulSets In this article, we explore storage in StatefulSets and explain how persistent storage operates in Kubernetes. We begin with a review of persistent volumes (PV) and persistent volume claims (PVC) used with Pods, then dive into different provisioning methods and their use in StatefulSets. Persistent Volumes and Static Provisioning Static provisioning involves a three-step process: Create a PersistentVolume. Create a PersistentVolumeClaim. Reference the PVC in your Pod specification. Below is an example of each configuration: Tip For static provisioning, ensure that the PV capacity and access modes match the requirements of your applications. PersistentVolume Definition # pv-definition.yaml
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-vol1
spec:
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 500Mi
  gcePersistentDisk:
    pdName: pd-disk
  fsType: ext4 PersistentVolumeClaim Definition # pvc-definition.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-volume
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: google-storage
  resources:
    requests:
      storage: 500Mi Pod Definition Referencing the PVC # pod-definition.yaml
apiVersion: v1
kind: Pod
metadata:
  name: mysql
spec:
  containers:
    - image: mysql
      name: mysql
      volumeMounts:
        - mountPath: /var/lib/mysql
          name: data-volume
  volumes:
    - name: data-volume
      persistentVolumeClaim:
        claimName: data-volume Dynamic Provisioning with StorageClasses Dynamic provisioning simplifies the process by automatically creating PVs when you define a PVC and reference a StorageClass. This eliminates the need to manually provision PVs. StorageClass Definition # sc-definition.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: google-storage
provisioner: kubernetes.io/gce-pd PVC Definition Using Dynamic Provisioning # pvc-definition.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data-volume
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: google-storage
  resources:
    requests:
      storage: 500Mi Pod Definition Referencing the Dynamically Provisioned PVC # pod-definition.yaml
apiVersion: v1
kind: Pod
metadata:
  name: mysql
spec:
  containers:
    - image: mysql
      name: mysql
      volumeMounts:
        - mountPath: /var/lib/mysql
          name: data-volume
  volumes:
    - name: data-volume
      persistentVolumeClaim:
        claimName: data-volume Using StatefulSets with Shared Storage StatefulSets support scenarios where multiple replicas share the same volume. If you reference a common PVC within a StatefulSet, all replicas will attempt to access the same storage. This setup works if your underlying storage supports multi-reader or multi-writer capabilities. Shared Storage StatefulSet Example ---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
        - name: mysql
          image: mysql
          volumeMounts:
            - mountPath: /var/lib/mysql
              name: data-volume
      volumes:
        - name: data-volume
          persistentVolumeClaim:
            claimName: data-volume Important Ensure that your storage solution supports concurrent access if you plan to share the same volume across multiple Pods. Separate Volumes for Each Pod Using VolumeClaimTemplates For scenarios like MySQL replication where each Pod requires dedicated storage, a volume claim template allows Kubernetes to automatically create a unique PVC for each Pod in a StatefulSet. Step 1: Define the StorageClass # sc-definition.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: google-storage
provisioner: kubernetes.io/gce-pd Step 2: Create a StatefulSet with a VolumeClaimTemplate # statefulset-definition.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
  labels:
    app: mysql
spec:
  replicas: 3
  selector:
    matchLabels:
      app: mysql
  template:
    metadata:
      labels:
        app: mysql
    spec:
      containers:
        - name: mysql
          image: mysql
          volumeMounts:
            - mountPath: /var/lib/mysql
              name: data-volume
  volumeClaimTemplates:
    - metadata:
        name: data-volume
      spec:
        accessModes:
          - ReadWriteOnce
        storageClassName: google-storage
        resources:
          requests:
            storage: 500Mi In this configuration, Kubernetes provisions a unique PVC for each Pod automatically based on the volume claim template. This ensures that every Pod receives its dedicated storage. Additionally, StatefulSets maintain stable storage even if a Pod is rescheduled; the associated PVC and underlying PV remain intact and are reattached to the new Pod instance. That concludes our discussion on storage in StatefulSets. For more information on Kubernetes storage concepts, visit the Kubernetes Documentation . Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,KubeConfig,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Security/KubeConfig,"Certified Kubernetes Application Developer - CKAD Security KubeConfig Welcome to this guide on kubeconfig files in Kubernetes. In this article, we will explore how kubeconfig files streamline authentication and context management for kubectl, enhancing your workflow by reducing repetitive command-line options. So far, you learned how to generate a certificate for a user and how a client can use the certificate file and key to query the Kubernetes REST API. For example, assume your cluster is named ""my kube playground."" You can send a curl request to the Kubernetes API server with the client certificate, key, and CA certificate: curl https://my-kube-playground:6443/api/v1/pods \
  --key admin.key \
  --cert admin.crt \
  --cacert ca.crt The API server validates the certificate and responds with output similar to: {
  ""kind"": ""PodList"",
  ""apiVersion"": ""v1"",
  ""metadata"": {
    ""selfLink"": ""/api/v1/pods""
  },
  ""items"": []
} When using kubectl, you normally pass the same connection information with the corresponding options: kubectl get pods \
  --server my-kube-playground:6443 \
  --client-key admin.key \
  --client-certificate admin.crt \
  --certificate-authority ca.crt This command might return: No resources found. Typing these options every time can become tedious. To simplify your workflow, you can move the connection details into a configuration file known as a kubeconfig file. By default, kubectl looks for a file named config under the .kube directory in your home directory. If the kubeconfig file is in its default location, you don’t have to specify connection options for each command: kubectl get pods Tip Using a kubeconfig file saves you time by automatically applying connection settings, which means you no longer have to repeatedly supply options like --client-key and --certificate-authority . Kubeconfig Structure The kubeconfig file is organized into three primary sections: Clusters : Define the various Kubernetes clusters you need access to. You might have separate clusters for development, testing, production, or different cloud providers. Users : Define the user accounts holding credentials (such as client certificates and keys) needed to access these clusters. Contexts : Link clusters and users together. A context specifies which user credentials should be used to access a particular cluster. For example, you could have a context called “admin@production,” which uses the admin user’s credentials for the production cluster. These components work together to streamline connectivity and authentication in your Kubernetes environment. In our example, the server address and CA certificate information belong in the clusters section, while the admin user’s keys and certificates go in the users section. A context then binds these settings together. Below is a sample kubeconfig file in YAML format: apiVersion: v1
kind: Config
clusters:
- name: my-kube-playground  # values hidden for brevity
- name: development
- name: production
- name: google
contexts:
- name: my-kube-admin@my-kube-playground
- name: dev-user@google
- name: prod-user@production
users:
- name: my-kube-admin
- name: admin
- name: dev-user
- name: prod-user Note that you do not create Kubernetes objects for these configurations. Instead, kubectl reads this file to obtain the necessary connection details. kubectl selects a context from the kubeconfig based on the current-context field. For example, if you set: current-context: my-kube-admin@my-kube-playground kubectl will default to that context. Alternatively, you can specify a different kubeconfig file from the command line using the --kubeconfig flag: kubectl config view --kubeconfig=my-custom-config Default Kubeconfig Example An example output of the default kubeconfig file might be: apiVersion: v1
kind: Config
current-context: kubernetes-admin@kubernetes
clusters:
- cluster:
    certificate-authority-data: REDACTED
    server: https://172.17.0.5:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
users:
- name: kubernetes-admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED And when using a custom config file: kubectl config view --kubeconfig=my-custom-config
apiVersion: v1
kind: Config
current-context: my-kube-admin@my-kube-playground
clusters:
- name: my-kube-playground
- name: development
- name: production
contexts:
- name: my-kube-admin@my-kube-playground
- name: prod-user@production
users:
- name: my-kube-admin
- name: prod-user Switching Contexts To switch your current context—for example, changing from the my-kube-admin account on the playground cluster to the prod-user account on the production cluster—use the kubectl config use-context command: kubectl config use-context prod-user@production After running this command, your current-context in the kubeconfig file updates to prod-user@production . You can verify the change by viewing the configuration: apiVersion: v1
kind: Config
current-context: prod-user@production
clusters:
- name: my-kube-playground
- name: development
- name: production
contexts:
- name: my-kube-admin@my-kube-playground
- name: prod-user@production
users:
- name: my-kube-admin
- name: prod-user Additional variations of the kubectl config command let you update or delete entries within the kubeconfig file as needed. Configuring Namespaces Each Kubernetes cluster can span multiple namespaces. You can designate a default namespace within a context so that switching contexts automatically sets the working namespace. For example, here is a configuration for the production cluster that sets ""finance"" as the default namespace: apiVersion: v1
kind: Config
clusters:
- name: production
  cluster:
    certificate-authority: ca.crt
    server: https://172.17.0.51:6443
contexts:
- name: admin@production
  context:
    cluster: production
    user: admin
    namespace: finance
users:
- name: admin
  user:
    client-certificate: admin.crt
    client-key: admin.key When you switch to the admin@production context, kubectl will automatically use the finance namespace. Working with Certificates in Kubeconfig The kubeconfig file typically references certificate file paths. For clarity and robustness, it is best practice to use the full path to each certificate. Alternatively, you can embed the certificate data directly into the file by base64-encoding the certificate. For instance, instead of defining: apiVersion: v1
kind: Config
clusters:
- name: production
  cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt you can embed the certificate data: apiVersion: v1
kind: Config
clusters:
- name: production
  cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJU... If you encounter certificate data in base64 format and need to decode it, use the following command: echo ""LS0t...bnJ"" | base64 --decode This command will output the certificate in its standard PEM format: -----BEGIN CERTIFICATE-----
MIICDCCAuCAQAoA...AIBDwAw...-----END CERTIFICATE----- Security Note Always ensure that certificate and key files are stored securely and access to the kubeconfig file is restricted to trusted users. Summary In this article, we covered how kubeconfig files simplify connection management for Kubernetes by consolidating user credentials, cluster details, and context settings into a single file. Use this knowledge to streamline your kubectl commands and manage multiple Kubernetes environments effectively. Next, apply these concepts by creating and troubleshooting your kubeconfig files to enhance your Kubernetes workflow. For further reading, check out the following resources: Resource Type Description Link Kubernetes Concepts Overview of Kubernetes functionality Kubernetes Basics Kubernetes Documentation Complete documentation and guides Kubernetes Documentation Docker Hub Container images and registry Docker Hub Terraform Registry Infrastructure as Code modules Terraform Registry Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Solution KubeConfig,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Security/Solution-KubeConfig,"Certified Kubernetes Application Developer - CKAD Security Solution KubeConfig In this guide, we'll walk through a comprehensive lab exercise focused on exploring the kubeconfig file and switching contexts for kubectl. We'll inspect the default kubeconfig file’s clusters, users, and contexts, troubleshoot a certificate issue, and finally apply changes to set a new kubeconfig file as the default configuration. Locating the Default kubeconfig File First, determine the location of your default kubeconfig file by leveraging the HOME environment variable. The default file is stored in the hidden .kube directory: root@controlplane ~ # echo $HOME
/root
root@controlplane ~ # pwd
/root
root@controlplane ~ # ls .kube/
cache  config
root@controlplane ~ # ls .kube/config
.kube/config Viewing the file reveals that it contains one cluster, one user, and one context: root@controlplane ~ # cat .kube/config
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0t... (truncated for brevity)
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes Thus, the default kubeconfig file located at /root/.kube/config defines: A single cluster named ""kubernetes"" One context named ""kubernetes-admin@kubernetes"" One user named ""kubernetes-admin"" Examining kubeconfig Details Review the kubeconfig file to answer the following: Number of Clusters: The kubeconfig defines one cluster. Number of Users: Only one user is configured, which includes certificate information. Number of Contexts: There is a single context defined. User Configured in the Current Context: The current context ""kubernetes-admin@kubernetes"" specifies the user ""kubernetes-admin."" Note Although the context name might suggest a naming convention, always inspect the actual user field. Name of the Cluster in the Default Config: The cluster is named ""kubernetes."" For clarity, here is a more detailed excerpt from the default kubeconfig file: root@controlplane ~ # cat .kube/config
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: L0tSL1CRDJtiBRDVSUJZQ0FURS0tL0tSL0tCkt1SJmVakNDQWhZ0F3SUJBZ0lCQURBTkJn...
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users: Creating and Inspecting a New kubeconfig File A new kubeconfig file named ""my kube config"" has been created in the root directory. This configuration file contains multiple clusters, contexts, and users. Below is the complete configuration: contexts:
- name: test-user@development
  context:
    cluster: development
    user: test-user

- name: aws-user@kubernetes-on-aws
  context:
    cluster: kubernetes-on-aws
    user: aws-user

- name: test-user@production
  context:
    cluster: production
    user: test-user

- name: research
  context:
    cluster: test-cluster-1
    user: dev-user

users:
- name: test-user
  user:
    client-certificate: /etc/kubernetes/pki/users/test-user/test-user.crt
    client-key: /etc/kubernetes/pki/users/test-user/test-user.key

- name: dev-user
  user:
    client-certificate: /etc/kubernetes/pki/users/dev-user/developer-user.crt
    client-key: /etc/kubernetes/pki/users/dev-user/dev-user.key

- name: aws-user
  user:
    client-certificate: /etc/kubernetes/pki/users/aws-user/aws-user.crt
    client-key: /etc/kubernetes/pki/users/aws-user/aws-user.key

current-context: test-user@development
preferences: {} Key Details in the New Configuration Clusters: The file defines a total of 4 clusters. Contexts: There are 4 contexts configured. User in the ""research"" Context: The ""research"" context uses the ""dev-user."" AWS User’s Client Certificate: The AWS user’s client certificate is sourced from aws-user.crt . Current Context: Initially set to ""test-user@development."" Switching Context to Use the Dev User To access ""test-cluster-1"" using the ""research"" context (which utilizes the dev user), run the following command. Be sure to specify the kubeconfig file containing the desired configuration: root@controlplane ~ ⟶ kubectl config use-context research --kubeconfig /root/my-kube-config
Switched to context ""research"".
root@controlplane ~ ⟶ Running kubectl config view should now indicate that the current context is ""research."" Setting the New kubeconfig File as Default To avoid specifying the kubeconfig file with each command, move the new configuration file to the default location ( /root/.kube/config ). The updated file appears as follows: name: production
clusters:
- cluster:
    certificate-authority: /etc/kubernetes/pki/ca.crt
    server: https://controlplane:6443
  name: test-cluster-1
contexts:
- context:
    cluster: kubernetes-on-aws
    user: aws-user
  name: aws-user@kubernetes-on-aws
- context:
    cluster: test-cluster-1
    user: dev-user
  name: research
- context:
    cluster: development
    user: test-user
  name: test-user@development
- context:
    cluster: production
    user: test-user
  name: test-user@production
current-context: research
kind: Config
preferences: {}
users:
- name: aws-user
  user:
    client-certificate: /etc/kubernetes/pki/users/aws-user/aws-user.crt
    client-key: /etc/kubernetes/pki/users/aws-user/aws-user.key
- name: dev-user
  user:
    client-certificate: /etc/kubernetes/pki/users/dev-user/developer-user.crt
    client-key: /etc/kubernetes/pki/users/dev-user/dev-user.key
- name: test-user
  user:
    client-certificate: /etc/kubernetes/pki/users/test-user/test-user.crt
    client-key: /etc/kubernetes/pki/users/test-user/test-user.key Open the file in your preferred editor to confirm that all changes are in place and that the current context is correctly set to ""research."" Troubleshooting a Certificate Error With the current context set to ""research,"" you might run into a certificate error when trying to access the cluster: root@controlplane ~ ➜  kubectl get nodes
error: unable to read client-cert /etc/kubernetes/pki/users/dev-user/developer-user.crt for dev-user due to open /etc/kubernetes/pki/users/dev-user/developer-user.crt: no such file or directory Inspect the certificate directory to verify file names: root@controlplane ~ ⟶ ls /etc/kubernetes/pki/users/
aws-user  dev-user  test-user
root@controlplane ~ ⟶ ls /etc/kubernetes/pki/users/dev-user/
dev-user.crt  dev-user.csr  dev-user.key The error is due to the configuration mistakenly referring to the certificate as developer-user.crt instead of the correct dev-user.crt . Fixing the Issue Update the ""dev-user"" entry in your kubeconfig file by changing: client-certificate: /etc/kubernetes/pki/users/dev-user/developer-user.crt to: client-certificate: /etc/kubernetes/pki/users/dev-user/dev-user.crt After saving, verify the fix by running: root@controlplane ~  # kubectl get nodes
NAME           STATUS   ROLES                    AGE   VERSION
controlplane   Ready    control-plane,master     25m   v1.23.0
root@controlplane ~  # The command should now list the nodes, confirming that the configuration is successful. This completes the lab exercise for configuring and troubleshooting the kubeconfig file. For more detailed Kubernetes documentation and troubleshooting guides, refer to the Kubernetes Documentation . Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Authorization,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Security/Authorization,"Certified Kubernetes Application Developer - CKAD Security Authorization In previous lessons, we explored authentication and demonstrated how users or processes gain access to a cluster. With access granted, the next important question is: what actions can they perform? This is where authorization comes into play. As a cluster administrator, you hold full control over operations such as viewing cluster objects (e.g., Pods, Nodes, and Deployments) and modifying them. For example: kubectl get pods
NAME    READY   STATUS    RESTARTS   AGE
nginx   1/1     Running   0          53s

kubectl get nodes
NAME        STATUS   ROLES    AGE     VERSION
worker-1    Ready    <none>   5d21h   v1.13.0
worker-2    Ready    <none>   5d21h   v1.13.0 Administrators can also create or delete objects. For instance, to remove a node from the cluster: kubectl delete node worker-2
Node worker-2 Deleted! Minimum Privilege Principle It is best practice to grant users only the minimum required permissions. For example, a developer might be allowed to deploy applications but should not have permissions to modify cluster-wide configurations like nodes, storage, or networking. Similarly, service accounts should be configured only with the necessary permissions. Consider the following interactions for a user with restricted privileges: kubectl get pods
Error from server (Forbidden): pods is forbidden: User ""Bot-1"" cannot list ""pods""

kubectl get nodes
Error from server (Forbidden): nodes is forbidden: User ""Bot-1"" cannot get ""nodes""

kubectl delete node worker-2
Error from server (Forbidden): nodes ""worker-2"" is forbidden: User ""developer"" cannot delete resource ""nodes"" Authorization in Kubernetes is implemented through multiple mechanisms, including Node authorization, Attribute-Based Access Control (ABAC), Role-Based Access Control (RBAC), and Webhook modes. Each mechanism plays a distinct role in managing access control: The Kubernetes API server is not only accessed by administrators but also by kubelets (nodes) that report the status of nodes and pods. Kubelets use a special node authorizer to handle requests from users whose names are prefixed by ""system:node"" and who belong to the ""system:nodes"" group. This node authorizer validates each request based on naming conventions and group memberships, ensuring that node-level actions are properly verified. Below is an overview of this authorization workflow: For external API access, ABAC lets you tie individual users or groups to a set of permissions. For example, you can define a policy in a JSON file to allow a developer to view, create, and delete pods: {""kind"": ""Policy"", ""spec"": {""user"": ""dev-user"", ""namespace"": ""*"", ""resource"": ""pods"", ""apiGroup"": ""*""}}
{""kind"": ""Policy"", ""spec"": {""user"": ""dev-user-2"", ""namespace"": ""*"", ""resource"": ""pods"", ""apiGroup"": ""*""}}
{""kind"": ""Policy"", ""spec"": {""group"": ""dev-users"", ""namespace"": ""*"", ""resource"": ""pods"", ""apiGroup"": ""*""}}
{""kind"": ""Policy"", ""spec"": {""user"": ""security-1"", ""namespace"": ""*"", ""resource"": ""csr"", ""apiGroup"": ""*""}} However, changes to a user's permissions require editing the policy file and restarting the Kubernetes API server, making ABAC somewhat cumbersome. This is why many organizations prefer Role-Based Access Control (RBAC). RBAC simplifies management by allowing the creation of roles that bundle specific permissions, which can then be assigned to users or groups. For example, you can create a role for developers that permits application deployments without granting access to modify cluster configurations, while a separate role for security personnel might include permissions to approve certificate signing requests (CSRs). The structure of these RBAC roles is summarized in the following image: In addition to the built-in mechanisms, you can delegate authorization to external tools such as Open Policy Agent (OPA) . With OPA, Kubernetes sends an API call containing user details and the request to an external service, which then decides whether the request should be approved. Kubernetes also provides two simple modes: AlwaysAllow and AlwaysDeny. As their names imply, AlwaysAllow permits all requests without checks, whereas AlwaysDeny rejects all requests. These modes are configured via the authorization mode option when starting the Kubernetes API server. If no option is specified, the server defaults to AlwaysAllow. For example, here is how you configure the AlwaysAllow mode: ExecStart=/usr/local/bin/kube-apiserver \
  --advertise-address=${INTERNAL_IP} \
  --allow-privileged=true \
  --apiserver-count=3 \
  --authorization-mode=AlwaysAllow \
  --bind-address=0.0.0.0 \
  --enable-swagger-ui=true \
  --etcd-cafile=/var/lib/kubernetes/ca.pem \
  --etcd-certfile=/var/lib/kubernetes/apiserver-etcd-client.crt \
  --etcd-keyfile=/var/lib/kubernetes/apiserver-etcd-client.key \
  --etcd-servers=https://127.0.0.1:2379 \
  --event-ttl=1h \
  --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \
  --kubelet-client-certificate=/var/lib/kubernetes/apiserver-etcd-client.crt \
  --kubelet-client-key=/var/lib/kubernetes/apiserver-etcd-client.key \
  --service-node-port-range=30000-32767 \
  --client-ca-file=/var/lib/kubernetes/ca.pem \
  --tls-cert-file=/var/lib/kubernetes/apiserver.crt \
  --tls-private-key-file=/var/lib/kubernetes/apiserver.key \
  -v=2 Alternatively, you can configure multiple authorization modes at once with a comma-separated list. The following configuration enables Node, RBAC, and Webhook modes: ExecStart=/usr/local/bin/kube-apiserver \
  --advertise-address=${INTERNAL_IP} \
  --allow-privileged=true \
  --apiserver-count=3 \
  --authorization-mode=Node,RBAC,Webhook \
  --bind-address=0.0.0.0 \
  --enable-swagger-ui=true \
  --etcd-cafile=/var/lib/kubernetes/ca.pem \
  --etcd-certfile=/var/lib/kubernetes/apiserver-etcd-client.crt \
  --etcd-keyfile=/var/lib/kubernetes/apiserver-etcd-client.key \
  --etcd-servers=https://127.0.0.1:2379 \
  --event-ttl=1h \
  --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \
  --kubelet-client-certificate=/var/lib/kubernetes/apiserver-etcd-client.crt \
  --kubelet-client-key=/var/lib/kubernetes/apiserver-etcd-client.key \
  --service-node-port-range=30000-32767 \
  --client-ca-file=/var/lib/kubernetes/ca.crt \
  --tls-cert-file=/var/lib/kubernetes/apiserver.crt \
  --tls-private-key-file=/var/lib/kubernetes/apiserver.key \
  --v=2 When multiple authorization modes are enabled, each incoming request is processed sequentially in the specified order. For example, the request is first evaluated by the Node authorizer; if the request is either not applicable or denied, it then passes to the next module (such as RBAC). As soon as one module approves the request, the evaluation stops and access is granted. That concludes our lesson on authorization. In upcoming lessons, we will dive deeper into role-based access controls and explore their implementation in Kubernetes. Further Reading Kubernetes Documentation Open Policy Agent Jenkins Continuous Delivery Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Role Based Access Controls,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Security/Role-Based-Access-Controls,"Certified Kubernetes Application Developer - CKAD Security Role Based Access Controls In this lesson, we dive into Kubernetes Role-Based Access Control (RBAC), providing step-by-step instructions on how to create and manage roles and role bindings within a namespace. You'll learn how to define specific permissions for users, bind these permissions to users, and verify their access using kubectl commands. Creating a Role To start, you need to define a Role object. Create a YAML file (e.g., developer-role.yaml) and set the API version to rbac.authorization.k8s.io/v1 and the kind to Role . In this example, we define a role named ""developer"" with permissions that allow developers to manage pods and create configmaps. Each permission rule contains three key sections: API groups, resources, and verbs. For resources in the core API group, leave the API group field blank. apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
rules:
- apiGroups: [""""]
  resources: [""pods""]
  verbs: [""list"", ""get"", ""create"", ""update"", ""delete""]
- apiGroups: [""""]
  resources: [""configmaps""]
  verbs: [""create""] Create the role using the following command: kubectl create -f developer-role.yaml Note Remember that roles in Kubernetes are namespace-specific. Ensure the YAML definition targets the correct namespace if needed. Binding a User to the Role Once the role is defined, you need to link a user to this role by creating a RoleBinding. A RoleBinding associates a user with the specified role within a given namespace. In this example, we'll create a role binding named devuser-developer-binding that assigns the ""developer"" role to the user dev-user in the default namespace. Below is the YAML definition for the role binding (e.g., devuser-developer-binding.yaml): apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: devuser-developer-binding
subjects:
- kind: User
  name: dev-user
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: developer
  apiGroup: rbac.authorization.k8s.io Create the role binding with the command: kubectl create -f devuser-developer-binding.yaml Verifying Roles and Role Bindings After creating the role and role binding, you can verify them using the following kubectl commands: To list roles: kubectl get roles Example output: NAME         AGE
developer    4s To list role bindings: kubectl get rolebindings Example output: NAME                        AGE
devuser-developer-binding   24s To get detailed information about the role, run: kubectl describe role developer Sample output: Name:         developer
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources      Non-Resource URLs  Resource Names  Verbs
  --------       ------------------  ---------------  ----
  configmaps    []                  []               [create]
  pods          []                  []               [get watch list create delete] Similarly, to examine the role binding details, use: kubectl describe rolebinding devuser-developer-binding Expected output: Name:                   devuser-developer-binding
Labels:                 <none>
Annotations:            <none>
Role:
  Kind:                Role
  Name:                developer
Subjects:
  Kind   Name       Namespace
  ----   ----       ---------
  User   dev-user Testing Access Permissions You can test whether a user has access to specific Kubernetes resources using the kubectl auth can-i command: kubectl auth can-i create deployments Expected output: yes And if you test deleting nodes: kubectl auth can-i delete nodes Expected output: no If you need to simulate actions as another user, use the --as flag. Even though the developer role does not have permission to create deployments, it can create pods: kubectl auth can-i create deployments --as dev-user Expected output: no kubectl auth can-i create pods --as dev-user Expected output: yes You can also specify a particular namespace using the --namespace flag if the permissions are scoped accordingly. Granting Access to Specific Resources In some cases, you might want to restrict a user's permissions to specific resources. For instance, if you need to allow a user to manage only two pods named ""blue"" and ""orange"" within a namespace, refine the role rule by including the resourceNames field: apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
rules:
- apiGroups: [""""]
  resources: [""pods""]
  verbs: [""get"", ""create"", ""update""]
  resourceNames: [""blue"", ""orange""] Warning Be cautious when specifying resource names. Only include the exact resources you intend to allow access to, as this will restrict access to other resources of the same type. Conclusion This lesson has demonstrated how to set up and manage RBAC in Kubernetes. By creating roles and role bindings, you can control user permissions precisely within a namespace. For further practice, refer to Kubernetes' official documentation and experiment with real RBAC configurations in your environment. For additional resources, consider these links: Kubernetes Documentation Kubernetes RBAC Concepts Happy coding and secure your Kubernetes clusters with effective RBAC management! Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Solution Admission Controllers,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Security/Solution-Admission-Controllers,"Certified Kubernetes Application Developer - CKAD Security Solution Admission Controllers In this lesson, we will walk through the solutions for the admission controllers lab, explaining each step and providing the necessary commands and configuration snippets. Understanding Admission Controller Functions For the first question, you need to determine which option is NOT a function of admission controllers. The correct answer is ""authenticate users"" because admission controllers operate after authentication has already taken place; they do not handle user authentication. root@controlplane ~  [] Verifying Enabled Admission Controllers The next question asks which admission controller is not enabled by default. To find out, start by listing the pods in the kube-system namespace using the following command: root@controlplane ~ ➜ kubectl get pods -n kube-system
NAME                                         READY   STATUS    RESTARTS   AGE
coredns-74ff55c5b-88v8r                       1/1     Running   0          38m
coredns-74ff55c5b-tng7p                       1/1     Running   0          38m
etcd-controlplane                            1/1     Running   0          38m
kube-apiserver-controlplane                  1/1     Running   0          38m
kube-controller-manager-controlplane         1/1     Running   0          38m
kube-flannel-ds-wzlpx                         1/1     Running   0          38m
kube-proxy-hccb9                             1/1     Running   0          38m
kube-scheduler-controlplane                  1/1     Running   0          38m Next, run this command to check the enabled admission plugins: root@controlplane ~ ➜ kubectl exec -it kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | grep 'enable-admission-pl' The output lists default enabled plugins like NamespaceLifecycle , LimitRanger , ServiceAccount , among others. Notice that ""namespace auto-provision"" is not listed. Also, verifying the enabled webhooks gives you the following output: root@controlplane ~ ➜ kubectl exec -it kube-apiserver-controlplane -n kube-system -- kube-apiserver -h | grep 'enable-admission-plugins'
--enable-admission-plugins strings   admission plugins that should be enabled in addition to default enabled ones (NamespaceLifecycle, LimitRanger, ServiceAccount, TaintNodesByCondition, Priority, DefaultTolerationSeconds, DefaultStorageClass, StorageObjectInUseProtection, PersistentVolumeClaimResize, VolumeClasses, CertificateApproval, CertificateSigning, CertificateSubjectRestrictions, DefaultIngressClass, DefaultTolerations, CertificateExemption, DefaultIngressClass, MutatingAdmissionWebhook, ValidatingAdmissionWebhook, ResourceQuota). Since ""namespace auto-provision"" is missing from the default list, it is the correct answer for the question. Identifying Explicitly Enabled Admission Controllers The following question asks which admission controller is enabled in the cluster but is generally disabled by default. You can inspect the kube-apiserver configuration file located at /etc/kubernetes/manifests/kube-apiserver.yaml to determine the enabled plugins. Use your preferred text editor or run the following search command: root@controlplane ~ ➜ grep 'enable-admission-plugins' /etc/kubernetes/manifests/kube-apiserver.yaml A snippet from the configuration file might appear as follows: apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.49.219.9:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=10.49.219.9
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --insecure-port=0
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt In this configuration, the NodeRestriction admission controller is explicitly enabled, despite normally being disabled. Note By inspecting the kube-apiserver configuration, you can verify which admission controllers are enabled and adjust your settings accordingly. Enabling Namespace Auto Provision The next task is to create an NGINX pod in the ""blue"" namespace. However, the lab specifies that the ""blue"" namespace has not been created yet. Attempting to run the following command: root@controlplane ~ ➜ kubectl run nginx --image nginx --rm results in an error because the exists admission controller rejects requests for non-existent namespaces. To automatically create the namespace during pod creation, enable the NamespaceAutoProvision admission controller. Edit the /etc/kubernetes/manifests/kube-apiserver.yaml file and update the --enable-admission-plugins flag. Insert NamespaceAutoProvision after NodeRestriction (separated by a comma). The updated configuration should look similar to this: apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.49.219.9:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
      - kube-apiserver
      - --advertise-address=10.49.219.9
      - --allow-privileged=true
      - --authorization-mode=Node,RBAC
      - --client-ca-file=/etc/kubernetes/pki/ca.crt
      - --enable-admission-plugins=NodeRestriction,NamespaceAutoProvision
      - --enable-bootstrap-token-auth=true
      - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      - --etcd-servers=https://127.0.0.1:2379
      - --insecure-port=0
      - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt After saving and allowing the API server to restart, create the NGINX pod in the blue namespace: root@controlplane ~ ➜ kubectl run nginx --image nginx -n blue
pod/nginx created

root@controlplane ~ ➜ kubectl get ns
NAME              STATUS   AGE
blue              Active   7s
default           Active   50m
kube-node-lease   Active   50m
kube-public       Active   50m
kube-system       Active   50m As you can see, the blue namespace is automatically created due to the enabled NamespaceAutoProvision admission controller. Disabling the Default Storage Class Controller The final task is to disable the default storage class admission controller. To do this, update the /etc/kubernetes/manifests/kube-apiserver.yaml file by adding the --disable-admission-plugins flag on a new line immediately after the enabled plugins. For example: apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.49.219.9:6443
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
      - kube-apiserver
      - --advertise-address=10.49.219.9
      - --allow-privileged=true
      - --authorization-mode=Node,RBAC
      - --client-ca-file=/etc/kubernetes/pki/ca.crt
      - --enable-admission-plugins=NodeRestriction,NamespaceAutoProvision
      - --disable-admission-plugins=DefaultStorageClass
      - --enable-bootstrap-token-auth=true
      - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
      - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
      - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
      - --etcd-servers=https://127.0.0.1:2379
      - --insecure-port=0
      - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
      - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
      - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt Once the API server restarts, validate the changes by inspecting the running process with a command like: ps -ef | grep kube-apiserver This command will list both the enabled and disabled admission plugins. Completion With these steps, you have successfully completed the admission controllers lab by understanding plugin functions, verifying defaults, enabling automatic namespace creation, and disabling unwanted admission controllers. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solution Network Policies optional,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Services-Networking/Solution-Network-Policies-optional,"Certified Kubernetes Application Developer - CKAD Services Networking Solution Network Policies optional In this lesson, we explore a practice test on network policies in a Kubernetes cluster. The exercise involves inspecting deployed web applications, services, and network policies to understand how traffic is controlled within the environment. The exercise begins with the question: “How many network policies do you see in the environment?” In our setup, several web applications and services have been deployed, and a specific network policy is defined to control traffic to one of the pods. Let's walk through the environment. Inspecting the Pods and Services Start by checking the list of running pods with: root@controlplane:~# k get pods
NAME       READY   STATUS    RESTARTS   AGE
external   1/1     Running   0          2m20s
internal   1/1     Running   0          2m19s
mysql      1/1     Running   0          2m19s
payroll    1/1     Running   0          2m19s Next, view the services available in the cluster: root@controlplane:~# k get service
NAME                TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
db-service          ClusterIP   10.109.89.42    <none>        3306/TCP         2m42s
external-service    NodePort    10.108.170.44   <none>        8080:30080/TCP   2m42s
internal-service    NodePort    10.98.11.243    <none>        8080:30082/TCP   2m42s
kubernetes          ClusterIP   10.96.0.1       <none>        443/TCP          39m
payroll-service     NodePort    10.110.165.31   <none>        8080:30083/TCP   2m42s Note Here, notice that the payroll service is exposed on port 8080, the database service on port 3306, and both the external and internal services are available on port 8080. Checking the Network Policies When you list network policies using the following command: root@controlplane:~# k get networkpolicies
error: the server doesn't have a resource type ""networkpolicies"" You see an error because the short alias is not recognized. Using the short form, however, produces the expected result: root@controlplane:~# k get netpol
NAME            POD-SELECTOR   AGE
payroll-policy  name=payroll   3m31s This output confirms that there is one network policy named payroll-policy , which applies to pods with the label name=payroll . Reviewing the Network Policy Details Examine the details of the network policy with: root@controlplane:~# k describe netpol payroll-policy
Name:         payroll-policy
Namespace:    default
Created on:   2022-04-18 20:35:54 +0000 UTC
Labels:       <none>
Annotations:  <none>
Spec:
  PodSelector:     name=payroll
  Allowing ingress traffic:
    To Port: 8080/TCP
    From:
      PodSelector: name=internal
  Not affecting egress traffic
  Policy Types: Ingress This policy allows ingress traffic on TCP port 8080 to the payroll pod only if the traffic originates from pods with the label name=internal . Since no egress rules are defined, the payroll pod continues to allow all outgoing traffic. Understanding the Impact of the Network Policy By default, all pods allow both ingress and egress traffic. Once a network policy is applied, only the traffic permitted by the policy is allowed. In this scenario: Only ingress traffic from the internal pod on TCP port 8080 is allowed to reach the payroll pod. All egress traffic from the payroll pod remains unrestricted. Pods or sources without the label name=internal cannot access the payroll pod on port 8080. Connectivity tests revealed that: The internal-facing application successfully connected to the payroll service on port 8080. The external-facing application timed out when attempting to access the payroll service, confirming that the policy is working as intended. Creating a Custom Network Policy The lab exercise also requires creating a new network policy to allow traffic exclusively from the internal application to both the payroll and database (MySQL) services. This policy will restrict egress traffic from the internal pod so that it only communicates with the payroll pod on TCP port 8080 and the MySQL pod on TCP port 3306. Below is a sample YAML specification for this custom network policy. Save it as internal-policy.yaml : apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internal-policy
  namespace: default
spec:
  podSelector:
    matchLabels:
      name: internal
  policyTypes:
    - Egress
  egress:
    - to:
        podSelector:
          matchLabels:
            name: payroll
      ports:
        - protocol: TCP
          port: 8080
    - to:
        podSelector:
          matchLabels:
            name: mysql
      ports:
        - protocol: TCP
          port: 3306 Key Points This policy selects the internal pod (using the label name: internal ) and applies an egress rule that allows traffic: To the payroll pod on TCP port 8080. To the MySQL pod on TCP port 3306. To create the policy, run: root@controlplane:~# k create -f internal-policy.yaml
networkpolicy.networking.k8s.io/internal-policy created Verify the new policy details with: root@controlplane:~# k describe netpol internal-policy
Name:          internal-policy
Namespace:     default
Created on:    2022-04-18 20:53:13 +0000 UTC
Labels:        <none>
Annotations:   <none>
Spec:
  PodSelector:     name=internal
  Not affecting ingress traffic
  Allowing egress traffic:
    To:
      PodSelector: name=payroll
      Ports: 8080/TCP
    To:
      PodSelector: name=mysql
      Ports: 3306/TCP
  Policy Types: Egress This confirms that the internal pod is now restricted to sending traffic only to the payroll service on port 8080 and the MySQL service on port 3306. Final Connectivity Testing After applying these policies: The internal-facing application should be able to access both the payroll and database services. The external-facing application or any other source will be unable to access the payroll pod on port 8080. This completes our lab exercise on network policies, demonstrating how to control ingress and egress traffic within a Kubernetes environment effectively. For further information on Kubernetes network policies, consider exploring the Kubernetes Documentation . Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Authentication Authorization and Admission Control,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Security/Authentication-Authorization-and-Admission-Control,"Certified Kubernetes Application Developer - CKAD Security Authentication Authorization and Admission Control Welcome to this comprehensive lesson on Kubernetes security. In this module, we dive into the core security primitives essential for production-grade Kubernetes deployments. We'll explore how Kubernetes secures its API, manages user access, and controls inter-component communication. Note Before diving deeper, ensure your infrastructure hosts are secure by disabling root access, disabling password-based authentication, and exclusively using SSH key-based authentication. Securing Kubernetes Cluster Hosts The foundation of a secure Kubernetes environment is protecting the underlying physical or virtual infrastructure. Ensure that your cluster hosts adhere to industry security best practices: Disable root access. Turn off password-based authentication. Enable SSH key-based authentication. Remember, if the base infrastructure is compromised, the entire Kubernetes cluster is vulnerable. Protecting the Kube API Server At the heart of Kubernetes lies the Kube API Server, which serves as the gateway for all operations—be it through kubectl or direct API calls. As such, strict control over who can access the cluster and what actions they can perform is vital. Authentication Methods Authentication determines the identity of a user or a process accessing the API server. Kubernetes supports multiple authentication mechanisms, including: User IDs and Passwords: Stored in static files. Tokens: Issued for API access. Certificates: Securing communications. External Authentication Providers: For example, LDAP. Service Accounts: For machine-to-machine interactions. Authorization Strategies Once authenticated, authorization dictates what actions users are permitted to perform. Kubernetes primarily employs Role-Based Access Control (RBAC) to manage permissions. Additional strategies include: Attribute-Based Access Control (ABAC) Node Authorization Webhook Modes Securing Inter-Component Communications Every component within Kubernetes communicates over a network, sometimes traversing untrusted networks. To ensure data security, Kubernetes uses TLS encryption for communications among critical components such as: Kube API Server etcd Cluster Kube Controller Manager Kube Scheduler Kubelet and Kube Proxy on worker nodes Detailed procedures for configuring and managing TLS certificates are covered in the course content. Managing Pod Communications with Network Policies By default, all pods within a Kubernetes cluster can communicate freely, which may not be desirable for every workload or application. Network policies allow administrators to define rules that restrict pod-to-pod interactions, thereby reducing potential attack vectors and isolating sensitive components. This high-level overview provides a solid foundation on Kubernetes security primitives. Each of these topics is explored in greater depth throughout the course, ensuring you have the knowledge to build a robust and secure Kubernetes environment. Let’s move forward into the detailed modules. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solution Cluster Roles,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Security/Solution-Cluster-Roles,"Certified Kubernetes Application Developer - CKAD Security Solution Cluster Roles In this lesson, we will explore the concepts of cluster roles and cluster role bindings. We will inspect the existing cluster roles and their bindings, count them, and review their permissions before creating new roles and bindings for a new team member named Michelle. Inspecting Existing Cluster Roles Begin by examining the currently defined cluster roles in your cluster. These roles include cluster-admin , system-specific roles, and others. Run the following command to list them: system:volume-scheduler
system:certificates.k8s.io:legacy-unknown-approver
system:certificates.k8s.io:kubelet-serving-approver
system:certificates.k8s.io:kube-apiserver-client-approver
system:certificates.k8s.io:kube-apiserver-client-kubelet-approver
system:service-account-issuer-discovery
system:node-proxier
system:kube-scheduler
system:controller:attacheddetach-controller
system:controller:clusteroverlay-aggregation-controller
system:controller:cronjob-controller
system:controller:daemon-set-controller
system:controller:deployment-controller
system:controller:endpoint-controller
system:controller:endpointslice-mirroring-controller
system:controller:expand-controller
system:controller:ephemeral-volume-controller
system:controller:generic-garbage-collector
system:controller:job-controller
system:controller:namespace-controller
system:controller:node-controller
system:controller:persistent-volume-binder
system:controller:garbage-collector
system:controller:replicaset-controller
system:controller:replication-controller
system:controller:resourcequota-controller
system:controller:service-account-controller
system:controller:service-controller
system:controller:statefulset-controller
system:controller:ttl-controller
system:controller:vertical-pod-autoscaler
system:controller:volume-protection-controller
system:controller:ttl-after-finished-controller
system:controller:root-ca-cert-publisher
k3s-cloud-controller-manager You can count these roles manually by piping the output to wc -l . For example: k get clusterroles --no-headers | wc -l On one system, this command produced an output of 6, but keep in mind that this example shows a truncated list. In another instance, a total of 69 roles might be expected using a different counting method. Note Cluster roles are cluster-scoped and are not limited to any namespace. Inspecting Cluster Role Bindings Next, verify the cluster role bindings by counting them with the following command: k get clusterrolebindings --no-headers | wc -l This command returned 54 on the system in question, indicating that there are 54 cluster role bindings. To illustrate further, here is an excerpt displaying some cluster roles and their creation times: k get clusterroles
NAME                                                                  CREATED AT
cluster-admin                                                       2022-04-18T00:01:23Z
system:discovery                                                   2022-04-18T00:01:23Z
system:monitoring                                                  2022-04-18T00:01:23Z
system:basic-user                                                  2022-04-18T00:01:23Z
system:public-info-viewer                                          2022-04-18T00:01:23Z
system:aggregate-to-admin                                          2022-04-18T00:01:23Z
system:aggregate-to-edit                                           2022-04-18T00:01:23Z
system:aggregate-to-view                                           2022-04-18T00:01:23Z
system:heapster                                                    2022-04-18T00:01:23Z
system:node                                                       2022-04-18T00:01:23Z
... Note Both cluster roles and cluster role bindings are applied across the entire cluster and are not namespace-specific. Reviewing Cluster Admin Role Bindings The cluster-admin role represents the highest permission level and is bound to specific user groups via a ClusterRoleBinding. To identify the binding for the cluster-admin role, run: k get clusterrolebindings | grep cluster-admin The output may appear as follows: cluster-admin                    12m   ClusterRole/cluster-admin
helm-kube-system-traefik         12m   ClusterRole/cluster-admin
helm-kube-system-traefik-crd     12m   ClusterRole/cluster-admin You can then inspect the details of the cluster-admin binding with: k describe clusterrolebindings cluster-admin This command produces output similar to: Name:         cluster-admin
Labels:       kubernetes.io/bootstraping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
Role:
  Kind:  ClusterRole
  Name:  cluster-admin
Subjects:
  Kind  Name                 Namespace
  ----- ------------------- -----------
  Group system:masters This confirms that the cluster-admin role is bound by default to the system:masters group, thereby granting all possible operations (denoted by [*] for all actions on all resources). Warning Exercise caution when modifying cluster roles as they affect permissions across the entire cluster. Granting Node Access to a New User (Michelle) Michelle, a new team member, requires access to manage nodes. Initially, when Michelle runs: k get nodes --as michelle she encounters the following error: Error from server (Forbidden): nodes is forbidden: User ""michelle"" cannot list resource ""nodes"" in API group """" at the cluster scope To resolve this issue, follow these steps: Create the Cluster Role for Node Access Create a cluster role named michelle-role that allows Michelle to get, list, and watch nodes: k create clusterrole michelle-role --verb=get,list,watch --resource=nodes Verify the new role by describing it: k describe clusterrole michelle-role Expected output: Name:         michelle-role
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources   Non-Resource URLs   Resource Names   Verbs
  ---------   -----------------   ---------------  ---------
  nodes       []                  []               [get list watch] Bind the Cluster Role to Michelle Bind the role to Michelle’s user account with: k create clusterrolebinding michelle-role-binding --clusterrole=michelle-role --user=michelle Verify this binding with: k describe clusterrolebinding michelle-role-binding Expected output: Name:         michelle-role-binding
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  ClusterRole
  Name:  michelle-role
Subjects:
  Kind   Name      Namespace
  ----   ----      ---------
  User   michelle Finally, test Michelle’s access again: k get nodes --as michelle A successful command execution will display a list of nodes similar to: NAME         STATUS   ROLES                 AGE   VERSION
controlplane Ready    control-plane,master  16m   v1.23.3+k3s1 Expanding Michelle's Responsibilities to Storage Michelle’s roles are expanding to include storage management. To grant her access to storage-related resources, first verify the available API resources by running: kubectl api-resources This command lists resources along with their short names, API versions, and scope. For storage management, the key resources are persistent volumes and storage classes. Create a Cluster Role for Storage Administration Create a cluster role named storage-admin that allows listing, creating, getting, and watching persistent volumes and storage classes: k create clusterrole storage-admin --resource=persistentvolumes,storageclasses --verb=list,create,get,watch Verify the role in YAML format using: k get clusterrole storage-admin -o yaml An expected YAML output is as follows: apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  creationTimestamp: ""2022-04-18T00:20:48Z""
  name: storage-admin
  resourceVersion: ""921""
  uid: e0ee52a7-b32c-4a42-bb7a-a783b040cd4e
rules:
- apiGroups:
  - """"
  resources:
  - persistentvolumes
  verbs:
  - list
  - create
  - get
  - watch
- apiGroups:
  - storage.k8s.io
  resources:
  - storageclasses
  verbs:
  - list
  - create
  - get
  - watch Bind the Storage Admin Role to Michelle Bind the storage-admin role to Michelle with the following command: k create clusterrolebinding michelle-storage-admin --user=michelle --clusterrole=storage-admin Verify this binding: k describe clusterrolebinding michelle-storage-admin Expected output: Name:         michelle-storage-admin
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  ClusterRole
  Name:  storage-admin
Subjects:
  Kind   Name      Namespace
  ----   ----      ---------
  User   michelle Michelle should now have the necessary permissions to manage storage resources such as persistent volumes and storage classes. This concludes the lesson on inspecting, creating, and binding cluster roles and cluster role bindings. By following these steps, you learned how to review existing roles, grant specific permissions to a user, and expand those permissions as job responsibilities change. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solution Role Based Access Controls,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Security/Solution-Role-Based-Access-Controls,"Certified Kubernetes Application Developer - CKAD Security Solution Role Based Access Controls In this lab, we explore role-based access controls (RBAC) in Kubernetes. The tutorial covers inspecting the API server configuration, reviewing roles and permissions, and creating the necessary roles and bindings for a development user. Follow along to gain hands-on experience with securing your Kubernetes cluster. ────────────────────────────── Inspecting the API Server Configuration Start by inspecting the kube-apiserver manifest to identify the configured authorization modes. In the manifest snippet below, note the use of ""Node,RBAC"" for the authorization mode: creationTimestamp: null
labels:
  component: kube-apiserver
  tier: control-plane
name: kube-apiserver
namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=10.48.174.6
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --requestheader-allowed-names=front-proxy-client
    - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group
    - --requestheader-username-headers=X-Remote-User
    - --secure-port=6443
    - --service-account-issuer=https://kubernetes.default.svc.cluster.local
    - --service-account-key-file=/etc/kubernetes/pki/sa.pub
    - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key
    - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt
    - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
  image: k8s.gcr.io/kube-apiserver:v1.23.0
  imagePullPolicy: IfNotPresent
  livenessProbe: An alternative verification method is to inspect the running processes on the control plane. For instance, execute the following command: root@controlplane ~ ps aux | grep authorization
root      3403  0.0  0.0  830588 115420 ?        Ssl  22:54   0:55 kube-controller-manager --allocate-node-authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --authorization-kubeconfig=/etc/kubernetes/controller-manager.conf --bind-address=127.0.0.1 --client-ca-file=/etc/kubernetes/pki/ca.crt --cluster-cidr=10.244.0.0/16 --kubelet-client-certificate=/etc/kubernetes/pki/ca.key --controllers=*,bootstrapsigner,tokens --kubelet-client-key=/etc/kubernetes/pki/front-proxy-client.key --service-account-privileged=false --service-cluster-ip-range=10.96.0.0/12 --use-service-account-credentials=true
root      3614  0.0  0.0  759136  55292 ?        Ssl  22:55   0:10 kube-scheduler --authentication-kubeconfig=/etc/kubernetes/scheduler.conf --bind-address=127.0.0.1 --kubeconfig=/etc/kubernetes/scheduler.conf --leader-elect=true
root      3630  0.0  0.1 111896  316984 ?        S    22:55   2:07 kube-apiserver --advertise-address=10.4.8.174.6 --allow-privileged=true --authorization-mode=Node,RBAC --client-ca-file=/etc/kubernetes/pki/ca.crt --enable-admission-plugins=NodeRestriction --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key --requestheader-allowed-names=X-Remote-Group --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-client-ca.crt --requestheader-headers-prefix=X-Remote-Extra --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6443 --service-account-issuer=https://kubernetes.default.svc.cluster.local --service-account-key-file=/etc/kubernetes/pki/sa.key --service-cluster-ip-range=10.96.0.0/12 --tls-cert-file=/etc/kubernetes/pki/apiserver.crt --tls-private-key-file=/etc/kubernetes/pki/apiserver.key
root      25283  0.0  0.0  13444  1068 pts/0    S+   23:40   0:00 grep --color=auto authorization The output confirms that the kube-apiserver is running with the ""Node,RBAC"" authorization mode. ────────────────────────────── Reviewing Roles in the Cluster Begin by identifying the existing roles within the default namespace. The output below shows that no roles exist in that namespace: apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubectl.kubernetes.io/kube-apiserver.advertise-address.endpoint: 10.48.174.6:6443
  creationTimestamp: null
  labels:
    component: kube-apiserver
    tier: control-plane
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --advertise-address=10.48.174.6
    - --allow-privileged=true
    - --authorization-mode=Node,RBAC
    - --client-ca-file=/etc/kubernetes/pki/ca.crt
    - --enable-admission-plugins=NodeRestriction
    - --enable-bootstrap-token-auth=true
    - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt
    - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt
    - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key
    - --etcd-servers=https://127.0.0.1:2379
    - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
    - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
    - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
    - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
    - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
    - --request-header-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
    - --requestheader-extra-headers-prefix=X-Remote-Extra-
    - --requestheader-group-headers=X-Remote-Group To count the roles across all namespaces, use this command: root@controlplane ~ k get roles -A --no-headers | wc -l
12 There are 12 roles configured cluster-wide. ────────────────────────────── Examining the kube-proxy Role Next, examine the permissions assigned to the kube-proxy role in the kube-system namespace. Run the following command to display its details: root@controlplane ~ ✗ k describe role kube-proxy -n kube-system
Name:         kube-proxy
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources           Non-Resource URLs  Resource Names  Verbs
  ---------           -----------------  ---------------  -----
  configmaps          []                                   [get] From the output, the kube-proxy role is limited to performing the ""get"" action on ConfigMap resources only. It cannot delete or update ConfigMaps. ────────────────────────────── Checking Role Bindings for kube-proxy Identify which service account is bound to the kube-proxy role by examining the role bindings in the kube-system namespace: root@controlplane ~ ✗ k describe role kube-proxy -n kube-system
Name:         kube-proxy
Labels:       <none>
Annotations:  <none>
PolicyRules:
  Resources      Non-Resource URLs  Resource Names  Verbs
  -----------    -----------------  --------------  -----
  configmaps     []                  [kube-proxy]    [get] List the role bindings in the kube-system namespace: root@controlplane ~ ✗ k get rolebindings -n kube-system
NAME                                              AGE
kube-proxy                                        48m
kubeadm:kubelet-config-1.23                       48m
kubeadm:nodes-kubeadm-config                      48m
system:extension-apiserver-authentication-reader  48m
system:leader-locking-kube-controller-manager     48m
system:leader-locking-kube-scheduler               48m
system:controller:bootstrap-signer               48m
system:controller:cloud-provider                  48m
system:controller:token-cleaner                   48m Then, inspect the kube-proxy role binding: root@controlplane ~ ◦ k describe rolebindings kube-proxy -n kube-system
Name:         kube-proxy
Labels:       <none>
Annotations:  <none>
Role:
  Kind: Role
  Name: kube-proxy
Subjects:
  Kind   Name
  ----   ----
  Group  system:bootstrappers:kubeadm:default-node-token This confirms that the kube-proxy role is associated with the group ""system:bootstrappers:kubeadm:default-node-token"". ────────────────────────────── Inspecting the kubeconfig and User Permissions Review the kubeconfig file to verify the configured users on the cluster: root@controlplane ~ ⮀ kubectl config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://controlplane:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: {}
users:
- name: dev-user
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
- name: kubernetes-admin
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED Test the permissions for the ""dev-user"" by listing pods in the default namespace: root@controlplane ~ ⮀ k get pods --as dev-user
Error from server (Forbidden): pods is forbidden: User ""dev-user"" cannot list resource ""pods"" in API group """" in the namespace ""default"" This error confirms that the ""dev-user"" currently lacks permission to list pods in the default namespace. ────────────────────────────── Creating Roles and Role Bindings for the dev-user Before proceeding, review how to create a role in Kubernetes. The following command creates a role named ""developer"" granting list, create, and delete permissions on pods. Notice that the correct flag is --resource (singular): root@controlplane ~ ➜ k create role developer --verb=list,create,delete --resource=pods
role.rbac.authorization.k8s.io/developer created You can verify the created role by describing it: root@controlplane ~ ➜ k describe role developer
Name:         developer
Labels:       <none>
Annotations:  <none>
PolicyRule:
  Resources  Non-Resource URLs  Resource Names  Verbs
  ---------  -----------------  --------------  --------------
  pods       []                 []              [list create delete] Next, assign the ""developer"" role to ""dev-user"" by creating a role binding: root@controlplane ~ ⟩ k create rolebinding dev-user-binding --role=developer --user=dev-user
rolebinding.rbac.authorization.k8s.io/dev-user-binding created Verify the role binding: root@controlplane ~ ⟩ k describe rolebinding dev-user-binding
Name:         dev-user-binding
Labels:       <none>
Annotations:  <none>
Role:
  Kind:  Role
  Name:  developer
Subjects:
  Kind   Name
  ----   ----
  User   dev-user ────────────────────────────── Troubleshooting Access in the Blue Namespace Suppose the dev-user tries to retrieve details of a pod named ""dark-blue-app"" in the blue namespace: k --as dev-user get pod dark-blue-app -n blue The error received is: Error from server (Forbidden): pods ""dark-blue-app"" is forbidden: User ""dev-user"" cannot get resource ""pods"" in API group """" in the namespace ""blue"" Upon inspection, the blue namespace has an existing ""developer"" role and the ""dev-user-binding"" role binding: root@controlplane ~ ✗ k get roles -n blue
NAME        CREATED AT
developer   2022-04-17T23:25:04Z

root@controlplane ~ ✗ k get rolebindings -n blue
NAME              ROLE             AGE
dev-user-binding  Role/developer   24m Currently, the ""developer"" role in the blue namespace grants permission only for the pod ""dark-blue-app."" To provide the dev-user with the ability to manage that pod and to create new deployments, update the role to include rules for handling deployments in the ""apps"" API group. Below is an example of the updated role: apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: ""2022-04-17T23:25:04Z""
  name: developer
  namespace: blue
  resourceVersion: ""4534""
  uid: b1424ab8-9776-43b0-bddf-433dfff73c1e
rules:
- apiGroups:
  - """"
  resourceNames:
  - dark-blue-app
  resources:
  - pods
  verbs:
  - get
  - watch
  - create
  - delete
- apiGroups:
  - ""apps""
  resources:
  - deployments
  verbs:
  - get
  - watch
  - create
  - delete Save the changes, then test the configuration by creating a deployment as the dev-user: root@controlplane ~ ⟫ k --as dev-user create deployment nginx --image=nginx -n blue
deployment.apps/nginx created The successful creation of the deployment confirms that the necessary permissions have been granted. Note Always verify RBAC changes by testing user permissions in the designated namespace to ensure that the intended access is provided while maintaining security. ────────────────────────────── Conclusion In this lab, you have: Inspected the kube-apiserver configuration to confirm the use of Node,RBAC authorization mode. Reviewed the roles across namespaces and counted the total number of roles. Examined the kube-proxy role’s permissions and its corresponding role binding. Inspected the kubeconfig to determine that the dev-user initially lacked permission to list pods. Created a role and role binding for the dev-user to grant appropriate pod access in the default namespace. Troubleshooted a permission issue in the blue namespace by updating the ""developer"" role to allow managing deployments. This concludes the Kubernetes RBAC lab. For more details on Kubernetes security and RBAC best practices, refer to the Kubernetes Documentation . Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Cluster Roles,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Security/Cluster-Roles,"Certified Kubernetes Application Developer - CKAD Security Cluster Roles In earlier sections, we discussed namespaced resources and how roles and role bindings work within a specific namespace. By default, if you don't explicitly specify a namespace, these resources are created in the default namespace. Namespaced resources include pods, replica sets, deployments, services, secrets, and more. In contrast, some Kubernetes objects such as nodes, persistent volumes, certificate signing requests, and namespaces are classified as cluster-scoped resources, meaning they exist across the entire cluster rather than within a single namespace. To inspect the full list of available resources, you can run the following commands in your terminal: kubectl api-resources --namespaced=true kubectl api-resources --namespaced=false When managing access for namespaced resources, roles and role bindings are typically sufficient. However, for cluster-scoped resources such as nodes or persistent volumes, you need to configure cluster roles and cluster role bindings. For example, a cluster role can be created for a cluster administrator to view, create, or delete nodes, while a storage administrator might require a cluster role to manage persistent volumes and persistent volume claims. To define a cluster role, create a ClusterRole definition file that specifies the rules for the resources you wish to manage. Consider the following example, which grants permissions for managing nodes: apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-administrator
rules:
- apiGroups: [""""]
  resources: [""nodes""]
  verbs: [""list"", ""get"", ""create"", ""delete""]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-admin-role-binding
subjects:
- kind: User
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-administrator
  apiGroup: rbac.authorization.k8s.io This YAML file first defines the cluster role ""cluster-administrator"" with specific permissions for nodes. It then creates a ClusterRoleBinding that ties this role to the ""cluster-admin"" user. To apply these configurations to your cluster, run: kubectl create -f <filename>.yaml Important Note While cluster roles and bindings are mainly intended for cluster-scoped resources, they can also be used for namespaced resources. When a cluster role is applied to namespaced resources, the permissions are effective across all namespaces, unlike a namespaced role which restricts access to a specific namespace. Kubernetes automatically generates several cluster roles during the cluster setup. In subsequent sections, we will dive deeper into these default roles and how you can leverage them. Good luck with your lesson! Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Validating and Mutating Admission Controllers,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Security/Validating-and-Mutating-Admission-Controllers,"Certified Kubernetes Application Developer - CKAD Security Validating and Mutating Admission Controllers In this lesson, we explore the different types of Admission Controllers in Kubernetes and demonstrate how to configure custom controllers. We cover both validating and mutating admission controllers, explaining their functions and how Kubernetes processes them during the request lifecycle. Overview of Admission Controllers Admission Controllers are plugins that intercept requests to the Kubernetes API server before persisting an object. They are categorized into two main types: Validating Admission Controllers: These controllers examine incoming requests and reject them if they do not conform to specified policies. For example, a namespace existence validator checks if the requested namespace exists; if it doesn't, the request is rejected. Mutating Admission Controllers: These controllers modify incoming requests prior to their persistence. A common example is the default storage class admission controller. When you create a PersistentVolumeClaim (PVC) without specifying a storage class, this controller automatically adds the default storage class to the request. For instance, consider the following PVC creation request: apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi This request undergoes authentication and authorization, then passes through the admission controllers. Because no storage class is specified, the default storage class admission controller mutates the request. When the PVC is created, inspecting it reveals the storage class added: apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 500Mi
  storageClassName: default You can view the details of the PVC with: kubectl describe pvc myclaim
Name:           myclaim
Namespace:      default
StorageClass:   default
Status:         Pending
Volume:         <none>
Labels:         <none>
Annotations:    <none> Note Mutating controllers adjust the request (by adding configurations like the storage class), while validating controllers strictly allow or deny requests based on predefined criteria. In many cases, mutating controllers are invoked before validating ones to ensure that any modifications are considered during validation. If any admission controller in the chain rejects the request, Kubernetes halts further processing and returns an error message to the user. External Admission Controllers: Webhooks Kubernetes extends its built-in admission controllers with external admission controllers through webhooks. There are two primary webhook types: Mutating Admission Webhook Validating Admission Webhook These webhooks allow you to deploy a server—either inside or outside your Kubernetes cluster—that implements custom logic for modifying or validating requests. After a request clears all built-in controllers, it is sent as a JSON-formatted admission review object to the webhook server. The server responds with an admission review object indicating whether the request is allowed and, if applicable, provides JSON patches to mutate the request. For example, a validating webhook admission review request may look like: {
  ""apiVersion"": ""admission.k8s.io/v1"",
  ""kind"": ""AdmissionReview"",
  ""request"": {
    ""uid"": ""705ab4f5-6393-11e8-b7cc-42010aa80002"",
    ""kind"": { ""group"": ""autoscaling"", ""version"": ""v1"", ""kind"": ""Scale"" },
    ""resource"": { ""group"": ""apps"", ""version"": ""v1"", ""resource"": ""deployments"" },
    ""subResource"": ""scale"",
    ""requestKind"": { ""group"": ""autoscaling"", ""version"": ""v1"", ""kind"": ""Scale"" },
    ""requestResource"": { ""group"": ""apps"", ""version"": ""v1"", ""resource"": ""deployments"" }
  }
} And the corresponding webhook response approving the request would be: {
  ""apiVersion"": ""admission.k8s.io/v1"",
  ""kind"": ""AdmissionReview"",
  ""request"": {
    ""uid"": ""705ab4f5-6393-11e8-b7cc-42010aa80002"",
    ""kind"": {""group"": ""autoscaling"", ""version"": ""v1"", ""kind"": ""Scale""},
    ""resource"": {""group"": ""apps"", ""version"": ""v1"", ""resource"": ""deployments""},
    ""subResource"": ""scale"",
    ""requestKind"": {""group"": ""autoscaling"", ""version"": ""v1"", ""kind"": ""Scale""},
    ""requestResource"": {""group"": ""apps"", ""version"": ""v1"", ""resource"": ""deployments""}
  },
  ""response"": {
    ""uid"": ""<value_from request.uid>"",
    ""allowed"": true
  }
} Deploying an Admission Webhook Server To set up your own admission webhook, follow these general steps: Deploy your webhook server containing custom logic. Configure Kubernetes to integrate with your webhook by creating the appropriate webhook configuration object. Deploying the Webhook Server You can implement a webhook server in any programming language as long as it supports the mutate and validate APIs and returns the correct JSON response. Below is a simplified example of a webhook server written in Go. package main

import (
	""encoding/json""
	""flag""
	""fmt""
	""io/ioutil""
	""net/http""

	""k8s.io/api/admission/v1beta1""
	metav1 ""k8s.io/apimachinery/pkg/apis/meta/v1""
	""k8s.io/klog""
)

// toAdmissionResponse is a helper function to create an AdmissionResponse with an embedded error.
func toAdmissionResponse(err error) v1beta1.AdmissionResponse {
	return v1beta1.AdmissionResponse{
		Result: &metav1.Status{
			Message: err.Error(),
		},
	}
}

// admitFunc defines the function type for validators and mutators.
type admitFunc func(v1beta1.AdmissionReview) v1beta1.AdmissionResponse

// serve handles the HTTP part of a request before passing it to the admit function.
func serve(w http.ResponseWriter, r *http.Request, admit admitFunc) {
	var body []byte
	if r.Body != nil {
		data, err := ioutil.ReadAll(r.Body)
		if err == nil {
			body = data
		}
	}
	// Further processing of the admission review would occur here.
} You can implement a similar server in other languages. Below is pseudocode for a sample webhook server in Python that handles both validation and mutation: from flask import Flask, request, jsonify
import base64
import json

app = Flask(__name__)

@app.route(""/validate"", methods=[""POST""])
def validate():
    object_name = request.json[""object""][""metadata""][""name""]
    user_name = request.json[""request""][""userInfo""][""name""]
    allowed = True
    message = """"
    if object_name == user_name:
        message = ""You can't create objects with your own name.""
        allowed = False
    return jsonify({
        ""response"": {
            ""allowed"": allowed,
            ""uid"": request.json[""request""][""uid""],
            ""status"": {""message"": message},
        }
    })

@app.route(""/mutate"", methods=[""POST""])
def mutate():
    user_name = request.json[""request""][""userInfo""][""name""]
    patch = [{""op"": ""add"", ""path"": ""/metadata/labels/users"", ""value"": user_name}]
    # The patch must be base64 encoded before sending in the response.
    encoded_patch = base64.b64encode(json.dumps(patch).encode()).decode()
    return jsonify({
        ""response"": {
            ""allowed"": True,
            ""uid"": request.json[""request""][""uid""],
            ""patch"": encoded_patch,
            ""patchType"": ""JSONPatch"",
        }
    })

if __name__ == ""__main__"":
    app.run(host=""0.0.0.0"", port=443, ssl_context=(""path/to/tls.crt"", ""path/to/tls.key"")) Deployment Note In an exam or production environment, you are not expected to write the full webhook implementation code. The focus should be on understanding how admission webhook servers function and how they integrate with Kubernetes. Hosting and Configuring the Webhook Once your webhook server is ready, you can host it as a standalone server or containerize it and deploy it as a Deployment within your Kubernetes cluster. If you opt to deploy within the cluster, remember to create a corresponding Service so that the API server can communicate with your webhook. Next, configure your Kubernetes cluster by creating a webhook configuration object. For example, to direct pod creation requests to your validating webhook service, you could use the following configuration: apiVersion: admissionregistration.k8s.io/v1
kind: ValidatingWebhookConfiguration
metadata:
  name: ""pod-policy.example.com""
webhooks:
- name: ""pod-policy.example.com""
  clientConfig:
    service:
      namespace: ""webhook-namespace""
      name: ""webhook-service""
    caBundle: ""Ci0tLS0tQk.....tLS0K""
  rules:
  - apiGroups: [""""]
    apiVersions: [""v1""]
    operations: [""CREATE""]
    resources: [""pods""]
    scope: ""Namespaced"" For a mutating webhook, create a MutatingWebhookConfiguration object instead. In both cases, ensure that TLS is properly configured between the API server and your webhook server by generating the required certificates and providing the CA bundle in the configuration. With this configuration, every pod creation request triggers a call to your webhook service. Based on the response, the request will either be allowed or rejected. That concludes this lesson on Validating and Mutating Admission Controllers. Practice these concepts with hands-on exercises, and stay tuned for more advanced topics in the upcoming lessons. For further reading, check out the following resources: Kubernetes Documentation Admission Controllers Overview Webhook Concepts Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Solution Validating and Mutating Admission Controllers,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Security/Solution-Validating-and-Mutating-Admission-Controllers,"Certified Kubernetes Application Developer - CKAD Security Solution Validating and Mutating Admission Controllers In this lesson, we walk through the lab on validating and mutating admission controllers. You'll gain hands-on experience with namespace creation, TLS secret management, deploying webhook servers, and testing pod security contexts. The lab begins with a multiple-choice question asking which combination is correct for mutating and validating admission controllers. The key observation is that the namespace auto-provision admission controller performs a mutation by automatically creating or altering a namespace. On the other hand, the namespace existence check is strictly a validation step. Therefore, the correct combination is to treat namespace auto-provisioning as mutating and namespace existence checking as validating. Step 1: Create the Namespace First, create a namespace named ""webhook-demo"" to serve as the environment for the webhook operations. kubectl create ns webhook-demo After executing the command, verify that the namespace is created by listing all namespaces: kubectl get ns You should see the ""webhook-demo"" namespace included in the output. Step 2: Create a TLS Secret For secure webhook communication, create a TLS secret named ""webhook-server-tls"" in the ""webhook-demo"" namespace. Ensure you substitute the correct file paths for the certificate and key: kubectl -n webhook-demo create secret tls webhook-server-tls \
  --cert ""/root/keys/webhook-server-tls.crt"" \
  --key ""/root/keys/webhook-server-tls.key"" A successful creation message will confirm that the TLS secret has been established. Step 3: Deploy the Webhook Server Review the webhook server deployment definition provided in the file webhook-deployment.yaml : cat webhook-deployment.yaml Deploy the webhook server with the following command: kubectl apply -f webhook-deployment.yaml Step 4: Create the Webhook Service Create a service for the webhook server using the configuration in webhook-service.yaml : kubectl apply -f webhook-service.yaml Step 5: Apply the Mutating Webhook Configuration Next, apply the mutating webhook configuration defined in webhook-configuration.yaml . This file includes rules under the ""rules"" section to intercept ""CREATE"" operations for pods. kubectl apply -f webhook-configuration.yaml Deprecation Notice When applying the webhook configuration, you might receive a deprecation warning indicating that admissionregistration.k8s.io/v1beta1 is deprecated in favor of v1. Despite the warning, the configuration will still perform its intended function by denying pod creation requests that attempt to run as root when no security context is provided. In our lab, if no explicit value is given for ""runAsNonRoot"", a default value of true is applied, and the user ID is defaulted to 1234 unless overridden. Step 6: Test the Webhook by Deploying Pods Pod with Default Security Context The next stage involves deploying a pod that does not specify any security context so that the webhook can mutate the configuration. The YAML file pod-with-defaults.yaml contains the following configuration: # A pod with no securityContext specified.
# Without the webhook, it would run as user root (0). The webhook mutates it
# to run as the non-root user with uid 1234.
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-defaults
  labels:
    app: pod-with-defaults
spec:
  restartPolicy: OnFailure
  containers:
    - name: busybox
      image: busybox
      command: [""sh"", ""-c"", ""echo I am running as user $(id -u)""] Deploy the pod with: kubectl apply -f pod-with-defaults.yaml After deployment, confirm that the pod has been mutated by retrieving the pod details: kubectl get pod pod-with-defaults -o yaml The webhook should have added a mutated security context, setting runAsNonRoot: true and runAsUser: 1234 . Pod with Explicit Override Next, deploy a pod that explicitly sets its security context to allow running as root. Check the pod-with-override.yaml file where the security context is defined with runAsNonRoot set to false : kubectl apply -f pod-with-override.yaml Review the file to confirm that the override is applied as intended. Pod with Conflicting Security Context Finally, deploy a pod with a conflicting security configuration in the pod-with-conflict.yaml file. This configuration attempts to set runAsNonRoot: true while also specifying runAsUser: 0 , creating a conflict. apiVersion: v1
kind: Pod
metadata:
  name: pod-with-conflict
  labels:
    app: pod-with-conflict
spec:
  restartPolicy: OnFailure
  securityContext:
    runAsNonRoot: true
  containers:
    - name: busybox
      image: busybox
      command: [""sh"", ""-c"", ""echo I am running as user $(id -u)""] Deploy the conflicting pod with: kubectl apply -f pod-with-conflict.yaml Validation Rejection The admission webhook should reject the conflicting pod creation request, displaying an error message similar to the following: Error from server: error when creating ""pod-with-conflict.yaml"": admission webhook ""webhook-server.webhook-demo.svc"" denied the request: runAsNonRoot specified, but runAsUser set to 0 (the root user) This rejection confirms that the webhook validation is functioning as expected by preventing pods from running as root when it is not allowed. This comprehensive walkthrough demonstrates how to validate and mutate admission controllers effectively. By following these steps, you ensure that your Kubernetes environment enforces the desired security configurations. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,API Deprecations,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Security/API-Deprecations,"Certified Kubernetes Application Developer - CKAD Security API Deprecations In this lesson, we explore API deprecations and their impact on managing API lifecycles in Kubernetes. We explain how a single API group can support multiple versions, why multiple versions are necessary, how many versions should be supported, and when older versions can be removed. The API deprecation policy provides guidelines for efficient API lifecycle management. API Group Lifecycle Example Imagine contributing to the Kubernetes project by creating an API group named kodekloud.com that contains two resources: Course and Webinar . Initially, both resources are developed and tested in-house. Once ready for integration, a pull request is created, and the API group is released as an alpha version (specifically, v1alpha1 ). For example, a YAML file to create a Course might look like: apiVersion: kodekloud.com/v1alpha1
kind: Course
metadata:
  name: ckad
spec: If user feedback suggests that the Webinar resource is no longer needed, the first rule of the API deprecation policy states that API elements can only be removed by incrementing the API group's version. Therefore, the Webinar resource is removed in the subsequent version (v1alpha2), while v1alpha1 remains available. The Course definition in the two versions is as follows: apiVersion: kodekloud.com/v1alpha1
kind: Course
metadata:
  name: ckad
spec: apiVersion: kodekloud.com/v1alpha2
kind: Course
metadata:
  name: ckad
spec: Even though the in-memory resource remains stored as v1alpha1, the current API version gets updated to v1alpha2. Both versions need to be supported until the change is fully adopted. Eventually, one of these versions (typically v1alpha2) becomes the preferred or storage version—ensuring that while users may continue to use v1alpha1 YAML files, the resource is stored internally as v1alpha2. Round-Trip Conversion Between API Versions The second rule of the API deprecation policy mandates that API objects must support round-trip conversion between versions without any loss of information (except for complete REST resources that do not exist in all versions). For instance, consider a Course object created in v1alpha1 with a spec field: apiVersion: kodekloud.com/v1alpha1
kind: Course
metadata:
  name: ckad
spec:
  type: video In v1alpha2, a new field named duration is introduced. The conversion to v1alpha2 could be represented as: apiVersion: kodekloud.com/v1alpha2
kind: Course
metadata:
  name: ckad
spec:
  type: video
  duration: To ensure fidelity when converting back to v1alpha1, an equivalent field for duration must be provided in v1alpha1—even if only as a placeholder—so that the round-trip conversion retains its integrity. Evolving Through Beta to GA After addressing bugs and refining the API in the alpha stages, the next phase is beta. For example, after releasing the first beta version ( v1beta1 ), further refinements result in v1beta2 , and finally, the stable GA version ( v1 ) is released. The typical evolution follows this sequence: v1alpha1 → v1alpha2 (with potentially additional alpha versions) v1beta1 and v1beta2 (both beta versions are supported for a defined period) v1 GA (stable release) The API deprecation policy requires continued support for older versions as follows: GA: At least 12 months or three releases. Beta: At least nine months or three releases. Alpha: No support beyond the current release. For example, consider a Kubernetes release sequence: In release x, v1alpha1 is introduced. In release x+1, v1alpha2 is introduced. Since alpha versions do not have extended support beyond the next release, v1alpha1 is dropped. According to the deprecation policy (rule four), older API versions must be supported after deprecation for a mandated period. In release x+1, if v1alpha1 is removed, this change must be clearly documented in the release notes. Future updates may also include the removal of deprecated APIs, such as v1alpha2, urging users to migrate to newer versions. In release x+2, the first beta version ( v1beta1 ) appears. Over subsequent releases (x+3, x+4, etc.), both the current and previous beta versions are supported for compatibility. During this overlap: The older beta version (e.g., v1beta1 ) is deprecated but is not removed immediately. Users receive deprecation warnings prompting migration to the newer beta version. The preferred API version (and storage version) remains unchanged until the next release when only the newer version is supported. When the GA version (v1) is released (e.g., in release x+5), beta versions ( v1beta1 and v1beta2 ) remain available temporarily until they satisfy the support duration requirements. In release x+6, v1beta1 may be removed after supporting it for three releases, while v1beta2 continues to be supported until it completes its deprecation period and is eventually dropped (e.g., in release x+8). Introducing a New API Version (v2) When a new major API version such as v2alpha1 is introduced, the previous stable version (v1, GA) is not immediately deprecated. The third rule of the API deprecation policy specifies that an API version must not be deprecated until an equivalent or more stable version is available. Since v2alpha1 is an alpha release and v1 is in GA, v1 continues to be supported. The new version will then progress through its lifecycle—v2alpha2, v2beta1, v2beta2, and finally v2—before it can deprecate v1. Using kubectl convert As Kubernetes evolves and older API versions are removed, manifest files must be updated to the current API versions. For example, if you have a YAML file with a deployment definition using v1beta1 and your Kubernetes upgrade removes that version, you must update your manifest to use apps/v1 : apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec: For projects with numerous YAML files requiring conversion, the kubectl convert command is an efficient tool. To convert your deployment file to apps/v1 , run: kubectl convert -f nginx.yaml --output-version apps/v1 The converted YAML manifest will be displayed on your screen. !!! note ""Note"" The kubectl convert command might not be installed by default as it is available as a separate plugin. Follow these steps to install it: curl -LO ""https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl-convert"" curl -LO ""https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl-convert.sha256"" echo ""$(<kubectl-convert.sha256) kubectl-convert"" | sha256sum --check For further details, please refer to the Kubernetes documentation . These instructions, along with hands-on labs, will help you become proficient with the kubectl convert plugin. Thank you for your time, and see you in the next lesson. Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Admission Controllers,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Security/Admission-Controllers,"Certified Kubernetes Application Developer - CKAD Security Admission Controllers In this lesson, we’ll explore admission controllers—an essential component of Kubernetes that enforces additional security measures and modifies requests before they are persisted. Understanding how admission controllers work and how to configure them is key to enhancing your cluster’s security and operations. When you use the kubectl utility to interact with a Kubernetes cluster (for example, to create a pod), the request first reaches the API server. The following sequence outlines the process: The API server receives the request. It authenticates the request—commonly using certificates specified in your KubeConfig file. For example, inspect your KubeConfig with: cat ~/.kube/config
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0t...
    # Truncated for brevity Once authenticated, the request proceeds to the authorization phase where Kubernetes checks if the user has permission to perform the operation, typically using Role-Based Access Control (RBAC). For example, a developer role can be defined as follows: apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: developer
rules:
- apiGroups: [""""]
  resources: [""pods""]
  verbs: [""list"", ""get"", ""create"", ""update"", ""delete""]
  resourceNames: [""blue"", ""orange""] In this example, the developer role is limited to operations on pods named ""blue"" or ""orange"". These RBAC rules operate solely at the API level to control permitted operations. After authentication and authorization, you might need to enforce restrictions beyond what RBAC offers. Consider these scenarios: Preventing images from public Docker Hub by allowing only images from a specific internal registry. Enforcing that images never use the ""latest"" tag. Rejecting pod creation when a container is configured to run as the root user. Allowing specific capabilities (like ""MAC_ADMIN"") only under certain conditions. Ensuring resource metadata always includes specific labels. To address such policies, admission controllers are used. They operate after authentication and authorization but before the request is saved in etcd. Admission controllers can validate configurations, modify requests, or trigger additional operations. Below is an overview of the Kubernetes process flow with admission controllers positioned between the authorization phase and resource creation: Built-in Admission Controllers Kubernetes includes several built-in admission controllers, such as: Always Pull Images: Ensures that images are pulled every time a pod is created. Default Storage Class: Automatically assigns a default storage class to PersistentVolumeClaims (PVCs) when none is specified. Event Rate Limit: Throttles the number of requests processed by the API server to prevent overload. Namespace Exists: Rejects requests for resources in namespaces that do not exist. Example: Namespace Existence Check Consider the ""namespace exists"" admission controller. When you attempt to create a pod in a namespace that doesn’t exist, for instance: kubectl run nginx --image nginx --namespace blue The API server goes through authentication and RBAC authorization. Then, the namespace exists controller checks for the ""blue"" namespace. Since it isn’t found, the request is rejected with an error similar to: Error from server (NotFound): namespaces ""blue"" not found Namespace Auto-Provisioning Some clusters may have the namespace auto-provision admission controller enabled (disabled by default) to automatically create missing namespaces. To list the admission controllers enabled by default, run: kube-apiserver -h | grep enable-admission-plugins If you’re using a kubeadm-based setup, execute this command inside the kube-apiserver control plane pod using the kubectl exec command. Configuring Admission Controllers To add an admission controller, update the --enable-admission-plugins flag on the Kubernetes API server. In a kubeadm-based setup, modify the kube-apiserver manifest file accordingly. For example, to enable both NodeRestriction and NamespaceAutoProvision, update the ExecStart command as follows: ExecStart=/usr/local/bin/kube-apiserver \
  --advertise-address=${INTERNAL_IP} \
  --allow-privileged=true \
  --apiserver-count=3 \
  --authorization-mode=Node,RBAC \
  --bind-address=0.0.0.0 \
  --enable-swagger-ui=true \
  --etcd-servers=https://127.0.0.1:2379 \
  --event-ttl=1h \
  --runtime-config=api/all \
  --service-cluster-ip-range=10.32.0.0/24 \
  --service-node-port-range=30000-32767 \
  --v=2 \
  --enable-admission-plugins=NodeRestriction,NamespaceAutoProvision For clusters running the API server as a pod (common in kubeadm setups), the manifest might look like this: apiVersion: vl
kind: Pod
metadata:
  creationTimestamp: null
  name: kube-apiserver
  namespace: kube-system
spec:
  containers:
  - command:
    - kube-apiserver
    - --authorization-mode=Node,RBAC
    - --advertise-address=172.17.0.107
    - --allow-privileged=true
    - --enable-bootstrap-token-auth=true
    - --enable-admission-plugins=NodeRestriction,NamespaceAutoProvision
    image: k8s.gcr.io/kube-apiserver-amd64:vl.11.3
    name: kube-apiserver To disable specific admission controller plugins, use the --disable-admission-plugins flag similarly. Auto-Provisioning Example With NamespaceAutoProvision enabled, running the following command: kubectl run nginx --image nginx --namespace blue passes through authentication and authorization. The auto-provision controller detects that the ""blue"" namespace is missing, creates it automatically, and the pod creation completes successfully. The output would be: kubectl run nginx --image nginx --namespace blue
Pod/nginx created! Verifying the existing namespaces: kubectl get namespaces
NAME         STATUS   AGE
blue         Active   3m
default      Active   23m
kube-public  Active   24m
kube-system  Active   24m This example shows that admission controllers not only validate and potentially reject requests but can also perform background operations and modify the request content as needed. Deprecated Controllers Note that the NamespaceAutoProvision and NamespaceExists admission controllers are deprecated. They have been replaced by the Namespace Lifecycle admission controller, which now ensures that requests to non-existent namespaces are rejected and protects default namespaces (such as default, kube-system, and kube-public) from deletion. This concludes our lesson on admission controllers. By practicing with these controllers, you can gain a deeper understanding of how to enhance Kubernetes security and manage your cluster effectively. References Kubernetes Documentation Kubernetes Basics Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Custom Resource Definition,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Security/Custom-Resource-Definition,"Certified Kubernetes Application Developer - CKAD Security Custom Resource Definition In this lesson, we explore how Custom Resource Definitions (CRDs) work in Kubernetes. You will learn how standard Kubernetes resources, such as Deployments, are created, stored in etcd, and managed by built-in controllers. Then, we’ll demonstrate how to extend Kubernetes by defining and using a custom resource—illustrated here as a ""FlightTicket""—and explain why a dedicated custom controller is necessary to act upon these new resources. Standard Kubernetes Resource: Deployment When you create a Deployment in Kubernetes, the API server stores its state in etcd. A built-in controller, known as the deployment controller, continuously monitors the Deployment to ensure that the desired state (for example, maintaining three replicas) is met by creating or deleting pods as needed. Below is an example Deployment definition file: # deployment.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      type: front-end
  template:
    metadata:
      name: myapp-pod
      labels:
        type: front-end
    spec:
      containers:
      - image: nginx Using the file above, you can create, query, and delete the deployment with the following commands: kubectl create -f deployment.yml
# Output:
kubectl get deployments
# Output:
# NAME              DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
kubectl delete -f deployment.yml
# Output:
# deployment ""myapp-deployment"" deleted The deployment controller automatically creates a ReplicaSet, which manages the specified number of pods. This automation lies at the heart of Kubernetes resource management. Custom Resource: FlightTicket Imagine you want to manage something entirely new on your cluster—such as booking a flight ticket. In this example, we define a custom resource called FlightTicket that represents a flight ticket booking. The resource encapsulates details such as the departure and destination airports and the number of tickets required. Below is an example of a FlightTicket resource file: # flightticket.yml
apiVersion: flights.com/v1
kind: FlightTicket
metadata:
  name: my-flight-ticket
spec:
  from: Mumbai
  to: London
  number: 2 When you create this custom resource, you expect that: It is stored in etcd. A custom controller (which you will build) watches for create, update, or delete events. The controller automatically makes the necessary API calls (for example, to an external flight booking API) to either book or cancel the ticket. Important Attempting to create the FlightTicket resource without first informing Kubernetes of its existence will result in an error: kubectl create -f flightticket.yml
# Output:
# no matches for kind ""FlightTicket"" in version ""flights.com/v1"" This error means that Kubernetes does not yet recognize the FlightTicket type. Defining a Custom Resource Definition (CRD) To allow the Kubernetes API to accept FlightTicket objects, you must create a CRD that informs the API server about this new resource type. The CRD includes details such as API version, kind, metadata, spec, and schema information (including supported fields, types, and validation rules). Here’s an example of a CRD for the FlightTicket resource: # flightticket-custom-definition.yml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: flighttickets.flights.com
spec:
  group: flights.com
  scope: Namespaced
  names:
    plural: flighttickets
    singular: flightticket
    kind: FlightTicket
    shortNames:
      - ft
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                from:
                  type: string
                to:
                  type: string
                number:
                  type: integer
                  minimum: 1 Key points in the CRD: The API group is flights.com . The resource scope is namespace-scoped. Both singular and plural names are defined along with a short name ( ft ). Only one version ( v1 ) is served and defined as the storage version. An OpenAPI v3 schema enforces that the spec includes from and to as strings, and number as an integer with a minimum value of 1. After creating the CRD, you can create, retrieve, and delete FlightTicket resources using these commands: kubectl create -f flightticket-custom-definition.yml
# Output:
kubectl create -f flightticket.yml
# Output:
kubectl get flightticket
# Output:
# NAME             STATUS
kubectl delete -f flightticket.yml
# Output:
# flightticket ""my-flight-ticket"" deleted To verify that your new resource is recognized by Kubernetes, run: kubectl api-resources
# Output snippet:
# NAME            SHORTNAMES   APIGROUP     NAMESPACED   KIND
# flighttickets   ft           flights.com  true         FlightTicket The Role of a Custom Controller While defining a CRD enables Kubernetes to store and retrieve FlightTicket objects in etcd, these objects remain inactive without a controller. A custom controller, often implemented in Go, observes events related to FlightTicket resources and executes business logic (such as calling external APIs to book or cancel flights). Below is a simplified snippet of what such a controller might look like: package flightticket

var controllerKind = apps.SchemeGroupVersion.WithKind(""FlightTicket"")

// Run begins watching and syncing FlightTicket resources.
func (dc *FlightTicketController) Run(workers int, stopCh <-chan struct{}) {
    // Controller logic goes here
}

// callBookFlightAPI handles the API call to book a flight ticket.
func (dc *FlightTicketController) callBookFlightAPI(obj interface{}) {
    // API call implementation goes here
} Note Without a custom controller, any FlightTicket resource you create remains a passive data record in etcd without triggering any external actions. Conclusion In this lesson, we learned how: Kubernetes resources like Deployments are managed by built-in controllers. A custom resource (FlightTicket) can be defined to represent a new domain object. A CRD must be created so that Kubernetes recognizes your custom resource. A custom controller is essential to actively process and respond to events related to these resources. In upcoming lessons, we’ll dive deeper into developing custom controllers that monitor CRD events and execute automated tasks based on resource changes. Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,API Versions,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Security/API-Versions,"Certified Kubernetes Application Developer - CKAD Security API Versions In this lesson, we explore API versions in Kubernetes—a topic that builds upon core concepts such as APIs, API groups, resources, and verbs. Understanding the evolution of these API versions—from experimental to stable—is key to effectively managing your Kubernetes configurations. Overview of API Organization Kubernetes organizes everything under its API into API groups (e.g., apps, extensions, networking). Each API group can support multiple versions. A version labeled as ""v1"" typically indicates a generally available (GA) and stable release. Other labels like v1beta1 or v1alpha1 denote beta or alpha stages respectively. Let’s break down what each of these stages means. API Version Stages Alpha An alpha version represents the initial development stage of an API. Once an API is added to the Kubernetes code base and included in a release for the first time, it is marked as alpha (for example, v1alpha1, v1alpha2). At this stage, the API is not enabled by default, may lack comprehensive end-to-end tests, and could contain bugs. For instance, at the time of recording, the API group internal.apiserver.k8s.io (which includes the StorageVersion resource) exists only in its alpha form. If you attempt to create an object using the following YAML: apiVersion: internal.apiserver.k8s.io/v1alpha1
kind: StorageVersion
metadata:
  name: sv-1
spec: the API server will reject the creation because the alpha version is not enabled by default. This version is intended for expert users who want to test and provide early feedback. Beta After addressing major bugs in the alpha API and adding comprehensive tests, the API advances to beta (e.g., v1beta1, v1beta2). Beta APIs are enabled by default and include end-to-end tests. While minor bugs might still exist, there is a commitment from the community to advance these APIs to GA. For example, the flow-control group is currently in the beta stage. GA (Stable) An API promoted to GA has successfully navigated the beta phase, undergone multiple releases, and received numerous bug fixes. The version number drops any alpha or beta suffix, appearing simply as ""v1."" GA APIs are reliably enabled by default and become part of conformance tests. Most API groups, such as apps, authentication, authorization, certificates, and coordination, are now available in their GA versions. Note For production environments, always ensure you use GA versions of the APIs to maintain a stable and supported deployment. Supporting Multiple Versions in an API Group An API group can support several versions simultaneously. For example, the apps group might offer v1, v1beta1, and v1alpha1, allowing you to reference any of these versions in your YAML file. Below are examples of a Deployment resource defined using different API versions: # Example using apps/v1alpha1:
apiVersion: apps/v1alpha1
kind: Deployment
metadata:
  name: nginx
spec:
  # Deployment spec fields go here
---
# Example using apps/v1beta1:
apiVersion: apps/v1beta1
kind: Deployment
metadata:
  name: nginx
spec:
  # Deployment spec fields go here
---
# Example using apps/v1 (the preferred and storage version):
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  # Deployment spec fields go here While you can create an object with any supported API version, only one version is designated as the preferred version. This is the version used by default with commands like kubectl get deployment or kubectl explain deployment . Furthermore, only one version—the storage version —is used to persist objects in etcd. If you create an object using a non-preferred version (like apps/v1alpha1 or apps/v1beta1), Kubernetes will automatically convert it to the storage version (typically apps/v1) before saving. For example, after creating a Deployment using any API version, running these commands will show that it is stored as apps/v1: kubectl get deployment kubectl explain deployment KIND:     Deployment
VERSION:  apps/v1 Determining the Preferred and Storage Versions When multiple API versions are available within a group, the preferred version is listed in the API group details. For example, querying the batch API group might return: {
  ""kind"": ""APIGroup"",
  ""apiVersion"": ""v1"",
  ""name"": ""batch"",
  ""versions"": [
    {
      ""groupVersion"": ""batch/v1"",
      ""version"": ""v1""
    },
    {
      ""groupVersion"": ""batch/v1beta1"",
      ""version"": ""v1beta1""
    }
  ],
  ""preferredVersion"": {
    ""groupVersion"": ""batch/v1"",
    ""version"": ""v1""
  }
} While you can determine the preferred version via the API, the storage version remains hidden. To reveal the storage version, you can query the etcd database directly. For instance, using the etcdctl utility with the following command: ETCDCTL_API=3 etcdctl \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  get ""/registry/deployments/default/blue"" --print-value-only You might receive output similar to: apps/v1
Deployment

bluedefault""*${cf8dcd55-8819-4be2-85e7-bb71665c2ddf2ZB
successfully progressed8""2 This output confirms that the object is stored as apps/v1. Enabling or Disabling Specific API Versions To modify which API versions are enabled, adjust the --runtime-config parameter of the kube-apiserver. For instance, if you need to enable alpha APIs that aren’t enabled by default, update the kube-apiserver's service configuration. Below is an example ExecStart configuration for kube-apiserver: ExecStart=/usr/local/bin/kube-apiserver \
  --advertise-address=${INTERNAL_IP} \
  --allow-privileged=true \
  --apiserver-count=3 \
  --authorization-mode=Node,RBAC \
  --bind-address=0.0.0.0 \
  --enable-swagger-ui=true \
  --etcd-cafile=/var/lib/kubernetes/ca.pem \
  --etcd-certfile=/var/lib/kubernetes/apiserver-etcd-client.crt \
  --etcd-keyfile=/var/lib/kubernetes/apiserver-etcd-client.key \
  --etcd-servers=https://127.0.0.1:2379 \
  --event-ttl=1h \
  --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \
  --kubelet-client-certificate=/var/lib/kubernetes/apiserver-etcd-client.crt \
  --kubelet-client-key=/var/lib/kubernetes/apiserver-etcd-client.key \
  --kubelet-https=true \
  --runtime-config=api/all \
  --service-account-key-file=/var/lib/kubernetes/service-account.pem \
  --service-cluster-ip-range=10.32.0.0/24 \
  --service-node-port-range=30000-32767 \
  --client-ca-file=/var/lib/kubernetes/ca.pem \
  --tls-cert-file=/var/lib/kubernetes/apiserver.crt \
  --tls-private-key-file=/var/lib/kubernetes/apiserver.key \
  --v=2 After updating the configuration to enable the desired API versions (specified as comma-separated values in the --runtime-config parameter), restart the kube-apiserver service for the changes to take effect. Warning Always back up your configuration and validate your changes in a non-production environment before applying them to production. Conclusion This lesson provided an in-depth look at Kubernetes API versions, the significance of each version stage (Alpha, Beta, and GA), and how Kubernetes handles multiple API versions within a single API group. By understanding preferred and storage versions along with how to enable or disable specific API versions, you are better equipped to manage your Kubernetes infrastructure effectively. Happy learning and stay tuned for more in-depth explorations of the Kubernetes API components! Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Custom Controllers,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Security/Custom-Controllers,"Certified Kubernetes Application Developer - CKAD Security Custom Controllers In this lesson, we explore how to develop custom controllers for managing your Kubernetes resources. Building on our previous work with Custom Resource Definitions (CRDs), we now introduce FlightTicket objects. These objects, along with their details, are stored in etcd. The custom controller monitors the status of these FlightTicket objects in etcd and performs actions such as booking, editing, or canceling flight tickets by invoking the appropriate flight booking API. A controller is a process or piece of code that continuously observes the Kubernetes cluster for specific events (like changes to FlightTicket objects) and takes corresponding actions. For example, consider the following YAML definition of a FlightTicket resource: # flightticket.yml
apiVersion: flights.com/v1
kind: FlightTicket
metadata:
  name: my-flight-ticket
spec:
  from: Mumbai
  to: London
  number: 2 You can create this resource and verify its status using these commands: kubectl create -f flightticket.yml
# Output:
kubectl get flightticket
# Output:
# NAME              STATUS
# my-flight-ticket  Pending Note While you can implement controllers in various programming languages, using the Kubernetes Go client is recommended. The Go client (client-go library) provides shared informers that offer efficient caching and queuing mechanisms, making it ideal for building robust controllers. The same FlightTicket YAML definition provides context for this process: # flightticket.yml
apiVersion: flights.com/v1
kind: FlightTicket
metadata:
  name: my-flight-ticket
spec:
  from: Mumbai
  to: London
  number: 2 And here are the related creation commands: kubectl create -f flightticket.yml
# Output:
kubectl get flightticket
# Output:
# NAME              STATUS
# my-flight-ticket  Pending Using Go simplifies the development process due to its seamless integration with Kubernetes libraries that support robust controller patterns. Getting Started with a Custom Controller To build your custom controller, follow these steps: Clone the SampleController Repository Clone the repository from GitHub using the following command: git clone https://github.com/kubernetes/sample-controller.git
# Cloning into 'sample-controller'...
# Resolving deltas: 100% (15787/15787), done. Customize Your Controller Logic Navigate to the repository directory and modify the controller.go file to include your custom logic, such as invoking the flight booking API: cd sample-controller
go build -o sample-controller .
# Output during build might include:
# go: downloading k8s.io/client-go v0.0.0-20211001003700-dbfa30b9d908
# go: downloading golang.org/x/text v0.3.6 Run the Controller Execute the controller by specifying the kubeconfig file for authentication: ./sample-controller --kubeconfig=$HOME/.kube/config
# Example output:
# I1013 02:11:07.489479   4017 controller.go:115] Setting up event handlers
# I1013 02:11:07.489701   4017 controller.go:156] Starting FlightTicket controller When executed, the controller runs locally, monitors the creation of FlightTicket objects, and triggers the necessary API calls. Deployment Tip After verifying that your controller functions correctly, consider packaging it into a Docker image and deploying it inside your Kubernetes cluster as a pod or deployment. This approach eliminates the need for manual rebuilding and execution each time. Overview This article provides a high-level overview of building a custom controller. Although detailed coding questions about custom controllers are unlikely to appear in certification exams, it is essential to understand concepts such as: Custom Resource Definitions (CRDs) Managing CRD files Working with existing controller patterns For more in-depth information on Kubernetes resources, refer to the following links: Kubernetes Basics Kubernetes Documentation Docker Hub Operators extend these concepts further by automating more complex operational tasks in Kubernetes environments. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solution Install Helm,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Helm-Fundamentals/Solution-Install-Helm,"Certified Kubernetes Application Developer - CKAD Helm Fundamentals Solution Install Helm In this lesson, we will walk through the steps required for the Helm installation lab, covering the process from identifying your operating system to validating your Helm installation. 1. Identifying the Operating System Before installing Helm, you need to verify the operating system on your machine. Run the following command to view your OS details: root@controlplane ~ ➔ cat /etc/*release*
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=18.04
DISTRIB_CODENAME=bionic
DISTRIB_DESCRIPTION=""Ubuntu 18.04.6 LTS""
NAME=""Ubuntu""
VERSION=""18.04.6 LTS (Bionic Beaver)""
ID=ubuntu
ID_LIKE=debian
PRETTY_NAME=""Ubuntu 18.04.6 LTS""
VERSION_ID=""18.04""
HOME_URL=""https://www.ubuntu.com/""
SUPPORT_URL=""https://help.ubuntu.com/""
BUG_REPORT_URL=""https://bugs.launchpad.net/ubuntu/""
PRIVACY_POLICY_URL=""https://www.ubuntu.com/legal/terms-and-policies/privacy-policy""
VERSION_CODENAME=bionic
UBUNTU_CODENAME=bionic
root@controlplane ~ ➔ The output confirms that the machine is running Ubuntu. Therefore, we will choose Ubuntu-specific installation steps. 2. Installing Helm Refer to the official Helm documentation for the most accurate instructions. For Ubuntu, follow these commands. Depending on your package manager, use one of the methods below: Using Chocolatey: (For environments where Chocolatey is available) choco install kubernetes-helm Using Scoop: (For environments where Scoop is installed) scoop install helm Using apt (Preferred for Ubuntu): Copy and execute the commands in your terminal to add the Helm repository, update your package list, and install Helm: curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
sudo apt-get install apt-transport-https --yes
echo ""deb [arch=$(dpkg --print-architecture)] signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main"" | sudo tee /etc/apt/sources.list.d/helm.list
sudo apt-get update
sudo apt-get install helm Note Ensure you follow only the method applicable to your package management setup. 3. Validating the Helm Installation After installation, it's essential to confirm that Helm has been successfully installed on your system. 3.1. Installing Helm from the Repository When installing Helm from the repository using the apt command, you should see an output similar to the following: root@controlplane ~ ➜ sudo apt-get install helm
Reading package lists... Done
Building dependency tree
Reading state information... Done
The following NEW packages will be installed:
  helm
0 upgraded, 1 newly installed, 0 to remove and 47 not upgraded.
Need to get 14.0 MB of archives.
After this operation, 46.3 MB of additional disk space will be used.
Get:1 https://baltocdn.com/helm/stable/debian all/main amd64 helm 3.9.2-1 [14.0 MB]
Fetched 14.0 MB in 0s (51.8 MB/s)
debconf: delaying package configuration, since apt-utils is not installed
Selecting previously unselected package helm.
(Reading database ... 123456 files and directories currently installed.)
Preparing to unpack .../helm_3.9.2-1_amd64.deb ...
Unpacking helm (3.9.2-1) ...
Setting up helm (3.9.2-1) ...
Processing triggers for man-db (2.8.3-2ubuntu0.1) ...
root@controlplane ~ ➜ 3.2. Viewing Helm Commands You can explore Helm's available commands and options by invoking the help command: Usage:
helm [command]

Available Commands:
  completion  generate autocompletion scripts for the specified shell
  create      create a new chart with the given name
  dependency  manage a chart's dependencies
  env         helm client environment information
  get         download extended information of a named release
  help        Help about any command
  history     fetch release history
  install     install a chart
  lint        examine a chart for possible issues
  list        list releases
  package     package a chart directory into a chart archive
  plugin      install, list, or uninstall Helm plugins
  pull        download a chart from a repository and (optionally) unpack it in local directory
  push        push a chart to remote
  registry    login to or logout from a registry
  repo        add, list, remove, update, and index chart repositories
  rollback    roll back a release to a previous revision
  search      search for a keyword in charts
  show        show information of a chart
  status      display the status of the named release
  template    locally render templates
  test        run tests for a release
  uninstall   uninstall a release
  upgrade     upgrade a release Note Remember to use the ""env"" command in lowercase when checking the Helm client environment information. 3.3. Checking the Helm Version Verify that Helm is correctly installed and check its version by running: root@controlplane ~ ➜ helm version
version.BuildInfo{Version:""v3.9.2"", GitCommit:""1addefbfe665c30f4daf868a9adc5600cc064fd"", GitTreeState:""clean"", GoVersion:""go1.17.12""} This confirms that you are running Helm version 3.9.2. 3.4. Enabling Verbose Output For troubleshooting and deeper inspection, you can enable verbose output by adding the --debug flag when executing Helm commands. An excerpt from the Helm help output shows available flags: Flags:
  --debug                  enable verbose output
  -h, --help               help for helm
  --kube-apiserver string  the address and the port for the Kubernetes API server
  --kube-as-group stringArray  group to impersonate for the operation, this flag can be repeated to specify multiple groups.
  --kube-as-user string    username to impersonate for the operation
  --kube-ca-file string    path to the certificate authority file for the Kubernetes API server connection
  --kube-context string     name of the kubeconfig context to use
  --kube-token string       bearer token used for authentication
  --kubeconfig string       path to the kubeconfig file Note Using the --debug flag will provide more detailed output, which is useful for diagnosing any issues during Helm operations. This concludes the Helm installation lab. You now have Helm installed and verified on your Ubuntu machine, with the ability to leverage its extensive command set for managing Kubernetes deployments effectively. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Helm Concept,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Helm-Fundamentals/Helm-Concept,"Certified Kubernetes Application Developer - CKAD Helm Fundamentals Helm Concept In this lesson, we explore Helm concepts by examining how Helm charts simplify the deployment of applications like WordPress on Kubernetes. First, we addressed the deployment challenges of a WordPress application. Now, we will demonstrate how Helm charts resolve these challenges by converting static YAML configuration files into parameterized templates. YAML Files for WordPress Components Below are the YAML files that define the individual components of a WordPress application on Kubernetes: deployment.yaml secret.yaml svc.yaml pvc.yaml service.yaml These files provide the necessary configurations for deploying WordPress. Note that some values—such as the WordPress version, disk size, or admin password—might need to be adjusted between different environments. For example, you might deploy a different WordPress version or customize the disk size depending on your requirements. Standard Deployment Configuration The following code snippet is part of the standard deployment configuration (deployment.yaml): apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: frontend
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: frontend
    spec:
      containers:
      - image: wordpress:4.8-apache
        name: wordpress Persistent Volume and Claim Configurations A persistent volume configuration is provided in pv.yaml: apiVersion: v1
kind: PersistentVolume
metadata:
  name: wordpress-pv
spec:
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  gcePersistentDisk:
    pdName: wordpress-2
    fsType: ext4 The corresponding persistent volume claim (pvc.yaml) is defined as follows: apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wp-pv-claim
  labels:
    app: wordpress
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 20Gi Service and Secret Configurations The service used to expose WordPress is configured in service.yaml: apiVersion: v1
kind: Service
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  ports:
    - port: 80
  selector:
    app: wordpress
    tier: frontend
  type: LoadBalancer The admin password for WordPress is managed by a secret (secret.yaml): apiVersion: v1
kind: Secret
metadata:
  name: wordpress-admin-password
data:
  key: CahjWVUxSdzIZQzg0SERXhBQTVrQ1FzN2JE9PQ== Customization with Templates Because variables such as the admin password, image version, and disk size often vary by environment, the next step is to convert these static YAML files into reusable templates. Template variables, denoted by double curly braces (e.g., {{ .Values.image }}), fetch their corresponding values from a separate values.yaml file. This setup allows users to manage different deployment parameters by modifying only one file. Converting YAML to Helm Chart Templates Below is an example of a Helm chart template where key values are injected from values.yaml: apiVersion: apps/v1
kind: Deployment
metadata:
  name: wordpress
  labels:
    app: wordpress
spec:
  selector:
    matchLabels:
      app: wordpress
      tier: frontend
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: wordpress
        tier: frontend
    spec:
      containers:
      - image: {{ .Values.image }}
        name: wordpress
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: wordpress-pv
spec:
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOnce
  gcePersistentDisk:
    pdName: wordpress-2
    fsType: ext4
---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: wp-pv-claim
  labels:
    app: wordpress
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: {{ .Values.storage }}
---
apiVersion: v1
kind: Secret
metadata:
  name: wordpress-admin-password
data:
  key: CjhWVUxSdZIzQg0SERXhBQTVQ1FZnJE2P9Q== A Helm chart aggregates these templates along with the values.yaml file. In addition, it includes a Chart.yaml file that contains metadata about the chart, such as its name, version, description, keywords, and maintainer information. Below is an example of a Chart.yaml file: apiVersion: v2
name: Wordpress
version: 9.0.3
description: Web publishing platform for building blogs and websites.
keywords:
  - wordpress
  - cms
  - blog
  - http
  - web
  - application
  - php
home: http://www.wordpress.com/
sources:
  - https://github.com/bitnami/bitnami-docker-wordpress
maintainers:
  - email: [email protected] name: Bitnami Helm Charts and the Artifact Hub Helm charts simplify deploying applications like WordPress with a single package. You can also explore a wide range of existing charts available on the Artifact Hub at artifacthub.io . The Artifact Hub is a central repository of Helm charts, hosting over 5700 charts at the time of this writing. You can search for a chart using either the Artifact Hub website or from the command line. Searching from the Command Line To search the community-driven Artifact Hub via the command line, use the following command: helm search hub wordpress Example output: https://hub.helm.sh/charts/kube-wordpress/wordpress...
https://hub.helm.sh/charts/groundhog2k/wordpress   0.1.0
https://hub.helm.sh/charts/bitnami-aks/wordpress    1.2.3 To search for charts in a specific repository, such as Bitnami, first add the repository: helm repo add bitnami https://charts.bitnami.com/bitnami Then, use this command to search: helm search repo wordpress Example output: NAME CHART VERSION APP VERSION DESCRIPTION bitnami/wordpress 12.1.14 5.8.1 Web publishing platform for building blogs and ... Installing Helm Charts After identifying the desired chart, install it on your Kubernetes cluster using: helm install [release-name] [chart-name] For example, installing multiple independent releases of WordPress: helm install release-1 bitnami/wordpress
helm install release-2 bitnami/wordpress
helm install release-3 bitnami/wordpress Each installation (release) is independent, ensuring that you can manage multiple deployments of the same application. Additional Helm Commands To manage your Helm releases, use the following commands: List installed releases: helm list
NAME        NAMESPACE  REVISION  UPDATED                                   STATUS     CHART                   APP VERSION
my-release  default    1         2021-05-30 09:52:38.33818569 -0400 EDT  deployed   wordpress-11.0.12       5.7.2 Uninstall a release: helm uninstall my-release Pull (download) a chart without installing it: Use helm pull with the --untar option. This command downloads the chart as a tar archive and extracts its contents. You can then navigate into the chart directory to review or modify files (including values.yaml) before installation. Pro Tip Customize your chart configuration by editing the values.yaml file to fine-tune your deployments according to your environment's requirements. Conclusion This lesson provided an in-depth overview of Helm concepts, demonstrating how to transform static YAML files into parameterized templates using a values.yaml file and Chart.yaml metadata. Helm charts package these files together, simplifying the deployment and management of applications on a Kubernetes cluster. Experiment with Helm commands in your Kubernetes environment to further enhance your understanding. Stay tuned for more detailed insights into Helm and its advanced features. For additional resources, check out: Kubernetes Basics Kubernetes Documentation Docker Hub Terraform Registry Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Operator Framework,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Security/Operator-Framework,"Certified Kubernetes Application Developer - CKAD Security Operator Framework In this lesson, we'll explore the operator framework, a powerful method for managing Kubernetes custom resources by combining Custom Resource Definitions (CRDs) and custom controllers into a unified deployment. Traditionally, you would manually create a CRD along with its corresponding controller (deployed as a Pod or Deployment). The operator framework streamlines this process by packaging both components together. For example, when you deploy a flight operator, it automatically creates the necessary CRD, provisions custom resources, and deploys the custom controller as a Deployment. Below is an example configuration for a flight ticket CRD and its corresponding controller: # flightticket-custom-definition.yml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: flighttickets.flights.com
spec:
  scope: Namespaced
  group: flights.com
  names:
    kind: FlightTicket
    singular: flightticket
    plural: flighttickets
    shortnames:
      - ft
  versions:
    - name: v1
      served: true
      storage: true package flightticket

import (
    ""k8s.io/api/apps/v1""
)

var controllerKind = v1.SchemeGroupVersion.WithKind(""Flightticket"")

// Run begins watching and syncing.
func (dc *FlightTicketController) Run(workers int, stopCh <-chan struct{}) {
    // Controller logic goes here.
}

// callBookFlightAPI initiates the flight booking process.
func (dc *FlightTicketController) callBookFlightAPI(obj interface{}) {
    // API call implementation.
} kubectl create -f flight-operator.yaml The operator framework offers extensive automation capabilities beyond simple deployments. A notable real-world example is the etcd operator—which deploys and manages an etcd cluster within Kubernetes. It includes a CRD for defining the etcd cluster and a custom controller that continuously monitors etcd resources. This operator also simplifies advanced operations like backups and restores by enabling these actions with the creation of dedicated CRDs. The diagram below visualizes an operator framework structure that includes CRDs and custom controllers such as EtcdCluster, EtcdBackup, EtcdRestore, ETCD Controller, and Backup Operator: Note Operators simplify complex processes by bundling CRDs and controllers, making tasks like deployment, backup, and recovery more efficient. However, keep in mind that understanding CRDs remains essential, especially for certification exams. Deploying an operator typically involves three main steps: Installing the Operator Lifecycle Manager (OLM). Installing the operator. Verifying the installation. For instance, to deploy the etcd operator, you might use the following commands: curl -sL https://github.com/operator-framework/operator-lifecycle-manager/releases/download/v0.19.1/install.sh | bash -s v0.19.1 kubectl create -f https://operatorhub.io/install/etcd.yaml kubectl get csv -n my-etcd Important While operators offer significant advantages in automating deployment and management tasks, ensure that you have a solid understanding of CRDs, as they are a core component of Kubernetes and a primary focus of the exam curriculum. Thank you for reading, and stay tuned for the next lesson to further enhance your Kubernetes expertise. Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Install Helm,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Helm-Fundamentals/Install-Helm,"Certified Kubernetes Application Developer - CKAD Helm Fundamentals Install Helm Before installing Helm, ensure you have a working Kubernetes cluster and a correctly configured kubectl utility on your local machine. A valid kubeconfig file containing the proper credentials for your target cluster is essential. Prerequisites Verify that your Kubernetes setup is operational and that kubectl is set up before proceeding with the Helm installation. Helm supports Linux, Windows, and macOS environments. This guide focuses on the installation process for Linux systems. Installing Helm on Linux Using Snap If your Linux distribution supports Snap, you can install Helm using the Snap package manager. Snap's classic confinement allows Helm unrestricted access to locate your kubeconfig file (typically in your home directory). Execute the following command: sudo snap install helm --classic Installing Helm on APT-Based Distributions For Debian, Ubuntu, or similar APT-based distributions, follow these steps: # Install Helm via Snap with classic confinement (if using Snap)
sudo snap install helm --classic

# Add the Helm GPG key to verify package authenticity
curl https://baltocdn.com/helm/signing.asc | sudo apt-key add -

# Install apt-transport-https to handle HTTPS repositories
sudo apt-get install apt-transport-https --yes

# Add the official Helm stable repository to your APT sources list
echo ""deb https://baltocdn.com/helm/stable/debian/ all main"" | sudo tee /etc/apt/sources.list.d/helm-stable-debian.list

# Update APT and install Helm
sudo apt-get update
sudo apt-get install helm

# Alternatively, on some package-based systems, you might use:
pkg install helm Additional Resources For the most up-to-date installation instructions and additional configuration details, refer to the official Helm documentation . /images/Python_Basics-Comments/frame_100.jpg Watch Video Watch video content Practice Lab Practice lab"
Certified Kubernetes Application Developer CKAD,Solution Helm Concepts,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Helm-Fundamentals/Solution-Helm-Concepts,"Certified Kubernetes Application Developer - CKAD Helm Fundamentals Solution Helm Concepts In this lesson, you’ll learn how to perform various tasks with Helm—from searching for chart packages and adding repositories to configuring and installing charts. Follow the steps below, and refer to the command outputs to validate your progress. 1. Searching for a Helm Chart Package To search for a WordPress Helm chart package on Artifact Hub, run the following command: helm search hub wordpress This command displays all available WordPress-related chart packages. 2. Adding a Bitnami Helm Chart Repository First, add the Bitnami Helm chart repository on your control plane node by executing: helm repo add bitnami https://charts.bitnami.com/bitnami Once added, search the repository for the Joomla package with: helm search repo joomla A sample output might look like: root@controlplane ~ ➜ helm repo add bitnami https://charts.bitnami.com/bitnami
""bitnami"" has been added to your repositories

root@controlplane ~ ➜ helm search repo joomla
NAME           CHART VERSION APP VERSION DESCRIPTION
bitnami/joomla 13.2.16     4.1.5     Joomla! is an award winning open source CMS pla... From this output, observe that the Joomla package has an app version of 4.1.5 and a chart version of 13.2.16. 3. Listing Helm Repositories To review all repositories configured on the control plane node, run: helm repo list A typical output might be: root@controlplane ~ ➜ helm repo list
NAME      URL
bitnami   https://charts.bitnami.com/bitnami
puppet    https://puppetlabs.github.io/puppetserver-helm-chart
hashicorp https://helm.releases.hashicorp.com This indicates that there are three repositories currently added. 4. Installing and Uninstalling a Drupal Helm Chart Installing the Chart Install the Drupal Helm chart from the Bitnami repository using the release name ""bravo"": helm install bravo bitnami/drupal After installation, verify the release by listing all Helm releases: helm list The output should include an entry for ""bravo"", similar to: root@controlplane ~ ➜ helm list
NAME    NAMESPACE   REVISION   UPDATED                                STATUS    CHART          APP VERSION
bravo   default     1          2022-08-04 18:50:12.967402693 +0000 UTC deployed  drupal-12.3.3  9.4.4 Uninstalling the Chart To remove the Drupal Helm package (release ""bravo""), execute: helm uninstall bravo The command should confirm the uninstallation: root@controlplane ~ ➜ helm uninstall bravo
release ""bravo"" uninstalled 5. Downloading the Bitnami Apache Package Download the Bitnami Apache package from the repository to your /root directory without installing it. Use the --untar flag: helm pull --untar bitnami/apache After downloading, verify that a new directory named apache has been created: root@controlplane ~ ➜ ls
apache Then, change to the Apache directory with: cd apache/ 6. Modifying the Apache Helm Chart Configuration Open the values.yaml file within the Apache chart directory to make two essential changes: Set the replica count for the web server to 2. Configure the HTTP service to expose NodePort 30080. Search the file for the replica settings. You might see a section like: ## @section Global parameters
global:
  imageRegistry: """"
  imagePullSecrets: []
  storageClass: """"
  
## @section Common parameters
kubeVersion: """"
nameOverride: """"
fullnameOverride: """" Make the necessary modifications by adding or updating the entries for the replica count and the service configuration. For assistance on the file structure, refer to the official Helm or Bitnami documentation. Once the changes are complete, install the Apache chart from the current directory using the release name ""mywebapp"": helm install mywebapp . Confirm the deployment by listing your Helm releases: helm list 7. Accessing the Apache Service Retrieve and set the service IP by extracting the external IP of the Apache service, then print the URL: export SERVICE_IP=$(kubectl get svc --namespace default mywebapp-apache --template ""{{ range (index .status.loadBalancer.ingress 0) }}{{ . }}{{ end }}"")
echo URL : http://$SERVICE_IP/ Visit the printed URL in your web browser to confirm that the Apache page is successfully loading. Summary This lesson demonstrated how to interact with Helm in a Kubernetes environment: from searching and adding repositories to deploying, configuring, and accessing services using Helm. For further details and best practices, consider exploring the Helm Documentation . Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Helm Introduction,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Helm-Fundamentals/Helm-Introduction,"Certified Kubernetes Application Developer - CKAD Helm Fundamentals Helm Introduction In this article, we will explore Helm and its role in simplifying the management of complex Kubernetes applications. While Kubernetes excels at orchestrating infrastructure, handling multiple interconnected objects in a single application can become challenging. For example, deploying a WordPress site might require several components, including: A Deployment to run the pods for components such as MySQL database servers or web servers. A Persistent Volume for database storage. A Persistent Volume Claim to request storage. A Service to expose the web server. A Secret to securely store the admin password. Traditionally, each of these objects would be defined in separate YAML files and applied individually using the kubectl apply command. Managing configurations across multiple files becomes tedious and error-prone—if, for instance, you need to increase the persistent volume size from 20 GB to a higher capacity, every related YAML file must be manually updated. Additionally, upgrading or removing the application involves tracking down and managing all individual objects. Consider the following YAML snippet: apiVersion: v1
kind: Secret
metadata:
  name: wordpress-admin-password
data:
  key: CajnHWVUxSdzIZQzg0SERXhBQTvQ1FzN2JE9PQ==
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: wordpress-pv
spec:
  capacity:
    storage: 20Gi
  accessModes:
    - ReadWriteOne
  gcePersistentDisk:
    pdName: wordpress-2
    fsType: ext4 While organizing related objects into separate files (for example, placing deployment configurations in mysql-deployment.yaml ) can offer some clarity, it still requires searching across multiple files to update settings. Enter Helm. Unlike Kubernetes, which considers each object independently, Helm recognizes that these objects are part of a cohesive package—such as a WordPress application. With Helm, you manage the application as a single unit instead of juggling multiple YAML files. Consider this analogy: a computer game is composed of hundreds or thousands of files (executables, audio, graphics, configuration data). Instead of downloading and organizing each file manually, you run an installer that efficiently places everything in the correct locations. Helm provides a similar level of abstraction for your Kubernetes manifests. It allows you to install, upgrade, roll back, and uninstall an application, regardless of how many individual objects it encompasses. For example, to install a WordPress package using Helm, you would execute: helm install wordpress ... Helm leverages a central configuration file—typically named values.yaml —to manage custom settings. This file might look like: wordpressUsername: user
wordpressEmail: [email protected] wordpressFirstName: FirstName
wordpressLastName: LastName This centralized configuration lets you adjust critical settings—such as persistent volume sizes, website names, admin passwords, and database configurations—in one place, eliminating the need to modify multiple YAML files. Tip By centralizing configuration management, Helm significantly streamlines application lifecycle management in Kubernetes. Managing the application lifecycle becomes even simpler with Helm. With a single command, you can upgrade your application, and Helm calculates the necessary changes to each object. Rolling back to a previous version or uninstalling the application is equally straightforward. For example, a typical workflow might involve: helm install wordpress ...
helm upgrade wordpress ...
helm rollback wordpress ...
helm uninstall wordpress ... By treating Kubernetes applications as cohesive packages rather than isolated objects, Helm reduces administrative overhead and simplifies application management. This package and release management approach allows you to focus on application development rather than micromanaging individual Kubernetes objects. For further reading, consider visiting the Kubernetes Documentation and learning more about Helm . Watch Video Watch video content"
Certified Kubernetes Application Developer CKAD,Solution API VersionsDeprecations,https://notes.kodekloud.com/docs/Certified-Kubernetes-Application-Developer-CKAD/Security/Solution-API-VersionsDeprecations,"Certified Kubernetes Application Developer - CKAD Security Solution API VersionsDeprecations In this lesson, you'll learn how to address API versioning and deprecation issues in Kubernetes. We'll cover how to find resource short names, determine API groups, discover preferred API versions, and enable deprecated API endpoints. Follow along with the steps and code blocks below. Short Names for Resources First, identify the short names for key resources such as Deployments, ReplicaSets, CronJobs, and Custom Resource Definitions (CRDs). Run the following command: kubectl api-resources Review the output to locate the following mappings: Deployments: deploy ReplicaSets: rs CronJobs: cj Custom Resource Definitions: crd (or crds ) An excerpt from the command output may look like this: deployments                deploy
replicasets                rs
cronjobs                   cj
customresourcedefinitions  crd, crds Quick Tip Use the output of kubectl api-resources to quickly reference the abbreviated names in your commands. Determining the API Group for a Resource To determine which API group a resource belongs to—for example, the Job resource—execute: kubectl explain job At the top of the output, you'll find details similar to: KIND: Job
VERSION: batch/v1 This indicates that the Job belongs to the batch API group (the section before the slash) and is using version v1 . Finding the Preferred Version for a Specific API Group Next, find the preferred version for the authorization.k8s.io API group by following these steps: Start a Local API Proxy: Run the following command to create a local proxy to the Kubernetes API server: kubectl proxy --port=8001 & Query the API Group Details: With the proxy running in the background, use curl to fetch the API group's details: curl localhost:8001/apis/authorization.k8s.io The JSON response will include a section for the preferred version: {
   ""kind"": ""APIGroup"",
   ""apiVersion"": ""v1"",
   ""name"": ""authorization.k8s.io"",
   ""versions"": [
      {
         ""groupVersion"": ""authorization.k8s.io/v1"",
         ""version"": ""v1""
      }
   ],
   ""preferredVersion"": {
      ""groupVersion"": ""authorization.k8s.io/v1"",
      ""version"": ""v1""
   }
} This confirms that the preferred version for authorization.k8s.io is v1 . Enabling the v1alpha1 Version for RBAC.authorization.k8s.io To enable the v1alpha1 version for the RBAC.authorization.k8s.io API group on the control plane node, perform the following steps: Backup the Current Manifest: The kube-apiserver manifest is located at /etc/kubernetes/manifests/kube-apiserver.yaml . Back it up with: cp /etc/kubernetes/manifests/kube-apiserver.yaml /root/kube-apiserver.yaml.backup Modify the Manifest: Open the manifest file in your preferred text editor. Scroll down to the section containing command-line arguments and add the following flag at the bottom of the list: --runtime-config=rbac.authorization.k8s.io/v1alpha1 Below is an excerpt from the modified section: --etcd-key-file=/etc/kubernetes/pki/apiserver-etcd-client.key
--etcd-servers=https://127.0.0.1:2379
--kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt
--kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key
--kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
--proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt
--proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key
--requestheader-allowed-names=front-proxy-client
--requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt
--requestheader-extra-headers-prefix=X-Remote-Extra-
--requestheader-group-headers=X-Remote-Group
--requestheader-username-headers=X-Remote-User
--secure-port=6443
--service-account-issuer=https://kubernetes.default.svc.cluster.local
--service-account-signing-key-file=/etc/kubernetes/pki/sa.pub
--service-cluster-ip-range=10.96.0.0/12
--tls-cert-file=/etc/kubernetes/pki/apiserver.crt
--tls-private-key-file=/etc/kubernetes/pki/apiserver.key
--runtime-config=rbac.authorization.k8s.io/v1alpha1
image: k8s.gcr.io/kube-apiserver:v1.23.0
imagePullPolicy: IfNotPresent
livenessProbe:
  failureThreshold: 8
  httpGet:
    host: 10.69.230.9
    path: /livez
    port: 6443
    scheme: HTTPS
  initialDelaySeconds: 10 Verify the Update: After saving the changes, the kube-apiserver pod will automatically restart. Confirm its status by running: kubectl get pod -n kube-system Initially, the API server pod might display as Pending , but it should soon change to Running . Caution Always back up your kube-apiserver manifest before making any modifications to ensure you can revert changes if needed. Installing the kubectl-convert Plugin The kubectl-convert plugin is a versatile tool to convert manifest files between different API versions. Follow these steps to install it on the control plane node: Download the Plugin: Retrieve the binary using the commands below: curl -LO ""https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl-convert""
curl -LO ""https://dl.k8s.io/release/stable.txt"" | sha256sum --check Make the Binary Executable and Move It: Change its permissions and move it to /usr/local/bin with these commands: chmod +x kubectl-convert
mv kubectl-convert /usr/local/bin/ Verify the Installation: Execute the help command to ensure the plugin is installed correctly: kubectl convert --help The help output confirms the successful installation of the kubectl-convert plugin. Converting an Ingress Manifest Finally, update an existing Ingress manifest from the deprecated API version ( v1beta1 ) to the current networking.k8s.io/v1 . Follow these steps: Convert the Manifest: The old ingress manifest is located at /root/ingress-old.yaml . Convert the API version using the following command: kubectl convert -f ingress-old.yaml --output-version networking.k8s.io/v1 > ingress-new.yaml This command creates a new file named ingress-new.yaml with the updated API version. Apply the New Manifest: Deploy the updated Ingress configuration by running: kubectl apply -f ingress-new.yaml If successful, you should see an output similar to: ingress.networking.k8s.io/ingress-space created Congratulations! You have successfully updated your Kubernetes resources and managed API deprecations. For further reading, check out Kubernetes Documentation and explore related topics such as Kubernetes Basics . Enjoy your journey with Kubernetes! Watch Video Watch video content"
Docker Certified Associate Exam Course,Demo Docker Engine Setup CentOS,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine/Demo-Docker-Engine-Setup-CentOS,"Docker Certified Associate Exam Course Docker Engine Demo Docker Engine Setup CentOS Learn how to quickly install and configure Docker Engine (Community Edition) on a CentOS 7 server. This guide covers uninstalling old packages, setting up the official Docker repository, installing Docker CE, and verifying your installation. Note Always refer to the official Docker documentation for the most up-to-date installation instructions. Prerequisites Requirement Details Operating System CentOS 7 (x86_64) User Account A non-root user with sudo privileges Enabled Repository CentOS Extras Server Access SSH access to your instance or VM Log in to your server: ssh centos@docker-centos 1. Remove Older Docker Versions Before installing Docker CE, remove any legacy packages to prevent conflicts: sudo yum remove -y docker \
                  docker-client \
                  docker-client-latest \
                  docker-common \
                  docker-latest \
                  docker-logrotate \
                  docker-engine Verify your enabled repos: sudo yum repolist You should see base , extras , and updates in the list. Warning Removing old Docker packages will not delete your images or containers stored under /var/lib/docker , but it’s a good idea to back up any critical data before proceeding. 2. Install Dependencies & Configure the Docker Repository Install yum-utils , which provides the yum-config-manager utility: sudo yum install -y yum-utils Add the official Docker CE repository: sudo yum-config-manager \
  --add-repo \
  https://download.docker.com/linux/centos/docker-ce.repo Confirm the new repo is enabled: sudo yum repolist | grep docker-ce You should see an entry similar to docker-ce-stable/x86_64 . 3. Install Docker Engine (Docker CE) Install Docker Engine and its core components: sudo yum install -y docker-ce docker-ce-cli containerd.io Verify that the Docker packages are installed: sudo rpm -qa | grep -i docker Expected packages in the output: docker-ce docker-ce-cli containerd.io 4. Start and Enable the Docker Service Check the Docker service status: systemctl status docker If it’s not running, start and enable it to launch on boot: sudo systemctl start docker
sudo systemctl enable docker Re-check to ensure Docker is active: systemctl status docker 5. Verify Your Docker Installation Check Docker version: sudo docker --version Sample output: Docker version 19.03.13, build 4484c46d9d View detailed client/server info: sudo docker version Display full system information: sudo docker system info When you see information about the Engine, containerd, runc, and your host environment, Docker is installed and running correctly. Congratulations! You have successfully installed and configured Docker Engine on CentOS 7. Links and References Docker Get Started Guide Docker Engine Overview CentOS Linux Documentation Watch Video Watch video content"
Docker Certified Associate Exam Course,Docker Engine Architecture,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine/Docker-Engine-Architecture,"Docker Certified Associate Exam Course Docker Engine Docker Engine Architecture In this article, we dive into Docker Engine architecture, exploring its core components, how it evolved from LXC to Libcontainer, and the standards defined by the Open Container Initiative (OCI). You’ll also learn about key Docker objects, the registry model, the container creation flow, and how to verify your Docker installation. Key Components Docker Engine consists of three primary parts that work together to build, ship, and run containers: Docker Daemon ( dockerd ) The background service that manages images, containers, networks, and volumes on your host. REST API A set of HTTP endpoints that expose the daemon’s functionality to clients and automation tools. Docker CLI ( docker ) The command-line interface that sends commands to the REST API. From LXC to Libcontainer When Docker launched in 2013, it used Linux Containers (LXC) to isolate processes via namespaces and cgroups. By version 0.9, Docker introduced Libcontainer , a Go library that interfaces directly with kernel primitives—eliminating the LXC dependency and simplifying container management. The Open Container Initiative (OCI) Before 2015, container formats and runtimes were fragmented. Docker, CoreOS, and other industry leaders formed the Open Container Initiative (OCI) to standardize: Runtime Specification Defines lifecycle operations ( create , start , delete , etc.). Image Specification Specifies how container images are formatted and distributed. With these standards, Docker Engine 1.11 was refactored into modular components: runC The OCI-compliant runtime that handles low-level container operations. containerd A daemon responsible for managing runC instances, image transfer, and storage. containerd-shim Allows containers to keep running independently of containerd, ensuring resilience if the daemon restarts. Core Docker Objects Docker Engine manages four primary resource types: Object Description Images Read-only templates composed of layered filesystem snapshots and metadata. Containers Instances of images providing a writable layer and running processes. Networks Virtual networks enabling container-to-container and external communication. Volumes Persistent storage volumes decoupled from container lifecycles. Docker Registry A registry is a service for storing and distributing Docker images: Docker Hub (default public registry) Private Registry (self-hosted) Docker Trusted Registry (DTR) (enterprise-grade, on-premises) Container Creation Flow When you run docker run , Docker follows a series of steps: CLI to API The Docker CLI translates your command into a REST API call. Daemon Processing The daemon checks for the image locally or pulls it from the registry. containerd Converts the image into an OCI bundle. containerd-shim Hands off the bundle to runC and monitors the container’s lifecycle. runC Uses kernel namespaces and cgroups to spawn and isolate the container. Example: docker container run -it ubuntu Verifying Your Installation After installing Docker on CentOS or Ubuntu, confirm that everything is set up correctly: docker version Sample output: Client: Docker Engine - Community
 Version:           19.03.5
 API version:       1.40
 Go version:        go1.12.12

Server: Docker Engine - Community
 Engine:
  Version:          19.03.5
  API version:      1.40 (minimum version 1.12)
 containerd:
  Version:          1.2.10
 runc:
  Version:          1.0.0-rc8+dev Check the CLI version: docker --version
# Docker version 19.03.5, build 633a0ea And get system-wide details: docker system info Sample excerpt: Server:
 Containers: 0
 Running: 0
 Images: 0
 Server Version: 19.03.5
 Storage Driver: overlay2 References Docker Official Documentation Open Container Initiative Docker Hub Watch Video Watch video content"
Docker Certified Associate Exam Course,Section Introduction,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine/Section-Introduction,"Docker Certified Associate Exam Course Docker Engine Section Introduction Welcome to our in-depth exploration of Docker Engine. In this lesson, you’ll gain a comprehensive understanding of: Docker Engine architecture Installation steps and daemon configuration Container and image lifecycle management Storage backends and volume handling Networking models and security best practices Prerequisites You should be comfortable with basic Docker concepts covered in the Beginner’s Docker Course before proceeding. Key Learning Outcomes Topic Description Docker Engine Architecture How the client, daemon, and registry interact Installation & Daemon Configuration Installing Docker Engine on Linux, Windows, macOS, and tuning Container & Image Management Building, tagging, and pushing images; running and managing containers Storage & Volume Strategies Local volumes, bind mounts, and advanced storage drivers Networking & Security Bridge, overlay, MACVLAN networks, and securing containers Advanced Concepts Once you’ve mastered the fundamentals, we’ll dive deeper into: Restart policies and container troubleshooting Customizing the Docker Daemon ( daemon.json ) and choosing logging drivers Crafting optimized, minimal images using build context, cache layers, and Multi-Stage Builds Designing robust networking topologies and scalable storage solutions Let’s get started on building a rock-solid Docker Engine foundation! Links and References Docker Engine Documentation Docker CLI Reference Understanding Docker Storage Drivers Watch Video Watch video content"
Docker Certified Associate Exam Course,Basic Container Operations,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine/Basic-Container-Operations,"Docker Certified Associate Exam Course Docker Engine Basic Container Operations Master the essentials of Docker CLI to create, manage, and troubleshoot containers efficiently. This guide covers key commands, options, and best practices for container operations. Table of Contents Quick Syntax Reference Command Syntax Styles Creating a Container Listing Containers Starting a Container Create & Start in One Step Ephemeral Containers Interactive Shells Exiting Containers Naming Containers Detached Mode Links and References Quick Syntax Reference Use this general pattern for Docker commands: docker [object] [command] [options] [arguments] Examples: docker image ls
docker container run -it ubuntu
docker image build .
docker container attach <container>
docker container kill <container> Command Syntax Styles Docker supports both legacy and grouped syntax. We’ll use the grouped style throughout this guide. Syntax Type Example Grouped (new) docker container run -it ubuntu Legacy (old) docker run -it ubuntu Creating a Container To create (but not start) a container: docker container create httpd Sample output: Unable to find image 'httpd:latest' locally
latest: Pulling from library/httpd
…
Status: Downloaded newer image for httpd:latest
36a391532e10d45f772f2c9430c2cc38dad4b441aa7a1c444d59f6fa3d78c6b6 On Linux, Docker stores images and container metadata under /var/lib/docker/ : ls /var/lib/docker/
# builder  buildkit  containers  image  network  overlay2  plugins  runtimes  swarm  tmp  trust  volumes Each container’s data lives in containers/<container-ID>/ . Listing Containers Use docker container ls with options to filter the output: Command Description docker container ls Show running containers docker container ls -a Show all containers (running & stopped) docker container ls -l Show the most recently created container docker container ls -q Display only container IDs (running) docker container ls -aq Display all container IDs docker container ls -a Sample output: CONTAINER ID   IMAGE     COMMAND              CREATED         STATUS                     NAMES
36a391532e10   httpd     ""httpd-foreground""   2 minutes ago   Created                    charming_wiles Docker assigns a random human-readable name if you don’t provide one. Starting a Container Start an existing container by its ID or name: docker container start 36a391532e10
docker container ls Create & Start in One Step Pull the image, create, and start the container: docker container run httpd Ephemeral Containers Some images (like ubuntu ) don’t run a persistent process: docker container run ubuntu
docker container ls -a Output: CONTAINER ID   IMAGE    COMMAND     CREATED         STATUS                     NAMES
d969ecdb44ea   ubuntu   ""/bin/bash"" 2 minutes ago   Exited (0) 2 minutes ago   intelligent_almeida Once the primary process exits, the container stops. Interactive Shells Keep STDIN open and allocate a pseudo-TTY with -it : docker container run -it ubuntu Inside the container: root@6caba272c8f5:/# hostname
6caba272c8f5 On the host: docker container ls Shows: CONTAINER ID   IMAGE    COMMAND      CREATED          STATUS              NAMES
6caba272c8f5   ubuntu   ""/bin/bash""  About a minute   Up About a minute   quizzical_austin Tip Always place options ( -i , -t , -d , --name ) before the image name. Anything after the image name is interpreted as the container’s command. Exiting Containers Running exit inside an interactive shell stops the container: docker container run -it ubuntu
root@6caba272c8f5:/# exit
exit
docker container ls -a Naming Containers Assign a custom name at creation: docker container run -itd --name webapp ubuntu
docker container ls -l Output: CONTAINER ID   IMAGE    COMMAND      CREATED          STATUS             NAMES
59aa5eacd88c   ubuntu   ""/bin/bash""  20 seconds ago   Up 19 seconds      webapp Rename an existing container: docker container rename intelligent_almeida webapp2 Detached Mode Run containers in the background with -d : docker container run -d httpd Sample output: 11cbd7fe7e65a9da453e159ed0fe163592dcc8a7845abc91b8305c78f50ac70 To reattach: docker container attach 11cbd7fe7e65 A unique ID prefix is sufficient if it’s unambiguous. Links and References Docker CLI Reference Docker Container Commands Docker Official Documentation Watch Video Watch video content"
Docker Certified Associate Exam Course,Inspecting a Container,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine/Inspecting-a-Container,"Docker Certified Associate Exam Course Docker Engine Inspecting a Container In this guide, you’ll learn how to explore and troubleshoot Docker containers by using built-in inspection commands. We’ll cover: Listing containers Retrieving detailed JSON metadata Monitoring live resource usage Viewing in-container processes Fetching and streaming logs Streaming Docker system events 1. List Running Containers Before diving deeper, get a quick overview of containers on your host: docker container ls You can extend this with flags: Command Description docker container ls Show running containers docker container ls -a Include stopped containers docker container ls --filter Filter by status, name, label, etc. 2. Inspect Container Details To view in-depth information—configuration, network settings, volumes—use: docker container inspect <container_name_or_ID> This outputs a JSON array. Example: [
  {
    ""Id"": ""59aa5eacd88c42970754cd6005ce315944a2efcd32288df998b29267ae54c152"",
    ""Created"": ""2020-01-14T13:23:01.225868339Z"",
    ""Path"": ""/bin/bash"",
    ""Args"": [],
    ""State"": {
      ""Status"": ""running"",
      ""Running"": true,
      ""Paused"": false,
      ""Restarting"": false,
      ""IPAddress"": ""172.17.0.5"",
      ""IPPrefixLen"": 16,
      ""MacAddress"": ""02:42:ac:11:00:05""
    }
  }
] JSON Field Description Id Unique container identifier Created Timestamp when the container was instantiated Path Entrypoint command executed in the container Args Arguments passed to the entrypoint State Current runtime status and networking information Note Use -f json to pipe this output into jq or other JSON parsers for selective querying. 3. Monitor Resource Usage Docker can stream real-time metrics—CPU, memory, network I/O, block I/O—across all running containers: docker container stats Sample output: CONTAINER ID   NAME           CPU %     MEM USAGE / LIMIT   MEM %   NET I/O        BLOCK I/O
59aa5eacd88c   webapp         50.00%    400KiB / 989.4MiB   0.04%   656B / 0B      0B / 0B Note The stats command runs continuously. Press <kbd>Ctrl+C</kbd> to stop the stream. 4. List Processes Inside a Container Identify which processes are consuming resources within a specific container: docker container top webapp Example: UID     PID    PPID  C STIME TTY TIME     CMD
root    17001  16985 0 13:23 ?   00:00:00 stress This shows the stress process (host PID 17001) running inside webapp . 5. Fetch and Stream Container Logs To retrieve application logs: docker container logs <container_name_or_ID> Follow logs in real time with: docker container logs -f <container_name_or_ID> Warning If your logs are large, consider limiting output with options like --since or --tail to avoid overwhelming your terminal. 6. Stream Docker Events Docker records events for containers, networks, volumes, and more. To see recent events—for example, within the last hour—run: docker system events --since 60m Sample output: 2020-01-14T18:30:30Z network connect ... (container=68649c8b..., name=bridge)
2020-01-14T18:30:30Z container start ... (image=ubuntu, name=casethree) All resource lifecycle events can be retrieved using docker system events . Summary of Inspection Commands Command Purpose docker container ls List active containers docker container inspect <id> Show detailed JSON metadata docker container stats Stream live resource usage docker container top <name> List processes inside the container docker container logs <id> Retrieve container logs docker container logs -f <id> Follow logs in real time docker system events --since 60m Stream Docker engine events from last hour Links and References Docker Inspect Documentation Docker Stats Documentation Docker Logs Documentation Docker Events Documentation Watch Video Watch video content"
Docker Certified Associate Exam Course,Exam Details,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Introduction/Exam-Details,"Docker Certified Associate Exam Course Introduction Exam Details In this lesson, we’ll dive into the structure, question formats, and logistics of the Docker Certified Associate (DCA) exam. You’ll learn how the exam is organized, what to expect on test day, and where to find official resources. Exam Structure Total Questions: 55 Duration: 90 minutes Passing Score: 55% (subject to change) Question Formats The DCA exam uses two distinct question types: Format Description Standard Multiple Choice Select one correct answer from a list of options. Discrete Option Multiple Choice (DOMC) Each option appears one at a time. Respond “Yes” or “No” to indicate whether the displayed choice is correct. Note DOMC questions end immediately upon an incorrect response. Build a strong foundation in Docker networking, storage, and security to tackle these efficiently. Example: Standard Multiple Choice Select the correct answer and click Submit . In contrast, a DOMC question shows one option at a time. If you mark it correctly, you move on; a wrong answer ends the question immediately. Remote Testing Requirements You can take the DCA exam from home under online proctoring. Ensure the following: A private, walled room with no interruptions A webcam-enabled laptop A stable internet connection Warning Your testing environment must be free of unauthorized materials. Proctors may pause or terminate your exam if requirements aren’t met. Frequently Asked Questions Question Answer Can I take the exam remotely? Yes—online proctoring is available from your home or office, provided you meet the environment requirements. What is the exam fee? USD 195 (subject to change). What is the passing score? 55% or higher (subject to change). When will I get my results? Results are available immediately after you complete the exam. Links and References Mirantis DCA Certification Exam Docker Official Documentation Docker Certified Associate Program Watch Video Watch video content"
Docker Certified Associate Exam Course,Exam Course Tips,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Introduction/Exam-Course-Tips,"Docker Certified Associate Exam Course Introduction Exam Course Tips Before diving into hands-on labs and mock questions, here’s an overview to help you prepare for the Docker Certified Associate exam. If certification isn’t your goal, skip ahead to Section 1 . The DCA exam is multiple-choice, so you should: Build a solid conceptual foundation through clear explanations and demos. Practice commands and workflows in a real or virtual Docker lab. Familiarize yourself with the official Docker documentation . 1. Lab Setup Reinforce your learning with a personal Docker environment: Follow our Docker for Beginners course to run Docker-in-Docker labs. Spin up cloud-based demos on AWS (using free-tier credits). Use an instant online playground such as Katacoda . In most scenarios, the Docker CLI alone is enough to explore commands and flags. 2. Mastering the Documentation While the DCA exam is closed-book, being adept at navigating Docker’s documentation will speed up your workflow in real projects. We’ve created research questions to guide your reading: Multiple-choice format for true exam-style practice. Open-book: consult your notes, the docs, or your lab. Direct links to relevant documentation pages for quick lookup. Example option list in a research question: docker
dockerd
docker-engine
docker --start-engine These exercises push you to explore command behaviors and flags beyond the videos, building a habit of verifying your understanding. 3. Revision Strategy We’ve built revision checkpoints throughout the course: Stage Format Purpose Research Questions Open-book, multiple-choice Practice lookup and exam-style questioning Section Practice Tests Closed-book Assess recall and concept retention Mock Exams DMC & MCQ Simulate the real DCA exam environment This cycle ensures continuous engagement with multiple-choice questions and hands-on application. 4. Planning Your Study Schedule A balanced study plan avoids burnout while ensuring coverage of all topics: Daily Study Time Estimated Duration 2 hours ~3 months 4 hours ~2 months 6+ hours ~1 month Note You’ll find a detailed breakdown of estimated study hours per major section in the supplementary materials. If you’re already familiar with Kubernetes (e.g., CKAD ), you can reduce your study time for container orchestration topics. 5. Types of Exam Questions The DCA exam covers: Core Docker commands and common options Default file paths and configuration locations Reading and interpreting Dockerfiles, Compose files, stack files, and YAML manifests For instance, you might encounter a question about this Kubernetes Service manifest: apiVersion: v1
kind: Service
metadata:
  name: dca
spec:
  type: ClusterIP
  selector:
    app: nginx
  ports:
    - port: 8080
      targetPort: 80
    - port: 4443
      targetPort: 443 You’ll need to explain its purpose or adapt it for Docker Swarm or Compose scenarios. That’s all for the DCA exam tips. Good luck with your Docker certification journey! See you in the first lecture. Links and References Docker Documentation Katacoda Docker Scenarios CKAD Certification Watch Video Watch video content"
Docker Certified Associate Exam Course,Demo Docker Engine Setup Ubuntu,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine/Demo-Docker-Engine-Setup-Ubuntu,"Docker Certified Associate Exam Course Docker Engine Demo Docker Engine Setup Ubuntu In this step-by-step guide, you’ll learn how to install and configure the Docker Engine on an Ubuntu host. By the end of this tutorial, your Ubuntu VM will be ready to run containers and support your Docker-based workflows. Prerequisites A machine running Ubuntu 16.04 or newer (this example uses Ubuntu 18.04 “Bionic Beaver” on x86_64 ). A user account with sudo privileges. ubuntu@dockerubuntu:~$ cat /etc/lsb-release
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=18.04
DISTRIB_CODENAME=bionic
DISTRIB_DESCRIPTION=""Ubuntu 18.04.5 LTS""
ubuntu@dockerubuntu:~$ uname -m
x86_64 Note Make sure your system is up to date before proceeding. Run sudo apt-get update && sudo apt-get upgrade if you haven’t recently updated your packages. 1. Remove Old Docker Versions If you have any legacy Docker packages installed, remove them to avoid conflicts: sudo apt-get remove docker docker-engine docker.io containerd runc 2. Install Required Packages Docker’s repository requires HTTPS transport, certificate management, and command-line tools. Update the package index and install these dependencies: sudo apt-get update
sudo apt-get install \
  apt-transport-https \
  ca-certificates \
  curl \
  gnupg-agent \
  software-properties-common Prerequisite Packages Package Purpose apt-transport-https Allows apt to use repositories over HTTPS ca-certificates Provides SSL certificates for HTTPS curl Downloads files from the command line gnupg-agent Manages OpenPGP keys software-properties-common Adds and manages apt repositories 3. Add Docker’s Official GPG Key Import Docker’s GPG key to verify package signatures: curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add - Verify the fingerprint to ensure integrity: sudo apt-key fingerprint 0EBFCD88 Warning If the fingerprint does not match 9DC8 5822 9FC7 DD38 854A E2D8 8D81 803C 0EBF CD88 , do not continue. Verify your network and retry the key import. 4. Set Up the Docker APT Repository Add Docker’s stable repository to your system: sudo add-apt-repository \
  ""deb [arch=amd64] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) \
  stable"" 5. Install Docker Engine Refresh the package index and install Docker Engine, CLI, and containerd: sudo apt-get update
sudo apt-get install docker-ce docker-ce-cli containerd.io 6. Verify the Installation List installed Docker packages: dpkg -l | grep -i docker Check that the Docker service is active and enabled at boot: sudo systemctl status docker 7. View Docker Version and System Information Display the Docker client version: docker --version Show detailed version information for both client and server: docker version Inspect system-wide Docker details (containers, images, drivers, plugins): docker info Congratulations! You now have Docker Engine installed and running on your Ubuntu host. Use this environment to build, ship, and run containerized applications. Links and References Docker Engine Documentation Ubuntu Official Site Docker Installation Tutorials Watch Video Watch video content"
Docker Certified Associate Exam Course,Stopping and Removing a Container,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine/Stopping-and-Removing-a-Container,"Docker Certified Associate Exam Course Docker Engine Stopping and Removing a Container In this guide, we’ll explore how to manage Docker containers by stopping, pausing, resuming, and removing them. You’ll also learn how Linux signals map to Docker commands, plus best practices for cleaning up containers to reclaim resources. 1. Linux Signals Refresher Linux processes respond to signals for control and shutdown. Below is a quick overview: Signal Action Description SIGSTOP Pause Suspends the process, cannot be caught. SIGCONT Resume Continues a paused process. SIGTERM Graceful shutdown Allows process cleanup before exit. SIGKILL Forceful kill Immediately terminates; cannot be trapped. # Pause Apache httpd
kill -SIGSTOP 11663

# Resume it
kill -SIGCONT $(pgrep httpd)

# Graceful shutdown
kill -SIGTERM $(pgrep httpd)

# Forceful kill
kill -SIGKILL $(pgrep httpd)
# shorthand
kill -9 $(pgrep httpd) Note SIGTERM is the preferred way to stop a process because it allows cleanup. SIGKILL should be used only if the process does not terminate gracefully. 2. Docker Container Equivalents Docker uses similar primitives at the container level. The table below shows the mappings: Action Linux Signal Docker Command Pause SIGSTOP/SIGCONT docker pause / docker unpause Stop SIGTERM → SIGKILL docker stop Kill SIGKILL docker kill --signal=SIGKILL Remove — docker rm 2.1 Running an HTTPD Container docker run --name web httpd 2.2 Pause and Resume docker pause web
docker unpause web 2.3 Stop (SIGTERM then SIGKILL) docker stop web Docker sends SIGTERM , waits the default 10 seconds, then sends SIGKILL if the container is still running. 3. Sending Custom Signals You can target any signal to a container’s main process: # Send SIGKILL by name
docker kill --signal=SIGKILL web

# Or by number
docker kill --signal=9 web 4. Removing a Container Containers must be stopped before removal: docker stop web
docker rm web Attempting to remove a running container yields an error: $ docker rm web
Error response from daemon:
You cannot remove a running container ... Stop the container before attempting removal or use --force 5. Batch Stopping and Removing When managing multiple containers, leverage docker ps -q and docker ps -aq : # Stop all running containers
docker stop $(docker ps -q)

# Remove all containers (running & exited)
docker rm $(docker ps -aq) 6. Pruning Stopped Containers To delete all stopped containers and free disk space: docker container prune Warning docker container prune permanently deletes all stopped containers. There is no undoing this action. 7. Automatic Cleanup with --rm For ephemeral containers, use --rm to remove them automatically after exit: docker run --rm ubuntu expr 4 + 5
# Output: 9 This is ideal for one-off tasks, CI jobs, or simple shell commands. Links and References Docker CLI Commands Docker Pause Documentation Linux Signals Watch Video Watch video content"
Docker Certified Associate Exam Course,Docker Service Configuration,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine/Docker-Service-Configuration,"Docker Certified Associate Exam Course Docker Engine Docker Service Configuration Master the essentials of configuring the Docker daemon ( dockerd ) on Linux. This guide covers systemd management, foreground debugging, socket tuning, remote access, TLS security, and persistent configuration. Table of Contents Managing Docker with systemd Running the Daemon in Foreground Default Unix Socket Exposing the Daemon on TCP Securing the Daemon with TLS Persisting Configuration in daemon.json Flag vs Configuration File Conflicts References Managing Docker with systemd Use systemd to start, stop, and inspect the Docker service. By default, Docker is enabled to launch on boot. Command Description sudo systemctl start docker Start the Docker service sudo systemctl stop docker Stop the Docker service sudo systemctl restart docker Restart the service sudo systemctl status docker Show current status and logs sudo systemctl enable docker Enable docker at startup sudo systemctl disable docker Disable automatic startup Example status output: ● docker.service - Docker Application Container Engine
   Loaded: loaded (/lib/systemd/system/docker.service; enabled; vendor preset: enabled)
   Active: active (running) since Wed 2020-10-21 04:21:01 UTC; 3 days ago
     Docs: https://docs.docker.com
 Main PID: 4197 (dockerd)
    Tasks: 13
   Memory: 129.7M
      CPU: 9min 6.980s
   CGroup: /system.slice/docker.service
           └─4197 /usr/bin/dockerd -H fd:// -H tcp://0.0.0.0 --containerd=/run/containerd/containerd.sock Note If you make changes to /etc/docker/daemon.json , restart Docker with sudo systemctl restart docker to apply them. Running the Daemon in Foreground Troubleshoot or capture real-time logs by launching dockerd interactively. # Launch daemon in foreground
dockerd

# Enable debug logging
dockerd --debug Sample debug output: INFO[2020-10-24T08:29:00.331Z] Starting up
DEBU[2020-10-24T08:29:00.332Z] Listener created for HTTP on unix (/var/run/docker.sock)
DEBU[2020-10-24T08:29:00.333Z] Golang's threads limit set to 6930
WARN[2020-10-24T08:29:00.364Z] Your kernel does not support cgroup runtime Note Foreground mode is ideal for capturing logs in CI pipelines or debugging startup failures. Default Unix Socket By default, Docker listens on a Unix domain socket. This restricts access to local clients only: Socket path: /var/run/docker.sock Access: Local IPC (no remote connections) The Docker CLI uses this socket unless DOCKER_HOST is overridden. Exposing the Daemon on TCP To allow remote management, bind dockerd to both the Unix socket and a TCP port: dockerd \
  --host=unix:///var/run/docker.sock \
  --host=tcp://192.168.1.10:2375 On a remote client: export DOCKER_HOST=""tcp://192.168.1.10:2375""
docker ps Warning Port 2375 is unencrypted and unauthenticated. Exposing it publicly invites unauthorized access and potential malicious use. Only enable on secured networks or for testing. Securing the Daemon with TLS Encrypt and authenticate connections on port 2376 by enabling TLS: Generate CA, server, and client certificates. Place server.pem and serverkey.pem in a secure directory. Start dockerd with TLS options: dockerd \
  --host=unix:///var/run/docker.sock \
  --host=tcp://192.168.1.10:2376 \
  --tls=true \
  --tlscert=/var/docker/server.pem \
  --tlskey=/var/docker/serverkey.pem Clients must reference the CA and their own certs: docker --tlsverify \
  --tlscacert=ca.pem \
  --tlscert=client.pem \
  --tlskey=client-key.pem \
  -H=tcp://192.168.1.10:2376 info Note Using TLS ensures confidentiality, integrity, and authentication for remote Docker API calls. Persisting Configuration in daemon.json Avoid long startup flags by defining options in /etc/docker/daemon.json : {
  ""debug"": true,
  ""hosts"": [
    ""unix:///var/run/docker.sock"",
    ""tcp://192.168.1.10:2376""
  ],
  ""tls"": true,
  ""tlscert"": ""/var/docker/server.pem"",
  ""tlskey"": ""/var/docker/serverkey.pem""
} Then reload Docker: sudo systemctl restart docker Flag vs Configuration File Conflicts Mixing CLI flags and daemon.json entries can lead to startup errors: # Conflicting debug settings
dockerd --debug=false Error: unable to configure the Docker daemon with file /etc/docker/daemon.json:
the following directives are specified both as a flag and in the configuration file:
 debug: (from flag: false, from file: true) Resolution: Keep all overrides in one place—either CLI flags or the JSON file. References Docker Daemon Configuration Docker CLI Environment Variables Systemd Service Files Docker Security Best Practices Watch Video Watch video content"
Docker Certified Associate Exam Course,Interacting with a Running Container,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine/Interacting-with-a-Running-Container,"Docker Certified Associate Exam Course Docker Engine Interacting with a Running Container This article demonstrates how to manage Docker containers that are already running. You’ll learn how to exit, detach, execute commands, open interactive shells, and reattach to containers without interrupting critical processes. Table of Common Docker Container Operations Action Command Description Start interactive container docker run -it ubuntu Launches Ubuntu with an interactive TTY shell Exit container exit Terminates the shell and stops the container Detach without stopping Press Ctrl-p Ctrl-q Leaves the container running in the background Execute a one-off command docker exec CONTAINER COMMAND Runs a command inside a running container Open an interactive shell docker exec -it CONTAINER /bin/bash Starts a new shell session for troubleshooting Attach to the primary process docker attach CONTAINER Reconnects your terminal to the container’s main shell 1. Exiting an Interactive Container When you run a container with -i (interactive) and -t (TTY), typing exit in the shell will stop both the shell and the container: docker run -it ubuntu
root@6caba272c8f5:/# exit
exit Verify the container status: docker container ls -l
CONTAINER ID   IMAGE   COMMAND     CREATED         STATUS                      PORTS   NAMES
6caba272c8f5   ubuntu  ""/bin/bash"" 2 minutes ago   Exited (0) 10 seconds ago           loving_ritchie 2. Detaching Without Stopping To keep the container running after leaving the shell, press Ctrl-p followed by Ctrl-q : docker run -it ubuntu
root@b71f15d33b60:/#   # Press Ctrl-p then Ctrl-q Note The detach sequence Ctrl-p + Ctrl-q does not stop the container; it simply returns you to the host shell. Confirm the container is still running: docker container ls
CONTAINER ID   IMAGE   COMMAND     CREATED        STATUS       PORTS    NAMES
b71f15d33b60   ubuntu  ""/bin/bash"" 5 seconds ago  Up 3 seconds          goofy_bell 3. Executing a One-off Command To run a single command in an existing container, use docker exec : docker exec b71f15d33b60 hostname
b71f15d33b60 This outputs the container’s hostname (its short ID). 4. Opening an Interactive Shell When you need to troubleshoot or inspect the environment, start a new shell inside the container: docker exec -it b71f15d33b60 /bin/bash
root@b71f15d33b60:/# ps -ef
UID        PID  PPID  C STIME TTY          TIME CMD
root         1     0  0 12:00 pts/0    00:00:00 /bin/bash
root        10     1  0 12:01 pts/0    00:00:00 ps -ef

root@b71f15d33b60:/# tty
/dev/pts/0

root@b71f15d33b60:/# exit
exit 5. Attaching to a Running Container To reconnect to the container’s initial process (PID 1), use: docker attach b71f15d33b60
root@b71f15d33b60:/# Warning Exiting the primary shell (PID 1) will stop the container. To avoid unintentionally shutting it down, detach ( Ctrl-p Ctrl-q ) instead of exiting. References Docker Documentation docker exec reference Managing containers Watch Video Watch video content"
Docker Certified Associate Exam Course,Course Curriculum,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Introduction/Course-Curriculum,"Docker Certified Associate Exam Course Introduction Course Curriculum Note This curriculum is designed to align with the Docker Certified Associate (DCA) exam domains, ensuring comprehensive coverage of key Docker Engine, Swarm, Kubernetes, and Enterprise topics. Below is the full breakdown of modules and their primary focus areas: Module Focus Areas 1. Installation & Configuration System requirements, Engine & Swarm install, UCP/DTR, daemon config, CA auth 2. Image Management Dockerfile optimization, CLI image ops, registry deployment, best practices 3. Storage & Volumes Storage drivers, layer inspection, volumes, Kubernetes PV/PVC, StorageClass 4. Networking Network namespaces, bridge/overlay, port publishing, DNS, K8s Services/Ingress, Policies 5. Security Docker Content Trust, Engine & Swarm hardening, RBAC with UCP, image scanning 6. Orchestration Swarm services/stacks, Kubernetes Pods/Deployments/Services, scaling, rollouts 1. Installation & Configuration We begin with the foundation: Sizing & System Requirements : RAM, CPU, and OS compatibility Installing Docker Engine : Official packages, repository setup Swarm Setup : Initializing a Swarm cluster, manager/worker roles Docker Enterprise Components : UCP and DTR overview and deployment User & Team Management : Role-Based Access Control (RBAC) Daemon Configuration : JSON configs, daemon.json options, TLS certs Namespaces & cgroups : Resource isolation fundamentals Troubleshooting & DR : Backup strategies, disaster recovery planning 2. Image Management Master image lifecycle and optimization: Writing Efficient Dockerfiles Multi-Stage Builds & Build Cache best practices Docker CLI : docker push , pull , rmi , inspect Tagging & Layer Inspection : docker history , layer minimization Registry Operations : Deploying a private registry, searching, and cleanup 3. Storage & Volumes Understand how containers persist and share data: Storage Drivers : overlay2, aufs, aufs alternatives per OS Inspecting Image Layers on the host filesystem Volume Management : Creating, mounting, pruning unused volumes Kubernetes Storage : Persistent Volumes (PV), Persistent Volume Claims (PVC), Storage Classes 4. Networking Dive into container connectivity and service exposure: Linux Network Namespaces & CNM Built-in Drivers : bridge , host , overlay Port Publishing & External DNS setup Overlay Networks : Deploying multi-host services Network Troubleshooting : docker network inspect , packet captures Kubernetes Traffic Routing : Services, Ingress Controllers, Network Policies 5. Security Implement best practices to harden your container environment: Docker Content Trust & Image Signing Engine & Swarm Security : TLS, mutual auth, secret management Identity & RBAC with UCP managers and workers Image Scanning : Static analysis tools, vulnerability reports Integrating UCP with LDAP/AD for centralized auth 6. Orchestration Compare and contrast Docker Swarm with Kubernetes: Swarm Architecture : Managers, workers, Raft consensus Swarm Services & Stacks : Compose file v3, rolling updates Kubernetes Fundamentals : Pods, Deployments, ReplicaSets, Services Advanced K8s : Horizontal Pod Autoscaling, liveness/readiness probes Integration with Docker Enterprise : UCP scheduling, DTR-based images The curriculum is organized into four progressive sections: Docker Engine – Core installation, config, and basic operations Docker Swarm – Native orchestration model Kubernetes – Industry-standard orchestration Docker Enterprise – UCP/DTR integrations and advanced management Each technology module revisits installation, management, storage, networking, and security in context. To maximize your learning experience, we recommend: Completion of Docker for Beginners Completion of Kubernetes for Beginners If you’re already familiar with: Docker Swarm fundamentals Certified Kubernetes Application Developer (CKAD) you can breeze through the orchestration sections. Otherwise, all core concepts are introduced from the ground up. In the next lesson, we’ll start with Module 1: Docker Engine Installation & Configuration . Links and References Docker Documentation Kubernetes Documentation Docker Certified Associate Exam Domains Docker Content Trust Guide Watch Video Watch video content"
Docker Certified Associate Exam Course,Restart Policies,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine/Restart-Policies,"Docker Certified Associate Exam Course Docker Engine Restart Policies Docker restart policies let you control if and when a container is automatically restarted. Whether you’re running a critical web service or a batch job, these policies ensure your containers recover from failures or daemon restarts. Why Containers Stop A container can exit for several reasons: Normal completion The primary process finishes successfully (exit code 0), for example, a script completes its task. Failure The process crashes or throws an error, exiting with a non-zero code (e.g., bad input). Manual intervention Running docker container stop sends a SIGTERM, then a SIGKILL after a timeout: If the process traps SIGTERM and exits cleanly, it may return code 0. If it’s killed with SIGKILL, it usually exits with a non-zero code. When you need a container—say, a production API or CI runner—to restart immediately after a crash, Docker’s restart policies come into play. Docker Restart Policies Specify a restart policy with --restart when you run a container: Policy Behavior CLI Flag no (default) Never restart automatically. --restart=no on-failure Restart only if exit code ≠ 0. Optionally limit retries: on-failure[:<max-retries>] . --restart=on-failure:5 always Always restart regardless of exit status. If you manually stop the container, it will restart on daemon reboot. --restart=always unless-stopped Like always , but honors manual stops across daemon restarts. --restart=unless-stopped Note Use on-failure[:<max-retries>] to prevent infinite restart loops. Docker immediately retries with no backoff delay. Quick Reference no : No auto-restart. on-failure : Auto-restart only on errors (non-zero exit). always : Auto-restart on any exit. unless-stopped : Auto-restart on any exit, but not after a manual stop. Examples 1. Default ( no ) docker run --name test-no --restart=no ubuntu \
  expr 3 + 5
# Exits with 0; Docker does not restart. 2. On Failure docker run --name test-fail --restart=on-failure:3 ubuntu \
  expr three + 5
# Exits with 1; Docker retries up to 3 times, then stops. 3. Always docker run --name test-always --restart=always ubuntu \
  sleep 5 After sleep 5 finishes (exit 0), Docker restarts immediately. docker stop test-always prevents restart only until the next Docker daemon reboot. 4. Unless-Stopped docker run --name test-unless --restart=unless-stopped ubuntu \
  sleep 5 Behaves like always on crashes or normal exit. Honors manual docker stop even if the daemon restarts later. Live Restore of Containers By default, stopping the Docker daemon halts all containers. With live restore , containers remain running when the daemon is down. Edit or create /etc/docker/daemon.json : {
  ""live-restore"": true
} Restart the Docker service: sudo systemctl restart docker
# or
sudo systemctl reload docker Verifying Live Restore Without live restore: docker run --name web httpd
sudo systemctl stop docker
# Containers stop when daemon stops.
sudo systemctl start docker
docker ps
# web is not running With live restore enabled: docker run --name web httpd
sudo systemctl stop docker
# web stays running.
sudo systemctl start docker
docker ps
# web is still running Warning Live restore requires compatible Docker versions and proper permissions. Check the Docker daemon.json reference before enabling. Links and References Docker Restart Policy Documentation Docker Daemon Configuration Docker Live Restore Deep Dive Watch Video Watch video content"
Docker Certified Associate Exam Course,Demo Docker Container Operations,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine/Demo-Docker-Container-Operations,"Docker Certified Associate Exam Course Docker Engine Demo Docker Container Operations In this tutorial, we’ll cover essential Docker container lifecycle commands using the Docker CLI. You’ll learn how to create, start, list, interact with, monitor, and clean up containers. 1. Creating and Starting a Container Create a container from the official httpd image. If the image isn’t available locally, Docker pulls it from Docker Hub. docker container create httpd List containers (none are running yet): docker container ls Include stopped containers with -a : docker container ls -a Start the container by its CONTAINER ID: docker container start d52fad69ea76 Verify it’s running : docker container ls Table: docker container ls Field Descriptions Field Description CONTAINER ID Short 12-character container ID IMAGE Name of the image COMMAND Entrypoint command CREATED Timestamp when created STATUS Current state and uptime PORTS Exposed ports NAMES Auto-generated container name 2. Listing Options Use different flags with docker container ls to customize output: Flag Description -a Show all containers (running and stopped) -l Show the latest created container -q Only display numeric IDs of running containers -aq Display numeric IDs of all containers (all states) Example—list only IDs of running containers: docker container ls -q 3. Running Containers Interactively Combine create and start with run : docker container run -it ubuntu If ubuntu:latest isn’t local, you’ll see the pull progress, then a shell prompt: root@0afdaf794887:/# ps -ef
UID        PID PPID  C STIME TTY          TIME CMD
root         1     0  0 06:47 pts/0    00:00:00 /bin/bash
root         9     1  0 06:48 pts/0    00:00:00 ps -ef
root@0afdaf794887:/# exit Exiting the shell stops the container: docker container ls -l Detaching Without Stopping To leave a container running and return to the host shell, press Ctrl+P then Ctrl+Q : docker container run -it ubuntu
# Press Ctrl+P, Ctrl+Q
docker container ls Note Detaching this way leaves the container running in the background. 4. Executing Commands in Running Containers Run additional commands inside an active container using exec : docker container exec -it 9fe83b47dc1f /bin/bash
root@9fe83b47dc1f:/# ps -ef You can even use partial container IDs: docker container exec -it 65 cat /etc/lsb-release 5. Attaching to a Container Use attach to connect to the primary process of a running container: docker container attach <container_id>
# then:
exit  # This will stop the container Warning Exiting an attached session ( exit ) will terminate the container’s main process. 6. Stopping and Removing Containers Stop a running container: docker container stop 14fc5c1661f9 Remove a stopped container: docker container rm 14fc5c1661f9 Prune all stopped containers: docker container prune Warning docker container prune removes all stopped containers. Use with caution. Or combine stop and remove for all containers: docker container stop $(docker container ls -q)
docker container rm $(docker container ls -aq) 7. Detached Mode and Naming Run in detached mode ( -d ) and assign a custom name: docker container run -itd --name=kodekloud ubuntu Rename an existing container: docker container rename kodekloud yogish-codecloud 8. Inspecting Container Details Retrieve full metadata with inspect : docker container inspect yogish-codecloud Sample output: [
  {
    ""Id"": ""5c2b2b5fc32f..."",
    ""Created"": ""2020-05-04T07:04:13.230760175Z"",
    ""Path"": ""/bin/bash"",
    ""Args"": [],
    ""State"": {
      ""Status"": ""running"",
      ""Running"": true,
      ""Paused"": false,
      ""Restarting"": false,
      ""OOMKilled"": false,
      ""Dead"": false,
      ""Pid"": 14776,
      ""ExitCode"": 0,
      ""StartedAt"": ""2020-05-04T07:04:13.598111123Z"",
      ""FinishedAt"": ""0001-01-01T00:00:00Z""
    },
    ""Image"": ""sha256:1d622ef86b1..."",
    ""Name"": ""/yogish-codecloud"",
    ""Driver"": ""overlay2""
  }
] 9. Monitoring Containers 9.1 Resource Usage Display real-time stats: docker container stats 9.2 Process List Show host PIDs inside a container: docker container top reverent_hopper 9.3 Logs View past logs: docker container logs d52fad69ea76 Follow logs live: docker container logs -f d52fad69ea76 Conclusion You’ve now mastered: Creating, starting, and listing containers Interactive sessions with run , exec , and attach Naming, renaming, and inspecting container details Monitoring resource usage and logs Cleaning up with stop , rm , and prune For more commands, see the Docker CLI reference . Watch Video Watch video content"
Docker Certified Associate Exam Course,Setting a Container Hostname,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine/Setting-a-Container-Hostname,"Docker Certified Associate Exam Course Docker Engine Setting a Container Hostname When you start a Docker container, you often assign it a name with --name , but this doesn’t change the hostname inside the container. Understanding the difference between a container’s name and its hostname is crucial when your application uses hostname-based logic for logging, inter-service communication, or constructing URLs. Container Name vs Hostname Option Scope Affects --name <name> Docker Engine User-friendly container identifier at the CLI --hostname <name> Container OS The hostname returned by hostname inside the container Note The container name and hostname serve different purposes. Some applications generate logs or metrics based on the hostname, so setting it appropriately simplifies debugging and monitoring. Default Hostname Behavior By default, Docker sets the hostname to the short version of the container’s unique ID. For example: docker container run -it --name webapp ubuntu Inside that container, checking the hostname shows the truncated ID: root@3484d738:/# hostname
3484d738 Here, 3484d738 is the container ID—not the friendly webapp name you provided. Overriding the Hostname To assign a meaningful hostname inside your container, use the --hostname (or -h ) flag. This helps when services rely on consistent hostnames: docker container run -it \
  --name webapp \
  --hostname webapp \
  ubuntu Now, the hostname command returns your custom name: root@webapp:/# hostname
webapp Your application can now reference a predictable, human-readable hostname. References Docker Run Reference Docker Container CLI Watch Video Watch video content"
Docker Certified Associate Exam Course,Demo Docker Container Operations Continued,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine/Demo-Docker-Container-Operations-Continued,"Docker Certified Associate Exam Course Docker Engine Demo Docker Container Operations Continued In this lesson, we’ll explore advanced Docker flags and best practices for managing containers. You’ll learn how to: Automatically clean up ephemeral containers Customize container hostnames Control container restarts with various policies Inspect Docker events for troubleshooting Copy files between host and containers Expose ports using random and static mappings Automatically Remove Ephemeral Containers with --rm When running short-lived or CI/CD containers, you can use --rm to automatically delete them upon exit. This helps keep your environment clean and free of stale containers. docker container run -itd --name=kodekloud --rm ubuntu
docker container ls -l   # Lists only the most recent container Stop and watch it disappear: docker container stop kodekloud Note Using --rm is ideal for one-off or CI tasks. Remember that you won’t be able to inspect logs or the filesystem after the container exits. Setting Container Name vs. Hostname Docker allows you to assign both a unique container name and an internal hostname: docker container run -itd \
  --name=yogesh \
  --hostname=kodekloud \
  --rm ubuntu

docker container ls -l
docker exec -it yogesh hostname Container names must be unique per host, but you can reuse the same internal hostname across multiple containers. Restart Policies Docker’s --restart flag offers fine-grained control over container uptime. Below is a summary of each policy: Policy Description no (default) Container will not restart after exit. on-failure[:N] Restart only on non-zero exit status. Optionally limit retries to N . always Always restart the container regardless of exit code or Docker daemon restarts. unless-stopped Like always , but the container won’t restart if you manually stopped it. 1. --restart=no (Default) docker container run -itd --name=caseone --restart=no ubuntu
docker container stop caseone
docker container ls -l The container will remain stopped until you manually start it again. 2. --restart=on-failure docker container run -itd --name=casetwo --restart=on-failure ubuntu
docker container kill casetwo   # Simulate a failure
sleep 2
docker container ls -l The container automatically restarts if it exits with a non-zero status. 3. --restart=always docker container run -itd --name=casethree --restart=always ubuntu
docker container stop casethree
docker container ls -l Regardless of exit reason or daemon restart, the container will be restarted. 4. --restart=unless-stopped docker container run -itd --name=casefour --restart=unless-stopped ubuntu
docker container stop casefour
docker container ls -l Even if Docker restarts, casefour stays down after a manual stop. Warning Use always or unless-stopped for critical services. Excessive restart loops on failure can degrade performance—consider on-failure with a retry limit. Inspecting Docker Events To diagnose restart loops or networking events, view real-time Docker system events: docker system events --since 60m Sample output: 2020-05-04T08:38:43.747Z network connect cf10… (container=2daf…, name=bridge) 2020-05-04T08:38:43.976Z container start … (image=ubuntu, name=casethree) 2020-05-04T08:39:43.633Z network connect cf10… (container=74b0…, name=bridge) 2020-05-04T08:39:43.890Z container start … (image=ubuntu, name=casefour) Copying Files Between Host and Container ( docker cp ) You don’t need an interactive shell to move files; docker cp handles it directly. Host → Container mkdir -p /var/temp
echo ""hello for KodeKloud"" > /var/temp/yogishtest

docker container run -itd --name=copytest --rm ubuntu
docker container exec copytest mkdir -p /root
docker container cp /var/temp/yogishtest copytest:/root/
docker container exec -it copytest cat /root/yogishtest Container → Host docker container exec copytest bash -c ""mkdir -p /var/temp && echo 'container to host' > /var/temp/docker-host""
docker container cp copytest:/var/temp/docker-host /var/temp/
ls -l /var/temp/docker-host
cat /var/temp/docker-host Publishing Ports By default, containers don’t expose ports to the host. You can use random or static port mappings: Option Description Access URL none No mapping; container ports are isolated Unreachable from host -P Random high host port mapped to container’s port http://<host-ip>:<random> -p Static host-to-container port mapping (e.g., 82:80) http://<host-ip>:82 No Port Mapping docker container run -itd --name=case1 httpd
docker container ls -l Accessing via host IP will fail. Random Port Mapping ( -P ) docker container run -itd -P --name=case2 httpd
docker container ls -l Docker picks a random port (e.g., 32768) for container port 80. Static Port Mapping ( -p ) docker container run -itd --name=case3 -p 82:80 httpd
docker container ls -l Now host port 82 forwards to container port 80. Port Mapping on Restart docker container restart case2
docker container restart case3
docker container ls -l With -P , Docker may assign a different random port after restart. With -p , the mapping remains consistent. Best Practice For production services, always use -p for predictable port assignments and easier firewall configuration. References Docker Run Reference Docker Container CP Docker Restart Policies We hope this lesson helps you master advanced Docker container operations! Watch Video Watch video content"
Docker Certified Associate Exam Course,Publishing Ports,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine/Publishing-Ports,"Docker Certified Associate Exam Course Docker Engine Publishing Ports In this lesson, we’ll explore how Docker publishes container ports to the host system. We begin with basic port mapping, then move on to advanced options like interface binding, dynamic port allocation, and automatic exposure. By the end, you’ll understand how Docker leverages iptables to route traffic between host and container. 1. Container vs Host IP A containerized web application typically listens on an internal port (e.g., 5000 ). Every container receives an internal IP (for example, 172.17.0.2 ), which is only reachable from the Docker host: curl http://172.17.0.2:5000 However, this IP isn’t accessible from other machines. To allow external access, you must map the container port to a port on the host (e.g., 192.168.1.5 ). 2. Publishing a Fixed Port ( -p ) To map container port 5000 to host port 80 , run: docker run -p 80:5000 kodekloud/simple-webapp Now your application is accessible at: http://192.168.1.5:80 Multiple Instances on Different Ports You can launch multiple containers binding the same internal port to different host ports: docker run -d -p 8000:5000 kodekloud/simple-webapp
docker run -d -p 8001:5000 kodekloud/simple-webapp For database services: docker run -d -p 3306:3306 mysql
docker run -d -p 8306:3306 mysql
# The next command fails if port 8306 is in use:
docker run -d -p 8306:3306 mysql Port Collision Warning Host ports must be unique. Attempting to bind the same host port twice will cause Docker to error out. 3. Binding to Specific Host Interfaces If your machine has multiple network interfaces, you can restrict port binding to a particular IP: # Bind only on 192.168.1.5
docker run -p 192.168.1.5:8000:5000 kodekloud/simple-webapp

# Bind only on loopback (accessible locally)
docker run -p 127.0.0.1:8000:5000 kodekloud/simple-webapp 4. Dynamic Host Port Allocation Omitting the host port lets Docker assign a random port (default range 32768–60999): docker run -d -p 5000 kodekloud/simple-webapp To view the port range: cat /proc/sys/net/ipv4/ip_local_port_range
# Example output:
# 32768 60999 5. Publishing All Exposed Ports ( -P ) If an image’s Dockerfile declares one or more EXPOSE ports, you can automatically map them to random host ports: # Dockerfile snippet
FROM ubuntu:16.04
RUN apt-get update && apt-get install -y python3 python3-pip
RUN pip3 install flask
COPY app.py /opt/
ENTRYPOINT [""flask"", ""run"", ""--host=0.0.0.0""]
EXPOSE 5000 Build and run: docker build -t simple-webapp .
docker run -P simple-webapp You can also expose additional ports at runtime: docker run -P --expose=8080 simple-webapp Inspect the exposed ports: docker inspect simple-webapp --format '{{json .NetworkSettings.Ports}}'
# Example output:
# {""5000/tcp"":[{""HostIp"":""0.0.0.0"",""HostPort"":""32768""}],
#  ""8080/tcp"":[{""HostIp"":""0.0.0.0"",""HostPort"":""32769""}]} 6. Port Publishing Options at a Glance Option Description Syntax -p Map specific host and container ports -p [host_ip:]host_port:container_port -P Publish all EXPOSE d ports to random host ports -P --expose Expose additional container ports (no host bind) --expose=port[/protocol] 7. Under the Hood: iptables NAT Docker uses Linux iptables to forward traffic from host ports to container IPs. It creates custom chains ( DOCKER , DOCKER-USER ) in the nat table: Packet arrives on the host port. PREROUTING chain directs it to the DOCKER chain. A DNAT rule rewrites the packet’s destination to the container’s IP and port. The packet is forwarded to the container. Response packets are SNAT’d or MASQUERADE’d back to the host. Inspect Docker’s NAT rules: iptables -t nat -S DOCKER
# Sample output:
# -N DOCKER
# -A DOCKER ! -i docker0 -p tcp -m tcp --dport 41232 \
#     -j DNAT --to-destination 172.17.0.3:5000 Note You can insert custom rules in the DOCKER-USER chain to filter or modify traffic before Docker’s own rules apply. Further Reading and References Docker Networking Overview Docker Run Reference iptables Manual Watch Video Watch video content"
Docker Certified Associate Exam Course,Demo Docker Debug Mode,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine/Demo-Docker-Debug-Mode,"Docker Certified Associate Exam Course Docker Engine Demo Docker Debug Mode In this demo, you’ll learn how to turn on Docker daemon debug mode, verify that it’s active, and inspect verbose logs during container operations. 1. Check Current Debug Status Start by viewing your Docker daemon’s current settings: docker system info Look for the Debug Mode line in the output: Debug Mode: false 2. Create a Test Container & Inspect Default Logs Launch a simple HTTPD container: docker run -d --name test httpd:latest Then review your system log (e.g., /var/log/messages or journalctl -u docker.service ). You should only see high-level entries about container creation: tail -n 20 /var/log/messages 3. Enable Docker Debug Mode To capture detailed debug output, edit the Docker daemon configuration: Open /etc/docker/daemon.json (create it if missing) and add: {
  ""debug"": true
} Note If the file doesn’t exist, you can create it. Make sure the JSON remains valid—use a JSON linter if needed. Reload the Docker daemon: sudo systemctl reload docker 4. Confirm Debug Mode Is Active Run the inspect command again: docker system info Now you should see Debug Mode: true along with extra metrics: Debug Mode: true
File Descriptors: 23
Goroutines: 36
System Time: 2020-05-21T11:38:33.79317432Z
EventsListeners: 0 5. Generate and View Verbose Logs Create a new container called test_debug : docker run -d --name test_debug httpd:latest Then tail your logs to see debug-level details: tail -n 20 /var/log/messages You’ll notice granular messages describing each step of the container lifecycle. 6. Reload Docker with SIGHUP & Disable Debug If you prefer a manual reload instead of systemctl : Update /etc/docker/daemon.json to disable debug mode: {
  ""debug"": false
} Identify the Docker daemon PID and send SIGHUP : pid=$(pgrep dockerd)
sudo kill -SIGHUP $pid Warning Always verify you have the correct PID before sending signals. Killing the wrong process can disrupt your system. Check that debug is now off: docker system info | grep ""Debug Mode"" Command Reference Action Command Check debug status docker system info Launch a container docker run -d --name <name> httpd:latest View system logs tail -n 20 /var/log/messages Enable debug in daemon.json Add ""debug"": true to /etc/docker/daemon.json Reload Docker via systemd sudo systemctl reload docker Reload Docker via SIGHUP sudo kill -SIGHUP $(pgrep dockerd) Disable debug in daemon.json Change ""debug"": false in /etc/docker/daemon.json Verify debug flag only `docker system info Links and References Docker Daemon Configuration Understanding the Docker Logging Driver Docker System Commands Watch Video Watch video content"
Docker Certified Associate Exam Course,Copying Contents into Container,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine/Copying-Contents-into-Container,"Docker Certified Associate Exam Course Docker Engine Copying Contents into Container In this guide, you’ll learn how to transfer files and directories between your Docker host and a running container using the docker container cp command. This utility works in both directions: Host → Container Container → Host Assume you have: A container named webapp running on your Docker host A file on the host at /tmp/web.conf Copying from Host to Container Syntax docker container cp [HOST_PATH] [CONTAINER_NAME]:[CONTAINER_PATH] Example docker container cp /tmp/web.conf webapp:/etc/web.conf Source (host): /tmp/web.conf Destination (container): webapp:/etc/web.conf Note If you specify a directory as the destination, ensure that directory already exists inside the container. Copying from Container to Host Simply reverse the source and destination: docker container cp webapp:/etc/web.conf /tmp/web.conf Source (container): webapp:/etc/web.conf Destination (host): /tmp/web.conf Warning Any existing file at the destination path will be overwritten without confirmation. Copying Entire Directories To copy a complete directory (including its contents), include trailing slashes: docker container cp ./config/ webapp:/etc/config This command transfers your local config folder into /etc/config inside the webapp container. Ensure /etc/config exists in the container before running the command. Command Reference Direction Command Syntax Description Host → Container docker container cp /path/on/host container_name:/path/in/container Copy files or directories into container Container → Host docker container cp container_name:/path/in/container /path/on/host Copy files or directories to host Links and References Docker CLI Reference docker container cp Documentation Watch Video Watch video content"
Docker Certified Associate Exam Course,Demo Image Registry and Operations,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Image-Management/Demo-Image-Registry-and-Operations,"Docker Certified Associate Exam Course Docker Image Management Demo Image Registry and Operations In this lesson, we’ll dive into Docker Hub and walk through essential image operations using both the web interface and Docker CLI. You’ll learn how to find, pull, tag, push, inspect, save, and remove images efficiently. 1. Exploring Docker Hub Open your browser and go to hub.docker.com . Note If you don’t have a Docker Hub account yet, sign up now—you’ll need it to push images later. Docker Hub classifies images into three categories: Resource Type Description Identifier Example Official images Maintained by Docker; carries an official badge httpd Verified publisher images Provided by ecosystem partners; marked verified puppet/puppet-agent Community (user) images Uploaded by users; named with username/imagename yogeshraheja/wordpress Search for the Apache HTTP Server image ( httpd ) and click httpd . You’ll see: Supported Tags : Available versions (e.g., latest , alpine ). Dockerfile links: How the image is built. Quick info on architectures, update history, and help resources. 2. Listing and Pulling Images Locally First, see which images are already on your host: docker image ls Sample output: REPOSITORY   TAG       IMAGE ID       CREATED        SIZE
ubuntu       latest    1d622ef86b13   10 days ago    73.9MB Pull the default HTTPD image ( httpd:latest ): docker pull httpd Verify it’s downloaded: docker image ls 3. Searching Images via CLI Instead of the web UI, search Docker Hub from your terminal: docker search httpd To refine results: docker search --limit 2 httpd
docker search --filter stars=10 httpd
docker search --filter stars=10 --filter is-official=true httpd 4. Pulling Specific Tags and Tagging Images Grab the Alpine-based HTTPD variant: docker pull httpd:alpine Verify both images: docker image ls Tag httpd:alpine locally: docker image tag httpd:alpine httpd:kodekloudv1
docker image ls Both tags share the same IMAGE ID and SIZE , since they reference the same layers. 5. Checking Disk Usage Assess disk space consumed by images, containers, and volumes: docker system df 6. Pushing Images to Docker Hub Log in to Docker Hub: docker login Retag your image with your Docker Hub username (replace <username> ): docker tag httpd:kodekloudv1 <username>/httpd:kodekloudv1 Push the image: docker push <username>/httpd:kodekloudv1 Warning If you try docker push httpd:kodekloudv1 without your username, you’ll get an “access denied” error. Always retag with your Docker Hub namespace. After a successful push, confirm locally: docker image ls You should see: REPOSITORY                 TAG           IMAGE ID       CREATED        SIZE
<username>/httpd           kodekloudv1   eee6a6a3a3c9   9 days ago     107MB On Docker Hub, navigate to your repositories to view it. 7. Removing Images Locally Remove a single tag: docker image rm httpd:kodekloudv1 If the image ID is still referenced by other tags, remove all of them: docker image rm httpd:alpine
docker image rm <username>/httpd:kodekloudv1 Confirm removal: docker image ls On Docker Hub Sign in to Docker Hub and go to your account. Select the repository to delete. Click Settings → Delete Repository . Type the repository name to confirm. 8. Inspecting and Exploring Images View an image’s layer history: docker image history ubuntu Inspect detailed metadata: docker image inspect httpd 9. Saving and Loading Images Export an image to a tarball: docker image save alpine:latest -o alpine-latest.tar Remove the local image: docker image rm alpine:latest Load it back: docker image load -i alpine-latest.tar This is useful for air-gapped or offline transfers. 10. Exporting and Importing a Container Filesystem Run a container: docker container run -itd --name test alpine Export its filesystem: docker export test > test-container.tar Import as a new image: docker image import test-container.tar test-image:latest Verify: docker image ls That covers image registry and operations with Docker Hub and CLI. Happy Dockering! Links and References Docker Documentation Docker Hub Docker CLI Reference Watch Video Watch video content"
Docker Certified Associate Exam Course,Demo Logging Driver,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine/Demo-Logging-Driver,"Docker Certified Associate Exam Course Docker Engine Demo Logging Driver In this tutorial, you’ll learn how to manage Docker’s logging drivers—check the default, switch the daemon-wide setting, apply advanced options, and override the driver for individual containers. 1. Check the Default Logging Driver Docker uses the json-file driver by default, storing container logs as JSON on the host. # Verify the current logging driver
docker system info | grep -i ""logging driver""
# Output: Logging Driver: json-file Note The json-file driver is the standard Docker logging backend. It’s easy to parse and works out of the box. 2. Create and Inspect a Test Container Run an Ubuntu container to see its inherited log configuration: # Start a detached Ubuntu container
docker container run -itd --name test-container ubuntu Inspect its log settings: docker container inspect test-container \
  --format='{{json .HostConfig.LogConfig}}'
# Output:
# {
#   ""Type"": ""json-file"",
#   ""Config"": {}
# } 3. Supported Logging Drivers Docker supports multiple logging backends for different use cases. You can find the full list in the official docs: Configure containers → Logging Driver Use Case json-file Local JSON logs, simple parsing syslog Centralized logging to syslog daemon journald Integration with systemd’s journal fluentd Forward logs to a Fluentd collector awslogs Ship logs to Amazon CloudWatch Logs splunk Send logs to a Splunk HTTP Event Collector (HEC) … And others (gcplogs, logentries, etc.) 4. Change the Default Driver to Syslog To switch the daemon-wide driver to syslog , edit /etc/docker/daemon.json : Warning Modifying daemon.json requires restarting the Docker daemon. Existing containers will continue using their current driver until recreated. Stop Docker: sudo systemctl stop docker Update /etc/docker/daemon.json : {
  ""log-driver"": ""syslog""
} Restart Docker: sudo systemctl start docker Verify: docker system info | grep -i ""logging driver""
# Output: Logging Driver: syslog 5. Advanced Logging Options You can fine-tune log behavior with log-opts . For example, to limit file size and rotation on json-file : {
  ""log-driver"": ""json-file"",
  ""log-opts"": {
    ""max-size"": ""10m"",
    ""max-file"": ""3""
  },
  ""labels"": ""production_status"",
  ""env"": ""os_customer""
} Retrieve the current default driver in scripts: docker info --format '{{.LoggingDriver}}'
# e.g., json-file 6. Override the Logging Driver per Container Even when the daemon default is syslog , you can pick a different driver for a specific container: docker container run -itd \
  --name logtest \
  --log-driver journald \
  ubuntu Confirm the override: docker container inspect logtest \
  --format='{{json .HostConfig.LogConfig}}'
# Output:
# {
#   ""Type"": ""journald"",
#   ""Config"": {}
# } 7. Conclusion You’ve learned how to: Check and view Docker’s default logging driver Change the daemon-wide driver in /etc/docker/daemon.json Apply advanced options like rotation and size limits Override logging drivers for individual containers Happy logging! References Docker Logging Drivers Documentation Docker System Info Docker Container Inspect Watch Video Watch video content"
Docker Certified Associate Exam Course,Authenticating to Registries,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Image-Management/Authenticating-to-Registries,"Docker Certified Associate Exam Course Docker Image Management Authenticating to Registries In this guide, you’ll learn how to: Pull images from public and private registries Authenticate with docker login Retag (rename) images Push images to private registries Inspect actual disk usage with Docker 1. Pulling Images from a Public Registry Pulling from a public registry (for example, the official Ubuntu image on Docker Hub) requires no authentication: docker pull ubuntu Note By default, docker pull ubuntu retrieves the latest tag. To pull a specific version, append the tag (e.g., ubuntu:20.04 ). 2. Accessing Private Repositories Attempting to pull an image from a private registry without logging in will result in an access denied error: docker pull gcr.io/organization/ubuntu Using default tag: latest
Error response from daemon: pull access denied for gcr.io/organization/ubuntu, repository does not exist or may require 'docker login': denied: requested access to the resource is denied Similarly, pushing without authentication will fail: docker push ubuntu The push refers to repository [docker.io/library/ubuntu]
128fa0b0fb81: Layer already exists
c0151ca45f27: Layer already exists
b2fd17df2071: Layer already exists
[DEPRECATION NOTICE] registry v2 schema1 support will be removed...
errors:
  denied: requested access to the resource is denied
  unauthorized: authentication required Warning You must authenticate before pulling from or pushing to private registries. Make sure your credentials have the necessary permissions. 3. Logging In to a Registry Use docker login to authenticate. By default, it targets Docker Hub ( docker.io ). docker login docker.io Example: Username: registry-user
Password:
WARNING! Your password will be stored unencrypted in /home/vagrant/.docker/config.json.
Login Succeeded For other registries such as Google Container Registry: docker login gcr.io Once logged in, subsequent docker pull and docker push commands will use your credentials. 4. Retagging (Renaming) an Image Docker images cannot be renamed directly. Instead, you can create a new tag—a “soft link” to the same image ID. List local images: docker image ls Retag httpd:alpine to httpd:customv1 : docker image tag httpd:alpine httpd:customv1 Confirm the new tag: docker image ls Example output: REPOSITORY  TAG        IMAGE ID       CREATED       SIZE
httpd       alpine     52862a02e4e9   2 weeks ago   112MB
httpd       customv1   52862a02e4e9   2 weeks ago   112MB
httpd       latest     c2aa7e16edd8   2 weeks ago   165MB
ubuntu      latest     549b9b86cb8d   4 weeks ago   64.2MB Notice httpd:alpine and httpd:customv1 share the same IMAGE ID—no duplicate layers are created. 5. Pushing to a Private Registry To push your retagged image: docker image tag httpd:customv1 gcr.io/organization/httpd:customv1
docker push gcr.io/organization/httpd:customv1 Make sure you are logged in ( docker login gcr.io ) before pushing. 6. Checking Actual Disk Usage Use docker system df to inspect disk usage across images, containers, volumes, and build cache: docker system df Example: TYPE                TOTAL     ACTIVE    SIZE      RECLAIMABLE
Images              3         0         341.9MB   341.9MB (100%)
Containers          0
Local Volumes       0
Build Cache         0 Even if docker image ls lists four tags, docker system df shows only three unique images. The SIZE column reflects unique layers, and RECLAIMABLE indicates potential space savings. Command Reference Table Action Command Example Pull a public image docker pull ubuntu Pull a private image docker pull gcr.io/myorg/ubuntu Login to Docker Hub docker login docker.io Login to Google Container Reg. docker login gcr.io Retag an image docker image tag httpd:alpine httpd:customv1 Push to a private registry docker push gcr.io/organization/httpd:customv1 View disk usage docker system df Links and References Docker CLI Reference Docker Hub Documentation Google Container Registry Auth Watch Video Watch video content"
Docker Certified Associate Exam Course,Logging Driver,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine/Logging-Driver,"Docker Certified Associate Exam Course Docker Engine Logging Driver In Docker, logging drivers determine how container and service logs are captured, formatted, and stored. By choosing the right logging driver, you can centralize logs, integrate with external systems, and manage log retention efficiently. You can always view a container’s output with: docker logs <container-name> but behind the scenes the logging driver dictates where and how those logs are persisted. What Is the Default json-file Logging Driver? Docker’s default logging driver is json-file . It collects container output and writes it as JSON objects on the host filesystem. To confirm your daemon’s default driver: docker system info Look for: Server:
  ...
  Logging Driver: json-file
  Cgroup Driver: cgroupfs
Plugins:
  Log: awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog Run and inspect logs under json-file : docker run -d --name nginx nginx
docker logs nginx Sample output: /docker-entrypoint.sh: Launching /docker-entrypoint.d/20-envsubst-on-templates.sh
/docker-entrypoint.sh: Configuration complete; ready for start up Where Are json-file Logs Stored? By default, log files are stored under /var/lib/docker/containers/<container-id> . Each container directory contains a file named <container-id>-json.log : docker ps
cd /var/lib/docker/containers
ls
cat f3997637c0df66becf4dd4662d3c172bf16f916a3b9289b95f0994675102de17-json.log Note Rotating or cleaning up old JSON logs prevents disk exhaustion. Consider using Docker’s log-opts settings like max-size and max-file . Supported Docker Logging Drivers Beyond json-file , Docker integrates with multiple logging backends. Choose the one that matches your infrastructure for centralized log aggregation: Driver Use Case More Info awslogs Send logs to Amazon CloudWatch Logs AWS CloudWatch Logs fluentd Forward logs to a Fluentd collector Fluentd gcplogs Ship logs to Google Cloud Logging Google Cloud Logging gelf Graylog Extended Log Format GELF Specs journald Use systemd’s journald journald splunk Forward to Splunk Splunk Docs syslog Standard syslog protocol — local Fast, built-in local driver — none Disable logging (no docker logs ) — Warning Using the none driver disables all logs for the container. Only use this when you intentionally want zero log output. Changing the Daemon’s Default Logging Driver To set a different default logging driver, modify (or create) /etc/docker/daemon.json : {
  ""log-driver"": ""awslogs"",
  ""log-opts"": {
    ""awslogs-region"": ""us-east-1""
  }
} If you need TLS, custom hosts, or debug mode, include those settings alongside: {
  ""debug"": true,
  ""hosts"": [""tcp://0.0.0.0:2376""],
  ""tls"": true,
  ""tlscert"": ""/var/docker/server.pem"",
  ""tlskey"": ""/var/docker/serverkey.pem"",
  ""log-driver"": ""awslogs"",
  ""log-opts"": {
    ""awslogs-region"": ""us-east-1""
  }
} Restart Docker to apply changes: sudo systemctl restart docker Overriding the Logging Driver Per Container You can override the daemon default for individual containers with the --log-driver flag: docker run -d \
  --name myapp \
  --log-driver=fluentd \
  nginx Add driver-specific options via --log-opt : docker run -d \
  --name myapp \
  --log-driver=fluentd \
  --log-opt fluentd-address=localhost:24224 \
  nginx Inspecting a Container’s Logging Configuration To verify which logging driver a container is using: docker container inspect nginx Search for the HostConfig.LogConfig section: ""HostConfig"": {
  ""LogConfig"": {
    ""Type"": ""json-file"",
    ""Config"": {}
  }
} For a concise output with a Go template: docker container inspect -f '{{.HostConfig.LogConfig.Type}}' nginx This prints only the driver name. Links and References Docker Logging Drivers Kubernetes Logging with json-file AWS CloudWatch Logs Fluentd Official Website Google Cloud Logging Overview Splunk Container Logging Watch Video Watch video content"
Docker Certified Associate Exam Course,Image Addressing Convention,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Image-Management/Image-Addressing-Convention,"Docker Certified Associate Exam Course Docker Image Management Image Addressing Convention In this lesson, you’ll learn how Docker interprets image names when you pull or reference them. Understanding these conventions helps you avoid naming conflicts and ensures you’re pulling or pushing images to the correct registry. Pulling an Image For example, running: docker image pull httpd might look simple—but what does httpd actually represent, and where does Docker retrieve it from? Docker Image Naming Components A complete Docker image reference can include up to three parts: Component Description Example Registry Hostname of the registry (defaults to Docker Hub) docker.io Namespace User or organization under which the image lives library (for official images) Repository Name of the image project httpd Implicit Namespace When you specify only httpd , Docker assumes you want the official image from Docker Hub’s library namespace. Effectively, Docker interprets: image: library/httpd Here, library is the namespace for curated, official images on Docker Hub, and httpd is the repository name. Default Registry By default, Docker pulls from Docker Hub ( docker.io ). Omitting the registry is shorthand for: image: docker.io/library/httpd The registry is where images are stored. When you build and push an image, it goes to this registry; when you pull, it comes from here. Note You can verify the full reference of an existing image with: docker image inspect httpd --format '{{.RepoDigests}}' Referencing Other Registries If your image lives in a different registry—such as Google Container Registry or a private registry—you must prepend the registry hostname: # Google Container Registry (GCR)
image: gcr.io/your-project/httpd

# Private registry
image: registry.example.com/your-namespace/httpd Before interacting with private registries, authenticate using: docker login <registry-hostname> Warning Always ensure you’re logged in to the correct registry. Pushing to the wrong registry can overwrite critical images. Links and References Docker Image Overview Docker Hub Documentation Google Container Registry Watch Video Watch video content"
Docker Certified Associate Exam Course,Docker commit method,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Image-Management/Docker-commit-method,"Docker Certified Associate Exam Course Docker Image Management Docker commit method In this guide, you’ll learn how to create a Docker image from a running container using docker container commit . This technique can be useful for rapid prototyping or debugging changes without writing a Dockerfile. For production-grade images, you should still prefer the Dockerfile approach to ensure repeatability and version control. Overview Typically, custom images are built with a Dockerfile: docker build -t myapp:latest . Alternatively, you can: Launch a container from a base image (e.g., httpd ). Modify files or install packages inside the container. Commit the container’s state as a new image. Warning The docker commit workflow is not recommended for production systems. Use a Dockerfile for maintainability, readability, and versioning. When to Use docker commit Scenario Recommended? Alternative One-off experiments Yes Dockerfile (optional) Capturing state for debugging Yes Volumes, logging Production-ready, repeatable CI No Dockerfile Step-by-Step Example Run an httpd container in detached mode docker run -d --name httpd httpd Enter the container and update the default web page docker exec -it httpd bash
root@container:/# cat > /usr/local/apache2/htdocs/index.html <<EOF
Welcome to my custom web application
EOF
root@container:/# exit Commit the container state to a new image docker container commit -a ""Ravi"" httpd customhttpd Verify the new image docker image ls
REPOSITORY    TAG       IMAGE ID       CREATED         SIZE
customhttpd   latest    adac0f56a7df   5 seconds ago   138MB
httpd         latest    417af7dc28bc   8 days ago      138MB Comparison: Dockerfile vs. docker commit Feature Dockerfile docker commit Version control Yes (plain text) No Automation CI/CD pipelines Manual or scripted Reproducibility High Low Ease of simple tweaks Moderate Very fast Best practice for production ✔ ✖ References Dockerfile reference docker container commit docker exec documentation Docker image management Watch Video Watch video content"
Docker Certified Associate Exam Course,Save and Load Images,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Image-Management/Save-and-Load-Images,"Docker Certified Associate Exam Course Docker Image Management Save and Load Images Learn how to export Docker images and containers to tar archives, move them into air-gapped or restricted environments, and load or import them back into Docker without direct internet access. Table of Contents Exporting and Transferring Docker Images Exporting a Container Filesystem and Importing as an Image Docker CLI Reference Links and References 1. Exporting and Transferring Docker Images When you cannot pull directly from Docker Hub (for example, in a restricted environment), follow these steps: Step-by-Step Guide Pull the desired image on an internet-connected host: docker pull alpine:latest Save the image into a tarball: docker image save alpine:latest -o alpine.tar Transfer alpine.tar to the target system (USB drive, SCP, shared NFS, etc.). Load the tarball on the restricted host: docker image load -i alpine.tar You should see output similar to: beee9f30bc1f: Loading layer [==================>]  5.862MB/5.862MB
Loaded image: alpine:latest Confirm the image is available locally: docker image ls Example output: REPOSITORY   TAG       IMAGE ID       CREATED        SIZE
alpine       latest    a187dde48cd2   4 weeks ago    5.6MB Note Always verify you’re using the correct <repository>:<tag> when saving and loading images to avoid version mismatches. Warning Ensure you have sufficient disk space on both source and target systems before exporting large images. 2. Exporting a Container Filesystem and Importing as an Image You can snapshot a running container’s filesystem and convert it into a new Docker image: Export the container to a tar archive: docker export <container-name> > testcontainer.tar Import the tarball as a fresh image: docker image import testcontainer.tar newimage:latest Docker returns a new image ID, e.g.: sha256:8909b7da236bb21aa2e52e6e04dff4b7103753e4046e15457a3daf6dfa723a12 List local images to confirm the import: docker image ls Example output: REPOSITORY    TAG       IMAGE ID       CREATED         SIZE
newimage      latest    8090b7da236b   2 minutes ago   5.6MB
alpine        latest    a187dde48cd2   4 weeks ago     5.6MB Docker CLI Reference Command Description docker pull <repo>:<tag> Download image from Docker Hub or private registry docker image save <image> -o <file>.tar Save one or more images to a tar archive docker image load -i <file>.tar Load image(s) from a tar archive docker export <container> Export container filesystem as a tar archive docker image import <file>.tar <name>:<tag> Create a new image from a container tarball docker image ls List local Docker images Links and References Docker Save Command Docker Load Command Docker Export and Import Docker CLI Overview Watch Video Watch video content"
Docker Certified Associate Exam Course,Docker Image Registry,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Image-Management/Docker-Image-Registry,"Docker Certified Associate Exam Course Docker Image Management Docker Image Registry In this lesson, we explore Docker’s image registry. Throughout our work, we’ve run many containers using images—but where do these images reside, and how do you access them? All images are stored in a central repository called an image registry . By default, Docker uses Docker Hub , a public registry hosting thousands of both public and private images. You can push your own images to Docker Hub and choose to keep them private. Default Registries On top of Docker Hub, organizations can deploy private registries internally or use managed services from cloud providers: Registry Type Description Link Docker Hub Docker’s public registry hub.docker.com Docker Trusted Registry On-premises private registry docs.docker.com/enterprise/registry Google Container Registry Managed registry by Google Cloud cloud.google.com/container-registry Amazon Elastic Container Registry Managed registry by AWS aws.amazon.com/ecr Azure Container Registry Managed registry by Microsoft Azure azure.microsoft.com/services/container-registry/ Image Categories on Docker Hub On Docker Hub, images are organized into three categories: Category Maintained By Examples Official Images Docker ubuntu , nginx , node , mongo Verified Images Trusted vendors oracle , splunk , datadog , dynatrace User Images Community contributors Various open-source and custom applications Searching for Images on Docker Hub You can browse and search images via the web interface. For example, searching for “Ubuntu” displays official Ubuntu images along with download counts and star ratings: Working with Image Tags Each image can have multiple tags. When you pull or run an image without specifying a tag, Docker uses the default tag: latest . This tag points to the version designated by the image maintainers. Note Pulling ubuntu is equivalent to pulling ubuntu:latest , which currently refers to Ubuntu 20.04. # Pulls the Ubuntu image with the default 'latest' tag
docker pull ubuntu

# Both commands are equivalent
docker run ubuntu
docker run ubuntu:latest To pull a specific version, specify its tag. For example, Ubuntu 18.04 is tagged as bionic , and Ubuntu 14.04 as trusty : docker pull ubuntu:bionic Listing and Searching Images from the CLI List Local Images To display all images on your host: docker image ls Example output: REPOSITORY   TAG       IMAGE ID       CREATED         SIZE
ubuntu       latest    549b9b86cb8d   4 weeks ago     64.2MB Search Docker Hub from Terminal docker search httpd By default, this returns up to 25 results. To limit the output (maximum 100): docker search httpd --limit 2 Filter Search Results Use filters like stars and is-official to refine results: # HTTPD images with at least 10 stars
docker search httpd --filter stars=10

# Only official HTTPD images
docker search httpd --filter is-official=true

# Combine multiple filters
docker search httpd --filter stars=10 --filter is-official=true Pulling Images Without Running Containers If you only want to download an image without starting a container: docker image pull httpd Example output: Using default tag: latest
latest: Pulling from library/httpd
8ec398bc0356: Pull complete
354e6904d655: Pull complete
27298e4c749a: Pull complete
10e27104ba69: Pull complete
36412f6b2f6e: Pull complete
Digest: sha256:769018135ba22d3a7a2b91cb898de711562cdf51ad6621b2b13e95f3798de
Status: Downloaded newer image for httpd:latest
docker.io/library/httpd:latest Verify the image is present: docker image ls That concludes this lesson on Docker image registries. In the next lesson, we’ll dive into container management. References Docker Hub Docker Trusted Registry Google Container Registry Amazon Elastic Container Registry Azure Container Registry Watch Video Watch video content"
Docker Certified Associate Exam Course,Removing a Docker Image,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Image-Management/Removing-a-Docker-Image,"Docker Certified Associate Exam Course Docker Image Management Removing a Docker Image Cleaning up unused Docker images helps reclaim disk space and keep your environment tidy. Before deleting an image, ensure no containers are running from it. Stop and remove any dependent containers first. Note You cannot remove an image if there are existing containers based on it. Always run docker container ls -a and docker container rm <container_id> as needed. 1. List All Images To view all images on your host: docker image ls Example output: REPOSITORY   TAG        IMAGE ID       CREATED      SIZE
httpd        alpine     52862a02e4e9   2 weeks ago  112MB
httpd        customv1   52862a02e4e9   2 weeks ago  112MB
httpd        latest     c2aa7e16edd8   2 weeks ago  165MB
ubuntu       latest     549b9b86cb8d   4 weeks ago  64.2MB In this example, the image ID 52862a02e4e9 has two tags: httpd:alpine and httpd:customv1 . 2. Remove a Single Tag When you run docker image rm <repository>:<tag> , Docker: Removes the tag (soft link). Deletes the image layers only if no other tags reference them. Remove the customv1 tag: docker image rm httpd:customv1 Output: Untagged: httpd:customv1 Since httpd:alpine still points to the same layers, only the tag is removed. Verify the remaining images: docker image ls Result: REPOSITORY   TAG        IMAGE ID       CREATED      SIZE
httpd        alpine     52862a02e4e9   2 weeks ago  112MB
httpd        latest     c2aa7e16edd8   2 weeks ago  165MB
ubuntu       latest     549b9b86cb8d   4 weeks ago  64.2MB Removing the last tag deletes the layers and reclaims space: docker image rm httpd:alpine Output: Untagged: httpd:alpine
Deleted: sha256:52862a02e4e9...
Deleted: sha256:...
Total reclaimed space: 112MB 3. Prune All Unused Images If you have many dangling or unreferenced images, use: docker image prune -a Warning This command deletes all images not currently used by at least one container. Use with caution in production. You will be prompted to confirm: WARNING! This will remove all images without at least one container associated to them.
Are you sure you want to continue? [y/N] y Sample output: Deleted Images:
untagged: ubuntu:latest
deleted: sha256:549b9b86c8d75a2668c21c50ee927...
untagged: httpd:latest
deleted: sha256:c2aa7e16d855da8827aa0ccf9761...
Total reclaimed space: 229.4MB Common Docker Image Removal Commands Command Description docker image ls List all images docker image rm <repository>:<tag> Remove a specific image tag (and layers if orphan) docker image prune Delete dangling images (untagged) docker image prune -a Delete all unused images docker container ls -a List all containers (to identify dependencies) docker container rm <container_id> Remove specified container Links and References Docker Image Management Docker Container Management Docker CLI Prune Commands Watch Video Watch video content"
Docker Certified Associate Exam Course,Inspecting a Docker Image,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Image-Management/Inspecting-a-Docker-Image,"Docker Certified Associate Exam Course Docker Image Management Inspecting a Docker Image Understanding how Docker images are built from multiple layers can help you optimize, debug, and secure your container workflows. In this guide, we’ll explore essential Docker CLI commands to: List local images Examine image history Inspect detailed metadata Filter output with JSONPath Table of Contents Prerequisites Listing Local Images Viewing Image History Inspecting Image Metadata Filtering with JSONPath References Prerequisites Note Ensure you have Docker installed and running. Check your version with docker version . For full installation steps, visit the Docker Docs . Listing Local Images To see all images stored on your host machine, use: docker image list Example output: REPOSITORY    TAG       IMAGE ID       CREATED       SIZE
httpd         latest    c2aa7e16edd8   2 weeks ago   165MB
ubuntu        latest    549b9b86cb8d   4 weeks ago   64.2MB Command Description docker image list List all images locally Viewing Image History Examining an image’s history reveals each layer and the command that created it. This is especially valuable if the Dockerfile isn’t available. docker image history ubuntu Sample output: IMAGE          CREATED        CREATED BY                                      SIZE
549b9b86cb8d   4 weeks ago    /bin/sh -c #(nop) CMD [""/bin/bash""]             0B
<missing>      4 weeks ago    /bin/sh -c mkdir -p /run/systemd && echo 'do…   7B
<missing>      4 weeks ago    /bin/sh -c set -xe && echo '#!/bin/sh' > ./…    745B
<missing>      4 weeks ago    /bin/sh -c [ -z ""$(apt-get indextargets)"" ]      987kB
<missing>      4 weeks ago    /bin/sh -c #(nop) ADD file:53f100793e6c0adfc…   63.2MB Command Description docker image history <IMAGE> Show layer-by-layer build commands Inspecting Image Metadata The inspect command returns comprehensive image metadata in JSON format, including environment variables, exposed ports, volumes, and more. docker image inspect ubuntu Abbreviated example: [
  {
    ""Id"": ""549b9b86cb8d..."",
    ""RepoTags"": [""ubuntu:latest""],
    ""Created"": ""2020-09-15T23:05:57.348340124Z"",
    ""ContainerConfig"": {
      ""ExposedPorts"": {
        ""80/tcp"": {}
      }
    },
    ""DockerVersion"": ""18.09.7"",
    ""Architecture"": ""amd64"",
    ""Os"": ""linux"",
    ""Size"": 137532780
  }
] Warning Inspecting very large images may output extensive JSON. Use filtering (see next section) or redirect output to a file: docker image inspect ubuntu > ubuntu-inspect.json Command Description docker image inspect <IMAGE> Display full image metadata (JSON) Filtering with JSONPath Docker’s inspect supports the -f flag with JSONPath templates to extract specific fields. Common Filters Filter Template Description -f '{{.Os}}' Display the OS -f '{{.Architecture}}' Show the architecture -f '{{.Architecture}} {{.Os}}' Combine architecture and OS docker image inspect httpd -f '{{.Os}}'
docker image inspect httpd -f '{{.Architecture}}'
docker image inspect httpd -f '{{.Architecture}} {{.Os}}'
# Output: amd64 linux Retrieving Exposed Ports To list all ports exposed by an image: docker image inspect httpd \
  -f '{{range $p := .ContainerConfig.ExposedPorts}}{{printf ""%s "" $p}}{{end}}' Example output: 80/tcp References Docker CLI Reference Docker Inspect JSONPath Guide Watch Video Watch video content"
Docker Certified Associate Exam Course,Demo Build HTTPD image,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Image-Management/Demo-Build-HTTPD-image,"Docker Certified Associate Exam Course Docker Image Management Demo Build HTTPD image In this tutorial, you’ll learn how to build a lightweight, custom Docker image for an HTTPD (Apache) web server on CentOS 7. We’ll cover: Setting up the build context Crafting an optimized Dockerfile Adding a simple index.html Building, testing, and pushing your image to Docker Hub By following these steps, you’ll gain hands-on experience with multi-layered Docker images and best practices for containerized web servers. 1. Prepare the Build Context First, create an isolated directory for all build artifacts. This ensures that nothing outside the folder is accidentally added to your image. cd /tmp
mkdir firstimage && cd firstimage
ls -1
# (should be empty) Build Context Everything in this directory (including subdirectories) is sent to the Docker daemon during the build. Keep it lean to speed up image creation. 2. Write the Dockerfile Create a file named Dockerfile : vi Dockerfile Populate it with the following content: # Base image: CentOS 7
FROM centos:7

# 1. Update and install HTTPD
RUN yum -y update && \
    yum -y install httpd

# 2. Copy a custom HTML page into the default document root
COPY index.html /var/www/html/index.html

# 3. Expose port 80 for HTTP traffic
EXPOSE 80

# 4. Start HTTPD in the foreground
CMD [""httpd"", ""-D"", ""FOREGROUND""] Instruction Description FROM Specifies the base image (CentOS 7) RUN Runs commands in a new layer COPY Copies files from build context EXPOSE Documents the port on which the container listens CMD Defines the default command at container start Security Reminder Running containers as root can pose risks. For production workloads, consider adding a non-root user and switching with USER . For more details on Dockerfile syntax, see the Dockerfile reference . 3. Create the HTML Page Add a simple index.html in the same directory: vi index.html Example content: <!DOCTYPE html>
<html lang=""en"">
<head>
  <meta charset=""UTF-8"">
  <title>Welcome</title>
</head>
<body>
  <h1>Hello from KodeKloud Again</h1>
</body>
</html> Verify both files are present: ls -l
# total 8
# -rw-r--r-- 1 root root 199 May  4 13:38 Dockerfile
# -rw-r--r-- 1 root root  98 May  4 13:39 index.html 4. Build the Docker Image Use a clear, versioned tag for your image: docker image build -t yogeshraheja/kodekloud-web-image:v1 . You should see output for each of the five build steps. Once complete, confirm the image exists: docker image ls Inspect the image layers: docker image history yogeshraheja/kodekloud-web-image:v1 5. Test the Container Run a container, mapping host port 82 to container port 80: docker container run -d \
  -p 82:80 \
  --name httpd-test \
  yogeshraheja/kodekloud-web-image:v1 Open your browser to http://<host_ip>:82 . You should see your custom page: 6. Push to Docker Hub Authenticate and push your image so others can pull it: docker login
docker push yogeshraheja/kodekloud-web-image:v1 Verify on Docker Hub that your repository is public and the tag is available. Quick Command Reference Command Purpose docker image build -t user/repo:tag . Build an image with a tag from the current directory docker container run -d -p host_port:container_port --name name image Run a container detached with port mapping docker image ls List all local images docker image history image_name:tag Show the history of image layers docker push user/repo:tag Push a local image to Docker Hub Links and References Dockerfile Reference Docker CLI Overview Docker Hub Documentation CentOS 7 on Docker Hub Watch Video Watch video content"
Docker Certified Associate Exam Course,Troubleshooting Docker Daemon,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine/Troubleshooting-Docker-Daemon,"Docker Certified Associate Exam Course Docker Engine Troubleshooting Docker Daemon When Docker commands fail to communicate with the Docker daemon, follow these steps to diagnose and resolve the issue. 1. “Cannot connect to the Docker daemon” Error If you encounter: $ docker ps
Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running? the Docker Engine service (daemon) isn’t reachable. Determine if you’re targeting a local socket or a remote host. a. Remote Access via DOCKER_HOST Set the DOCKER_HOST environment variable to point at your remote Docker endpoint: export DOCKER_HOST=""tcp://192.168.1.10:2376""
docker ps Port Encryption Description 2375 None Unencrypted traffic 2376 TLS Secure, encrypted Note When using port 2376, ensure you have valid certificates configured on both client and server. If the error persists, SSH into the remote host and check the Docker service status. 2. Checking the Docker Service Status On most Linux distributions with systemd , Docker runs as a service. Verify its state: sudo systemctl status docker A healthy daemon appears as: ● docker.service - Docker Application Container Engine
   Loaded: loaded (/lib/systemd/system/docker.service; enabled; preset: enabled)
   Active: active (running) since Wed 2020-10-21 04:21:01 UTC; 3 days ago
     Docs: https://docs.docker.com
 Main PID: 4197 (dockerd)
    Tasks: 13
   Memory: 130M
     CPU: 9min 6.980s
 CGroup: /system.slice/docker.service
         └─4197 /usr/bin/dockerd -H fd:// -H tcp://0.0.0.0 --containerd=/run/containerd/containerd.sock If you see inactive or dead : ● docker.service - Docker Application Container Engine
   Loaded: loaded (/lib/systemd/system/docker.service; enabled; preset: enabled)
   Active: inactive (dead) since Sat 2020-10-24 07:42:08 UTC; 21s ago
     Docs: https://docs.docker.com
  Process: 4197 ExecStart=/usr/bin/dockerd -H fd:// -H tcp://0.0.0.0 --containerd=/run/containerd/containerd.sock (code=exited, status=0/SUCCESS) Start or restart the service: sudo systemctl start docker 3. Inspecting Service Logs Use journalctl to pinpoint errors and warnings: sudo journalctl -u docker.service --since ""1 hour ago"" Example log excerpt: Oct 21 04:05:42 ubuntu-xenial systemd[1]: Starting Docker Application Container Engine...
Oct 21 04:05:42 time=""2020-10-21T04:05:42.565Z"" level=info msg=""parsed scheme: \""unix\""""
Oct 21 04:05:42 time=""2020-10-21T04:05:42.847Z"" level=warning msg=""Your kernel does not support cgroup cfs""
Oct 21 04:05:43 time=""2020-10-21T04:05:43.873Z"" level=error msg=""Error (Unable to complete operation)"" Note Adjust the --since flag to narrow down log entries for faster troubleshooting. 4. Verifying Daemon Configuration Inspect /etc/docker/daemon.json for JSON syntax errors or conflicting settings: {
  ""debug"": true,
  ""hosts"": [""tcp://192.168.1.10:2376""],
  ""tls"": true,
  ""tlscert"": ""/var/docker/server.pem"",
  ""tlskey"": ""/var/docker/serverkey.pem""
} Warning A conflict between daemon flags (in daemon.json ) and CLI or systemd overrides can prevent Docker from starting. Remove duplicate host or TLS settings. After any change, reload and restart: sudo systemctl daemon-reload
sudo systemctl restart docker 5. Ensuring Sufficient Disk Space Docker stores images, containers, and volumes under /var/lib/docker . A full filesystem can crash the daemon. Check disk usage: df -h Example: Filesystem     Size  Used Avail Use% Mounted on
/dev/sda1       19G   14.7G   15M  99% /
tmpfs          369M     0  369M   0% /dev/shm Clean up unused resources: docker container prune   # remove all stopped containers
docker image prune       # remove dangling images Command Description docker container prune Delete stopped containers docker image prune Remove dangling or unused images docker volume prune Clean up unused volumes Warning Pruning operations are irreversible. Use docker system df to preview reclaimable space. 6. Examining System Information and Events Once the daemon is running, validate your environment: docker system info Sample output: Client:
 Debug Mode: false

Server:
 Containers: 0
  Running: 0
  Paused: 0
  Stopped: 0
 Images: 0
 Server Version: 19.03.5
 Storage Driver: overlay2
 Backing Filesystem: xfs
 Experimental: false
 Insecure Registries:
  127.0.0.0/8
 Live Restore Enabled: false To view real-time Docker events (container lifecycle, network changes, etc.): docker system events Further Reading Docker Engine overview Docker daemon.json reference Docker CLI commands Managing Docker storage With these steps—verifying connection methods, service status, logs, configuration, disk space, and system information—you can reliably troubleshoot Docker daemon issues. Watch Video Watch video content"
Docker Certified Associate Exam Course,Building a custom image,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Image-Management/Building-a-custom-image,"Docker Certified Associate Exam Course Docker Image Management Building a custom image Creating a custom Docker image ensures consistent deployments and lets you include exactly the dependencies your Python Flask app needs. Whether you require specialized system libraries or a private base image, building your own container image is straightforward. 1. Planning Your Dockerfile Before writing any code, outline the manual steps you’d perform to deploy your Flask application: Select a base image (e.g., Ubuntu or python:3-slim ). Update and install OS packages ( apt-get update && apt-get install ). Install Python dependencies ( pip install ). Copy application source code into the image. Configure environment variables and expose required ports. Define the container’s startup command. Tip Using an official Python base image (for example, python:3.9-slim ) can reduce image size and simplify dependency installation. 2. Writing the Dockerfile Create a file named Dockerfile at your project root and add the following contents: # 1. Base image
FROM ubuntu:20.04

# 2. OS-level dependencies
RUN apt-get update && \
    apt-get install -y python3 python3-pip && \
    rm -rf /var/lib/apt/lists/*

# 3. Install Python packages
RUN pip3 install --no-cache-dir flask flask-mysql

# 4. Copy application source
WORKDIR /opt/source-code
COPY . .

# 5. Environment and ports
ENV FLASK_APP=app.py
EXPOSE 5000

# 6. Start the Flask server
ENTRYPOINT [""flask"", ""run"", ""--host=0.0.0.0""] Instruction Purpose Example FROM Sets the base image FROM ubuntu:20.04 RUN Executes commands in a new layer RUN pip3 install flask flask-mysql WORKDIR Sets working directory inside the container WORKDIR /opt/source-code COPY Copies files from host to container COPY . . EXPOSE Documents the port the container listens on EXPOSE 5000 ENTRYPOINT Defines the startup command ENTRYPOINT [""flask"", ""run"", ...] 3. Build, Tag, and Push Your Image Build the image locally and add a tag: docker build -t your-dockerhub-username/flask-app:latest . Push to Docker Hub (replace with your repository): docker push your-dockerhub-username/flask-app:latest 4. Inspecting Image Layers with docker history Each Dockerfile instruction creates a new image layer. To view these layers and their sizes, run: docker history your-dockerhub-username/flask-app:latest The output lists layers in reverse order, showing the command and size of each layer. Warning If you frequently change application code but not OS dependencies, structure your Dockerfile so that COPY . . appears after installing system and Python packages. This maximizes cache reuse and speeds up rebuilds. 5. Leveraging Build Cache for Faster Iteration Docker caches successful build steps. On subsequent builds: Steps unchanged since the last build use cached layers. Only modified steps (and those that follow) are re-executed. Example: $ docker build -t flask-app:latest .
...
Step 2/6 : RUN apt-get update && apt-get install -y python3 python3-pip
 ---> Using cache
...
Step 5/6 : COPY . .
 ---> 123abc456def      # Rebuilds only this layer and ENTRYPOINT step By isolating frequently changing instructions toward the end of your Dockerfile, you accelerate development cycles. Links and References Dockerfile reference Flask Documentation Docker Best Practices Watch Video Watch video content"
Docker Certified Associate Exam Course,Demo Build Tomcat image,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Image-Management/Demo-Build-Tomcat-image,"Docker Certified Associate Exam Course Docker Image Management Demo Build Tomcat image In this tutorial, you’ll learn how to create a Docker image for Apache Tomcat on a CentOS 7 base. We’ll introduce a build-time argument ( ARG ) for Tomcat versioning, and demonstrate how to override it at build time. Table of Contents Prerequisites Clone the Repository Reviewing the Dockerfile Dockerfile Instruction Reference Building and Running the Default Image Building with a Custom Tomcat Version Links and References Prerequisites Docker Engine installed (≥ 19.03) Basic familiarity with Docker commands Internet access to download Tomcat and sample WAR Note Ensure you have enough disk space (~500 MB) and proper network connectivity to Apache archives. Clone the Repository Get the sample project containing our Dockerfile : cd /tmp
git clone https://github.com/yogeshraheja/dockertomcat.git
cd dockertomcat
ls -ltr
# README.md  Dockerfile Reviewing the Dockerfile Below is the complete Dockerfile . It installs OpenJDK 8, downloads Tomcat into /opt/tomcat , sets permissions, and deploys a sample WAR file. FROM centos:7

# Define a build-time variable for Tomcat version
ARG tomcat_version=8.5.6

# Install prerequisites
RUN yum install -y epel-release java-1.8.0-openjdk.x86_64 wget

# Create tomcat group and home directory
RUN groupadd tomcat && mkdir -p /opt/tomcat

# Create non-interactive tomcat user
RUN useradd -s /bin/nologin -g tomcat -d /opt/tomcat tomcat

# Download and extract Tomcat
WORKDIR /
RUN wget https://archive.apache.org/dist/tomcat/tomcat-8/v${tomcat_version}/bin/apache-tomcat-${tomcat_version}.tar.gz \
 && tar -zxvf apache-tomcat-${tomcat_version}.tar.gz -C /opt/tomcat --strip-components=1

# Set ownership and permissions
RUN cd /opt/tomcat \
 && chgrp -R tomcat conf bin lib logs temp webapps work \
 && chmod g+rx conf \
 && chmod g+r conf/* \
 && chown -R tomcat tomcat logs temp webapps work \
 && chmod g+r bin/*

# Deploy sample application
WORKDIR /opt/tomcat/webapps
RUN wget https://tomcat.apache.org/tomcat-7.0-doc/appdev/sample/sample.war

# Expose port and define startup command
EXPOSE 8080
CMD [""/opt/tomcat/bin/catalina.sh"",""run""] Dockerfile Instruction Reference Instruction Description Example FROM Base image centos:7 ARG Build-time variable; default Tomcat version ARG tomcat_version=8.5.6 RUN Execute shell commands (install, create, download) yum install -y java-1.8.0-openjdk wget WORKDIR Set working directory /opt/tomcat/webapps EXPOSE Document container port 8080 CMD Default command when container starts [""/opt/tomcat/bin/catalina.sh"",""run""] Building and Running the Default Image Build with the default Tomcat version ( 8.5.6 ): docker build -t yogeshraheja/tomcatone:v1 . Expected output: Successfully built <IMAGE_ID>
Successfully tagged yogeshraheja/tomcatone:v1 Run the container, mapping host port 84 to container port 8080: docker run -d --name tomcat_default -p 84:8080 yogeshraheja/tomcatone:v1 Now visit http://<host>:84 in your browser. You should see the Tomcat welcome page for version 8.5.6. Building with a Custom Tomcat Version You can override tomcat_version at build time to use any release from the Apache Tomcat Archive : docker build \
  --build-arg tomcat_version=8.5.8 \
  -t yogeshraheja/tomcatone:v2 . Sample output: Successfully built <NEW_IMAGE_ID>
Successfully tagged yogeshraheja/tomcatone:v2 Verify both images: docker image ls yogeshraheja/tomcatone
# REPOSITORY             TAG    IMAGE ID       SIZE
# yogeshraheja/tomcatone  v2     <NEW_IMAGE_ID> 497MB
# yogeshraheja/tomcatone  v1     <OLD_IMAGE_ID> 497MB Run the new container on port 86: docker run -d --name tomcat_custom -p 86:8080 yogeshraheja/tomcatone:v2 Now open http://<host>:86 to confirm you’re running Tomcat version 8.5.8. Warning Always verify that the Tomcat version you specify in --build-arg exists in the archive. Incorrect versions will cause the build to fail. Links and References Dockerfile Reference Apache Tomcat Archive Docker Documentation GitHub: dockertomcat Watch Video Watch video content"
Docker Certified Associate Exam Course,CGroups,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine-Security/CGroups,"Docker Certified Associate Exam Course Docker Engine Security CGroups Linux control groups (cgroups) are a critical Linux kernel feature that provides fine-grained control over system resources—such as CPU, memory, network bandwidth, and block I/O—by organizing processes into hierarchical groups. Container platforms like Docker rely on cgroups to enforce resource constraints, ensuring each container consumes only its allocated share of host resources. This isolation improves performance predictability, security, and density on shared infrastructure. Note Before you begin, verify that your host kernel supports the desired cgroups version. Modern distributions default to cgroups v2, while Docker remains compatible with both v1 and v2. Resource Type Docker Flag Description CPU --cpus , --cpu-shares Limit CPU cores or adjust relative CPU weight Memory --memory , --memory-swap Set maximum RAM usage and optional swap space Block I/O --blkio-weight Control disk I/O priority (range: 10–1000) Network docker run --network Configure network mode; use tc for bandwidth caps In the following sections, we will demonstrate how to apply cgroup-based resource limits to Docker containers, with practical examples for CPU , memory , block I/O , and network configurations. Watch Video Watch video content"
Docker Certified Associate Exam Course,COPY vs ADD,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Image-Management/COPY-vs-ADD,"Docker Certified Associate Exam Course Docker Image Management COPY vs ADD In this guide, we’ll compare the COPY and ADD directives in a Dockerfile, highlight their differences, and share best practices for keeping your images predictable and lean. Why It Matters Both COPY and ADD bring files and directories from your build context into the container’s filesystem. However, ADD has two extra behaviors that can be surprising: Automatic extraction of local archives Remote URL download at build time By understanding these differences, you can write clearer Dockerfiles and avoid unintended side effects. Feature Comparison Directive Copies Local Files/Dirs Extracts Local Archives Downloads Remote URLs COPY ✔️ ❌ ❌ ADD ✔️ ✔️ ✔️ Warning Overusing ADD can introduce unexpected files or extra layers. If you only need to transfer files, prefer COPY . Simple Usage Examples 1. Using COPY A straightforward copy of testdir from your context into the image: FROM centos:7
COPY testdir /testdir 2. Using ADD for a Local Directory Functionally identical to COPY when the source is a directory: FROM centos:7
ADD testdir /testdir 3. ADD to Extract a Local Archive Automatically unpack app.tar.xz into /testdir : FROM centos:7
ADD app.tar.xz /testdir Consolidating Steps with RUN Multiple RUN instructions add layers. Combine download, extraction, build, and cleanup in one RUN to keep images small: FROM centos:7

RUN curl -fsSL http://example.com/app.tar.xz \
    | tar -xJ -C /testdir \
    && cd /testdir \
    && yarn build This single-layer approach removes the archive stream in-flight, leaving no temporary files behind. When You Need ADD for Remote Files If you prefer ADD to fetch a URL, then extract manually: FROM centos:7

# Downloads to /testdir/app.tar.xz
ADD http://example.com/app.tar.xz /testdir

# Extract & build in a separate step
RUN tar -xJf /testdir/app.tar.xz -C /tmp/app \
    && make -C /tmp/app Note For clarity and layer reduction, consider using a single RUN with curl and tar instead of ADD . Best Practices Use COPY for straightforward file and directory transfers. Reserve ADD for: Local archive auto-extraction ( .tar , .tar.gz , etc.). Quick remote downloads without further processing. Combine commands in a single RUN to minimize image layers and overall size. Links and References Dockerfile reference: COPY Dockerfile reference: ADD Docker best practices Watch Video Watch video content"
Docker Certified Associate Exam Course,Networking Deep Dive Namespaces,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine-Networking/Networking-Deep-Dive-Namespaces,"Docker Certified Associate Exam Course Docker Engine Networking Networking Deep Dive Namespaces In this tutorial, we take a deep dive into Linux network namespaces —the building blocks of container network isolation (e.g., in Docker ). Think of your host as a house and each network namespace as a private room: containers inside one room cannot see interfaces or processes in another. The host, however, has a global view of all “rooms.” Note Most of these commands require root privileges or sudo . Ensure you have the appropriate permissions before proceeding. 1. Process Isolation Inside a container’s PID namespace, a process always appears as PID 1. From the host’s root namespace, the same process has a distinct PID among all host processes: # Inside the container (PID namespace)
ps aux
# ...
root     1   0.0  0.0   4528   828 ?   Ss   03:06  0:00 nginx

# On the host
ps aux
# ...
root   3816  1.0  0.0   4528   828 ?   Ss   06:06  0:00 nginx 2. Creating Network Namespaces By default, the host’s network stack is isolated to its own namespace. To spin up isolated network domains: # Create two namespaces
ip netns add red
ip netns add blue

# Verify namespaces
ip netns list
# red
# blue Command Description ip netns add NAME Create a network namespace ip netns delete NAME Remove a namespace ip netns list List existing namespaces ip netns exec NAME <cmd> Run <cmd> inside namespace NAME 3. Inspecting Interfaces Inside a Namespace On the host, you’ll see all physical and virtual interfaces: ip link show
# 1: lo: <LOOPBACK,UP,LOWER_UP> ...
# 2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> ... Within red , only the loopback interface exists: # Method 1: ip netns exec
ip netns exec red ip link show

# Method 2: shorthand
ip -n red link show

# Output:
# 1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 ... No host interfaces (like eth0 ) appear in red . ARP and routing tables start empty: ip netns exec red arp           # no entries
ip netns exec red ip route      # default only loopback 4. Connecting Two Namespaces with veth Pairs To create a virtual “cable” between red and blue , use a veth pair: # Create veth pair
ip link add veth-red type veth peer name veth-blue

# Move each end into its namespace
ip link set veth-red  netns red
ip link set veth-blue netns blue

# Assign IPs and bring up interfaces
ip -n red  addr add 192.168.15.1/24 dev veth-red
ip -n red  link set veth-red up

ip -n blue addr add 192.168.15.2/24 dev veth-blue
ip -n blue link set veth-blue up Test connectivity: ip -n red ping -c1 192.168.15.2
# 64 bytes from 192.168.15.2: icmp_seq=1 ttl=64 time=0.xxx ms ARP tables populate automatically: ip -n red  arp
ip -n blue arp
# 192.168.15.1 ether 7a:9d:9b:c8:3b:7f C veth-blue 5. Building a Virtual Switch with a Bridge Connecting many namespaces via direct veth pairs is impractical. Instead, create a Linux bridge on the host: # Create and enable bridge
ip link add v-net-0 type bridge
ip link set v-net-0 up Remove the direct link in red : ip -n red link del veth-red Recreate veth pairs for each namespace and attach them to the bridge: # red ↔ bridge
ip link add veth-red     type veth peer name veth-red-br
ip link set veth-red netns red
ip link set veth-red-br master v-net-0

# blue ↔ bridge
ip link add veth-blue    type veth peer name veth-blue-br
ip link set veth-blue netns blue
ip link set veth-blue-br master v-net-0 Assign IPs and bring them up: ip -n red  addr add 192.168.15.1/24 dev veth-red
ip -n red  link set veth-red up

ip -n blue addr add 192.168.15.2/24 dev veth-blue
ip -n blue link set veth-blue up All namespaces on v-net-0 can now communicate via the bridge. 6. Host–Namespace Connectivity To let the host join this virtual network, assign v-net-0 an IP in the same subnet: ip addr add 192.168.15.5/24 dev v-net-0 Now the host can ping into any namespace: ping -c1 192.168.15.1
# 64 bytes from 192.168.15.1: icmp_seq=1 ttl=64 time=0.xxx ms 7. Namespace → LAN Connectivity via Host By default, namespaces cannot reach external LANs: ip -n blue ping 192.168.1.3
# Connect: Network is unreachable Check blue ’s routes: ip -n blue ip route
# 192.168.15.0/24 dev veth-blue proto kernel scope link Add a route via the host (gateway 192.168.15.5 ): ip -n blue ip route add 192.168.1.0/24 via 192.168.15.5 Enabling NAT on the Host Warning Be careful when modifying iptables rules on production systems. Always test in a safe environment first. # Masquerade outbound traffic from your virtual subnet
iptables -t nat -A POSTROUTING -s 192.168.15.0/24 -j MASQUERADE

# Set default route in the namespace
ip -n blue ip route add default via 192.168.15.5 Now blue can reach the internet (e.g., 8.8.8.8 ): ip -n blue ping -c1 8.8.8.8
# 64 bytes from 8.8.8.8: icmp_seq=1 ttl=115 time=XX ms 8. Port Forwarding into a Namespace To expose a service (e.g., HTTP on port 80) running in blue , use iptables DNAT on the host: iptables -t nat -A PREROUTING --dport 80 \
  -j DNAT --to-destination 192.168.15.2:80 Now requests to the host’s port 80 are transparently forwarded into blue . 9. Summary of Key Commands Task Command Example Create namespace ip netns add NAME Execute in namespace ip netns exec NAME <cmd> Create veth pair ip link add veth-A type veth peer name veth-B Create bridge ip link add BRIDGE type bridge Attach interface to bridge ip link set IFACE master BRIDGE Assign IP ip -n NS addr add IP/MASK dev IFACE Enable masquerading (NAT) iptables -t nat -A POSTROUTING -s SUBNET -j MASQUERADE DNAT for port forwarding iptables -t nat -A PREROUTING --dport PORT -j DNAT --to-destination IP:PORT Links and References Kubernetes Documentation iproute2 Manual Linux Bridge Wiki Docker Networking Watch Video Watch video content"
Docker Certified Associate Exam Course,Networking Deep Dive Docker,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine-Networking/Networking-Deep-Dive-Docker,"Docker Certified Associate Exam Course Docker Engine Networking Networking Deep Dive Docker In this lesson, we explore Docker networking, covering built-in modes and Linux network namespaces. You’ll learn how Docker uses a bridge network, veth pairs, and iptables NAT to connect containers and expose services. Docker Networking Modes Docker offers several network modes on a single host (e.g., host IP 192.168.1.10 on eth0 ): Mode Behavior Example none No network interfaces except loopback docker run --network none nginx host Shares the host’s network stack directly docker run --network host nginx bridge Default: containers attach to the docker0 bridge docker run nginx none The container only has a loopback interface and cannot send or receive external traffic. host Containers share the host network namespace directly. Warning Using --network host removes network isolation. Ports in the container map directly to the host and may conflict with other services. bridge The default mode creates a docker0 bridge with a 172.17.0.0/16 subnet. Containers receive an IP on this network. List Docker networks and host interfaces: docker network ls
ip link show You’ll see an interface named docker0 : 3: docker0: <NO-CARRIER,BROADCAST,MULTICAST> mtu 1500 qdisc noqueue state DOWN 
    link/ether 02:42:88:56:50:83 brd ff:ff:ff:ff:ff:ff Inspect its IP address: ip addr show docker0 Docker and Network Namespaces Each container runs in its own Linux network namespace. To view Docker namespaces on the host: sudo ip netns
# Example output:
b3165c10a92b Inspect a container’s sandbox and namespace path: docker inspect 942d70e585b2 \
  --format '{{json .NetworkSettings}}' {
  ""Bridge"": """",
  ""SandboxID"": ""b3165c10a92b50edc4c8aa5f37273e180907ded31"",
  ""SandboxKey"": ""/var/run/docker/netns/b3165c10a92b""
} When a container starts, Docker creates a veth pair: One end attaches to the host bridge ( docker0 ). The other end goes inside the container namespace as eth0 . Host side: ip link show
# Example:
8: vethbb1c343@i7f: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue master docker0
    link/ether 9e:71:37:83:9f:50 brd ff:ff:ff:ff:ff:ff link-netnsid 1 Container side: ip -n b3165c10a92b addr
# Example:
7: eth0@if8: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue 
    link/ether 02:42:ac:11:00:03 brd ff:ff:ff:ff:ff:ff 
    inet 172.17.0.3/16 brd 172.17.255.255 scope global eth0 Each new container repeats this process, assigning a unique IP in 172.17.0.0/16 . Container-to-Container and Host Communication Containers on the same bridge can communicate by IP. The host also reaches them directly: curl http://172.17.0.3:80
# => Welcome to nginx! Note External clients cannot access container IPs on the bridge network without port publishing. Publishing Ports (Port Mapping) Expose container ports to external clients with -p hostPort:containerPort : docker run -p 8080:80 nginx Access via http://192.168.1.10:8080 : curl http://192.168.1.10:8080
# => Welcome to nginx! Behind the Scenes: iptables NAT Docker adds iptables NAT rules to forward traffic: iptables -t nat -A DOCKER -p tcp --dport 8080 \
  -j DNAT --to-destination 172.17.0.3:80 This ensures incoming connections on host port 8080 are redirected to the container’s port 80. References Docker Networking Linux Network Namespaces iptables Documentation Watch Video Watch video content"
Docker Certified Associate Exam Course,Demo Image Creation Docker Commit Method,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Image-Management/Demo-Image-Creation-Docker-Commit-Method,"Docker Certified Associate Exam Course Docker Image Management Demo Image Creation Docker Commit Method In this tutorial, you’ll learn how to build an HTTPD webserver image on top of a CentOS 7 base image using the Docker commit method. By the end, you’ll have a reusable image that includes your custom Apache configuration and index page. 1. Pull the CentOS 7 Base Image Start by pulling the official CentOS 7 image from Docker Hub: docker image pull centos:7 You should see output like: 7: Pulling from library/centos
75f829a71a1c: Pull complete
Digest: sha256:19a79828ca2505eae0ff38c2f39901f4826737295157cc5212b7a372cd2b
Status: Downloaded newer image for centos:7
docker.io/library/centos:7 Verify it’s available locally: docker image ls Example: REPOSITORY   TAG   IMAGE ID       CREATED        SIZE
centos       7     7e6257c9f8d8   2 months ago   203MB 2. Create and Start a Container Create a container named test from the CentOS 7 image: docker container create --name test centos:7 Start it: docker container start test Attach an interactive shell: docker container exec -it test /bin/bash 3. Install HTTPD and Customize the Web Page Inside the container shell: yum -y update
yum install -y httpd
echo ""<h1>Hello from KodeKloud</h1>"" > /var/www/html/index.html This installs Apache HTTPD and replaces the default index page. Tip You can test the Apache service within the container before committing: httpd -k start
curl http://localhost
httpd -k stop 4. Commit the Container to a New Image Exit and stop the container: exit
docker container stop test Review container status: docker container ls -l Commit with metadata and a default CMD . Here’s an overview of common flags: Flag Description Example -a Specify the author -a ""Yogesh Raheja"" -m Add a commit message -m ""Add HTTPD and custom index"" -c Set a Dockerfile instruction (e.g., CMD) -c 'CMD [""httpd"",""-D"",""FOREGROUND""]' Run docker commit : docker container commit \
  -a ""Yogesh Raheja"" \
  -m ""Add HTTPD and custom index"" \
  -c 'CMD [""httpd"", ""-D"", ""FOREGROUND""]' \
  test webtest:v1 You’ll get a new image ID, for example: sha256:9cd11553a2e7... Verify the image list: docker image ls Expected: REPOSITORY   TAG   IMAGE ID       CREATED         SIZE
webtest      v1    9cd11553a2e7   30 seconds ago  328MB
centos       7     7e6257c9f8d8   2 months ago    203MB 5. Test Your Custom Image Launch a container from webtest:v1 , mapping port 80: docker container run -d --name webtesting -p 80:80 webtest:v1 Confirm it’s running: docker container ls -l Open http://<DockerHostIP> in your browser. You should see: Hello from KodeKloud This verifies that your HTTPD configuration and custom index page are baked into the image. 6. Tag and Push to Docker Hub Tag the image for your repository (replace <your-dockerhub-username> ): docker image tag webtest:v1 <your-dockerhub-username>/codekloud-webtest:v1 Log in to Docker Hub: docker login Push the tagged image: docker push <your-dockerhub-username>/codekloud-webtest:v1 Visit your Docker Hub repository to confirm the v1 tag is published. Congratulations! You’ve successfully created and published a custom Docker image using the docker commit method. References Docker Commit Documentation Docker Image Best Practices CentOS on Docker Hub Watch Video Watch video content"
Docker Certified Associate Exam Course,CMD vs Entrypoint,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Image-Management/CMD-vs-Entrypoint,"Docker Certified Associate Exam Course Docker Image Management CMD vs Entrypoint In this guide, we’ll explore how Docker uses the CMD and ENTRYPOINT instructions to define the default process of a container. You’ll learn how to override or extend these defaults at runtime and bake permanent changes into your images. Why Containers Exit Immediately When you run a container without specifying a command, Docker launches the default process defined in the image’s Dockerfile. If that process ends, the container exits: docker run ubuntu
docker ps              # no running containers
docker ps -a           # shows the new container in exited state Unlike virtual machines, containers are lightweight and designed to run a single task—such as a web server, database, or script. When that main process completes or fails, the container stops. Note A container only runs as long as its main process is alive. Defining a long-running service or shell will keep it running. Examining Official Images Popular Docker images set up their primary service using CMD or ENTRYPOINT . Let’s look at two examples: Nginx Dockerfile Excerpt # Install Nginx.
RUN \
    add-apt-repository -y ppa:nginx/stable && \
    apt-get update && \
    apt-get install -y nginx && \
    rm -rf /var/lib/apt/lists/* && \
    echo ""\ndaemon off;"" >> /etc/nginx/nginx.conf && \
    chown -R www-data:www-data /var/lib/nginx

# Define mountable directories.
VOLUME [""/etc/nginx/sites-enabled"", ""/etc/nginx/certs""]

# Define working directory.
WORKDIR /etc/nginx

# Define default command.
CMD [""nginx""] MySQL Dockerfile Excerpt # Install server and dependencies
RUN rpmkeys --import https://repo.mysql.com/RPM-GPG-KEY-mysql \
    && yum install -y $MYSQL_SERVER_PACKAGE_URL $MYSQL_SHELL_PACKAGE_URL libpwquality \
    && yum clean all \
    && mkdir /docker-entrypoint-initdb.d

VOLUME /var/lib/mysql

COPY docker-entrypoint.sh /entrypoint.sh
COPY healthcheck.sh /healthcheck.sh

# Define entrypoint script.
ENTRYPOINT [""/entrypoint.sh""]

# Healthcheck.
HEALTHCHECK CMD /healthcheck.sh

EXPOSE 3306 33060

# Default command to start the server.
CMD [""mysqld""] The Default Ubuntu Container Runs Bash The official Ubuntu image uses bash as its default command. Without an interactive TTY, Bash exits immediately: FROM ubuntu:14.04

RUN \
    sed -i 's/# \(.*multiverse$\)/\1/g' /etc/apt/sources.list && \
    apt-get update && \
    apt-get -y upgrade && \
    apt-get install -y build-essential software-properties-common \
                       byobu curl git htop man unzip vim wget && \
    rm -rf /var/lib/apt/lists/*

ADD root/.bashrc /root/.bashrc
ADD root/.gitconfig /root/.gitconfig
ADD root/.scripts /root/.scripts

ENV HOME /root
WORKDIR /root

CMD [""bash""] Running docker run ubuntu without -t gives Bash no TTY, so it exits immediately—causing the container to stop. Overriding CMD at Runtime You can override the default CMD by appending your own command in docker run : docker run ubuntu sleep 5 This runs sleep 5 , keeps the container alive for 5 seconds, then exits. Making the Change Permanent with CMD To bake a default command into your image, declare a new CMD in your Dockerfile: FROM ubuntu
CMD [""sleep"", ""5""] Build and run: docker build -t ubuntu-sleeper .
docker run ubuntu-sleeper    # sleeps 5 seconds and exits Shell Form vs Exec Form CMD can use shell form: CMD sleep 5 or exec form (JSON array): CMD [""sleep"", ""5""] With exec form, Docker does not invoke a shell, and the first element must be the executable. ENTRYPOINT vs CMD Use ENTRYPOINT to fix the executable but allow arguments to vary: FROM ubuntu
ENTRYPOINT [""sleep""] Then: docker run ubuntu-sleeper 10    # runs: sleep 10 With only CMD , any arguments passed to docker run replace the entire command line. Note ENTRYPOINT locks in your executable. Combine it with CMD to set default parameters. Combining ENTRYPOINT and CMD To specify both a fixed executable and default arguments: FROM ubuntu
ENTRYPOINT [""sleep""]
CMD [""5""] docker run ubuntu-sleeper runs sleep 5 docker run ubuntu-sleeper 10 runs sleep 10 Warning Always use the JSON array form for both ENTRYPOINT and CMD when combining them. This ensures proper argument handling. Quick Comparison Instruction Purpose Override Behavior CMD Default command or parameters Replaced by arguments on docker run ENTRYPOINT Fixed executable for the image Overridable with --entrypoint flag Overriding ENTRYPOINT You can also override ENTRYPOINT at runtime: docker run --entrypoint sleep2.0 ubuntu-sleeper 10 This runs sleep2.0 10 instead of the original sleep . Links and References Dockerfile reference docker run reference Docker Official Images Watch Video Watch video content"
Docker Certified Associate Exam Course,Base vs Parent Image,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Image-Management/Base-vs-Parent-Image,"Docker Certified Associate Exam Course Docker Image Management Base vs Parent Image In Docker, every custom image starts “FROM” another image—its parent. Tracing this chain leads to the special scratch image, the true base for all builds. Note A base image is the origin of an image chain (often scratch ), while a parent image is the one directly referenced by your FROM instruction. 1. Custom Web Application Image Create a minimal Apache-based web server by extending the official HTTPD image: FROM httpd
COPY index.html /usr/local/apache2/htdocs/index.html Parent image : httpd Your COPY command layers application files on top of the Apache HTTPD server. 2. Inside the HTTPD Official Image The official HTTPD image itself builds on Debian: FROM debian:buster-slim

ENV HTTPD_PREFIX=/usr/local/apache2
ENV PATH=$HTTPD_PREFIX/bin:$PATH
WORKDIR $HTTPD_PREFIX

# Installation steps for Apache HTTPD... Parent image : debian:buster-slim Installs Apache HTTPD on a minimal Debian base. 3. Exploring the Debian Base Image The Debian “slim” image originates from scratch : FROM scratch
ADD rootfs.tar.xz /
CMD [""bash""] Base image : scratch Unpacks a minimal Debian filesystem from a tarball. Warning You cannot push or pull the scratch image—it's only referenced in FROM scratch directives. Table: Image Lineage at a Glance Layer Image Tag Parent Description Application custom-web-app:latest httpd:latest Apache HTTPD plus your index.html HTTPD Official httpd:latest debian:buster-slim Official Apache HTTPD on Debian Debian Base debian:buster-slim scratch Minimal Debian filesystem from scratch 4. Additional Official Image Chains Ubuntu → MongoDB Ubuntu minimal from scratch : FROM scratch
ADD ubuntu-xenial-core-cloudimg-amd64-root.tar.gz / MongoDB on Ubuntu : FROM ubuntu:xenial

RUN groupadd -r mongodb \
 && useradd -r -g mongodb mongodb

RUN set -eux; \
    apt-get update; \
    apt-get install -y --no-install-recommends \
        ca-certificates jq numactl; \
    if ! command -v ps >/dev/null; then \
        apt-get install -y --no-install-recommends procps; \
    fi WordPress → PHP → Debian The official WordPress image builds on the official PHP image , which in turn uses Debian—demonstrating another scratch → Debian → PHP → WordPress chain. 5. The scratch Image: Docker’s True Base scratch is Docker’s reserved, empty image. It marks the very beginning of any build. By adding a minimal OS filesystem on top of scratch , maintainers create base images (Debian, Alpine, Ubuntu), which then serve as parents for application and service images. For more details on building and using base images, see the official Docker documentation on base images . Links and References Docker Base Image Documentation HTTPD Official Image Debian Official Image Ubuntu Official Image MongoDB Official Image WordPress Official Image PHP Official Image Watch Video Watch video content"
Docker Certified Associate Exam Course,Build Cache,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Image-Management/Build-Cache,"Docker Certified Associate Exam Course Docker Image Management Build Cache Efficient use of Docker’s build cache can dramatically speed up your image builds. Docker creates a cache layer for each instruction in your Dockerfile. When you rebuild the image, Docker reuses layers whose instructions and contexts haven’t changed, avoiding redundant work. How Docker’s Build Cache Works FROM ubuntu
RUN apt-get update
RUN apt-get install -y python python3-pip
RUN pip3 install flask
COPY app.py /opt/source-code
ENTRYPOINT [""flask"", ""run""] Each RUN , COPY , or ADD instruction produces a layer. After a successful build, layers are stored in the local cache. On subsequent builds, Docker compares: The instruction itself. Any files referenced by COPY / ADD . If both match the cached layer, Docker reuses it. Any change invalidates that layer and all subsequent layers , triggering a rebuild from that point. Cache Invalidation Example Changing the pip install command: RUN pip3 install flask flask-mysql Invalidates the pip3 install layer and everything that follows—earlier layers remain cached. Similarly, updating app.py in: COPY app.py /opt/source-code busts the cache from that layer onward. Cache Busting with Combined Instructions Separating apt-get update and apt-get install can lead to stale package lists: RUN apt-get update
RUN apt-get install -y python python3-pip python-dev Warning Stale package lists may cause installation of outdated or missing packages. Instead, combine them: RUN apt-get update && \
    apt-get install -y \
      python \
      python-dev \
      python3-pip Forces an update immediately before installation. Lists packages alphabetically and on separate lines for readability. Note Always include && rm -rf /var/lib/apt/lists/* if you want to reduce image size. Version Pinning Pinning package versions ensures consistent builds across environments: RUN apt-get update && \
    apt-get install -y \
      python \
      python-dev \
      python3-pip=20.0.2 Optimizing Instruction Order Place instructions that change least frequently at the top of your Dockerfile. This maximizes cache reuse. Instruction Type Change Frequency Caching Benefit Base Image & System Low Cached once unless you change the base Package Installation Low–Medium Reused until you add or remove packages Application Dependencies Medium Rebuilt when dependencies change Application Code High Only this layer rebuilds on code changes Example: Optimal Order FROM ubuntu

# 1. Infrequently changed – cached once
RUN apt-get update && \
    apt-get install -y \
      python \
      python-dev \
      python3-pip=20.0.2

# 2. Dependencies – rebuild when you add/remove libs
RUN pip3 install flask flask-mysql

# 3. Code – fastest iteration on changes
COPY app.py /opt/source-code

ENTRYPOINT [""flask"", ""run""] By contrast, placing COPY app.py first forces Docker to rerun all subsequent layers on every code update, significantly slowing builds. Further Reading Dockerfile Best Practices Docker Build Cache Documentation Watch Video Watch video content"
Docker Certified Associate Exam Course,Build Contexts,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Image-Management/Build-Contexts,"Docker Certified Associate Exam Course Docker Image Management Build Contexts In this lesson, we'll explore what a build context is and how it influences the Docker build process. Understanding build contexts helps you optimize build times and reduce image size by sending only the necessary files to the Docker daemon. What Is a Build Context? The build context is the set of files and folders the Docker CLI packages and sends to the Docker daemon when running docker build . By default, Docker uses the current directory ( . ) as the build context. docker build . -t my-custom-app This command: Archives everything under . . Sends it to the Docker daemon. Unpacks it into a temporary directory (e.g., /var/lib/docker/tmp/... ). Executes the instructions in your Dockerfile . Note If you omit the -t (tag) flag, Docker builds the image and assigns the latest tag by default: docker build .
# results in an image tagged: IMAGE_ID:latest Example Dockerfile for a Flask App FROM ubuntu

# Install Python and pip
RUN apt-get update && apt-get install -y python python-pip

# Install Flask and MySQL connector
RUN pip install flask flask-mysql

# Copy application source into the image
COPY . /opt/source-code

# Set environment variable and entrypoint
ENV FLASK_APP=/opt/source-code/app.py
ENTRYPOINT [""flask"", ""run""] Specifying a Different Build Context You can point Docker to any local directory containing your Dockerfile : docker build /opt/my-custom-app -t my-custom-app Docker will look for /opt/my-custom-app/Dockerfile and include all files under /opt/my-custom-app in the context. Common Context Sources Context Source Command Example Description Current directory docker build . -t my-custom-app Sends . as the context Local path docker build /opt/my-custom-app -t my-custom-app Uses a specified folder Git repository docker build https://github.com/myaccount/myapp.git#feature-branch Clones a repo (or branch) as the build context Custom Dockerfile docker build -f Dockerfile.dev https://github.com/myaccount/myapp.git Specifies an alternative Dockerfile location Managing Context Size with .dockerignore Sending large or unnecessary files (logs, build artifacts) can slow down builds, especially when the daemon is remote. To prevent this, create a .dockerignore file in your context root: tmp
logs
build Docker will exclude these paths when packaging the build context. Warning Be careful: missing important source files in .dockerignore can lead to build failures or incomplete images. Remote Docker Daemon Output When using a remote Docker daemon, you’ll see output similar to: Sending build context to Docker daemon  2.048kB
Step 1/7 : FROM ubuntu
... This confirms the context has been sent over the network before the build steps execute. Building from a Git Repository Docker can directly use Git URLs as the build context: # Clone the default branch
docker build https://github.com/myaccount/myapp

# Build a specific branch
docker build https://github.com/myaccount/myapp#feature-branch

# Build only a subfolder within the repo
docker build https://github.com/myaccount/myapp.git#docker By default, Docker looks for Dockerfile at the root of the checked‐out code. Use -f to point to a different file: docker build -f Dockerfile.dev https://github.com/myaccount/myapp Summary The build context defines what files are sent to the Docker daemon. Use .dockerignore to exclude unnecessary files and speed up builds. You can build from local paths or Git repositories. The -f flag lets you specify a non-default Dockerfile. Links and References Docker Build Reference Dockerignore Documentation Watch Video Watch video content"
Docker Certified Associate Exam Course,Multi Stage Builds,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Image-Management/Multi-Stage-Builds,"Docker Certified Associate Exam Course Docker Image Management Multi Stage Builds Containerizing a Node.js web application often involves separate build and packaging steps. Docker’s multi-stage builds streamline this process into a single, maintainable Dockerfile that produces smaller, more consistent images. 1. Local Build and Basic Containerization First, you might compile your app locally: npm run build This generates a dist/ folder with your production assets. To serve it via Nginx, you could write: # Dockerfile (production)
FROM nginx
COPY dist /usr/share/nginx/html
CMD [""nginx"", ""-g"", ""daemon off;""] Build and run: docker build -t my-app .
docker run -d -p 80:80 my-app Command Description npm run build Compile source into dist/ docker build -t my-app . Build Docker image docker run -d -p 80:80 my-app Launch container on host port 80 Drawbacks of This Approach Issue Impact Environment Drift Builds may vary across developer machines Manual Packaging Two-step process: build locally, then containerize CI/CD Complexity Every pipeline must replicate your local environment exactly 2. Using a Separate Builder Image To ensure repeatable builds, move compilation into its own container: # Dockerfile.builder
FROM node:16-alpine
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
RUN npm run build You still use the production Dockerfile from before. Then: docker build -f Dockerfile.builder -t builder .
docker build -f Dockerfile          -t my-app . Now you have: builder image with dist/ my-app image ready to serve via Nginx Warning Manually extracting artifacts involves creating temporary containers and copying files. This adds complexity and slows down CI/CD pipelines. 3. Simplifying with Multi-Stage Builds Multi-stage builds merge builder and final stages: # Dockerfile (multi-stage)
FROM node:16-alpine AS builder
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
RUN npm run build

FROM nginx:stable-alpine
COPY --from=builder /app/dist /usr/share/nginx/html
CMD [""nginx"", ""-g"", ""daemon off;""] Just build once: docker build -t my-app . What happens: builder stage installs dependencies and compiles into dist/ . final stage pulls only the built assets into a minimal Nginx image. 3.1 Using Numeric Stage References Instead of names, you can refer to stages by index: FROM node:16-alpine
# (stage 0)
WORKDIR /app
COPY . .
RUN npm install && npm run build

FROM nginx:stable-alpine
# (stage 1)
COPY --from=0 /app/dist /usr/share/nginx/html
CMD [""nginx"", ""-g"", ""daemon off;""] Note Using named stages (e.g., AS builder ) improves readability in complex Dockerfiles. 3.2 Building a Specific Stage For debugging or CI-cache purposes, target only the build stage: docker build --target builder -t my-app-builder . 4. Benefits of Multi-Stage Builds Benefit Explanation Smaller Final Image Excludes build tools and source code Single Dockerfile Easier maintenance and less duplication Faster CI/CD Leverages Docker cache across stages Enhanced Security Only runtime dependencies end up in the final image Links and References Docker Multi-Stage Builds Dockerfile reference Node.js Official Image Nginx Official Image Watch Video Watch video content"
Docker Certified Associate Exam Course,Docker Networking,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine-Networking/Docker-Networking,"Docker Certified Associate Exam Course Docker Engine Networking Docker Networking Docker simplifies container networking by providing built-in networks and easy-to-use commands for creating custom networks. Whether you need isolated environments or seamless inter-container communication, this guide covers everything from default networks to user-defined bridges, inspection commands, and internal mechanics. Built-in Docker Networks Docker creates three networks upon installation: Network Name Description Typical Use Case bridge Default private internal network on the host General container communication host Shares host’s network namespace—no isolation High-performance networking, host apps none No network interfaces except loopback Security-isolated or self-managed setups You can attach containers to any network using the --network flag: docker run --network=<network_name> ubuntu 1. Bridge Network The bridge network is Docker’s default. Each container on this network gets an internal IP (typically in 172.17.x.x ). Containers on the same bridge can communicate directly. Port Mapping Expose container ports to the host with -p : docker run -d -p 8080:80 nginx This maps port 80 in the container to port 8080 on your Docker host. Note If you omit -d , the container runs in the foreground. 2. Host Network Running with --network=host makes the container share your host’s network stack: docker run --network=host ubuntu Key points: No port mapping needed Ports in the container are the same as on the host Cannot run multiple containers on the same host port Warning Using the host network removes isolation. Only use this when you trust the container’s network behavior. 3. None Network The none network disables all external interfaces, leaving only the loopback: docker run --network=none ubuntu Use this for maximum network isolation when <em>no</em> connectivity is desired. Creating a User-Defined Bridge Network Custom bridge networks let you isolate groups of containers and define subnets: docker network create \
  --driver bridge \
  --subnet 182.18.0.0/16 \
  custom-isolated-network List all available networks: docker network ls Example output: NETWORK ID          NAME                         DRIVER    SCOPE
dba0fb9370fe        bridge                       bridge    local
4d60768bc9          custom-isolated-network      bridge    local
6de6865ce1c6        docker_gwbridge              bridge    local
e29d81be47          host                         host      local
mmrho7vb9rm         ingress                      overlay   swarm
d371b4009142        simplewebappdocker_default   bridge    local Inspecting a Container’s Network Settings To retrieve a container’s IP address and network details: docker inspect <container_id_or_name> Search for the NetworkSettings section in the JSON output: ""NetworkSettings"": {
  ""Gateway"": ""172.17.0.1"",
  ""IPAddress"": ""172.17.0.6"",
  ""MacAddress"": ""02:42:ac:11:00:06"",
  ""Networks"": {
    ""bridge"": {
      ""Gateway"": ""172.17.0.1"",
      ""IPAddress"": ""172.17.0.6"",
      ""MacAddress"": ""02:42:ac:11:00:06""
    }
  }
} Tip Use jq to filter output: docker inspect <id> | jq '.[0].NetworkSettings' Name-Based Container Communication Docker’s embedded DNS (at 127.0.0.11 ) lets containers resolve each other by name: mysql.connect(mysql) Here, mysql refers to the target container’s name. No static IPs required. Under the Hood: Namespaces & veth Pairs Docker uses Linux network namespaces to isolate containers. Communication between a container and the host bridge relies on veth (virtual Ethernet) pairs: One end lives in the container’s namespace The other end attaches to the host bridge This setup ensures both isolation and connectivity. Links and References Docker Networking Overview docker network create Linux Network Namespaces Watch Video Watch video content"
Docker Certified Associate Exam Course,Resource Limits CPU,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine-Security/Resource-Limits-CPU,"Docker Certified Associate Exam Course Docker Engine Security Resource Limits CPU In this lesson, we’ll explore how to control CPU resource usage for Docker containers using cgroups. Without limits, containers share the host kernel and can consume all available CPU and memory. If memory runs out, the Linux kernel’s OOM killer may terminate processes—including critical services—to free resources. Understanding how the Linux scheduler allocates CPU time is key to setting effective limits. CPU Scheduling on Linux On a host with a single CPU core, two processes cannot truly run simultaneously. The Linux scheduler gives each process a tiny time slice (measured in microseconds), switching rapidly so they appear parallel. When one process has more weight, it receives more or longer slices. CPU Shares and Weights CPU shares are relative weights, not hard caps. For example: Process A: 1024 shares Process B: 512 shares When both run, A gets twice the CPU time B gets. If A runs alone, it can still use 100% of the CPU despite its share count. Docker leverages the Completely Fair Scheduler (CFS) by default. While Docker 1.13+ supports a real-time scheduler, we’ll focus on CFS here. Applying CPU Shares to Containers By default, each container gets 1024 CPU shares. To change this weight: docker run --cpu-shares=512 --name webapp4 webapp In this example, webapp4 has half the CPU weight of any container using the default 1024 shares. Note CPU shares only affect relative scheduling. A container with fewer shares can still use 100% of CPU if it’s the only active workload. Restricting Containers to Specific CPUs To pin a container to specific cores, use --cpuset-cpus . On a 4-core host (0–3): docker run --cpuset-cpus=""0-1"" --name webapp1 webapp
docker run --cpuset-cpus=""0-1"" --name webapp2 webapp
docker run --cpuset-cpus=""2""   --name webapp3 webapp
docker run --cpuset-cpus=""2""   --name webapp4 webapp Here, webapp1 and webapp2 run only on cores 0 and 1; webapp3 and webapp4 on core 2. Core 3 stays free for other tasks. Limiting CPU Usage with --cpus Since Docker 1.13, you can enforce a hard CPU cap: docker run --cpus=2.5 --name webapp4 webapp This restricts webapp4 to 2.5 CPU cores (≈62.5% of a 4-core host). To update an existing container: docker update --cpus=2.5 webapp4 Warning Without proper CPU limits, a container can monopolize the host CPU, starving other containers or the Docker daemon and causing an unresponsive system. CPU Limit Configuration Options Option Description Example --cpu-shares Relative CPU weight (default: 1024 shares) docker run --cpu-shares=512 nginx --cpuset-cpus Pin container to specific CPU cores docker run --cpuset-cpus=""0-1"" nginx --cpus Hard limit on number of CPU cores docker run --cpus=2.5 nginx Links and References Docker CPU and Memory Constraints Linux CFS Scheduler Docker Control Groups (cgroups) Docker Certified Associate Exam Course Watch Video Watch video content"
Docker Certified Associate Exam Course,Networking additional commands,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine-Networking/Networking-additional-commands,"Docker Certified Associate Exam Course Docker Engine Networking Networking additional commands Manage Docker networks beyond the basics. In this guide, you’ll learn how to list default networks, inspect their settings, connect or disconnect containers, and remove unused networks to keep your Docker environment clean. Table of Contents Listing Default Networks Inspecting a Network Connecting and Disconnecting Containers Removing Networks Listing Default Networks Docker provides three built-in networks out of the box: bridge , host , and none . Use the following command to see them: docker network ls Example output: NETWORK ID          NAME      DRIVER    SCOPE
599dcaf4e856        bridge    bridge    local
c817f1bca596        host      host      local
e6508d3404a3        none      null      local You can also get a quick overview in tabular form: Network Name Driver Scope Description bridge bridge local Default network for standalone containers host host local Containers share the host’s network namespace none null local Containers have no network interfaces Note The bridge network uses the 172.17.0.0/16 subnet by default, with gateway 172.17.0.1 assigned to the docker0 interface on the host. Inspecting a Network To dive deeper into a network’s configuration—such as its IPAM settings, subnets, gateways, and attached containers—run: docker network inspect bridge Sample output (abridged): [
  {
    ""Name"": ""bridge"",
    ""Id"": ""599dcaf4e85648c3a111baa52b7530f097853b96485a8a3ffcd9088b20f0cb"",
    ""Scope"": ""local"",
    ""Driver"": ""bridge"",
    ""IPAM"": {
      ""Driver"": ""default"",
      ""Config"": [
        {
          ""Subnet"": ""172.17.0.0/16"",
          ""Gateway"": ""172.17.0.1""
        }
      ]
    },
    ""Containers"": {
      // attached container details
    }
  }
] This output shows the network’s driver, IPAM configuration, and any containers currently connected. Connecting and Disconnecting Containers You can attach a running container to additional networks or remove it from one without restarting: # Connect a container to a custom network
docker network connect custom-net my-container

# Disconnect a container from a network
docker network disconnect custom-net my-container These commands make it easy to adjust a container’s network access on the fly. Removing Networks Clean up unused networks to avoid clutter: # Remove a specific network
docker network rm custom-net

# Remove all unused networks
docker network prune Warning This will remove all networks not used by at least one container. Are you sure you want to continue? [y/N] Links and References Docker Networking Overview Docker CLI: network commands Docker Compose Networking Watch Video Watch video content"
Docker Certified Associate Exam Course,Namespaces and Capabilities,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine-Security/Namespaces-and-Capabilities,"Docker Certified Associate Exam Course Docker Engine Security Namespaces and Capabilities In this lesson, we explore how Docker leverages Linux namespaces and capabilities to isolate and secure containerized processes. You’ll learn about PID namespaces, user mappings, and fine-grained capability controls. Process Isolation with PID Namespaces Docker uses Linux namespaces to give each container its own view of system resources—PIDs, network interfaces, IPC, mounts, and time-sharing clocks: When Linux boots, it creates a single init process (PID 1) and forks all other processes from it. On the host, PIDs must remain unique, but containers also need a PID 1 without colliding with host IDs. PID namespaces solve this by providing each container its own PID space. Processes inside the container see only PIDs 1 and 2, while the host maps them to PIDs 5 and 6. Demonstration Launch a container that sleeps for an hour: docker run -d --name pid-namespace-test ubuntu sleep 3600 Inside the container, list processes: docker exec -it pid-namespace-test bash -c ""ps aux"" USER   PID %CPU %MEM    VSZ   RSS TTY STAT START   TIME COMMAND
root     1  0.0  0.0   4528   828 ?    Ss   03:06   0:00 sleep 3600 On the host, verify the same process has a different PID: ps aux | grep sleep USER   PID %CPU %MEM    VSZ   RSS TTY STAT START   TIME COMMAND
root  3816  0.0  0.0   4528   828 ?    Ss   06:06   0:00 sleep 3600 The output shows how PID namespaces isolate container processes from the host. User and Process Ownership By default, Docker runs container processes as root inside the container, which maps to root on the host. To confirm: docker run --rm ubuntu sleep 1
docker ps To run processes as a non-root user, use the --user flag: docker run --rm --user 1000 ubuntu sleep 3600
docker exec -it <container_id> bash -c ""ps aux"" USER   PID %CPU %MEM    VSZ   RSS TTY STAT START   TIME COMMAND
1000     1  0.0  0.0   4528   828 ?    Ss   03:06   0:00 sleep 3600 You can also set a default user in your Dockerfile : FROM ubuntu
USER 1000 docker build -t my-ubuntu-image .
docker run --rm my-ubuntu-image sleep 3600
docker exec -it <container_id> bash -c ""ps aux"" USER   PID %CPU %MEM    VSZ   RSS TTY STAT START   TIME COMMAND
1000     1  0.0  0.0   4528   828 ?    Ss   03:06   0:00 sleep 3600 Linux Capabilities Beyond namespaces, Docker restricts container privileges by dropping most Linux capabilities from the root user inside a container. This prevents operations like rebooting the host or altering network configurations. By default, containers run with a minimal set of capabilities. You can customize them using --cap-add , --cap-drop , or the --privileged flag: Command Description --cap-add=<CAPABILITY> Add a specific capability --cap-drop=<CAPABILITY> Remove a specific capability --privileged Grant all capabilities (not recommended) Warning Granting --privileged mode gives the container all host capabilities, which can compromise security. Use it only when absolutely necessary. Note For a full list of Linux capabilities, see Kernel Capabilities . That’s it for namespaces and capabilities in Docker. Proceed to the next lesson for more on container networking and storage. Links and References Docker run reference Linux namespaces overview Docker security documentation Watch Video Watch video content"
Docker Certified Associate Exam Course,Demo Resource Limits,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine-Security/Demo-Resource-Limits,"Docker Certified Associate Exam Course Docker Engine Security Demo Resource Limits In this lesson, you’ll learn how to manage memory and CPU usage in Docker containers. Proper resource constraints help ensure predictable performance and protect the host from runaway containers. Table of Contents Memory Limits Default Behavior Hard Memory Limit Unlimited Swap Soft Memory Reservation Memory Flags Summary CPU Limits Limiting CPU Count Pinning to Specific Cores CPU Flags Summary Memory Limits Docker containers run without memory constraints by default. You can view and modify these settings using docker run flags. Default Behavior Run a container named testone without any memory restrictions: docker container run -itd --name=testone ubuntu Inspect its memory settings: docker container inspect testone | grep -i mem Output: ""Memory"": 0,
""KernelMemory"": 0,
""KernelMemoryTCP"": 0,
""MemoryReservation"": 0,
""MemorySwap"": 0,
""MemorySwappiness"": null A value of 0 indicates “no limit.” Hard Memory Limit Set a hard cap on RAM usage with --memory . The following command limits the container to 200 MiB : docker container run -itd --name=testtwo --memory=200m ubuntu Verify: docker container inspect testtwo | grep -i mem Expected: ""Memory"": 209715200,
""KernelMemory"": 0,
""KernelMemoryTCP"": 0,
""MemoryReservation"": 0,
""MemorySwap"": 0,
""MemorySwappiness"": null Unlimited Swap By default, swap is limited to the same size as --memory . To allow unlimited swap, use --memory-swap=-1 : docker container run -itd \
  --name=testthree \
  --memory=200m \
  --memory-swap=-1 \
  ubuntu Inspect: docker container inspect testthree | grep -i mem Result: ""Memory"": 209715200,
""KernelMemory"": 0,
""KernelMemoryTCP"": 0,
""MemoryReservation"": 0,
""MemorySwap"": -1,
""MemorySwappiness"": null Warning Unlimited swap ( MemorySwap: -1 ) can lead to performance degradation if the host starts swapping heavily. Monitor your containers closely when using this option. Soft Memory Reservation With --memory-reservation , you set a soft limit that Docker tries to enforce under memory contention: docker container run -itd \
  --name=testfour \
  --memory=200m \
  --memory-reservation=100m \
  --memory-swap=-1 \
  ubuntu Inspect: docker container inspect testfour | grep -i mem Output: ""Memory"": 209715200,
""KernelMemory"": 0,
""KernelMemoryTCP"": 0,
""MemoryReservation"": 104857600,
""MemorySwap"": -1,
""MemorySwappiness"": null This configuration guarantees between 100 MiB and 200 MiB of RAM, with unlimited swap. Memory Flags Summary Flag Description Example --memory Hard limit for container memory --memory=200m --memory-reservation Soft (guaranteed) memory reservation --memory-reservation=100m --memory-swap Total memory + swap limit ( -1 = unlimited) --memory-swap=-1 --memory-swappiness Tendency to swap (0–100) --memory-swappiness=10 CPU Limits Docker lets you restrict CPU usage by setting the number of CPUs or pinning containers to specific cores. Limiting CPU Count First, check your host’s CPU count: nproc Assuming 2 cores, limit the container to 1 CPU: docker container run -itd --name=testcpu --cpus=1 ubuntu Inspect: docker container inspect testcpu | grep -i nano You’ll see: ""NanoCpus"": 1000000000, NanoCpus: 1000000000 represents 1 full CPU core. Pinning to Specific Cores Use --cpuset-cpus to bind a container to specific cores. For example, pin to core 1 : docker container run -itd --name=testcpus --cpuset-cpus=""1"" ubuntu Inspect: docker container inspect testcpus | grep -i cpuset Output: ""CpusetCpus"": ""1"", This container will only run on CPU core 1. CPU Flags Summary Flag Description Example --cpus Number of CPU cores (fractional OK) --cpus=1.5 --cpuset-cpus Bind container to specific cores --cpuset-cpus=""0,1"" Links and References Docker Run Reference Docker Resource Management Kubernetes Limits and Requests Watch Video Watch video content"
Docker Certified Associate Exam Course,Docker Daemon Security,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine-Security/Docker-Daemon-Security,"Docker Certified Associate Exam Course Docker Engine Security Docker Daemon Security Securing the Docker daemon is critical to protecting your containers, data, and host. If an attacker gains access to the Docker API, they could: Stop or delete running containers, impacting applications and users Remove volumes, causing irreversible data loss Launch malicious containers (e.g., crypto miners) Escalate to root on the host via privileged containers Docker’s default socket ( /var/run/docker.sock ) restricts access to local users. Before exposing the daemon over TCP, ensure your host follows standard hardening best practices: Disable direct root SSH logins Enforce SSH key–based authentication; disable passwords Close unused ports; restrict firewall rules Limit user accounts on the host Exposing the Docker API Over TCP To manage Docker remotely (from a CI server or management host), you can bind the daemon to a TCP endpoint. Edit /etc/docker/daemon.json and add a hosts entry: {
  ""hosts"": [
    ""unix:///var/run/docker.sock"",
    ""tcp://192.168.1.10:2375""
  ]
} Restart the daemon: sudo systemctl restart docker Warning Never expose 2375 (unencrypted) on a public interface. Always bind to a private network or VPN. Encrypting the Docker Remote API with TLS Unencrypted TCP traffic can be intercepted. To enable TLS: Create your own Certificate Authority (CA) Generate a server key ( server-key.pem ) and certificate ( server.pem ) Place them on the Docker host (e.g., /var/docker/ ) Update /etc/docker/daemon.json : {
  ""hosts"": [""tcp://192.168.1.10:2376""],
  ""tls"": true,
  ""tlscert"": ""/var/docker/server.pem"",
  ""tlskey"": ""/var/docker/server-key.pem""
} Restart Docker: sudo systemctl restart docker Client Configuration for Encryption Only On the client machine: export DOCKER_HOST=""tcp://192.168.1.10:2376""
export DOCKER_TLS=true
docker ps Warning This setup encrypts traffic but does not verify client identity. Anyone with DOCKER_TLS=true and the host address can connect. Enabling Mutual TLS Authentication (mTLS) To ensure only authorized clients connect, enable client cert verification: Generate a client key ( client-key.pem ) and certificate signing request (CSR). Sign the CSR with your CA to create client.pem . Distribute client.pem , client-key.pem , and cacert.pem to each client securely. Update /etc/docker/daemon.json : {
  ""hosts"": [""tcp://192.168.1.10:2376""],
  ""tls"": true,
  ""tlsverify"": true,
  ""tlscacert"": ""/var/docker/cacert.pem"",
  ""tlscert"": ""/var/docker/server.pem"",
  ""tlskey"": ""/var/docker/server-key.pem""
} Restart Docker: sudo systemctl restart docker Client Usage with mTLS Option 1: Place certificates in ~/.docker/ and use environment variables: export DOCKER_HOST=""tcp://192.168.1.10:2376""
export DOCKER_TLS_VERIFY=true
docker ps Note By default, Docker looks in ~/.docker/ for ca.pem , cert.pem , and key.pem . Rename your files accordingly for automatic discovery. Option 2: Specify paths explicitly: docker --tlscacert=/path/to/cacert.pem \
       --tlscert=/path/to/client.pem \
       --tlskey=/path/to/client-key.pem \
       --tlsverify \
       -H tcp://192.168.1.10:2376 ps Security Modes Overview Mode Encryption Client Auth Use Case Default (Unix socket) No N/A Local development TCP without TLS No N/A Not recommended TLS only Yes No Encrypt traffic Mutual TLS ( tlsverify ) Yes Yes Production, CI/CD environments References Docker Engine Security Docker Daemon CLI options OpenSSL Tutorial Watch Video Watch video content"
Docker Certified Associate Exam Course,Resource Limits Memory,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine-Security/Resource-Limits-Memory,"Docker Certified Associate Exam Course Docker Engine Security Resource Limits Memory In this guide, you’ll learn how to apply memory constraints to Docker containers and understand how Linux handles memory under the hood. Properly limiting container memory prevents individual workloads from exhausting host resources, improving stability and predictability. Table of Contents Linux Memory Allocation Docker’s Default Memory Behavior Setting a Hard RAM Limit with --memory Controlling Swap with --memory-swap Memory Flags Comparison Best Practices References Linux Memory Allocation A typical Linux host provides: Physical RAM (e.g., 2 GB, 4 GB, 8 GB) Swap space : disk-backed extension of RAM By default, processes may consume all available RAM. Once RAM is exhausted, the kernel resorts to swap. If both RAM and swap fill up, an Out-Of-Memory (OOM) event is triggered, and the kernel terminates processes to free memory. Docker’s Default Memory Behavior Without explicit flags, Docker containers can use all host memory (RAM + swap). This can lead to a single container consuming all resources and destabilizing the host. Note Always set memory limits in production to avoid unexpected OOM kills on the host. Setting a Hard RAM Limit with --memory Use --memory (or -m ) to cap a container’s physical RAM usage. Specify a value with a suffix: B (bytes) K (kilobytes) M (megabytes) G (gigabytes) Example: Limit RAM to 512 MB docker run --memory=512m my-webapp If the container exceeds this limit, Docker immediately kills the process with an OOM error. Unlike CPU, memory is not throttled—it’s enforced as a hard cap. Warning Exceeding the --memory limit results in an immediate container termination. Monitor your application’s memory usage with tools like docker stats . Controlling Swap with --memory-swap By default, setting only --memory allows unlimited swap usage (up to the host’s swap). To enforce a combined RAM+swap limit, use --memory-swap . The value you provide is the total memory budget: Total limit = --memory + ( --memory-swap − --memory ) Common Swap Configurations Disable swap entirely Set both flags to the same value: docker run \
  --memory=512m \
  --memory-swap=512m \
  my-webapp Here, swap available = 512 MB − 512 MB = 0 MB. Allocate specific swap Allow 256 MB swap on top of 512 MB RAM: docker run \
  --memory=512m \
  --memory-swap=768m \
  my-webapp Here, swap available = 768 MB − 512 MB = 256 MB. Memory Flags Comparison Flag Purpose Example --memory Hard cap on container’s physical RAM --memory=512m --memory-swap Total RAM + swap limit (must be ≥ --memory ) --memory=512m --memory-swap=768m --memory-swappiness Kernel swap tendency (0–100) --memory-swappiness=10 Best Practices Always set both --memory and --memory-swap in production. Use monitoring (e.g., cAdvisor , Prometheus) to track container memory. Tune --memory-swappiness to control how aggressively a container uses swap. Test under load to identify realistic memory requirements. References Docker Run Reference Managing Docker Resources Linux Memory Management Watch Video Watch video content"
Docker Certified Associate Exam Course,Demo Docker Network,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine-Networking/Demo-Docker-Network,"Docker Certified Associate Exam Course Docker Engine Networking Demo Docker Network In this lesson, we’ll explore Docker networking fundamentals: default networks, custom bridge networks, DNS resolution, and how to connect or disconnect containers from networks. By the end, you’ll understand how Docker manages container networking and how to customize it for your applications. Listing Default Networks Docker comes with three built-in networks: Network Driver Scope Description bridge bridge local Default network for newly created containers host host local Container shares the host’s network stack none null local No networking; containers are isolated To see these networks: docker network ls Example output: $ docker network ls
NETWORK ID     NAME      DRIVER    SCOPE
cf10938f5edf   bridge    bridge    local
d4f46412e7e9   host      host      local
b5b0ab8c1665   none      null      local Inspecting the Bridge Network To view details such as subnet configuration and gateway: docker network inspect bridge Key fields: [
  {
    ""Name"": ""bridge"",
    ""Driver"": ""bridge"",
    ""IPAM"": {
      ""Config"": [
        {
          ""Subnet"": ""172.17.0.0/16"",
          ""Gateway"": ""172.17.0.1""
        }
      ]
    },
    ""Options"": {
      ""com.docker.network.bridge.default_bridge"": ""true"",
      ""com.docker.network.bridge.enable_icc"": ""true"",
      ""com.docker.network.bridge.enable_ip_masquerade"": ""true"",
      ""com.docker.network.bridge.host_binding_ipv4"": ""0.0.0.0""
    }
  }
] Note The IPAM (IP Address Management) section shows how Docker assigns subnets and gateways. Running Containers on the Default Bridge When you start a container without specifying a network, it’s attached to bridge : docker run -itd --name first centos:7 Inspect its network settings: docker inspect first --format '{{json .NetworkSettings}}' | jq {
  ""Gateway"": ""172.17.0.1"",
  ""IPAddress"": ""172.17.0.2"",
  ""IPPrefixLen"": 16,
  ""MacAddress"": ""02:42:ac:11:00:02"",
  ""Networks"": {
    ""bridge"": {
      ""IPAddress"": ""172.17.0.2"",
      ""Gateway"": ""172.17.0.1"",
      ""MacAddress"": ""02:42:ac:11:00:02""
    }
  }
} Create a second container: docker run -itd --name second centos:7 On the default bridge, embedded DNS is not enabled. Attempting to ping by container name fails: docker exec first ping -c 2 second
# ping: second: Name or service not known Creating a User-Defined Bridge Network User-defined bridge networks include built-in DNS and automatic name resolution. Create one with a custom subnet: docker network create \
  --driver bridge \
  --subnet 192.168.10.0/24 \
  kodekloudnet Verify its presence: docker network ls $ docker network ls
NETWORK ID     NAME           DRIVER    SCOPE
cf10938f5edf   bridge         bridge    local
d4f46412e7e9   host           host      local
f22c791ef1ad   kodekloudnet   bridge    local
b5b0ab8c1665   none           null      local Running Containers on the Custom Network Launch two containers on kodekloudnet : docker run -itd --name customfirst --net kodekloudnet centos:7
docker run -itd --name customsecond --net kodekloudnet centos:7 They now receive IPs within 192.168.10.0/24 , and DNS-based name resolution works: docker exec customfirst ping -c 4 customsecond PING customsecond (192.168.10.3): 56 data bytes
64 bytes from customsecond.kodekloudnet (192.168.10.3): icmp_seq=1 ttl=64 time=0.07 ms
... Connecting an Existing Container to a Network By default, containers attach only to the default bridge. To connect first to kodekloudnet : docker network connect kodekloudnet first Verify both network endpoints: docker inspect first \
  --format '{{json .NetworkSettings.Networks}}' | jq {
  ""bridge"": {
    ""IPAddress"": ""172.17.0.2""
  },
  ""kodekloudnet"": {
    ""IPAddress"": ""192.168.10.4""
  }
} Now ping first from customfirst : docker exec customfirst ping -c 2 first Disconnecting a Container from a Network To detach a container: docker network disconnect kodekloudnet first After disconnecting, customfirst will no longer reach first on that network. Removing Networks Docker prevents removing networks with active endpoints. To delete kodekloudnet : Stop and remove containers: docker container stop $(docker ps -q)
docker container rm $(docker ps -aq) Remove the network: docker network rm kodekloudnet You can also prune all unused user-defined networks: docker network prune Warning docker network prune removes only user-defined networks without active containers. Default networks ( bridge , host , none ) are not affected. Links and References Docker Networking Overview Docker Network Commands Kubernetes Networking Watch Video Watch video content"
Docker Certified Associate Exam Course,Docker Storage,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine-Storage/Docker-Storage,"Docker Certified Associate Exam Course Docker Engine Storage Docker Storage Understanding how storage works in Docker is a critical first step before tackling storage in Kubernetes . Docker storage is built on two core components: Storage drivers that manage the container’s writable layer Volume driver plugins that provide persistent storage beyond the container lifecycle In this guide, we’ll explore both in detail—starting with storage drivers, and then moving on to volume drivers. Why Docker Storage Matters Effective storage management ensures data integrity, performance isolation, and smooth scaling. Whether you’re running single-node applications or distributed clusters, choosing the right storage mechanism can make or break your deployment. Note Docker containers are ephemeral by design. Without proper use of volumes or external storage, any data written inside a container will be lost when the container stops or is removed. Key Concepts Component Function Typical Use Case Storage Drivers Manages how the container filesystem is graphically layered and stored on the host. Optimizing I/O performance and space utilization. Volume Driver Plugins Interfaces with external storage systems (local, cloud, SAN, NFS). Persisting data across container restarts and nodes. Storage Drivers Storage drivers implement the copy-on-write layers for images and containers. Docker supports several drivers— overlay2 , aufs , btrfs , devicemapper , and more—each with trade-offs in performance and features. Common Storage Drivers overlay2 : The recommended default on modern Linux kernels. aufs : Historically popular but less maintained. btrfs : Offers snapshots and quotas. devicemapper : Used for block-level storage on older systems. For a full list and configuration options, see the Docker Storage Drivers documentation . Volume Driver Plugins While storage drivers handle the container’s writable layer, volumes are designed for persistent data. Docker volumes can be managed by built-in drivers or extended via plugins to connect with cloud or network storage. Built-in Volume Drivers local : Stores data on the Docker host filesystem. tmpfs : In-memory storage, cleared at shutdown. Third-Party Plugins You can integrate with storage solutions like AWS EBS, Azure Disk, or NFS using volume driver plugins: docker plugin install vieux/sshfs
docker volume create \
  --driver vieux/sshfs \
  --opt sshcmd=user@host:/remote/path \
  ssh_volume Warning When using network-backed volumes, verify your plugin’s compatibility and performance constraints to prevent I/O bottlenecks in production. Next Steps Explore Docker Volumes for advanced volume management. Learn how to mount volumes into containers with docker run --mount and docker-compose . Compare Docker’s storage options with Kubernetes PersistentVolumes and CSI drivers in our Kubernetes storage overview . Links and References Docker Storage Drivers Docker Volumes Kubernetes Persistent Volumes Watch Video Watch video content"
Docker Certified Associate Exam Course,Demo Docker Volume,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine-Storage/Demo-Docker-Volume,"Docker Certified Associate Exam Course Docker Engine Storage Demo Docker Volume In this tutorial, you’ll learn how to create, inspect, mount, and remove Docker volumes. Volumes are the preferred mechanism for persisting container data, ensuring it lives beyond the lifecycle of individual containers. 1. List and Create a Volume Before you begin, list existing volumes (you should see none): $ docker volume ls
DRIVER    VOLUME NAME Create a new volume called testvol : $ docker volume create testvol
testvol

$ docker volume ls
DRIVER    VOLUME NAME
local     testvol 2. Mounting a Volume with -v You can mount testvol into a container at /yogesh using the shorthand -v flag: $ docker run -itd --name test -v testvol:/yogesh centos:7 Note The -v shorthand syntax is concise but less explicit than --mount . For new scripts, consider using --mount (see Section 5). Verify the mount inside the container: $ docker exec -it test df -h | grep /yogesh
/dev/mapper/centos-root  50G  1.2G   49G   3% /yogesh 3. Inspect Volume Details On the host, inspect testvol to discover its data directory: $ docker volume inspect testvol
[
  {
    ""CreatedAt"": ""2020-05-04T17:34:47Z"",
    ""Driver"": ""local"",
    ""Mountpoint"": ""/var/lib/docker/volumes/testvol/_data"",
    ""Name"": ""testvol"",
    ""Scope"": ""local""
  }
] Create sample files inside the volume: $ docker exec test bash -c ""touch /yogesh/abc /yogesh/def"" 4. Removing Containers vs. Volumes Stopping or removing the container does not delete the attached volume: $ docker stop test
$ docker rm test

$ docker volume ls
DRIVER    VOLUME NAME
local     testvol To remove the volume and reclaim space: $ docker volume rm testvol
testvol Warning Removing a volume deletes all data stored in it. Make sure you no longer need its contents before running docker volume rm . 5. Mounting a Volume with --mount For clearer syntax and advanced options, use --mount : $ docker volume create testvol
testvol

$ docker run -itd --name test \
    --mount source=testvol,destination=/yogesh \
    centos:7 Check that your files persist: $ docker exec -it test ls /yogesh
abc  def Comparing -v vs --mount Option Syntax When to Use -v -v volume_name:/container_path Quick tests --mount --mount type=volume,source=volume_name,destination=/container_path Production & scripts 6. Handling Volume-in-Use Errors If you try to remove a volume that’s still attached, you’ll see an error: $ docker volume rm testvol
Error response from daemon: remove testvol: volume is in use - [container_id] Stop and remove the container, then retry: $ docker stop test
$ docker rm test
$ docker volume rm testvol
testvol You can also remove all unused volumes in one command: $ docker volume prune
WARNING! This will remove all local volumes not used by at least one container.
Are you sure you want to continue? [y/N] y
Total reclaimed space: 0B 7. Read-Write vs Read-Only Mounts By default, mounts are read-write ( rw ). Here’s how to confirm: $ docker volume create data_vol1
data_vol1

$ docker run -itd --name volume_rw \
    --mount source=data_vol1,destination=/yogesh \
    centos:7

$ docker inspect volume_rw --format '{{json .Mounts}}'
[{""Destination"":""/yogesh"",""Mode"":""rw"", ...}] To mount that same volume as read-only ( ro ): $ docker run -itd --name volume_ro \
    --mount source=data_vol1,destination=/yogesh,readonly=true \
    centos:7

$ docker inspect volume_ro --format '{{json .Mounts}}'
[{""Destination"":""/yogesh"",""Mode"":""ro"", ...}] Clean up: $ docker stop volume_rw volume_ro
$ docker rm volume_rw volume_ro
$ docker volume prune -f 8. Bind Mounts Bind mounts let you map any host directory directly into a container: $ mkdir /data

$ docker run -itd --name bindtest \
    --mount type=bind,source=/data,destination=/yogesh \
    centos:7 Inside bindtest , the path /yogesh is backed by /data on the host. Note that bind mounts do not show up in docker volume ls . Note Use bind mounts when you need to share host files or configuration with a container. For managed, portable storage, prefer Docker volumes. Links and References Docker Volumes Bind Mounts Docker Run Reference Watch Video Watch video content"
Docker Certified Associate Exam Course,Container Orchestration Introduction,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Container-Orchestration-Introduction,"Docker Certified Associate Exam Course Docker Swarm Container Orchestration Introduction In this lesson, we’ll explore container orchestration and why it’s essential for running containerized applications at scale. Up to this point, we’ve used Docker to launch single instances of applications: docker run nodejs While this works for development or low-traffic scenarios, it becomes cumbersome when you need to: Deploy multiple instances Monitor container health Automate restarts on failure Handle host-level outages The Challenge of Manual Scaling Imagine your Node.js application starts receiving more traffic. You’d manually spin up additional containers: docker run nodejs
docker run nodejs
docker run nodejs You also need to: Monitor each container’s CPU, memory, and response time Restart containers when they crash Migrate workloads if a Docker host fails Warning Manual scripts can help automate tasks, but they often become brittle as you scale. Maintaining and debugging those scripts can turn into a full-time job. At small scale, manual intervention is possible. But with tens of thousands of containers , you need a more robust, automated solution. Enter Container Orchestration Container orchestration platforms let you define desired state and let the system handle: Container placement across hosts Health checks and automatic restarts Load balancing and service discovery Cluster auto-scaling Configuration management For example, with Docker Swarm you can scale your service to 100 replicas in one command: docker service create --name my-node-app --replicas=100 nodejs Or update an existing service: docker service scale my-node-app=100 Key Features of Orchestration Platforms Feature Description Automatic Scaling Increase or decrease replicas based on resource usage Self-Healing Detect failed containers and reschedule replacements Rolling Updates & Rollbacks Deploy changes without downtime Service Discovery & Load Balancing Expose services with DNS and virtual IPs Storage Orchestration Attach persistent volumes dynamically Configuration & Secret Management Securely inject configuration and secrets Popular Orchestration Solutions Platform Pros Cons Docker Swarm Easy to set up; native Docker integration Limited auto-scaling; smaller ecosystem Apache Mesos Highly scalable; multi-tenant support Complex to configure and maintain Kubernetes Extensive community support; rich ecosystem; cloud-native Steeper learning curve For a deeper dive, see Kubernetes Basics . Note Kubernetes is supported by all major cloud providers (AWS, GCP, Azure) and offers a vast plugin ecosystem for networking, storage, and security. What’s Next In upcoming lessons, we’ll walk through: Deploying applications with Docker Swarm Setting up a Kubernetes cluster Implementing auto-scaling , rolling updates , and persistent storage References Kubernetes Documentation Docker Documentation Terraform Registry Watch Video Watch video content"
Docker Certified Associate Exam Course,Section Introduction,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Section-Introduction,"Docker Certified Associate Exam Course Docker Swarm Section Introduction In this lesson, we explore Docker Swarm from its core architecture to advanced deployments. We begin by examining the Swarm architecture, understanding how manager and worker nodes interact, and ensuring high availability through quorum. You’ll learn best practices for selecting the optimal number of manager nodes to maintain cluster resilience and avoid split-brain scenarios. Next, we dive into Swarm services: Creating and scaling services with docker service create and docker service scale Performing rolling updates and rollbacks to minimize downtime Managing configurations and secrets for secure deployments After covering service fundamentals, we’ll explore advanced topics: Service placement strategies for optimized resource utilization Health checks and failure recovery configurations Overlay networks and built-in service discovery Deploying multi-service applications with docker stack Prerequisites Make sure you have Docker Engine installed and a basic understanding of containerization concepts. Refer to the Docker Installation Guide if you need setup instructions. By the end of this section, you’ll be fully prepared to design, deploy, and manage production-grade Docker Swarm clusters. Watch Video Watch video content"
Docker Certified Associate Exam Course,Demo Swarm Cluster Setup,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Demo-Swarm-Cluster-Setup,"Docker Certified Associate Exam Course Docker Swarm Demo Swarm Cluster Setup In this step-by-step guide, you will learn how to bootstrap a resilient Docker Swarm cluster on CentOS 7.6, manage node roles, control availability, and gracefully remove nodes. By the end of this tutorial, you’ll have a three-node swarm (one manager, two workers) and know how to: Initialize the swarm Join worker nodes Promote and demote managers Drain and reactivate nodes Remove nodes from the cluster This tutorial assumes all nodes can resolve each other by hostname, have open swarm ports, and run Docker Engine v19.03.8 or later. Prerequisites Three CentOS 7.6 servers ( managerone , workerone , workertwo ) 2 CPU cores and 4 GB RAM each Hostname resolution (e.g., via /etc/hosts or DNS) Open ports: TCP 2377 for cluster management TCP/UDP 7946 for node discovery UDP 4789 for overlay networking Docker Engine installed and running System Overview Node OS Release CPUs Memory (MiB) Docker Version managerone CentOS Linux release 7.6.1810 (Core) 2 3787 19.03.8 workerone CentOS Linux release 7.6.1810 (Core) 2 3787 19.03.8 workertwo CentOS Linux release 7.6.1810 (Core) 2 3787 19.03.8 1. Initialize the Swarm on managerone Confirm Swarm is inactive: docker system info | grep -i swarm
# Swarm: inactive Initialize with the manager’s advertise address: docker swarm init --advertise-addr 172.31.42.232 After initialization, note the worker join command output: docker swarm join --token <SWARM_WORKER_TOKEN> 172.31.42.232:2377 2. Add the First Worker ( workerone ) On workerone , run the join command displayed by the manager: docker swarm join \
  --token SWMTKN-1-3fdj9fgrjcrrj5t0pekb42n45tj96zgwxodtwd4ujv4qnhl-cop40spyhgc1tmzythfss49xn \
  172.31.42.232:2377
# This node joined a swarm as a worker. Back on managerone , verify: docker node ls
# ID       HOSTNAME    STATUS  AVAILABILITY  MANAGER STATUS  ENGINE VERSION
# ...      managerone  Ready   Active        Leader          19.03.8
# ...      workerone   Ready   Active                        19.03.8 3. Add the Second Worker ( workertwo ) 3.1. Prepare workertwo Ensure OS, CPU, memory, Docker version, and connectivity: cat /etc/centos-release
nproc
free -m
docker --version
systemctl status docker
ping -c3 managerone
ping -c3 workerone 3.2. Retrieve and Run the Join Token On managerone , print the worker join command: docker swarm join-token worker Copy the docker swarm join --token … line and execute it on workertwo . Then confirm on managerone : docker node ls
# ... workertwo   Ready   Active   19.03.8 4. Promote a Worker to Manager List current nodes: docker node ls Promote workerone : docker node promote workerone
# Node workerone promoted to a manager in the swarm. Verify the new manager status: docker node ls
# workerone now shows MANAGER STATUS: Reachable (Optional) Inspect workerone in detail: docker node inspect workerone --pretty 5. Demote a Manager back to Worker If you need to revert workerone to a purely worker role: docker node demote workerone
docker node ls
# workerone returns to no MANAGER STATUS 6. Drain and Reactivate a Node To prevent new tasks from scheduling on workerone : docker node update --availability drain workerone
docker node ls
# AVAILABILITY: Drain When you’re ready to allow tasks again: docker node update --availability active workerone
docker node ls
# AVAILABILITY: Active 7. Remove a Node from the Swarm Drain workerone : docker node update --availability drain workerone Attempt removal (will fail if the node is still up): docker node rm workerone
# Error: node is not down and can't be removed On workerone , leave the swarm: docker swarm leave
# Node left the swarm. Back on managerone , remove the node: docker node rm workerone
docker node ls
# Only managerone and workertwo remain. Congratulations! You’ve successfully created, scaled, and managed a Docker Swarm cluster on CentOS 7.6. References and Further Reading Docker Swarm Overview Get Started with Swarm Mode Docker Engine Installation Guide Watch Video Watch video content"
Docker Certified Associate Exam Course,Docker Compose,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Compose/Docker-Compose,"Docker Certified Associate Exam Course Docker Compose Docker Compose Docker Compose lets you define and run multi‐container applications from a single YAML file. Instead of managing each docker run command manually, you declare services, networks, and volumes in docker-compose.yaml and launch the entire stack with one command: docker-compose up 1. Recap: Running Multiple Containers with docker run To illustrate the complexity of manual container orchestration, imagine starting four services individually: docker run -d --name web mmumshad/simple-webapp
docker run -d --name database mongo
docker run -d --name messaging redis:alpine
docker run -d --name orchestration ansible Each container runs, but wiring them together (networking, links, ports) quickly becomes tedious. 2. Defining Services in Compose In docker-compose.yaml , all services and their options live under the services: key: version: '3'
services:
  web:
    image: mmumshad/simple-webapp

  database:
    image: mongo

  messaging:
    image: redis:alpine

  orchestration:
    image: ansible Then bring the entire stack up: docker-compose up Note Every configuration change is version‐controlled in your Compose file—no more hunting down individual CLI commands. 3. Sample Voting Application Architecture We’ll demonstrate Compose using Docker’s sample voting app, composed of: Voting app (Python web UI): records “cats” or “dogs” votes in Redis. Worker (.NET): reads votes from Redis and updates PostgreSQL. PostgreSQL : stores persistent vote counts. Result app (Node.js web UI): displays tallied results from PostgreSQL. 4. Manual Stack Deployment with docker run If images already exist on Docker Hub, you might start containers like this: # Data layer
docker run -d --name redis redis
docker run -d --name db postgres

# Services
docker run -d --name vote   -p 5000:80 voting-app
docker run -d --name result -p 5001:80 result-app
docker run -d --name worker worker However, without networking configuration, containers cannot communicate. 4.1 Linking Containers (Deprecated) The --link flag creates /etc/hosts entries for cross-container DNS: docker run -d --name redis redis
docker run -d --name db postgres

docker run -d --name vote -p 5000:80 --link redis:redis voting-app
docker run -d --name result -p 5001:80 --link db:db result-app
docker run -d --name worker --link redis:redis --link db:db worker In your Node.js code, you’d connect via the hostname db : pg.connect('postgres://postgres@db/postgres', (err, client, done) => {
  if (err) console.error(""Waiting for db"");
  callback(err, client);
}); Warning Container links are deprecated . Instead, use user‐defined networks as shown in the next sections. 5. From docker run to docker-compose.yaml Convert your verified docker run commands into a Compose file: # Working commands for reference
docker run -d --name redis redis
docker run -d --name db postgres:9.4
docker run -d --name vote   -p 5000:80 --link redis:redis voting-app
docker run -d --name result -p 5001:80 --link db:db result-app
docker run -d --name worker --link db:db --link redis:redis worker Compose definition : version: '2'
services:
  redis:
    image: redis

  db:
    image: postgres:9.4

  vote:
    image: voting-app
    ports:
      - ""5000:80""
    links:
      - redis

  result:
    image: result-app
    ports:
      - ""5001:80""
    links:
      - db

  worker:
    image: worker
    links:
      - redis
      - db Launch with: docker-compose up 6. Building Local Images in Compose If your service images are built locally, specify a build: context instead of image: : version: '2'
services:
  vote:
    build: ./vote
    ports:
      - ""5000:80""
    links:
      - redis

  result:
    build: ./result
    ports:
      - ""5001:80""
    links:
      - db

  worker:
    build: ./worker
    links:
      - redis
      - db

  redis:
    image: redis

  db:
    image: postgres:9.4 Compose will build these images from each directory’s Dockerfile before starting the containers. 7. Compose File Versions Compared Different Compose versions introduce new features and schemas. Refer to this summary: Version Structure Highlights v1 No services: Legacy; no networks or depends_on v2 services: Built‐in network, depends_on support v3 Same as v2 Adds Swarm deployment settings Examples Version 1 version: '1'
redis:
  image: redis
db:
  image: postgres:9.4
vote:
  image: voting-app
  ports:
    - ""5000:80""
  links:
    - redis Version 2 version: '2'
services:
  redis:
    image: redis

  db:
    image: postgres:9.4

  vote:
    image: voting-app
    ports:
      - ""5000:80""
    depends_on:
      - redis Version 3 version: '3'
services:
  redis:
    image: redis

  db:
    image: postgres:9.4

  vote:
    image: voting-app
    ports:
      - ""5000:80"" For full details, see the Docker Compose file reference . 8. Defining Custom Networks By default, Compose creates a single bridge network. You can isolate traffic with multiple networks: version: '2'
services:
  redis:
    image: redis
    networks:
      - back-end

  db:
    image: postgres:9.4
    networks:
      - back-end

  vote:
    image: voting-app
    networks:
      - front-end
      - back-end

  result:
    image: result-app
    networks:
      - front-end
      - back-end

  worker:
    image: worker
    networks:
      - back-end

networks:
  front-end: {}
  back-end: {} Containers on front-end can only communicate with those also on back-end . Next Steps Now that you’ve mastered service definitions, version schemas, and custom networks, try creating and running your own docker-compose.yaml configurations in the exercises below. Links and References Docker Compose Overview Compose File Reference Kubernetes Basics Terraform Registry Watch Video Watch video content"
Docker Certified Associate Exam Course,Swarm Setup 2 node Cluster,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Swarm-Setup-2-node-Cluster,"Docker Certified Associate Exam Course Docker Swarm Swarm Setup 2 node Cluster In this guide, you'll learn how to set up a Docker Swarm cluster with one manager and two workers, perform basic node operations, and verify cluster status. By the end, you'll be ready to add, promote, or drain nodes in your Swarm.

## Prerequisites

1. Three machines (physical or virtual), on-premise or cloud.  
2. Static IPs assigned:
   - **manager1**: 172.31.46.126  
   - **worker1**: 172.31.46.127  
   - **worker2**: 172.31.46.128  
3. Install [Docker Engine](https://docs.docker.com/engine/install/) on each host.  
4. Open firewall ports for Swarm communication:

| Port     | Protocol | Description                          |
|----------|----------|--------------------------------------|
| 2377     | TCP      | Cluster management                   |
| 7946     | TCP/UDP  | Node-to-node communication           |
| 4789     | UDP      | Overlay network (VXLAN) traffic      |
,[object Object],

## Verify Docker and Swarm Status

After installation, run:

```bash
docker system info
```text

Look for:

```plain
Swarm: inactive
```text

If `Swarm: inactive`, the node is not yet part of a Swarm cluster.

## 1. Initialize the Swarm Manager

On **manager1**, execute:

```bash
docker swarm init --advertise-addr 172.31.46.126
```text

This:

- Configures the host as the Swarm manager  
- Prints the `docker swarm join` command with a token for workers  

Re-run `docker system info` to confirm:

```plain
Swarm: active
```text

## 2. Join Worker Nodes

Copy the join command from the manager’s output and run on each worker:

```bash
# On worker1
docker swarm join \
  --token SWMTKN-1-xxxxxxxxxxxxxxxxxxxxxxxx \
  172.31.46.126:2377

# On worker2
docker swarm join \
  --token SWMTKN-1-xxxxxxxxxxxxxxxxxxxxxxxx \
  172.31.46.126:2377
```text
,[object Object],

## 3. Verify Cluster Nodes

On the manager, list all nodes:

```bash
docker node ls
```text

Example output:

```plain
ID                            HOSTNAME   STATUS  AVAILABILITY  MANAGER STATUS  ENGINE VERSION
91uxgq6i78j1hu5v7moq7vgz *    manager1   Ready   Active        Leader          19.03.8
2lux7z6p96g6vtx0h6a2wo2r       worker1    Ready   Active        <none>          19.03.8
w0qr6k2cee3ojawmflc26pvp3      worker2    Ready   Active        <none>          19.03.8
```text

| Column            | Description                                                                                   |
|-------------------|-----------------------------------------------------------------------------------------------|
| **STATUS**        | Ready: node is healthy and participating                                                     |
| **AVAILABILITY**  | Active: scheduler assigns tasks<br>Pause: no new tasks, existing continue<br>Drain: tasks moved off |
| **MANAGER STATUS**| Leader: primary manager<br>Reachable: manager node able to take over<br>&lt;none&gt;: worker |

The `*` marks the node where the command was run.

## 4. Inspect a Node

Get detailed info on any node:

```bash
docker node inspect manager1 --pretty
```text

Sample output:

```plain
ID:             91uxgq6i78j1hu5v7moq7vgz
Hostname:       manager1
  State:        Ready
  Availability: Active
  Address:      172.31.46.126
Manager Status:
  Address:      172.31.46.126:2377
  Raft Status:  Reachable This reveals role, status, and network details. Next Steps In the next lesson, you’ll learn how to: Promote a worker to manager Drain nodes for maintenance Update and roll back services Links and References Docker Engine Documentation Docker Swarm Overview Swarm Mode Networking Watch Video Watch video content"
Docker Certified Associate Exam Course,Swarm in HA mode,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Swarm-in-HA-mode,"Docker Certified Associate Exam Course Docker Swarm Swarm in HA mode In this guide, you’ll learn how to configure a Docker Swarm cluster in HA mode with three manager nodes and three worker nodes. We’ll cover initialization, adding managers and workers, testing quorum, and restoring cluster health. Prerequisites Six CentOS 7.6 VMs (2 vCPU, 4 GB RAM) on AWS Docker 19.03 installed on all nodes Swarm ports open (2377, 7946 TCP/UDP, 4789 UDP) Fully qualified hostnames and name resolution for all nodes Hostname IP Address Role (after setup) managerone 172.31.42.232 Manager (Leader) managertwo 172.31.42.xxx Manager (Replica) managerthree 172.31.42.xxx Manager (Replica) workerone 172.31.39.115 Worker workertwo 172.31.39.xxx Worker workerthree 172.31.39.xxx Worker Verify OS and Docker: [root@managerone ~]# cat /etc/centos-release
CentOS Linux release 7.6.1810 (Core)
[root@managerone ~]# nproc
2
[root@managerone ~]# docker version --format '{{.Server.Version}}'
19.03.8 Verify network connectivity: [root@managerone ~]# ping -c2 workerone
PING workerone (172.31.39.115): 56 data bytes
64 bytes from workerone: icmp_seq=1 ttl=64 time=0.45 ms 1. Initialize the Swarm On managerone , initialize the Swarm and advertise its IP: [root@managerone ~]# docker swarm init --advertise-addr 172.31.42.232
Swarm initialized: current node (kvbht...) is now a manager. Confirm Swarm status: [root@managerone ~]# docker info --format '{{.Swarm.LocalNodeState}}'
active Note The --advertise-addr flag sets the manager’s reachable IP for new nodes. 2. Add Additional Managers 2.1 Retrieve Manager Join Token On managerone : [root@managerone ~]# docker swarm join-token manager --quiet
SWMTKN-1-xxxxx-xxxx 2.2 Join managertwo and managerthree On each additional manager node: [root@managertwo ~]# docker swarm join --token SWMTKN-1-xxxxx-xxxx 172.31.42.232:2377
This node joined a swarm as a manager. Repeat on managerthree . 2.3 Verify Managers From managerone : [root@managerone ~]# docker node ls
ID      HOSTNAME       STATUS  AVAILABILITY  MANAGER STATUS  ENGINE VERSION
* kvbht...  managerone     Ready   Active        Leader          19.03.8
  s2zym...  managertwo     Ready   Active        Reachable       19.03.8
  u81im...  managerthree   Ready   Active        Reachable       19.03.8 3. Add Worker Nodes 3.1 Retrieve Worker Join Token On managerone : [root@managerone ~]# docker swarm join-token worker --quiet
SWMTKN-1-yyyyy-yyyy 3.2 Join workerone , workertwo , workerthree On each worker: [root@workerone ~]# docker swarm join --token SWMTKN-1-yyyyy-yyyy 172.31.42.232:2377
This node joined a swarm as a worker. Verify all nodes: [root@managerone ~]# docker node ls
ID      HOSTNAME       ROLE      MANAGER STATUS  AVAILABILITY  ENGINE VERSION
* kvbht...  managerone   Manager   Leader          Active        19.03.8
  s2zym...  managertwo   Manager   Reachable       Active        19.03.8
  u81im...  managerthree Manager   Reachable       Active        19.03.8
  38oeh...  workerone    Worker    —               Active        19.03.8
  1pqdd...  workertwo    Worker    —               Active        19.03.8
  k4gc5...  workerthree  Worker    —               Active        19.03.8 4. Testing Manager Quorum Docker Swarm requires a majority of manager nodes to maintain a leader. 4.1 Simulate One Manager Failure On managertwo : [root@managertwo ~]# systemctl stop docker Check from managerone : [root@managerone ~]# docker node ls
… managertwo     Down    Active    Unreachable   19.03.8 The cluster remains healthy (2 of 3 managers online). 4.2 Simulate Two Manager Failures On managerthree : [root@managerthree ~]# systemctl stop docker Then on managerone : [root@managerone ~]# docker node ls
Error response from daemon: … The swarm does not have a leader … With only one manager online, the cluster loses quorum and cannot elect a leader. Warning Never simulate quorum loss in production. You will lose control over scheduling and service updates. 5. Restoring Quorum Start Docker on managertwo : [root@managertwo ~]# systemctl start docker Wait for the node to rejoin (quorum 2/3 restored). Start Docker on managerthree : [root@managerthree ~]# systemctl start docker Verify: [root@managerone ~]# docker node ls All managers should show Leader or Reachable . Conclusion You’ve now deployed a Docker Swarm in HA mode, added managers and workers, tested quorum behavior, and restored cluster health. This setup ensures fault tolerance and continuous service availability. Links and References Docker Swarm Mode Overview Swarm Init Swarm Join Watch Video Watch video content"
Docker Certified Associate Exam Course,Example Voting Application with Docker Compose,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Compose/Example-Voting-Application-with-Docker-Compose,"Docker Certified Associate Exam Course Docker Compose Example Voting Application with Docker Compose In this step-by-step tutorial, you’ll learn how to orchestrate a multi-service voting application using Docker Compose. By the end, you’ll have a running stack that includes Redis, PostgreSQL, a voting frontend, a worker processor, and a results dashboard. Prerequisites Docker Engine installed (version ≥ 19.03) Basic familiarity with docker CLI A terminal/SSH session on Linux, macOS, or Windows WSL Step 1: Install Docker Compose Docker Compose isn’t bundled with Docker Engine by default. Install it on Linux with: sudo curl -L ""https://github.com/docker/compose/releases/download/1.16.1/docker-compose-$(uname -s)-$(uname -m)"" \
  -o /usr/local/bin/docker-compose
sudo chmod +x /usr/local/bin/docker-compose
docker-compose --version Expected output: docker-compose version 1.16.1, build 1719ceb Note Replace 1.16.1 with the latest stable release. See the Compose releases on GitHub for details. Step 2: Clean Up Existing Containers Before deploying, stop any previous demo containers: # List running containers
docker ps

# Stop containers by ID or prefix
docker stop 69 54 5b 2f 0b

# Verify all relevant containers are stopped
docker ps Warning Stopping containers will terminate running services. Ensure you don’t have unsaved data in those containers. Step 3: Define Services in docker-compose.yml Create a file named docker-compose.yml with the following content. It leverages Compose file format version 3. version: '3'
services:
  redis:
    image: redis

  db:
    image: postgres:9.4

  vote:
    image: voting-app
    ports:
      - ""5000:80""
    depends_on:
      - redis

  worker:
    image: worker-app
    depends_on:
      - db
      - redis

  result:
    image: result-app
    ports:
      - ""5001:80""
    depends_on:
      - db Service Overview Service Image Ports Description redis redis – In-memory queue for incoming votes db postgres:9.4 – Persistent storage for vote records vote voting-app 5000→80 Frontend where users cast their vote worker worker-app – Processes queued votes into the PostgreSQL DB result result-app 5001→80 Displays aggregated vote results Note The depends_on key ensures containers start in the correct order, but it doesn’t wait for health checks. Consider adding healthchecks for production workloads. See the Compose file reference for advanced options. Step 4: Deploy the Stack From the directory containing docker-compose.yml , run: docker-compose up -d This command will pull images, create a default network, and start all five containers. Container names are prefixed by your folder name (e.g., root_redis_1 ). Verify everything is up: docker ps You should see containers for Redis, PostgreSQL, vote, worker, and result. Step 5: Access the Application Voting interface: http://localhost:5000 Results dashboard: http://localhost:5001 Cast a vote on the first page, then switch to the results page to see real-time counts. Clean Up When you’re done testing, stop and remove all services with: docker-compose down This command stops containers and removes the network. Volumes and images remain unless you add the --volumes or --rmi all flags. References and Further Reading Docker Compose Installation Compose File Versioning Docker CLI Reference Watch Video Watch video content"
Docker Certified Associate Exam Course,Auto Lock,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Auto-Lock,"Docker Certified Associate Exam Course Docker Swarm Auto Lock Docker Swarm automatically stores two critical keys in the manager’s in-memory keystore by default: Raft Encryption Key : Encrypts on-disk Raft logs TLS Key : Secures communication between Swarm nodes Enabling auto-lock moves key management out of the daemon’s memory. This lets you store keys in a hardware security module (HSM) or a dedicated key management service (KMS). Warning When you enable auto-lock, Swarm generates a one-time unlock key. Store it in a secure password manager—without it, you cannot unlock your manager after a restart. Enable Auto-Lock You can turn on auto-lock either during cluster initialization or on an existing Swarm: # Initialize a new Swarm with auto-lock enabled
docker swarm init --autolock=true

# Enable auto-lock on an existing Swarm
docker swarm update --autolock=true Example output: Swarm updated.
To unlock a swarm manager after it restarts, run the `docker swarm unlock` command and provide the following key:
SWMKEY-1-7K9wg5n85QeC4Zh7rZ0vSV0b5MteDsUvpVhG/lQnbl0
Please remember to store this key in a password manager, since without it you will not be able to restart the manager. Manager Restart and Unlocking After a manager restart, the Swarm remains locked . Any attempt to run Swarm commands will result in an error: $ docker node ls
Error response from daemon: Swarm is encrypted and needs to be unlocked before it can be used.
Please use ""docker swarm unlock"" to unlock it. To resume normal operation, unlock the manager: $ docker swarm unlock
Enter unlock key: SWMKEY-1-7K9wg5n85QeC4Zh7rZ0vSV0b5MteDsUvpVhG/lQnbl0 Once the manager is unlocked, it will rejoin disconnected nodes automatically. Quick Reference Command Description docker swarm init --autolock=true Initialize a new Swarm with auto-lock enabled docker swarm update --autolock=true Turn on auto-lock for an existing Swarm docker swarm unlock Unlock a locked Swarm manager after restart Further Reading Docker Swarm Security Overview High Availability in Docker Swarm Docker Swarm Autolock Deep Dive Watch Video Watch video content"
Docker Certified Associate Exam Course,Docker Volume,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Engine-Storage/Docker-Volume,"Docker Certified Associate Exam Course Docker Engine Storage Docker Volume In this tutorial, you’ll learn how to inspect, remove, prune, and configure Docker volumes. Managing volumes effectively helps persist data across container lifecycles and keeps your host system clean. Table of Contents Inspecting a Volume Removing a Volume Pruning Unused Volumes Verifying Mount Options Mounting a Volume as Read-Only References Inspecting a Volume Use docker volume inspect to retrieve metadata about your volume, including driver, mount point, labels, and scope: docker volume inspect data_volume Sample output: [
  {
    ""CreatedAt"": ""2020-01-20T19:52:34Z"",
    ""Driver"": ""local"",
    ""Labels"": {},
    ""Mountpoint"": ""/var/lib/docker/volumes/data_volume/_data"",
    ""Name"": ""data_volume"",
    ""Options"": {},
    ""Scope"": ""local""
  }
] This command is essential for troubleshooting mount permissions and verifying where Docker stores your volume data on the host. Removing a Volume To delete a volume that’s no longer used by any container: docker volume rm data_volume If the volume is active, Docker returns an error: Error response from daemon: remove data_volume: volume is in use - [2be4d9182296…] Stop or remove the container first, then run the same command again: docker volume rm data_volume
# data_volume Pruning Unused Volumes Clean up all dangling volumes in one step to free up disk space: docker volume prune You'll see a confirmation prompt: WARNING! This will remove all local volumes not used by at least one container.
Are you sure you want to continue? [y/N] y
Deleted Volumes:
  data_vol3
  data_vol1
  data_vol2
Total reclaimed space: 12MB Warning Pruning removes all unused volumes. Ensure you have backups of any critical data before proceeding. Verifying Mount Options By default, volumes are mounted read-write. To inspect a container’s mount configuration: docker container inspect my-container Look for the Mounts section: ""Mounts"": [
  {
    ""Type"": ""volume"",
    ""Name"": ""data_vol1"",
    ""Source"": ""/var/lib/docker/volumes/data_vol1/_data"",
    ""Destination"": ""/var/www/html/index.html"",
    ""Driver"": ""local"",
    ""Mode"": ""z"",
    ""RW"": true,
    ""Propagation"": """"
  }
] Note The ""RW"": true field confirms the volume is mounted read-write inside the container. Mounting a Volume as Read-Only To ensure data integrity, you can mount a volume as read-only. Use the --mount flag with readonly : docker container run \
  --mount type=volume,source=data_vol1,target=/var/www/html,index.html,readonly \
  httpd This is useful for scenarios where containers should not modify shared data. Common Docker Volume Commands Command Description Example docker volume ls List all volumes docker volume ls docker volume inspect [NAME] Show detailed info about a volume docker volume inspect data_volume docker volume rm [NAME] Remove a specific volume docker volume rm data_volume docker volume prune Remove all unused volumes docker volume prune References Docker Volumes Overview Use Bind Mounts or Volumes Docker CLI Reference Watch Video Watch video content"
Docker Certified Associate Exam Course,Swarm Operations,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Swarm-Operations,"Docker Certified Associate Exam Course Docker Swarm Swarm Operations In this lesson, you’ll learn how to: Promote and demote Swarm nodes Drain nodes for maintenance Remove nodes from your cluster These operations help maintain high availability and streamline cluster management. Promote and Demote Nodes Only Swarm manager nodes can orchestrate the cluster. You can elevate a worker node to manager or revert a manager back to a worker. Run these commands on any current manager. Promote a Worker to Manager docker node promote worker1 Note Promoting a node requires you to be connected to an active manager. Check your current context with docker info . Verify the updated node list: ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION 91uxgq6… manager1 Ready Active Leader 19.03.8 2lux7z6… worker1 Ready Active Reachable 19.03.8 w0qr6k2… worker2 Ready Active 19.03.8 Demote a Manager to Worker docker node demote worker1 Confirm the demotion: ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION 91uxgq6… manager1 Ready Active Leader 19.03.8 2lux7z6… worker1 Ready Active 19.03.8 w0qr6k2… worker2 Ready Active 19.03.8 Drain a Node for Maintenance Before patching or upgrading, remove workloads from a node to avoid downtime. The following diagram shows a Swarm cluster with one manager and two workers running a “Web” service. Drain worker1 to migrate its tasks: docker node update --availability drain worker1 Warning Draining a node stops new tasks from being scheduled and reschedules existing ones on other workers. Ensure your cluster has enough capacity before draining. Check the drain status: ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION 91uxgq6… manager1 Ready Active Leader 19.03.8 2lux7z6… worker1 Ready Drain 19.03.8 w0qr6k2… worker2 Ready Active 19.03.8 After maintenance, reactivate scheduling: docker node update --availability active worker1 Now worker1 will accept new tasks: ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION 91uxgq6… manager1 Ready Active Leader 19.03.8 2lux7z6… worker1 Ready Active 19.03.8 w0qr6k2… worker2 Ready Active 19.03.8 Remove a Node from the Swarm To permanently remove a worker node: Drain the node: docker node update --availability drain worker2 Log in to worker2 and leave the swarm: docker swarm leave Example confirmation: Node left the swarm. At this point, worker2 is safely removed, and your cluster continues with the remaining nodes. References Docker Swarm Overview Docker Node Management Docker Maintenance Best Practices Watch Video Watch video content"
Docker Certified Associate Exam Course,Swarm Architecture,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Swarm-Architecture,"Docker Certified Associate Exam Course Docker Swarm Swarm Architecture Running containers on a single Docker host is convenient for development or testing, but in production it introduces a single point of failure. If that host goes down, all your services become unavailable. Docker Swarm solves this by clustering multiple Docker hosts into one logical unit, providing high availability, load balancing, and seamless scaling. Swarm Cluster Components A Swarm cluster groups physical or virtual machines—on-premises or in the cloud—into a unified environment. Every node runs Docker Engine and joins the cluster either as a manager or a worker. Node Type Responsibilities Commands Manager Maintains desired state, schedules tasks, serves the API docker node ls <br/> docker node promote Worker Executes tasks assigned by managers, runs service containers docker node ls <br/> docker node demote Note By default, manager nodes can handle workloads in addition to management tasks. To dedicate a manager solely to orchestration, use docker node update --availability drain <node> . When you deploy an application, you submit a service definition to a manager. The manager translates it into tasks and distributes them across worker nodes, which then run the required containers. Declarative Service Definitions Docker Swarm uses declarative YAML files—similar to Docker Compose—to define multi-service applications. Store these files in version control to track changes and facilitate CI/CD workflows: # service-definition.yml
version: ""3.8""
services:
  web:
    image: ""simple-webapp:latest""
    ports:
      - ""80:80""
  database:
    image: ""mongo:5.0""
    volumes:
      - db-data:/data/db
  cache:
    image: ""redis:alpine""
    deploy:
      replicas: 2

volumes:
  db-data: Note Declarative definitions allow you to scale, update, and rollback services with a single command: docker stack deploy -c service-definition.yml my_stack . Key Features of Docker Swarm 1. Simplified Setup and Maintenance Swarm is built directly into Docker Engine, so there’s no extra software to install. With the Docker CLI you can: Initialize a new cluster: docker swarm init Join nodes to the cluster: docker swarm join --token <token> <manager-ip>:2377 Promote or demote managers: docker node promote <node> 2. Scalability and Load Balancing You can scale services on demand using: docker service scale web=10 Swarm’s internal load balancer distributes requests across all healthy containers. If you need an external load balancer, point it at any manager or worker node. 3. Rolling Updates and Self-Healing Swarm performs rolling updates by default, updating one container at a time: docker service update \
  --image simple-webapp:2.0 \
  --update-parallelism 2 \
  --update-delay 10s \
  web If a container crashes or fails health checks, Swarm automatically replaces it to match the desired state. Warning Always test updates in a staging environment before applying to production. Use --rollback to revert quickly if an update misbehaves. 4. Secure Networking and Service Discovery Node-to-node communication is secured with mutual TLS. Overlay networks let containers on different hosts communicate as if they were on the same LAN. Built-in DNS routing ensures each service name resolves to the correct VIP or container IP. Summary and Next Steps In this article, we covered the core architecture and features that make Docker Swarm a powerful container orchestration solution. You learned about: Cluster components and node roles Declarative service definitions Key features: setup, scaling, updates, and networking Next, dive into practical guides on: Setting up a multi-node Swarm cluster Deploying production-grade services Advanced networking patterns Links and References Docker Swarm Overview Docker Compose File Reference Docker Networking Guide Watch Video Watch video content"
Docker Certified Associate Exam Course,Demo Docker Compose,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Compose/Demo-Docker-Compose,"Docker Certified Associate Exam Course Docker Compose Demo Docker Compose In this guide, you’ll learn how to migrate a basic Docker Compose setup from version 1 to version 3. We’ll remove deprecated options, configure environment variables for PostgreSQL, and explore build and deployment best practices. 1. Starting with the Basic Compose File (Version 1) Here’s a simple Compose file in version 1 syntax defining five services: redis:
  image: redis

db:
  image: postgres:9.4

vote:
  image: voting-app
  ports:
    - ""5000:80""
  links:
    - redis

worker:
  image: worker-app
  links:
    - db
    - redis

result:
  image: result-app
  ports:
    - ""5011:80""
  links:
    - db Version 1 is straightforward but lacks support for: Automatic network creation Built-in DNS service discovery Named volumes and advanced deployment options 2. Upgrading to Version 3 Compose version 3 unlocks Docker Swarm compatibility, automatic networking, and enhanced resource definitions. Follow these steps: Add version: ""3"" at the top. Nest all services under the services: key. Remove links: —service names now resolve via DNS. version: ""3""
services:
  redis:
    image: redis
  db:
    image: postgres:9.4
  vote:
    image: voting-app
    ports:
      - ""5000:80""
  worker:
    image: worker-app
  result:
    image: result-app
    ports:
      - ""5001:80"" In version 3, Docker Compose automatically creates a default network. Services communicate by name, e.g., redis ↔ db . 3. Specifying Build Options You can define how images are built directly in your Compose file. Examples from Docker’s docs : Simple build context: version: ""3.9""
services:
  webapp:
    build: ./dir Custom Dockerfile and build args: version: ""3.9""
services:
  webapp:
    build:
      context: ./dir
      dockerfile: Dockerfile-alternate
      args:
        buildno: 1 Build with image tag: services:
  webapp:
    build: ./dir
    image: webapp:tag Warning In Swarm mode, docker stack deploy ignores the build option. You must build images locally with docker build or push them to a registry before deploying. 4. Deploying with Docker Compose With your version 3 Compose file in place, start all services: docker-compose up Sample output: WARNING: The Docker Engine you're using is running in swarm mode.
Compose does not use swarm mode to deploy services to multiple nodes
in a swarm. All containers will be scheduled on the current node.
To deploy your application across the swarm, use `docker stack deploy`.
Creating network ""code_default"" with the default driver
Creating code_redis_1    ... done
Creating code_db_1       ... done
Creating code_vote_1     ... done
Creating code_worker_1   ... done
Creating code_result_1   ... done Here, code_default is the auto-generated network, and all containers are prefixed with the project name ( code_ ). Note If you want to run in detached mode, add -d : docker-compose up -d 5. Handling PostgreSQL Initialization On first run, PostgreSQL requires a superuser password. Without it, you’ll see: db_1  | Error: Database is uninitialized and superuser password is not specified.
db_1  | You must specify POSTGRES_PASSWORD for the superuser. Use
db_1  | ""-e POSTGRES_PASSWORD=password"" to set it in ""docker run"". Add environment variables under the db service: version: ""3""
services:
  redis:
    image: redis
  db:
    image: postgres:9.4
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
  vote:
    image: voting-app
    ports:
      - ""5000:80""
  worker:
    image: worker-app
  result:
    image: result-app
    ports:
      - ""5001:80"" Restart in detached mode: docker-compose up -d PostgreSQL Environment Variables Variable Description POSTGRES_USER Superuser name (default: postgres ) POSTGRES_PASSWORD Superuser password (required on init) 6. Verifying the Setup Once all containers are running, open your browser and test the apps: Voting app: http://localhost:5000 Results app: http://localhost:5001 Cast a vote for one option (e.g., cats ). Confirm the result updates correctly. Vote again (e.g., dogs ) and verify real-time results. Your Docker Compose setup is now: Version 3 compliant Automatically networked Securely initializing PostgreSQL Links and References Docker Compose File Reference Compose and Docker Compatibility Matrix Docker Stack Deploy Further Reading Resource Description Docker Documentation Official guides and references Kubernetes Basics Overview of container orchestration (K8s) Terraform Registry Modules for automated infrastructure provisioning Watch Video Watch video content"
Docker Certified Associate Exam Course,Swarm High Availability Quorum,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Swarm-High-Availability-Quorum,"Docker Certified Associate Exam Course Docker Swarm Swarm High Availability Quorum In a Docker Swarm cluster, manager nodes are the control plane where the Swarm is initialized. Manager responsibilities include: Maintaining the cluster’s desired state Scheduling and orchestrating containers Adding or removing nodes Monitoring health and distributing services Relying on a single manager is risky: if it goes down, there’s no orchestrator. Deploying multiple managers increases resilience but introduces the risk of conflicting decisions. Docker Swarm avoids this by electing one manager as the leader , which alone makes scheduling decisions. All managers—including the leader—must agree on changes via a consensus protocol before they’re committed. Even the leader must replicate its decisions to a majority of managers to avoid split-brain scenarios. Docker implements this using the Raft consensus algorithm . Distributed Consensus with Raft Raft ensures that one leader is elected and all state changes are safely replicated: Each manager starts with a random election timeout. When a timeout expires, that node requests votes from its peers. Once it gathers a majority, it becomes leader. The leader sends periodic heartbeats to followers. If followers miss heartbeats, they trigger a new election. When the leader receives a request to change the cluster (e.g., add a worker or create a service), it: Appends the change as an entry in its Raft log. Sends the log entry to each follower. Waits for a majority of acknowledgments. Commits the change across all Raft logs. This process guarantees consistency even if the leader fails mid-update. Quorum and Fault Tolerance A quorum is the minimum number of managers required to make decisions. For n managers: quorum = ⌊n/2⌋ + 1 Fault tolerance is the number of manager failures the cluster can sustain: fault_tolerance = ⌊(n - 1) / 2⌋ Managers (n) Quorum (⌊n/2⌋+1) Fault Tolerance (⌊(n-1)/2⌋) 3 2 1 5 3 2 7 4 3 Docker recommends no more than seven managers per Swarm. More managers do not improve performance or scalability and only increase coordination overhead. Note Always keep an odd number of managers (3, 5, or 7) to prevent split-brain scenarios during network partitions. Best Practices for Manager Distribution Use an odd number of managers (3, 5, or 7). Spread managers across distinct failure domains (data centers or availability zones). For seven managers, a 3–2–2 distribution across three sites ensures that losing any single site still leaves a quorum. Failure Scenarios and Recovery Imagine a Swarm with three managers and five workers hosting a web application. The quorum is two managers. If two managers go offline: The remaining manager can no longer perform cluster changes (no new nodes, no service updates). Existing services continue to run, but self-healing and scaling are disabled. Recovering Quorum Bring failed managers back online. Once you restore at least one, the cluster regains quorum. If you cannot recover old managers and only one remains, force a new cluster: docker swarm init --force-new-cluster This single node becomes the manager, and existing workers resume running services. Re-add additional managers: # Promote an existing node to manager
docker node promote <NODE>

# Or join a new manager
docker swarm join --token <MANAGER_TOKEN> <MANAGER_IP>:2377 That covers high availability, quorum calculation, Raft consensus, and best practices for Docker Swarm manager nodes. Good luck! Watch Video Watch video content"
Docker Certified Associate Exam Course,Example for Voting Application,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Compose/Example-for-Voting-Application,"Docker Certified Associate Exam Course Docker Compose Example for Voting Application Welcome to this hands-on demo of the Example Voting App. This sample application demonstrates how to build and deploy a simple microservices-based voting system using Docker. The complete source code is available in the Docker samples repository on GitHub under example-voting-app : In this guide, we will: Review the overall architecture. Dive into each component’s source code and Dockerfile. Deploy all services step by step with docker run . Architecture Overview The voting system consists of five microservices: Component Language/Tech Responsibility vote Python (Flask) Web UI for casting votes redis Redis Message queue for vote events worker Java Consumes votes from Redis and writes to DB db PostgreSQL Stores vote records result Node.js/Express Displays aggregated voting results 1. vote (Python Flask) The vote service provides a simple web page to cast votes and pushes each vote into Redis. Navigate to the vote directory to explore its code and Dockerfile: app.py from flask import Flask, request, make_response, render_template, g
from redis import Redis
import os, random, json, socket

option_a = os.getenv(""OPTION_A"", ""Cats"")
option_b = os.getenv(""OPTION_B"", ""Dogs"")
hostname = socket.gethostname()

app = Flask(__name__)

def get_redis():
    if not hasattr(g, 'redis'):
        g.redis = Redis(host=""redis"", db=0, socket_timeout=5)
    return g.redis

@app.route(""/"", methods=[""GET"", ""POST""])
def vote_page():
    voter_id = request.cookies.get('voter_id')
    if not voter_id:
        voter_id = hex(random.getrandbits(64))[2:-1]

    vote = None
    if request.method == 'POST':
        redis_conn = get_redis()
        vote = request.form['vote']
        data = json.dumps({'voter_id': voter_id, 'vote': vote})
        redis_conn.rpush('votes', data)

    resp = make_response(render_template(
        'index.html',
        option_a=option_a,
        option_b=option_b,
        hostname=hostname,
        vote=vote,
    ))
    resp.set_cookie('voter_id', voter_id)
    return resp

if __name__ == ""__main__"":
    app.run(host=""0.0.0.0"", port=80, debug=True, threaded=True) Note The Redis host is referenced as redis . Ensure the Redis container is linked or networked under this name. Dockerfile # Use official Python runtime
FROM python:2.7-alpine
WORKDIR /app

# Install dependencies
ADD requirements.txt /app/
RUN pip install -r requirements.txt

# Copy application code
ADD . /app
EXPOSE 80

# Launch with Gunicorn
CMD [""gunicorn"", ""app:app"", ""-b"", ""0.0.0.0:80"", ""--workers"", ""4"", ""--keep-alive"", ""0""] 2. worker (Java) The worker service consumes vote messages from Redis and writes them into PostgreSQL. Worker.java import redis.clients.jedis.Jedis;
import org.json.JSONObject;
import java.sql.*;
import java.util.List;

class Worker {
    public static void main(String[] args) {
        try {
            Jedis redis = new Jedis(""redis"");
            Connection dbConn = DriverManager.getConnection(
                ""jdbc:postgresql://db/postgres"", ""postgres"", ""password""
            );
            System.err.println(""Watching vote queue"");

            while (true) {
                List<String> item = redis.blpop(0, ""votes"");
                String voteJSON = item.get(1);
                JSONObject voteData = new JSONObject(voteJSON);
                String voterID = voteData.getString(""voter_id"");
                String vote = voteData.getString(""vote"");

                System.err.printf(""Processing vote '%s' by '%s'%n"", vote, voterID);
                updateVote(dbConn, voterID, vote);
            }
        } catch (SQLException e) {
            e.printStackTrace();
            System.exit(1);
        }
    }

    static void updateVote(Connection dbConn, String voterID, String vote) throws SQLException {
        PreparedStatement stmt = dbConn.prepareStatement(
            ""INSERT INTO votes (id, vote) VALUES (?, ?)""
        );
        stmt.setString(1, voterID);
        stmt.setString(2, vote);
        stmt.executeUpdate();
    }
} Dockerfile # Use .NET SDK image
FROM microsoft/dotnet:1.1.1-sdk
WORKDIR /code

# Copy and restore/publish the worker
ADD src/Worker /code/src/Worker
RUN dotnet restore src/Worker && \
    dotnet publish -c Release -o out src/Worker

CMD [""dotnet"", ""out/Worker.dll""] 3. result (Node.js/Express) The result service queries PostgreSQL for vote counts and renders a results page. server.js const express = require('express');
const { Client } = require('pg');
const app = express();
const port = process.env.PORT || 80;

const client = new Client({
  host: 'db',
  user: 'postgres',
  password: 'password',
  database: 'postgres'
});
client.connect();

app.set('view engine', 'pug');
app.use(express.static('public'));

app.get('/', async (req, res) => {
  const result = await client.query(
    'SELECT vote, COUNT(*) AS count FROM votes GROUP BY vote'
  );
  res.render('results', { votes: result.rows });
});

app.listen(port, () => console.log(`Result app listening on ${port}`)); Dockerfile FROM node:5.11.0-slim
WORKDIR /app

# Global utilities
RUN npm install -g nodemon

ADD package.json /app/
RUN npm config set registry http://registry.npmjs.org && \
    npm install && npm ls

# Copy app code
ADD . /app
EXPOSE 80

CMD [""node"", ""server.js""] 4. Deploying with docker run Warning The --link flag is considered legacy. For production, prefer user-defined networks or Docker Compose. 4.1 Clone & Build the Voting UI git clone https://github.com/dockersamples/example-voting-app.git
cd example-voting-app/vote
docker build -t voting-app . 4.2 Start Redis & Voting UI # Redis message queue
docker run -d --name redis redis:latest

# Voting UI linked to Redis on port 5000
docker run -d --name vote-ui \
  -p 5000:80 \
  --link redis:redis \
  voting-app Open http://localhost:5000 to cast your vote. 4.3 Launch PostgreSQL Database docker run -d --name db \
  -e POSTGRES_PASSWORD=password \
  postgres:9.4 4.4 Build & Run the Worker cd ../worker
docker build -t worker-app .
docker run -d --name vote-worker \
  --link redis:redis \
  --link db:db \
  worker-app 4.5 Build & Run the Result App cd ../result
docker build -t result-app .
docker run -d --name vote-result \
  -p 5001:80 \
  --link db:db \
  result-app Visit http://localhost:5001 to see live voting results: Congratulations! You’ve manually deployed all services using docker run , linked them together, and completed a full voting workflow. Next, we’ll automate this setup with Docker Compose . References Docker Documentation Flask (Python) Documentation Redis Official Site PostgreSQL Documentation Express (Node.js) Guide Watch Video Watch video content"
Docker Certified Associate Exam Course,Swarm Services,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Swarm-Services,"Docker Certified Associate Exam Course Docker Swarm Swarm Services Docker services enable you to deploy, scale, and manage containers across a Swarm cluster. With built-in load balancing, health checks, and automatic rescheduling, services take container orchestration to the next level. From docker run to Swarm Services On a single Docker host, you might launch a web server like this: docker run --name webserver httpd But scaling out across multiple nodes manually requires SSHing into each host: docker run httpd Challenges of this approach: You must configure external load balancing. Health monitoring and restarts are manual. Deployment becomes error-prone as the cluster grows. Swarm services automate all of these tasks: Unified load balancing across nodes Built-in health monitoring and self-healing Declarative desired state and scaling Creating a Docker Service A Swarm service represents one or more replicas of the same container. On a manager node, run: docker service create \
  --name web \
  --replicas 3 \
  --publish 80:80 \
  httpd:alpine Flag Description Example --name Assigns a name to the service web --replicas Number of container replicas to run 3 --publish Maps a host port to container port 80:80 Image Container image (with optional tag) httpd:alpine Default Replicas If you omit --replicas , Docker defaults to a single replica ( 1 ). How Swarm Orchestration Works When you issue docker service create , the Swarm manager components collaborate: API Server Accepts the CLI/API request. Orchestrator Calculates desired tasks (one per replica). Allocator Assigns virtual IPs and ports to each task. Dispatcher Sends tasks to available worker nodes. Worker Node Launches containers and reports status back to the manager. Rescheduler Detects failures and automatically replaces failed tasks. Swarm Task vs. Container A task is the Swarm abstraction for running a container and maintaining its desired state. Managing Services Use the following commands to inspect and troubleshoot services: Command Description docker service ls List all services in the Swarm cluster docker service ps <service> View tasks (containers) for a specific service docker service inspect <service> --pretty Show detailed service configuration in human-readable form docker service logs <service> Retrieve aggregated logs from all service replicas docker service scale <service>=<replicas> Adjust the number of replicas on the fly Example: docker service ls ID            NAME  MODE        REPLICAS  IMAGE         PORTS
3zhe91mns5vz  web   replicated  3/3       httpd:alpine  *:80->80/tcp Sample logs: web.1.xxxxxx@worker1 | [mpm_event:notice] AH00489: Apache/2.4.43 configured
web.2.yyyyyy@worker2 | [core:notice] AH00994: Command line: 'httpd -D FOREGROUND'
web.3.zzzzzz@worker3 | 10.0.0.4 - - [24/Apr/2020] ""POST /cgi-bin/main.cgi"" 400 226 Removing a Service To tear down a service and its tasks: docker service rm web All associated containers will be stopped and removed. Links and References Docker Swarm Overview Deploy Services Docker CLI Reference Docker Networking Happy containerizing! See you in the next guide. Watch Video Watch video content"
Docker Certified Associate Exam Course,Demo Auto Lock,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Demo-Auto-Lock,"Docker Certified Associate Exam Course Docker Swarm Demo Auto Lock In this walkthrough, you’ll learn how to enable Docker Swarm’s Auto-Lock feature to encrypt Raft logs and TLS keys on disk. With Auto-Lock enabled, any manager restarting or rejoining the cluster must provide the unlock key—adding a robust layer of security. 1. Enable Auto-Lock on Your Swarm For an existing Swarm cluster, run: docker swarm update --autolock=true This outputs a one-time unlock key, for example: SWMKEY-1-izfTZG1yXBjIOY3VBkIHFDI+WcnpqeJKYV6daZW3o Note To enable Auto-Lock during cluster creation, use: docker swarm init --autolock=true 2. Store the Unlock Key Securely Save the key in a safe location. For demo purposes we’ll use /tmp/swarm-unlock.key . In production, consider a secrets manager or vault. echo ""SWMKEY-1-izfTZG1yXBjIOY3VBkIHFDI+WcnpqeJKYV6daZW3o"" > /tmp/swarm-unlock.key
chmod 600 /tmp/swarm-unlock.key Warning Losing this key means you cannot unlock your Swarm managers. Always back it up securely. 3. Quick Reference: Swarm Auto-Lock Commands Command Description docker swarm init --autolock=true Initialize a new Swarm with Auto-Lock enabled docker swarm update --autolock=true Turn on Auto-Lock for an existing Swarm docker swarm unlock Unlock a manager node after restart or rejoin 4. Verify Cluster Health on Manager 1 Even with Auto-Lock active, manager1 can query node status without unlocking: [root@manager1 ~]# docker node ls
ID                        HOSTNAME       STATUS  AVAILABILITY  MANAGER STATUS  ENGINE VERSION
kvbht486wmj881wp5vqxp53 * manager1       Ready   Active        Leader          19.03.8
u8imabedhzsu4cawtoz6jh32   manager3       Ready   Active        Reachable       19.03.8
s2ymqdbtfal661imydx31rlno  manager2       Ready   Active        Reachable       19.03.8
38oehhk79ss5rk2coejcavha   worker1        Ready   Active                        19.03.8
k4gc50oc0n8k6jm3f6bm2bph   worker3        Ready   Active                        19.03.8
1pqddmh2fcoy79vq9najr841d  worker2        Ready   Active                        19.03.8 5. Test Auto-Lock on Manager 2 Restart Docker on manager2: [root@manager2 ~]# systemctl stop docker
[root@manager2 ~]# systemctl start docker Attempt a Swarm command (should fail): [root@manager2 ~]# docker node ls
Error response from daemon: Swarm is encrypted and needs to be unlocked... Unlock the Swarm with your saved key: [root@manager2 ~]# docker swarm unlock
Please enter unlock key: [paste contents of /tmp/swarm-unlock.key] Confirm the node list again: [root@manager2 ~]# docker node ls
ID                        HOSTNAME       STATUS  AVAILABILITY  MANAGER STATUS  ENGINE VERSION
xbvhtg486wmj881wp5vkqx53 * manager1       Ready   Active        Leader          19.03.8
u8imabedhzsu4cawtoz6jh32   manager3       Ready   Active        Reachable       19.03.8
s2ymqdbtfal661imydx31rlno * manager2       Ready   Active        Reachable       19.03.8
38oehhth79bsfs7kco2jcvah   worker1        Ready   Active                        19.03.8
k4gcc5ooc0nm8xgl36fmb2pd   worker3        Ready   Active                        19.03.8
1pqddmhd2f0y7vq9najr841d   worker2        Ready   Active                        19.03.8 Congratulations! Manager 2 has rejoined securely with Auto-Lock enabled. Links and References Docker Swarm Overview Swarm Mode Commands Managing Docker Secrets Watch Video Watch video content"
Docker Certified Associate Exam Course,Swarm Service Types,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Swarm-Service-Types,"Docker Certified Associate Exam Course Docker Swarm Swarm Service Types In Docker Swarm, services define how containers are deployed and managed across a cluster. Swarm supports two primary service modes: replicated and global . This guide explains each mode, shows how to deploy them, and compares their use cases. Replicated Service A replicated service launches a specified number of identical tasks (containers). This is the default mode in Swarm. Note If you omit --replicas , Swarm defaults to 1 replica. You can also explicitly set --mode replicated if you prefer. To deploy five replicas of a web service: docker service create \
  --name web \
  --replicas 5 \
  nginx:latest Verify the service mode and replica count: docker service inspect web \
  --format '{{json .Spec.Mode}}' Sample output: {""Replicated"":{""Replicas"":5}} Key points for replicated services: Tasks are spread evenly across available nodes. Scale up or down by updating --replicas . Ideal for stateless applications like web servers or APIs. Global Service A global service ensures exactly one task runs on every node in the Swarm. Warning Do not specify --replicas with global services. Use only --mode global . To deploy a monitoring agent on all nodes: docker service create \
  --name agent \
  --mode global \
  my-monitoring-agent:1.0 Global mode behavior: New nodes automatically receive one task. When a node leaves, its task is removed and not rescheduled. Perfect for logging, monitoring, and security daemons. Comparison Table Service Type Replica Count Scheduling Behavior Common Use Case Replicated User-defined (N) Distributes tasks evenly Scalable web apps, microservices Global One per node Runs exactly one task on each node Agents for logging, monitoring References Docker Swarm Overview docker service create docker service inspect Docker Blog: Introduction to Swarm Mode Watch Video Watch video content"
Docker Certified Associate Exam Course,Scaling Rolling Updates and Rollbacks,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Scaling-Rolling-Updates-and-Rollbacks,"Docker Certified Associate Exam Course Docker Swarm Scaling Rolling Updates and Rollbacks In this guide, you’ll learn how to scale your services, perform seamless rolling updates, and execute rollbacks in Docker Swarm. We’ll run all commands on the manager node while Swarm orchestrates tasks on worker nodes. Table of Contents Deploying a Service Scaling Service Replicas Rolling Updates Default Update Behavior Introducing an Update Delay Parallel Updates Inspecting Update & Rollback Configuration Handling Update Failures Rolling Back a Service References Deploying a Service By default, creating a service launches a single replica. Here’s how to deploy a simple web service listening on port 80: docker service create \
  --name web \
  --publish 80:80 \
  web:latest Note Ensure the web:latest image is available locally or on your registry before creating the service. Scaling Service Replicas Adjust the replica count of an existing service with docker service update --replicas . Scale up to three replicas: docker service update \
  --replicas 3 \
  web Scale down to one replica: docker service update \
  --replicas 1 \
  web Docker Swarm will automatically add or remove tasks on the worker nodes to match your desired state. Rolling Updates Rolling updates replace containers one batch at a time, ensuring zero downtime. Default Update Behavior After tagging your new image (e.g., web:2.0 ), trigger the rolling update: docker service update \
  --image web:2.0 \
  web Swarm will: Stop one container Deploy a new one Wait for it to pass health checks Repeat until all replicas are updated Introducing an Update Delay Pause between updating each batch with --update-delay : docker service update \
  --image web:3.0 \
  --update-delay 30s \
  web This adds a 30-second wait after each batch to monitor stability. Parallel Updates Increase throughput by updating multiple tasks simultaneously using --update-parallelism : docker service update \
  --image web:2.1 \
  --update-parallelism 2 \
  web This example updates up to two replicas at once, balancing speed and reliability. Inspecting Update & Rollback Configuration Use docker service inspect to review how your service handles updates and rollbacks: docker service inspect web --format '{{json .UpdateStatus}}' Key fields in the output: Configuration Description Parallelism Number of tasks updated simultaneously Delay Time to wait between update batches Failure Action Behavior when an update fails ( pause , continue , rollback ) Monitoring Period Duration Swarm waits for a task to become healthy Handling Update Failures By default, Swarm pauses on the first failure. Change this with --update-failure-action : docker service update \
  --image web:2.2 \
  --update-failure-action rollback \
  web Failure Action Description pause Halt the update on error (default) continue Ignore failures and proceed to next batch rollback Revert immediately to the previous image/version Warning Using continue can leave your cluster in a mixed-version state. Test thoroughly before choosing this option in production. Rolling Back a Service If an update introduces instability, quickly revert to the last known good state: docker service update \
  --rollback \
  web This command restores the previous image and update configuration across all replicas. References Docker Service Commands Docker Swarm Overview Docker Rolling Updates Watch Video Watch video content"
Docker Certified Associate Exam Course,Placement in Swarm,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Placement-in-Swarm,"Docker Certified Associate Exam Course Docker Swarm Placement in Swarm Managing service placement in Docker Swarm ensures that each workload runs on the best-suited node. By default, Swarm distributes tasks evenly across any node with available resources. In real-world clusters, however, you often have nodes optimized for different workloads: Node Name Label Profile worker1 type=CPU-optimized CPU-optimized (batch processing) worker2 type=memory-optimized Memory-optimized (real-time analytics) worker3 type=GP General-purpose (web servers) And services such as: Service Name Resource Profile batch-processing CPU-intensive realtime-analytics Memory-intensive web General-purpose Without explicit placement constraints, Swarm might schedule a CPU-heavy task on a memory-optimized node or vice versa, causing contention and inefficiency. 1. Labeling Nodes Assign key-value labels to each node to reflect its resource profile: docker node update --label-add type=CPU-optimized    worker1
docker node update --label-add type=memory-optimized worker2
docker node update --label-add type=GP               worker3 Verify Node Labels After labeling, confirm with: docker node inspect --format '{{ .Description.Hostname }}: {{ .Spec.Labels }}' worker1 Below is a diagram illustrating the labeled nodes and associated workloads: 2. Applying Placement Constraints Use the --constraint flag with docker service create to bind services to nodes based on labels or built-in properties. 2.1 Match a Label Schedule CPU-intensive and memory-intensive services on their respective optimized nodes: docker service create \
  --name batch-processing \
  --constraint 'node.labels.type==CPU-optimized' \
  batch-processing

docker service create \
  --name realtime-analytics \
  --constraint 'node.labels.type==memory-optimized' \
  realtime-analytics 2.2 Exclude a Label Prevent the web service from running on memory-optimized nodes: docker service create \
  --name web \
  --constraint 'node.labels.type!=memory-optimized' \
  web This ensures web is placed on either the CPU-optimized or general-purpose node. Constraint Syntax Constraint expressions are case-sensitive and must be enclosed in quotes. For more options, see the Service Create reference . 2.3 Using Built-in Node Properties You can also constrain by node role. For example, to run a service only on worker nodes (excluding managers): docker service create \
  --name worker-only-service \
  --constraint 'node.role==worker' \
  alpine:latest ping docker.com 3. Summary of Constraints Service Name Constraint batch-processing node.labels.type==CPU-optimized realtime-analytics node.labels.type==memory-optimized web node.labels.type!=memory-optimized worker-only-service node.role==worker By combining node labels with placement constraints, you guarantee that each workload in your Swarm cluster runs on the most suitable hardware, improving both performance and resource utilization. Links and References Docker Swarm Overview Service Create reference Docker Engine CLI Watch Video Watch video content"
Docker Certified Associate Exam Course,Docker Overlay Network,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Docker-Overlay-Network,"Docker Certified Associate Exam Course Docker Swarm Docker Overlay Network Docker overlay networks provide a single, seamless virtual network across multiple Docker nodes, enabling containers on different hosts to communicate securely. This guide covers Docker’s built-in network drivers, the purpose of overlays, Swarm’s default networks, and how to create custom overlay networks. Revisiting Docker’s Built-In Networks Below is a quick reference for Docker’s default network drivers: Driver Use Case Example Command bridge Container isolation on a single host docker run -p 8080:80 my-web-app host Shares host network namespace docker run --network=host my-web-app none No networking (full isolation) docker run --network=none ubuntu bridge Docker’s default network driver. It creates a private bridge (usually 172.17.x.x ) on the host and connects containers to it: docker run -d --name web \
  -p 8080:80 \
  nginx host Containers share the host’s network namespace—ports inside the container map directly to the host without -p : docker run -d --network=host \
  --name api-server \
  my-api-image none Disables all networking for full isolation. Use this when you don’t need external access: docker run --network=none \
  --name isolated-container \
  ubuntu Why Overlay Networks? Each Docker host has its own bridge, so containers on different machines can’t talk by default. Overlay networks use VXLAN to create a virtual layer 2 network across hosts, making them essential for: Multi-host container communication Docker Swarm service discovery Secure, encrypted traffic between containers Ingress Network in Docker Swarm When you run docker swarm init , Swarm creates an ingress overlay network with a built-in load balancer and routing mesh: docker network ls
NETWORK ID     NAME        DRIVER    SCOPE
68abeefb1f2e   bridge      bridge    local
5bab4adc7d02   host        host      local
e43bd489dd57   none        null      local
mevcdh5b40zz   ingress     overlay   swarm Single Node Service Publishing Without Swarm, you’d expose a container port like this: docker run -p 80:5000 my-web-server With Swarm and two replicas, use --publish : docker service create \
  --replicas 2 \
  --publish 80:5000 \
  my-web-server Ingress’s load balancer listens on port 80 and routes traffic to port 5000 on both replicas. Multi-Node Routing Mesh In a multi-node Swarm, every node advertises the published port (80). Incoming requests on any node are automatically forwarded to an active replica, regardless of where it’s running. Default Swarm Networks Swarm init creates two essential networks: Name Driver Purpose ingress overlay Publishes service ports cluster-wide via routing mesh docker_gwbridge bridge Connects each node’s Docker daemon to the Swarm’s gateway port docker network ls
NETWORK ID     NAME             DRIVER    SCOPE
68abeefb1f2e   bridge           bridge    local
5bab4adc7d02   host             host      local
e43bd489dd57   none             null      local
mevcdh5b40zz   ingress          overlay   swarm
c8fb2c361202   docker_gwbridge  bridge    local Creating Custom Overlay Networks Create your own overlay network for services or standalone containers: docker network create \
  --driver overlay \
  my-overlay-network --attachable : Allows standalone containers to join the overlay. --opt encrypted : Enables AES-encrypted VXLAN for secure application traffic. Remove an overlay network or prune unused ones: docker network rm my-overlay-network
docker network prune Warning Ensure all Swarm nodes can communicate on the required ports (2377, 7946, 4789) to avoid network disruptions. Required Swarm Ports Port Publishing Formats Syntax Description -p 80:5000 Legacy short form --publish published=80,target=5000 Explicit new form -p 80:5000/udp Legacy with protocol --publish published=80,target=5000,protocol=udp New form with protocol # Legacy
docker service create -p 80:5000 my-web-server

# New explicit
docker service create \
  --publish published=80,target=5000 \
  my-web-server

# With UDP protocol
docker service create \
  --publish published=80,target=5000,protocol=udp \
  my-web-server Links and References Docker Networking Overview Docker Swarm Mode Docker CLI Reference VXLAN Protocol Watch Video Watch video content"
Docker Certified Associate Exam Course,Docker Config Objects,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Docker-Config-Objects,"Docker Certified Associate Exam Course Docker Swarm Docker Config Objects In this guide, you’ll learn how to distribute configuration files across multiple Docker nodes using Docker Config objects. We’ll start by overriding a container’s default config on a single host, examine the pitfalls of bind-mounts in a Swarm cluster, and then introduce Docker Config objects as the robust solution for managing configs in Swarm mode. 1. Override Configuration on a Single Host By default, an NGINX container uses its built-in nginx.conf . To replace this with your custom file at /tmp/nginx.conf on the Docker host: docker run -d \
  --name nginx-single \
  -v /tmp/nginx.conf:/etc/nginx/nginx.conf \
  nginx When the container starts, Docker bind-mounts /tmp/nginx.conf into the container’s /etc/nginx/nginx.conf , and NGINX loads your custom configuration. 2. Limitations of Bind-Mounts in a Swarm Cluster Attempting the same bind-mount approach in a Swarm cluster fails if not every node has the file: docker service create \
  --name nginx-service \
  --replicas 4 \
  -v /tmp/nginx.conf:/etc/nginx/nginx.conf \
  nginx Because Swarm schedules tasks across multiple nodes, the service will error out on any node missing /tmp/nginx.conf . Keeping config files in sync manually is error-prone and unscalable. 3. Introducing Docker Config Objects Docker Configs store configuration data in the Swarm control plane and inject it into containers at runtime. 3.1 Create a Config Object docker config create nginx-conf /tmp/nginx.conf 3.2 Deploy a Service with Config docker service create \
  --name nginx-service \
  --replicas 4 \
  --config src=nginx-conf,target=/etc/nginx/nginx.conf \
  nginx Each task receives the nginx-conf content mounted at /etc/nginx/nginx.conf . Configs are read-only and reside on the Swarm manager, eliminating the need for manual file distribution. Note Configs are not a replacement for persistent volumes. Use volumes when your service requires write access or persistent storage. Warning Docker Config objects are only available in Swarm mode. Standalone containers cannot consume configs. 4. Managing Docker Configs Use the following commands to modify or remove configs in your running services: Action Command Description Detach config from service docker service update --config-rm nginx-conf nginx-service Removes the config mount from the service. Delete a config object docker config rm nginx-conf Deletes the config from the Swarm after detachment. Rotate config for a service <ul><li> docker config create nginx-conf-new /tmp/nginx-new.conf </li><li>`docker service update \ --config-rm nginx-conf \ --config-add src=nginx-conf-new,target=/etc/nginx/nginx.conf \ nginx-service`</li></ul> Creates a new config and updates the service without downtime. 5. Further Reading and References Docker Config Documentation Docker Service Commands Docker Volumes vs. Configs Links and Resources Docker Swarm Mode Overview NGINX Official Image on Docker Hub Watch Video Watch video content"
Docker Certified Associate Exam Course,Demo Docker Stack,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Demo-Docker-Stack,"Docker Certified Associate Exam Course Docker Swarm Demo Docker Stack In this lesson, we'll deploy the Example Voting App using Docker Stacks on a Swarm cluster. We start by upgrading our Compose file to version 3, deploy it as a stack, verify running services, and then scale the voting service. 1. Prepare the Docker Compose File (Version 3) Below is the upgraded Compose file in version 3 format. Save this as docker-stack.yml on your Swarm manager: version: '3'
services:
  redis:
    image: redis
  db:
    image: postgres:9.4
  vote:
    image: dockersamples/examplevotingapp_vote
    ports:
      - ""5000:80""
  worker:
    image: dockersamples/examplevotingapp_worker
  result:
    image: dockersamples/examplevotingapp_result
    ports:
      - ""5001:80"" The dockersamples images are available on Docker Hub. Prerequisite Make sure Docker Swarm is initialized on your manager node: docker swarm init 2. Deploy the Stack SSH into your Swarm manager and create a project directory: mkdir ~/voting-app-stack && cd ~/voting-app-stack Place the docker-stack.yml file in this directory. Deploy the stack: docker stack deploy voting-app-stack --compose-file docker-stack.yml You should see output similar to: Creating network voting-app-stack_default
Creating service voting-app-stack_redis
Creating service voting-app-stack_db
Creating service voting-app-stack_vote
Creating service voting-app-stack_worker
Creating service voting-app-stack_result Verify all services are up: docker service ls Service Name Replicas Image voting-app-stack_redis 1/1 redis:latest voting-app-stack_db 1/1 postgres:9.4 voting-app-stack_vote 1/1 dockersamples/examplevotingapp_vote:latest voting-app-stack_worker 0/1 dockersamples/examplevotingapp_worker:latest voting-app-stack_result 1/1 dockersamples/examplevotingapp_result:latest 3. Inspect Running Tasks On any manager or worker node, list the active containers to ensure tasks are distributed: docker ps Example output: CONTAINER ID  IMAGE                                           NAMES
3940e228ce02  dockersamples/examplevotingapp_result:latest    voting-app-stack_result.1.abcde
9348d1d3d7f5  postgres:9.4                                    voting-app-stack_db.1.fghij 4. Access the Voting Application Open a browser and navigate to: Voting interface: http://<manager-ip>:5000 Results dashboard: http://<manager-ip>:5001 5. Scale the Voting Service To handle higher traffic, update docker-stack.yml by adding the deploy.replicas setting under the vote service: services:
  vote:
    image: dockersamples/examplevotingapp_vote
    ports:
      - ""5000:80""
    deploy:
      replicas: 2 Redeploy the updated stack: docker stack deploy voting-app-stack --compose-file docker-stack.yml Confirm that the voting service now has two replicas: docker service ls Service Name Replicas Image voting-app-stack_vote 2/2 dockersamples/examplevotingapp_vote:latest voting-app-stack_db 1/1 postgres:9.4 voting-app-stack_redis 1/1 redis:latest voting-app-stack_result 1/1 dockersamples/examplevotingapp_result:latest voting-app-stack_worker 1/1 dockersamples/examplevotingapp_worker:latest Inspect each task: docker service ps voting-app-stack_vote Links and References Docker Stack Deploy Docker Compose File Reference Docker Swarm Overview This demonstrates how to deploy, verify, and scale services using Docker Stacks in a Swarm cluster. Watch Video Watch video content"
Docker Certified Associate Exam Course,Demo Service in Swarm Replicated Global Parallelism Placements,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Demo-Service-in-Swarm-Replicated-Global-Parallelism-Placements,"Docker Certified Associate Exam Course Docker Swarm Demo Service in Swarm Replicated Global Parallelism Placements In this tutorial, you’ll learn how to optimize your Docker Swarm services by controlling update parallelism, scaling replicated services, running tasks globally, and enforcing placement constraints. Whether you’re rolling out updates or targeting specific nodes, these patterns will help you deploy reliably at scale. We’ll cover: Controlling update parallelism Creating replicated services Running services in global mode Constraining services to specific nodes 1. Update Parallelism By default, Swarm updates one task at a time. Adjusting the update parallelism speeds up rollouts or rollbacks across many tasks. docker service inspect secondservice --pretty Example output: UpdateConfig:
  Parallelism:         1
  On failure:          pause
  Monitoring Period:   5s
  Max failure ratio:   0
  Update order:        stop-first

RollbackConfig:
  Parallelism:         1
  On failure:          pause
  Monitoring Period:   5s
  Max failure ratio:   0
  Rollback order:      stop-first To upgrade two tasks at a time, use --update-parallelism : docker service update \
  --image httpd:2 \
  --update-parallelism 2 \
  secondservice Re-inspect to confirm the change: docker service inspect secondservice --pretty You’ll see UpdateConfig.Parallelism: 2 while the rollback parallelism remains unchanged. Note Increasing --update-parallelism accelerates rollouts but may spike resource usage. Tune based on your cluster capacity. 2. Replicated Service A replicated service ensures a fixed number of identical tasks across the swarm. docker service create \
  --name replicatedtest \
  --replicas 8 \
  redis:latest Progress output: overall progress: 8 out of 8 tasks
1/8: running    [==============>              ]
…
8/8: running    [==============================]
verify: Service converged Verify service and mode: docker service ls
docker service inspect replicatedtest --pretty | grep Mode Output: Mode: replicated
Replicas: 8 3. Global Service Global mode deploys exactly one task per active node. When nodes join or leave, tasks are added or removed automatically. docker service create \
  --mode global \
  --name globaltest \
  redis:latest You’ll see a task scheduled on each node: overall progress: 6 out of 6 tasks
a1b2c3d4e5f6: running [==============================>]
…
z9y8x7w6v5u4: running [==============================>]
verify: Service converged Confirm: docker service ls
docker node ls 4. Placement Constraints Placement constraints let you target services to nodes with specific labels. Label the node (replace <NODE_ID> ): docker node update --label-add env=dev <NODE_ID> Verify the label : docker node inspect <NODE_ID> --format '{{ .Spec.Labels }}' Create the constrained service : docker service create \
  --name placementtest \
  --constraint node.labels.env==dev \
  --replicas 3 \
  redis:latest All three tasks will run only on the node labeled env=dev . Warning If no nodes match the constraint, the service remains pending. Always verify labels before deployment. Service Mode Comparison Service Mode Description Example Flag Replicated Fixed number of tasks --replicas 8 Global One task per active node --mode global UpdateConfig Control rollout parallelism & order --update-parallelism Links and References Docker Swarm Overview docker service update Scheduling tasks in swarm mode That concludes this demo on Docker Swarm update parallelism, service modes, and placement constraints. Watch Video Watch video content"
Docker Certified Associate Exam Course,Swarm Service Discovery,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Swarm-Service-Discovery,"Docker Certified Associate Exam Course Docker Swarm Swarm Service Discovery Service discovery in Docker Swarm enables containers and services to locate and communicate with each other by name, rather than by changing IP addresses. This improves reliability and simplifies your microservices architecture. Container-to-Container Communication By default, Docker Engine allows containers on the same node to resolve each other by container name via its built-in DNS server at 127.0.0.11 . Relying on container names prevents issues when IPs change after restarts. # Not recommended: IP address may change on restart
mysql.connect(
    host='172.17.0.3',
    user='root',
    password='password',
    database='mydb'
)

# Recommended: use container name for stable resolution
mysql.connect(
    host='mysql',
    user='root',
    password='password',
    database='mydb'
) Service Discovery in Docker Swarm In Swarm mode, every service is assigned a DNS entry matching its service name. Load balancing is handled automatically across all replicas. # Create an API server with 2 replicas
docker service create \
  --name api-server \
  --replicas 2 \
  api-server-image

# Create a web frontend
docker service create \
  --name web \
  web-image The web service can connect to api-server simply by its name: curl http://api-server:8080/health Requests to api-server are distributed across its replicas. Using a Custom Overlay Network DNS-based service discovery only works on user-defined networks. The default bridge and ingress networks do not support inter-service name resolution. Create an overlay network to enable DNS resolution across multiple swarm nodes: # 1. Create a custom overlay network
docker network create \
  --driver overlay \
  app-network

# 2. Deploy services on the overlay network
docker service create \
  --name api-server \
  --replicas 2 \
  --network app-network \
  api-server-image

docker service create \
  --name web \
  --network app-network \
  web-image Now, api-server resolves to one of its replicas whenever any service on app-network queries its name. Note Always attach your services to a user-defined overlay network for reliable DNS-based service discovery in Swarm. Network Comparison Network Type DNS Resolution Between Services Scope Default bridge No Single host only Ingress Routing mesh only Cluster-wide load balancing User-defined overlay Yes Multi-host overlay Links and References Docker Swarm Overview Docker Networking Service Discovery in Swarm Watch Video Watch video content"
Docker Certified Associate Exam Course,MacVLAN Network,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/MacVLAN-Network,"Docker Certified Associate Exam Course Docker Swarm MacVLAN Network Containers typically use network namespaces for isolation, but some legacy applications require direct attachment to the physical LAN. Docker’s MACVLAN driver assigns each container its own MAC address on a virtual interface, making the container appear as a standalone host on your network. This guide covers how to create a MACVLAN network, the available modes, and a comparison of Docker’s built-in network drivers. Why Use MACVLAN? Direct Layer 2 connectivity with your physical network Unique MAC addresses for each container Support for legacy applications requiring their own IP on the LAN Note Before you begin, ensure the parent interface ( eth0 in these examples) is active and not part of another bridge. You may need to bring it up with ip link set eth0 up . 1. Creating a MACVLAN Network Use the macvlan driver when creating a Docker network: docker network create -d macvlan \
  --subnet=192.168.1.0/24 \
  --gateway=192.168.1.1 \
  -o parent=eth0 \
  my_macvlan_net Parameters: -d macvlan Selects the MACVLAN driver. --subnet / --gateway Defines the IP range and default gateway on the physical LAN. -o parent=eth0 Binds Docker’s MACVLAN to the host interface eth0 . my_macvlan_net Your custom network name. 2. MACVLAN Modes MACVLAN supports two primary modes for segmenting and isolating traffic: Mode Description Use Case Bridge ( bridge ) Creates a Layer 2 bridge on the parent interface. Simple flat network where all containers share a VLAN. 802.1Q Trunk ( 802.1q ) Tags traffic on a VLAN subinterface (e.g., eth0.100 ). Segmented VLAN routing and filtering per container. Warning Your physical switch must support 802.1Q tagging, and the parent interface must be configured as a trunk port to carry multiple VLANs. 3. Summary of Docker Network Drivers Here’s a quick reference table comparing Docker’s built-in network drivers: Driver Description Typical Use Case none Disables all networking for the container. Security testing, isolated workloads host Shares the host’s network namespace; removes network isolation. High-performance scenarios, monitoring tools bridge Default driver; creates a local L2 bridge on a single host. Single-host deployments, simple microservices overlay Creates an L3 overlay across multiple hosts (requires a key-value store backend). Multi-host Swarm services, cross-node traffic macvlan Assigns unique MAC addresses for L2 connectivity, available in bridge and VLAN modes. Legacy apps, direct LAN access ipvlan Operates at L2 but routes at host level for higher scalability in dense networks. Large-scale deployments with many endpoints 4. Next Steps Once your MACVLAN network is created, you can launch containers on it: docker run -d --network my_macvlan_net --name webserver nginx Each container will receive an IP from your defined subnet and appear as a physical host on the LAN. Links and References Docker Network Drivers Linux VLAN Documentation Kubernetes CNI MACVLAN Plugin That concludes this lesson on Docker MACVLAN networks. Advanced multi-VLAN and trunking scenarios will be covered in a future guide. Watch Video Watch video content"
Docker Certified Associate Exam Course,Docker Stack,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Docker-Stack,"Docker Certified Associate Exam Course Docker Swarm Docker Stack In this comprehensive guide, you'll learn how to move from single-host Docker Compose deployments to robust, multi-node orchestration using Docker Stack on a Swarm cluster. We’ll compare docker run vs. Compose, introduce Docker Swarm concepts, and walk through a real-world voting application example—complete with replicas, placement constraints, resource limits, and health checks. 1. Docker Run vs. Docker Compose When you start with containers, you often use docker run for each service: docker run simple-webapp
docker run mongodb
docker run redis:alpine However, for multi-service applications, Docker Compose simplifies management by defining services in a single YAML file: # docker-compose.yml
version: ""3""
services:
  web:
    image: simple-webapp
  database:
    image: mongodb
  messaging:
    image: redis:alpine Launch all services together: docker-compose up This approach centralizes configuration but is limited to a single host. 2. Introducing Docker Swarm and Stacks Docker Swarm enables clustering multiple Docker engines into a single, fault-tolerant Swarm cluster. Instead of docker run , you create services: docker service create --name web simple-webapp
docker service create --name database mongodb
docker service create --name messaging redis:alpine With Docker Stack, you can use your Compose file to deploy across the Swarm: docker stack deploy --compose-file docker-compose.yml mystack Note Docker Stack uses the same Compose file format (v3+), so you can reuse your existing docker-compose.yml with minimal changes. 3. Containers, Services, and Stacks Container : A running instance of an image, isolated with its dependencies. Service : A scalable set of containers of the same image, distributed across Swarm nodes. Stack : A collection of related services that define an application. 4. Example: Voting Application We’ll deploy a simple voting app with Redis, PostgreSQL, vote service, result service, and a worker. 4.1 Single-Host Deployment with Docker Compose version: ""3""
services:
  redis:
    image: redis
  db:
    image: postgres:9.4
  vote:
    image: voting-app
  result:
    image: result
  worker:
    image: worker docker-compose up All services run on your local Docker host. 4.2 Multi-Node Deployment with Docker Stack Assume a Swarm with one manager and two workers. Enhance the Compose file with a deploy section for Swarm-specific settings. 4.2.1 Replicas Scale services by defining replica counts: version: ""3""
services:
  redis:
    image: redis
    deploy:
      replicas: 1
  db:
    image: postgres:9.4
    deploy:
      replicas: 1
  vote:
    image: voting-app
    deploy:
      replicas: 2
  result:
    image: result
    deploy:
      replicas: 1
  worker:
    image: worker
    deploy:
      replicas: 1 Deploy the stack: docker stack deploy --compose-file docker-compose.yml voting 4.2.2 Placement Constraints Ensure critical services run only on manager nodes: db:
    image: postgres:9.4
    deploy:
      replicas: 1
      placement:
        constraints:
          - node.role == manager 4.2.3 Resource Limits Protect node resources by setting CPU and memory limits: vote:
    image: voting-app
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: ""0.01""
          memory: 50M 4.2.4 Health Checks Automatically monitor container health: vote:
    image: voting-app
    healthcheck:
      test: [""CMD"", ""curl"", ""-f"", ""http://localhost""]
      interval: 1m30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      replicas: 2
      resources:
        limits:
          cpus: ""0.01""
          memory: 50M Field Description test Command run inside the container to check health (e.g., curl ). interval Time between health checks (e.g., 1m30s ). timeout Maximum time to wait before marking a check as failed (e.g., 10s ). retries Number of consecutive failures before marking unhealthy (e.g., 3 ). start_period Grace period before starting health checks (e.g., 40s ). 5. Common Stack Commands Command Description docker stack deploy --compose-file <file> <name> Deploy or update a stack docker stack ls List all stacks docker stack services <stack_name> List services within a specific stack docker stack ps <stack_name> List tasks (containers) for a stack docker stack rm <stack_name> Remove an entire stack Warning Removing a stack stops and removes all associated services and containers. Use with caution in production environments. References Docker Stack Deploy Compose File Reference Docker Swarm Overview Docker Healthcheck Documentation Watch Video Watch video content"
Docker Certified Associate Exam Course,Demo Overlay Network,https://notes.kodekloud.com/docs/Docker-Certified-Associate-Exam-Course/Docker-Swarm/Demo-Overlay-Network,"Docker Certified Associate Exam Course Docker Swarm Demo Overlay Network In this tutorial, you'll learn how to create a custom Docker overlay network, deploy services in both ingress and host publish modes, and clean up all resources. This is essential for scalable, multi-host deployments in Docker Swarm. 1. List Existing Docker Networks Start by checking the current Docker networks on your Swarm manager: docker network ls Example output: NETWORK ID     NAME                DRIVER    SCOPE
a57b5746ed31   bridge              bridge    local
b407178964ed3  docker_gwbridge     bridge    local
b8a3f6b8b75    host                host      local
iudyepxwd79a   ingress             overlay   swarm
02733bc831dd   none                null      local Docker Swarm mode automatically creates two additional networks: Network Name Driver Purpose ingress overlay Ingress load-balancing across Swarm nodes docker_gwbridge bridge Host-to-container communication on manager 2. Create a Custom Overlay Network Overlay networks enable containers across multiple Docker hosts to communicate. To create a custom network named kodekloudnet , run: docker network create \
  --driver overlay \
  kodekloudnet Verify the new network appears in the list: docker network ls Inspect the network’s configuration: docker network inspect kodekloudnet Sample JSON output: [
  {
    ""Name"": ""kodekloudnet"",
    ""Id"": ""mr17hw7o7lnos8qyu3ndecv7"",
    ""Scope"": ""swarm"",
    ""Driver"": ""overlay"",
    ""IPAM"": {
      ""Config"": [
        {
          ""Subnet"": ""10.0.2.0/24"",
          ""Gateway"": ""10.0.2.1""
        }
      ]
    },
    ""Attachable"": false,
    ""Ingress"": false
  }
] Custom Subnet and Gateway To specify your own IP range, add --subnet and --gateway flags: docker network create \
  --driver overlay \
  --subnet 10.10.0.0/16 \
  --gateway 10.10.0.1 \
  kodekloudnet 3. Deploy a Service in Ingress Mode Ingress mode distributes traffic across all nodes on the published port. Even if a task isn’t running on a specific node, it will forward traffic to an active task elsewhere. docker service create \
  --name ingressservice \
  --publish published=80,target=80 \
  --replicas=2 \
  --network=kodekloudnet \
  yogeshraheja/kodekloudwebimage:v1 --publish published=80,target=80 uses the Swarm ingress load-balancer. Access the application via any manager or worker node IP: http://<node-ip>/ 4. Deploy a Service in Host Mode Host mode binds the published port directly on each node where a task is running. Nodes without a task on port 80 will not respond. docker service create \
  --name hostservice \
  --publish mode=host,published=80,target=80 \
  --replicas=2 \
  --network=kodekloudnet \
  yogeshraheja/kodekloudwebimage:v1 Publish Mode Behavior ingress Load-balances across all nodes host Binds port on each node running a task; other nodes won’t serve traffic Host Mode Considerations Traffic is only served on nodes hosting a task. Ensure your load balancer or DNS directs requests to the correct node IPs. 5. Clean Up Resources When you’re done, remove the services and the custom network: docker service rm ingressservice hostservice
docker network rm kodekloudnet Links and References Docker Networking Overview Docker Swarm Mode Overlay Network Driver Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Why it is critical for businesses to adopt new technology,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Introduction-to-Digital-Transformation-with-Google-Cloud/Why-it-is-critical-for-businesses-to-adopt-new-technology,"GCP Cloud Digital Leader Certification Introduction to Digital Transformation with Google Cloud Why it is critical for businesses to adopt new technology In today's fast-paced market, businesses must continually update their technology to remain competitive and meet evolving customer expectations. This guide delves into the risks of falling behind, the importance of ongoing upgrades, and the transformative impact of cloud computing on modern enterprises. The Consequences of Failing to Innovate Many well-known brands have struggled due to their inability to adapt to changing customer demands and technological trends. For example, Yahoo—once a leading force in online advertising—shifted its focus toward media rather than customer behavior and improved user experience. This oversight hindered its ability to monetize user views effectively. In contrast, companies like Google and Facebook thrive because they continuously invest in innovative technologies. Similarly, Sony, renowned for the iconic Walkman, failed to embrace digitalization. Without adopting technologies similar to those behind Apple's iPod, Sony lost its competitive edge. Such examples underscore the high cost of not adapting to emerging trends. The Importance of Continuous Upgrades Just as individuals upgrade their vehicles for better performance and efficiency, businesses must enhance their systems to streamline operations and boost customer engagement. Regular technological updates simplify processes and create a superior experience for both employees and clients. Sustaining Success Through Adaptation Certain companies have not only survived but also flourished by harnessing new technologies. For example, Equifax, a major credit reporting firm, has maintained its high reporting standards by adopting advanced data analysis tools and custom software solutions. Similarly, Walmart, established in 1962, has continuously integrated modern technologies to remain competitive, even in the face of e-commerce giants like Amazon. Insight Embracing innovation is key to securing long-term success and staying ahead in a competitive market. Four Key Requirements for Organizational Growth For organizations aiming to grow sustainably, focusing on these four fundamental areas is essential: Self-evaluate products and services. Adopt innovative technologies. Reorganize internal structures when necessary. Foster internal innovation through custom software solutions. Tip A strategic approach to evaluation and innovation can drive operational excellence and strengthen market positioning. The Role of Cloud Technology Cloud computing serves as a cornerstone for digital transformation. Its scalable, agile solutions empower organizations to adapt quickly to market demands and technological advances. In our next lesson, we will explore how integrating cloud solutions can enhance competitiveness and agility in an ever-evolving digital landscape. By embracing continuous innovation and advanced technologies, businesses can unlock new opportunities and build a robust foundation for future success. Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Lets understand a use case,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Introduction-to-Digital-Transformation-with-Google-Cloud/Lets-understand-a-use-case,"GCP Cloud Digital Leader Certification Introduction to Digital Transformation with Google Cloud Lets understand a use case In this article, we explore a real-world example that clearly demonstrates a common problem statement in the modern business landscape. Our example follows a pharmaceutical company that manufactures and ships healthcare products globally. The company's distribution network uses trucks within the USA and international shipments to places like Sri Lanka. This scenario underscores the broad use of various services in contemporary business operations. The company's operations can be broadly divided into three main sections: Production, Maintenance, and R&D Logistics Sales and Inventory Management Production, Maintenance, and R&D The Production, Maintenance, and R&D department plays a critical role by: Manufacturing healthcare products. Securing certifications from various government bodies. Maintaining machinery and supporting research and development initiatives. Note Ensure that each department seamlessly integrates technological solutions to enhance operational efficiency. Logistics The logistics department is responsible for: Tracking shipments of goods. Monitoring product quality during transit. Ensuring the efficient and safe delivery of products. Sales and Inventory Management The Sales and Inventory Management department focuses on: Tracking sales transactions. Streamlining the ordering process for new stock. Managing interactions with buyers. Each of these divisions consists of multiple teams working in close coordination to maintain the company’s robust operational framework. Building and Deploying Software Applications To support these core business functions, a variety of software applications are developed using diverse technical stacks. Users can access these applications either via mobile apps or web browsers. Given the company's extensive scale, hundreds of professionals including developers, quality assurance specialists, automation engineers, data engineers, security experts, and network engineers collaborate closely to roll out daily updates and innovative features. These applications are deployed across multiple servers grouped into data centers. A reliable supply of power, continuous internet connectivity, and round-the-clock security are critical in ensuring seamless operations. The Problem Statement Operating at a large scale and implementing daily updates often results in slowed processes. The problem statement we address in this article is: How can a large-scale company modernize its infrastructure using Google Cloud Platform (GCP)? Which GCP services can help replace or phase out legacy tools? And how can GCP's advanced offerings like AI, data analytics, and machine learning streamline performance and drive operational efficiency? With this clear problem statement in mind, let’s explore how GCP Cloud can modernize your infrastructure while enhancing overall operational performance. Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Demo Overview of GCP dashboard,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/GCP-account-and-Resource-hierarchy/Demo-Overview-of-GCP-dashboard,"GCP Cloud Digital Leader Certification GCP account and Resource hierarchy Demo Overview of GCP dashboard Welcome to this comprehensive guide on the Google Cloud Platform (GCP) Dashboard. In this lesson, we will walk you through the main components of the GCP console, highlighting important features and functionality available once you've activated your free trial. Introduction When you log into the GCP console, you'll encounter an interface similar to the one displayed on screen. This dashboard is designed to give you immediate access to your projects, notifications, and essential tools for managing your cloud resources. Navigating the Dashboard 1. Navigation Pane (Left Side) Google Cloud Platform Logo: Located at the top, this maintains GCP branding. GCP Projects Section: Just below the logo, you can view your current project (e.g., ""learning Google Cloud""). This section is crucial for project selection and management. Note For detailed information on managing projects, folders, and resource hierarchies, refer to our upcoming lessons. Quick Search Bar: Use this feature to quickly locate various GCP resources. 2. Cloud Shell Activation: Click the Cloud Shell icon to open an integrated terminal within the console, allowing you to execute both GCP-specific commands and standard Linux commands. Operation: Once activated, you can seamlessly interact with your cloud environment. (The terminal has been closed for demonstration purposes.) 3. Notification Section Alerts & Updates: This area provides notifications and alerts concerning your GCP activities. Project Identifiers: You'll find unique identifiers here, including the project number and project ID, which are essential for resource management. 4. Dashboard Page Comprehensive Overview: Selecting the dashboard option presents a detailed page that includes all critical aspects of your GCP account. Information on the Left Pane: This section displays: Project ID and Project Number Resources linked with your project Compute Engine details Billing information GCP status updates Additionally, you'll find segments highlighting the latest GCP news and related documentation. 5. Activity and Recommendations Activity Log: This section records all actions performed in the console, providing a useful audit trail. Recommendations: GCP offers suggestions to optimize your resource usage. For example, you might see recommendations to adjust project-level IAM role grants, potentially reducing excess permissions for improved security. Summary The GCP Dashboard is a user-friendly interface that centralizes all aspects of your cloud management experience—from monitoring projects and managing resources to accessing billing, compute, and notification details. This overview serves as a foundation for upcoming lessons, where we will dive deeper into specific tools and features within GCP. Thank you for reading this guide. Be sure to check out the next lesson for further insights into maximizing your GCP experience effectively. Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Hybrid and multi cloud infrastructures,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Introduction-to-Digital-Transformation-with-Google-Cloud/Hybrid-and-multi-cloud-infrastructures,"GCP Cloud Digital Leader Certification Introduction to Digital Transformation with Google Cloud Hybrid and multi cloud infrastructures Welcome back! In this section, we explore the deployment of hybrid and multi-cloud infrastructures with Google Cloud Platform (GCP) as part of our migration journey. As we configure GCP for our targeted use case, our environment currently operates in a transitional phase utilizing a hybrid cloud setup. A hybrid cloud combines a private cloud—or an on-premises data center—with a public cloud service like GCP. This approach is especially common in sectors such as banking and government, where maintaining dedicated data centers yields significant benefits. In contrast, a multi-cloud deployment leverages more than one public cloud provider. For example, some services may run on GCP while others operate on alternative platforms like Azure or AWS. Both hybrid and multi-cloud solutions are gaining popularity as organizations adopt architectures tailored to their distinct operational challenges and strategic requirements. Note Understanding the differences between hybrid and multi-cloud infrastructures is crucial for designing resilient and scalable IT environments. Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Demo Understanding Billing in GCP,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/GCP-account-and-Resource-hierarchy/Demo-Understanding-Billing-in-GCP,"GCP Cloud Digital Leader Certification GCP account and Resource hierarchy Demo Understanding Billing in GCP Hello and welcome back! In this article, we explore the intricacies of billing within the Google Cloud Platform (GCP). Whether you’re using a personal account or working within an enterprise environment, understanding how resource consumption is tracked and billed is crucial for effective cost management and optimization. GCP provides detailed insights into your resource usage. For example, when running multiple compute instances over a week, you can view granular utilization data for each instance. This detailed overview helps you pinpoint cost drivers and adjust your resource allocation to prevent unexpected overspending. Organizations typically conduct monthly reviews of GCP expenses. When costs increase, it’s essential to investigate whether the rising expenses are due to increased network usage, enhanced Kubernetes cluster activity, or heightened compute instance operations. Identifying the root cause helps determine whether the growth is organic or if resource allocation can be further optimized, potentially eliminating unnecessary expenditures. For personal accounts, the billing cycle ends monthly. In contrast, larger organizations that manage millions of dollars in GCP resources might have billing cycles that extend until the end of the year, based on their contractual agreements with GCP. Additionally, long-term contracts can unlock significant discounts on widely used resources. For instance, signing a five-year contract might qualify an organization for discounted pricing on several services. Accessing Billing Information in the GCP Console To begin, log into your GCP console. Start by typing “billing” in the search bar and select the billing option from the search results. Selecting this option directs you to the billing console. Note If your account is new, you might not see any billing data immediately. In our example, the dashboard displays a billing graph because the account has been active for several months. The dashboard offers an overview of your cost trends. To get more detailed information, click on ""Reports"" in the left-hand navigation pane. In the Reports section, you can review cost utilization for a specific period, such as November. Adjust the filter settings to view ""last month"" across all services, which generates a graph displaying resource consumption throughout the month. This report details the usage of various services like Cloud SQL, Compute Engine, Cloud Storage, and others. For example, the report might show that Cloud SQL incurred a charge of $0.41 with no discounts applied. While larger organizations might benefit from contract-based discounts, this personal account example shows a total cost of $0.43. To locate payment options, navigate to the Payment Settings on the left. Under “Payment Methods,” you can review your billing method. Clicking on ""Payment Method"" will display your card details. Adding a New Payment Method This interface allows you to add a new card if needed. The example shown uses a dummy card for demonstration purposes. Another valuable feature is the “Pricing” tab under cost optimization. Here, you can access comprehensive information about your resource usage. For instance, you might review specific details for services like Cloud Pub/Sub, including the number of units consumed and their associated costs. In this Pricing section, you may come across detailed rows such as a $40 charge for a particular service. By expanding the data to display more rows (e.g., selecting 50 rows per page), you gain additional insights into various resource utilizations—for example, detecting a $0.08 charge for specific Cloud Storage activities. Returning to the Overview page, scroll down to view the top services used. For November, the report highlights Cloud SQL as the highest cost resource at $0.41, followed by Compute Engine. This summary is invaluable when identifying which services are contributing the most to your monthly expenses. This concludes our in-depth look at understanding GCP Billing. We hope this guide helps you effectively manage and optimize your GCP costs. For more information on managing cloud expenses, please explore additional resources on the Google Cloud Documentation . Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Demo Setting up Billing alert in GCP,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/GCP-account-and-Resource-hierarchy/Demo-Setting-up-Billing-alert-in-GCP,"GCP Cloud Digital Leader Certification GCP account and Resource hierarchy Demo Setting up Billing alert in GCP Welcome to this lesson! In this guide, you'll learn how to set up billing alerts for your Google Cloud Platform (GCP) account to monitor and control your spending effectively. Before you begin, ensure that your GCP account is configured correctly and that you are familiar with its billing details. Quick Tip Billing alerts are an essential tool for staying within your budget and avoiding unexpected charges. Why Set Up Billing Alerts? When using GCP, especially with a personal account for learning and development, it's easy to leave resources running—like compute instances—that may exceed your free usage limits. Billing alerts notify you when your spending approaches specific thresholds, helping you avoid unforeseen charges. Accessing the Billing Console Log in to your GCP Console and verify you’re in the correct project. Use the top search bar to type “billing” and select the billing option, which will open the billing console. In the left-hand panel, click on Budgets & Alerts . Creating Your First Budget Alert If you don't have any budgets yet, select Create a Budget . Name your new budget. Choose the budget period. In this example, we will select a Monthly budget that applies to all services within your project. Click Next and set your desired budget amount. For this example, input 10 US dollars. The interface displays historical cost trends on the left, and you may notice that recent spending (e.g., around 0.4 US dollars in previous months) is well below your $10 threshold. Click Next to proceed to the alerts configuration. By default, three alert thresholds are set: 50% of the budget (5 US dollars) 90% of the budget (9 US dollars) 100% of the budget If the 50% alert threshold is not necessary, remove it by clicking the delete option. Define your alert method. Email notifications will be sent to the email address linked to your GCP account—so keep an eye on that inbox. Click Finish to create your billing alert. After completing these steps, you will receive an email notification whenever your spending exceeds the thresholds set for the budget. Creating a Resource-Level Budget Alert In addition to a general account-level budget, you can create a budget alert specifically for a service, such as Compute Engine. Click on Create Budget again. Select Resource-level Budget . Under services, choose all services by default, then filter by typing ""Compute"" and select Compute Engine . Set your budget amount specifically for Compute Engine usage. For example, set it to 5 US dollars. Click Next . If alert thresholds appear, remove any unnecessary ones (like the 50% threshold) and then click Finish . This process creates a second budget alert focused on monitoring Compute Engine consumption. Summary In this lesson, you learned how to set up two types of billing alerts in GCP: Overall Account-Level Alert: Monitors total spending across all services. Resource-Level Alert: Focuses on specific services such as Compute Engine. These billing alerts are crucial for ensuring you remain within your budget and maintain control over your cloud expenditures. Additional Resource For more detailed GCP billing insights and configurations, check out the Google Cloud Billing Documentation . Thank you for following along. You now have the knowledge to manage your GCP costs more efficiently while safeguarding against unexpected charges. Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Demo GCP project folder and resource,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/GCP-account-and-Resource-hierarchy/Demo-GCP-project-folder-and-resource,"GCP Cloud Digital Leader Certification GCP account and Resource hierarchy Demo GCP project folder and resource Welcome to this lesson on managing Google Cloud Platform (GCP) projects. In our previous session, we explored what a project is, reviewed the resource hierarchy, and discussed its importance in GCP. Today, we will demonstrate how to create a new GCP project step-by-step. Exploring the Project Selector Currently, I am working within a project named ""learning Google Cloud."" When you click on the project selector, you'll notice multiple projects listed—even if some share the same name. Although names may be identical, remember that the project IDs are unique. It's essential to ensure that project IDs remain unique across all projects. Project IDs Project IDs in GCP are globally unique. Always verify the project ID you are working with to avoid any configuration issues. Next, click on ""All"" to view complete project details. Creating a New Project Within the selector, you can also see that projects may belong to an organization—if one has been established—or exist without one. You can bookmark frequently used projects by starring them. To create a new project, follow these steps: Click ""New Project"" . Enter a project name. For example, I have named this project ""KodeKloud GCP Training"" . Leave the organization location unchanged. Click ""Create"" . After your request, a new project is generated. Click ""Select this project"" to switch to your newly created project. Understanding the Importance of GCP Projects Every project in GCP serves as a logical separation within an organization. For instance, in a company issuing credit cards, different departments—such as fraud detection, credit card payments, and notifications—might be organized as separate projects. This separation simplifies administration and resource management. A common question arises: Can resources in one project interact with those in another? The answer is yes. Provided the projects belong to the same folder and organization, inter-project interactions are possible. Resource Interaction Resources from separate projects can interact as long as they are in the same organizational context. This flexibility is particularly useful when managing projects across different departments. Throughout this training series, we will use the new project (""KodeKloud GCP Training"") to build and manage various resources. Best Practices for GCP Project Management When you join an organization with a pre-established GCP project structure, it is uncommon to create new projects daily. Often, organizations have been using GCP for many years, with the architecture set long before your involvement. New projects are typically created to support new departments or initiatives. Select the Correct Project Always double-check that you have selected the correct project in the GCP console before executing any operations. This helps ensure that your actions affect the intended resources. Conclusion This lesson demonstrated how to create a new GCP project and provided insights into its importance within an organization. Remember to always manage your projects carefully and verify your current project context in the GCP console. Thank you for joining this session. I look forward to seeing you in the next lesson. Additional Resources Google Cloud Documentation GCP Project and Resource Hierarchy Kubernetes Basics Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Creating of GCP account,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/GCP-account-and-Resource-hierarchy/Creating-of-GCP-account,"GCP Cloud Digital Leader Certification GCP account and Resource hierarchy Creating of GCP account Welcome to this comprehensive guide on creating a Google Cloud Platform (GCP) account. In this lesson, we will explore two primary methods to get started with GCP: Creating a new personal account. Utilizing KodeKloud Pro's integrated GCP playground. KodeKloud Pro offers an excellent platform that provides the necessary resources to help you prepare for the Cloud Digital Leader certification. Option 1: Create a Personal GCP Account If you decide to create your own personal GCP account, here's what you can expect as a first-time user: Explore a Wide Range of Services: Access more than 20 products from the 92 available services on GCP. Free Credit Offer: Benefit from a complimentary credit of $300 USD, which is valid for 90 days. Billing Considerations: After 90 days, any further usage might incur charges. Sign-Up Requirements: In certain regions, you may need to provide a credit card during registration. Important Reminder When practicing in your lab environment, always remember to delete any resources you create to prevent unexpected charges. Option 2: Use KodeKloud Pro KodeKloud Pro's GCP playground provides a hassle-free environment with all the necessary services to help you excel in your certification journey without stepping into potential billing issues on your personal account. Next Steps In the upcoming article, we will guide you through the detailed steps needed to set up and configure your personal GCP account. Stay tuned to learn more about navigating GCP's user interface and starting your cloud computing journey. Thank you for reading, and enjoy your cloud learning experience! For further information, explore the following resources: Google Cloud Platform Overview KodeKloud Pro Courses Cloud Digital Leader Certification Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Demo How to select a region,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Introduction-to-Digital-Transformation-with-Google-Cloud/Demo-How-to-select-a-region,"GCP Cloud Digital Leader Certification Introduction to Digital Transformation with Google Cloud Demo How to select a region Previously, we discussed the global infrastructure of GCP. In this article, we explore a handy GCP tool called the Region Picker. This tool helps you choose the optimal region for your application by evaluating three key parameters: Lower carbon footprint Lower price Lower latency Let's dive into a demonstration of how to use this tool. How to Use the Region Picker Open your browser and search for ""GCP Region Picker"". Click on the first link from the search results. On the landing page, you will notice two panels: Left Panel: Contains adjustable parameters. Right Panel: Displays a list of recommended regions. Tip Try experimenting with different settings. For example, increase the importance of having a lower carbon footprint, emphasize cost efficiency, or prioritize lower latency. Understanding Parameter Adjustments When you adjust the parameters, consider the origin of your traffic. For instance: If you select a region far away from your user base—say, the United Kingdom—you might see recommendations like Finland (Europe North One) as the optimal region. The tool may also suggest other regions, such as Belgium and locations in the USA. Alternatively, if you choose a different starting region such as Switzerland, the tool might then recommend Belgium as the best option. This demonstrates that the optimal region is closely tied to the origin of your incoming traffic. Why Use the Region Picker? This tool is invaluable if you want to ensure you're selecting the best region to accommodate your application's needs or if you're uncertain about where to deploy. By adjusting key parameters, you can balance factors like environmental impact, cost, and latency effectively. That is all for this article—thank you for reading. Watch Video Watch video content"
GCP Cloud Digital Leader Certification,GCP Resource hierarchy,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/GCP-account-and-Resource-hierarchy/GCP-Resource-hierarchy,"GCP Cloud Digital Leader Certification GCP account and Resource hierarchy GCP Resource hierarchy Hello and welcome back! In this lesson, we will explore the concept of the Google Cloud Platform (GCP) resource hierarchy and understand how it efficiently organizes your cloud resources into a structured, hierarchical model. This approach ensures better management, security, and scalability for all your cloud workloads. Hierarchical Structure Overview GCP organizes resources into a tiered hierarchy consisting of three primary levels: organizations, folders, and projects. Organization: The organization resource is at the top of the hierarchy and represents an entire company or legal entity. It serves as the root node for all subsequent resources. Folders: Folders provide an optional layer of grouping below the organization level. They can be used to separate different legal entities, departments, or teams, thus establishing clear isolation boundaries. Projects: Projects are the foundational level where you deploy your resources, including Kubernetes clusters, virtual machines (VMs), SQL databases, and more. Each project represents a unique workspace for your cloud workloads. Key Concept Each level of the hierarchy serves a specific purpose, helping organizations manage permissions, resource usage, and billing effectively. Real-World Example: Pharmaceutical Company Let's apply the resource hierarchy model to a pharmaceutical company use case. This example illustrates how an organization can be structured to support diverse departments while maintaining accountability and ease of management. On the left side of the diagram below, you see a visual representation of an organization's structure using the resource hierarchy model. On the right side, the same hierarchy starts with the organization as the root node. This pharmaceutical company serves as the central node in our GCP setup. Within this organization, various key departments are represented as folders. For example, departments such as Production & R&D, Logistics, and future departments act as individual folders. The Production & R&D folder is further subdivided into development and production projects, each containing specific resources like VMs, firewalls, Kubernetes clusters, SQL databases, etc. This structured approach allows the company to scale operations effectively, providing every team with a dedicated GCP environment. Moreover, it supports a microservices architecture by ensuring a clear separation of concerns, which not only fosters innovation but also simplifies cost tracking for deployed resources. The diagram below further illustrates the organizational setup within our pharmaceutical company, emphasizing the hierarchy of folders, projects, and resources. Organizational Benefits Utilizing GCP’s resource hierarchy model not only streamlines resource management but also enhances security and billing transparency. That concludes this lesson. We hope you now have a clear understanding of the GCP resource hierarchy and how it can be applied effectively using a real-world example from the pharmaceutical industry. Thank you for reading, and see you in the next article! Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Overview Compute in GCP,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/GCP-Compute-Part-1/Overview-Compute-in-GCP,"GCP Cloud Digital Leader Certification GCP Compute Part 1 Overview Compute in GCP Welcome to this lesson on the compute power options available in Google Cloud Platform (GCP). In this session, we will explore the variety of compute services that GCP offers and examine how each option can support diverse application needs. Compute power is a critical component in running applications efficiently. GCP provides a comprehensive range of services designed to handle varied workload requirements. Consider the following use case: a key department comprising Production, Maintenance, and R&D relies on several internally built software applications currently hosted in traditional data centers. Migrating these applications to the cloud necessitates the use of robust virtual machines or servers. For this migration, we have identified four suitable options from the extensive range available: Cloud Functions Compute Engine Cloud Run Google Kubernetes Engine (GKE) or Kubernetes clusters Key Takeaway Each compute service in GCP is designed to cater to different needs, offering flexibility and scalability for modern cloud architectures. In the following sections, we will analyze each option, discussing how they differ and the specific value they bring to your cloud infrastructure. Stay tuned as we dive into the details behind each GCP compute service to help you make informed decisions for your cloud migration strategy. Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Understanding compute Persistent disk and Firewall,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/GCP-Compute-Part-1/Understanding-compute-Persistent-disk-and-Firewall,"GCP Cloud Digital Leader Certification GCP Compute Part 1 Understanding compute Persistent disk and Firewall In this article, we explore key Google Cloud Platform (GCP) concepts, including compute instances, persistent disks, and VPC firewall rules, and illustrate how they work together to support and secure your applications. Google Cloud organizes resources into a hierarchical structure: organizations, projects, regions, and zones. For our examples, we'll use the ""US Central1"" region. Within this region, you choose a specific zone where your compute instances will run. Compute Instances A compute instance is a virtual machine that powers your application. You can customize these instances by configuring parameters such as CPU, RAM, and storage options. Unlike physical servers in a data center, compute instances are managed by GCP, meaning you have limited direct access to the underlying hardware. Persistent Disks Persistent disks provide the storage necessary for your compute instances. They host your operating system, applications, logs, and vital data. Depending on your workload requirements, you can choose from various disk types such as HDD or SSD, offering a balance between performance and cost. VPC Firewall Rules VPC firewall rules help manage and secure network access to your compute instances and the applications running on them. These rules define which ports are accessible and restrict connections based on IP ranges. For example, configuring a firewall rule with the source range of 0.0.0.0/0 exposes your instance to all IP addresses, which is typically not recommended. Instead, it is best practice to limit access to specific IP ranges, such as those of your office network or approved VPN configurations. Security Reminder When configuring VPC firewall rules, avoid using broad rules that expose your infrastructure to unnecessary risks. Always restrict access to trusted IP ranges. Integration of Services Compute instances, persistent disks, and VPC firewall rules work together to establish a secure and efficient environment on GCP. The integration includes: Compute: Hosts your virtual machines where applications run. Storage: Persistent disks hold operating systems, applications, and critical data. Networking: VPC firewall rules secure your environment by controlling incoming connections. Selecting the optimal region and zone is critical for performance. For example, hosting your application closer to your customer base (such as in Japan for Japanese users) can significantly reduce latency and improve response times. Conclusion By understanding and properly configuring compute instances, persistent disks, and VPC firewall rules, you can enhance the efficiency and security of your applications on GCP. In our next article, we will build on these basics by exploring advanced configuration scenarios. Thank you for reading, and we look forward to diving deeper into these topics in future articles. Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Demo Compute 02,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/GCP-Compute-Part-1/Demo-Compute-02,"GCP Cloud Digital Leader Certification GCP Compute Part 1 Demo Compute 02 2 Welcome to this lesson on connecting to and managing your compute instance. In our previous lesson, we explored the creation of compute instances and examined their key features. In this article, we will dive into various options for connecting to a compute instance, accessing its terminal, and managing its lifecycle—including how to stop and delete an instance. Accessing Your Virtual Machine To connect to a specific compute instance, click on the SSH option provided in the instance list. This action opens a new browser tab with an instantaneous terminal session, allowing you to interact directly with the virtual machine. Once connected via SSH, you will see a terminal interface where you can execute Linux commands and inspect system details. Managing Your Compute Instance Stopping the Instance To stop the virtual machine, click on the three-dot menu associated with the instance and select the ""Stop"" option. This action safely shuts down your VM, and you can later choose to delete the instance if it is no longer needed. Monitoring Performance For performance monitoring, click on the ""View Monitoring"" option. This takes you to a dedicated page where you can review vital statistics, including CPU usage, memory consumption, running processes, and other essential metrics. The monitoring page also categorizes logs by severity level, assisting you in troubleshooting any issues. To return to the main dashboard, simply click on ""VM Instances"" in the left-hand navigation menu. Deleting the Instance When you are finished, you might want to clean up unused resources. To delete the virtual machine, open the three-dot menu again, select ""Delete,"" and confirm your action. Note If you're utilizing a free trial, be aware that the time spent on the virtual machine counts against your trial resources. For non-trial users, charges may apply based on the usage duration. We highly recommend deleting unused virtual machines promptly to avoid unexpected billing. Conclusion This lesson covered the essential steps for connecting to and managing your compute instance, including accessing the terminal, monitoring performance, and safely stopping or deleting your VM. Following these guidelines will help you maintain an organized and resource-efficient environment. Thank you for reading. We look forward to guiding you in our next article. Watch Video Watch video content"
GCP Cloud Digital Leader Certification,What is GCP Global Infrastructure,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Introduction-to-Digital-Transformation-with-Google-Cloud/What-is-GCP-Global-Infrastructure,"GCP Cloud Digital Leader Certification Introduction to Digital Transformation with Google Cloud What is GCP Global Infrastructure Welcome to this article on Google Cloud Platform (GCP) global infrastructure. In this guide, we will explore how GCP supports global application delivery, addressing issues such as latency and downtime, and reveal strategies to improve availability by harnessing multiple regions and zones. Current Deployment Challenges Our application is currently hosted in the USA. For instance, while a pharmacist in the USA uses our web application seamlessly to place new orders, customers in Sri Lanka face significant latency due to the long distance from our US-based server. Moreover, if our US data center encounters issues, our entire application risks global downtime. This single-region deployment impacts the overall user experience and safety. Warning Relying solely on one region can lead to 100% downtime if that region becomes unavailable. Consider deploying across multiple regions to mitigate this risk. Enhancing Availability with GCP Infrastructure To improve application availability, we can deploy across multiple zones within a region. Multiple zones help distribute incoming requests and provide better fault tolerance. However, note that if an entire region experiences issues, even multiple zones within the region may not be sufficient. Deploying Across Regions Deploying the application in a region closer to the customer base can reduce latency and improve user experience. For instance, while our current deployment is in the USA, we can utilize another region, such as Mumbai, to serve customers in South Asia more efficiently. Combining this approach with the addition of zones within each region further enhances reliability. GCP's global infrastructure spans across North America, South America, Europe, Asia, and Australia. With 34 regions and 200+ country availability, GCP continuously maintains and upgrades its network with 24/7/365 support. As illustrated, GCP operates with a widespread network of regions interconnected through advanced networking, ensuring fault tolerance and reliability. This robust design enables businesses to locate resources close to their customers, reducing latency and boosting overall performance. For our pharmaceutical company, a strategic design would involve two primary regions: one in the USA and another in Mumbai. This configuration decreases latency by locating application instances nearer to end users, and it enhances availability through regional diversity and redundancy. Furthermore, our centralized engineering team can deploy updates regionally based on real-time demand, paving the way for seamless expansion as the customer base grows. Additionally, since GCP manages these global data centers, our focus can simply be on enabling the necessary services in the chosen regions instead of building our own infrastructure. Regions and Zones Explained In GCP, a region refers to a specific geographical area, such as the USA, where multiple zones exist. Zones are individual data centers within a region that are interconnected with low-latency links. Typically, a region comprises three or more zones, each playing a vital role in ensuring high availability and resilience. The diagram above outlines a typical multi-region setup, with our application deployed in two distinct regions — for example, US West 1 and Asia South 1. In each region, several zones host the compute instances, collectively enhancing application reliability and performance. Summary In this article, we covered: An overview of GCP global infrastructure and how it supports global application delivery. The current challenges of single-region deployment, including latency issues and potential downtime. The benefits of deploying applications across multiple regions and zones. Detailed explanations of regions and zones, emphasizing their roles in ensuring high availability. By leveraging GCP's robust global infrastructure, we can ensure that our application remains responsive and available to users worldwide. This strategic flexibility not only addresses immediate performance issues but also paves the way for future growth and global expansion. We look forward to sharing further insights in upcoming articles. Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Demo Creating a personal GCP account,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/GCP-account-and-Resource-hierarchy/Demo-Creating-a-personal-GCP-account,"GCP Cloud Digital Leader Certification GCP account and Resource hierarchy Demo Creating a personal GCP account Welcome to this step-by-step guide on creating your personal Google Cloud Platform (GCP) account. In this lesson, we will walk you through the process of setting up a GCP account, including details on activating your free trial and providing necessary billing information. Important Please note that the required information may vary depending on your region. In some regions, you might be asked to provide credit card details, so have them ready. Step 1: Accessing the GCP Console Begin by opening a fresh browser window or a new tab. Enter the following URL into your address bar: console.cloud.google.com Press Enter to navigate to the GCP console. You will be redirected to the sign-in page where you need to log in with your Gmail account. Step 2: Signing into Your Gmail Account Enter your Gmail account credentials and click the next button: On the next page, you will be prompted to enter your password. After providing your password, you will be redirected to the GCP account dashboard: Step 3: Activating Your Free Trial Once you access your account dashboard, you might see a pop-up inviting you to start your free trial. If the ""Start your free trial"" pop-up does not appear automatically, locate the gift box symbol within the blue tab. Clicking this symbol will reveal a white tab with the trial activation option. New GCP users are given $300 in credits to explore the platform for 90 days. This trial is perfect for testing various GCP services and learning the basics of cloud computing. To activate your free trial, follow these steps: Click on the Activate button. When prompted, complete the following: Country Selection: Choose the country where you reside. Purpose of the Trial: Select an option like ""Personal Project."" Terms and Services: Agree to the Terms and Services. Billing Information: Provide additional details including business information, primary contact data, and your credit card details. After completing all the required fields, click Start my free trial to proceed. Billing Information Reminder Your billing information is used solely for account verification. No charges will be incurred as long as you operate within the free trial limits and the 90-day period. Step 4: Exploring Advanced Features After activating your free trial, you now have full access to the GCP console. This allows you to explore a wide range of GCP services and features. For further learning, consider these additional resources: GCP Documentation Google Cloud Platform Tutorials GCP Free Tier Overview Thank you for following this guide. Enjoy exploring Google Cloud Platform and leveraging its powerful features for your personal projects! Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Demo Compute 01,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/GCP-Compute-Part-1/Demo-Compute-01,"GCP Cloud Digital Leader Certification GCP Compute Part 1 Demo Compute 01 Hello and welcome to this lesson on launching your very first Compute Engine instance on Google Cloud Platform (GCP). In this tutorial, we will guide you through the step-by-step process and review the critical parameters involved in setting up your virtual machine (VM). Step 1: Select Your Project Before you begin, ensure that you have selected the correct project. We have created a dedicated project for our learning process—make sure to choose this project from the drop-down menu. Step 2: Locate the Compute Engine After selecting your project, use the search bar to enter ""compute"" and select the corresponding compute option when it appears. The top result, ""Compute Engine,"" is your gateway to the Compute API dashboard. The Compute Engine API is disabled by default when you first use GCP resources. To utilize it, you must enable the API: Note If the API is already enabled due to previous usage, you may not see the enable prompt. Step 3: Enable the Compute Engine API Click the ""Enable"" button to activate the Compute Engine API. You might be prompted to provide billing information at this stage: Billing Information If you are using a free trial subscription or selected region without billing requirements, this prompt may not appear. Allow a few seconds for the API to activate. You will then be redirected to the Compute Engine console. Step 4: Create a New VM Instance On the Compute Engine dashboard, click the ""Create Instance"" option or switch to the VM instance tab. This action opens a form with comprehensive options for configuring your virtual machine (VM). Configuring Your VM Instance Name and Labels Assign a unique name to your VM. Optionally, add labels (e.g., ""environment: dev"") for better organization. Region and Zone Select the appropriate region and zone based on your requirements. For this demonstration, choose Montreal and an available zone. Consider factors like low CO2 emissions when selecting your region if that is important to your deployment strategy. Machine Configuration Choose among the four available machine families: General Purpose, Compute Optimized, Memory Optimized, and GPU. For development and testing, the default General Purpose settings are typically sufficient. Operating System and Boot Disk Click ""Change"" in the boot disk section to view the available operating system options. Debian is set as the default, but you can opt for alternatives like CentOS. Adjust the disk size or persistent disk options as needed. Firewall Configuration Scroll down to the firewall settings and enable HTTP traffic. This setting permits both HTTP and HTTPS traffic through the firewall. Advanced Networking Options For users interested in deeper network and disk configurations, advanced options are available. These settings will be covered in later lessons. Step 5: Launch Your Virtual Machine After finalizing your settings, click ""Create"" to launch your virtual machine. Remember, within the same project (KodeKloud GCP Training), you can create VMs in various regions without having to switch projects. Your project can support multiple VMs of different sizes, purposes, and geographic locations. The provisioning process may take a few minutes—typically around 2–3 minutes in this demo, but up to 10 minutes in some cases. Once your instance is ready, its details will appear on the console. Conclusion That concludes our lesson on creating your very first Compute Engine instance. In upcoming lessons, we will cover how to access your VM, install software like an Apache web server, configure firewall ports, and more. Thank you for joining, and see you in the next lesson! Additional Resources Google Cloud Compute Engine Documentation Getting Started with Google Cloud Watch Video Watch video content"
GCP Cloud Digital Leader Certification,How cloud technology is revolutionizing business,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Introduction-to-Digital-Transformation-with-Google-Cloud/How-cloud-technology-is-revolutionizing-business,"GCP Cloud Digital Leader Certification Introduction to Digital Transformation with Google Cloud How cloud technology is revolutionizing business Welcome to this lesson on the transformative impact of cloud technology. In today's discussion, we explore how cloud platforms, such as Google Cloud Platform (GCP) , are enabling businesses to achieve digital transformation by driving innovation, reducing costs, and improving operational efficiency. Cloud technology offers numerous advantages, including: Lower infrastructure and maintenance costs. A stronger focus on enhancing customer experience. Flexibility to experiment with minimal upfront investments. Improved collaboration among engineering teams. Robust disaster recovery solutions. On-demand scalability for adapting to fluctuating workloads. For example, flexibility allows companies to launch diverse sales campaigns with minimal initial expenditures. Moreover, cloud services empower teams to work independently on various products, reducing delays commonly associated with siloed structures. This results in faster development cycles and quicker launches of new services. One significant advantage of using GCP is its robust disaster recovery and reliability support. By transferring these responsibilities to GCP, organizations can leverage its robust infrastructure while still maintaining control over specific operational aspects. Scalability also plays a pivotal role; businesses can rapidly harness additional resources during high-demand periods—such as during major ad campaigns—and then scale back seamlessly when demand diminishes. GCP Cloud streamlines infrastructure management by providing and maintaining the necessary resources. This allows organizations to concentrate on enhancing customer experience and fostering innovation. As customer satisfaction rises, so does business growth—similar to the success seen in many popular mobile apps today. Key Insight Migrating to cloud technology not only reduces operational burdens but also empowers businesses to innovate and scale faster in a competitive market. Let's consider a practical use case involving a pharmaceutical company with three key departments driving its operations. Traditionally, such companies might rely on private data centers built in-house. However, moving to GCP can accelerate their digital transformation through a focused approach that includes: Migrating existing infrastructure and applications to the cloud. Leveraging GCP’s machine learning and AI tools to glean insights into customer trends and optimize internal processes. Maintaining security and operational standards within the cloud environment. These initiatives align with industry best practices and underscore how cloud technology can benefit sectors like pharmaceuticals by modernizing infrastructure and driving data innovation. That concludes this lesson on how cloud technology is revolutionizing business. Thank you for following along, and we look forward to sharing more insights in our next article. Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Demo GCP playground KodeKloud,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/GCP-account-and-Resource-hierarchy/Demo-GCP-playground-KodeKloud,"GCP Cloud Digital Leader Certification GCP account and Resource hierarchy Demo GCP playground KodeKloud Welcome to the GCP Playground! In this guide, we will walk you through accessing your exclusive Google Cloud Platform environment, provisioning resources, and understanding the sandbox restrictions. Follow the step-by-step instructions below to get started. Accessing the GCP Playground To begin, click the Start Lab button. Shortly after, you will receive your unique login credentials alongside the access instructions, as shown below: The credentials are valid for 3 hours. Copy the details provided: Console link: https://console.cloud.google.com
Username: [email protected] Password: prnA3TnC*%K
Expires in 179:45 Tip For a smooth login experience, open an incognito or private browsing window. Paste the username and password in the new window and complete the sign-in process. Once logged in, remember to accept the Google agreements for your new account. Once you have accepted the agreements, your screen will display an interface similar to the one below. Select the correct organization and project to view your GCP dashboard: On the left-hand side, services like Compute Engine, Google Kubernetes Engine, Cloud Storage, Cloud Run, and SQL Databases are grouped by type. Now let's start provisioning some resources. Provisioning a Compute Engine VM Instance To create a virtual machine (VM) instance, follow these steps: Click on the Compute Engine option. Select the Create Instance button. Sandbox Restrictions Remember that this sandbox environment enforces fair usage policies. Ensure you select allowed configurations while provisioning resources. When configuring the instance: Choose either an E2 Medium or N1 Standard machine type (or a smaller option). This guide uses E2 Medium . Scroll down to select the disk type. Change it from a balanced persistent disk to a standard persistent disk and set the disk size to 15 GB . Ensure that HTTP and HTTPS traffic are enabled. While the default image is Debian Linux 11 Bullseye, you may select another available VM image if necessary. Click the Create button to initiate the provisioning process. This step may take a few minutes. Setting Up Firewall Rules While your VM instance is being created, navigate to the firewalls page to configure additional rules: Verify that the default HTTP and HTTPS firewall rules are active. Create a new firewall rule to allow inbound SSH access with the following settings: Target: All instances in the default VPC (including your new VM). Source Range: 0.0.0.0/0. Allowed Port: TCP 22. After creating the rule, click on it to review its details and ensure it applies correctly to your instance. The new firewall rule should be applied to your instance (labeled as ""instance dash one""). Connecting to the VM Instance Once the firewall rule is set, return to the VM instances page and click the SSH button to connect via Cloud Shell. A new tab will open, establishing an SSH connection to your VM instance. Overview of Playground Restrictions Before proceeding further, familiarize yourself with some key restrictions in the GCP Playground: Service Restrictions Example/Note Compute Engine - Allowed machine types: E2 Medium or N1 Standard (or smaller)<br>- Allowed regions: US Central or US West 1 only<br>- Max CPU quota: 5 CPUs<br>- Disk size: Standard persistent only (up to 50 GB total) Ensure your instance configuration complies. Google Kubernetes Engine - Only GKE Standard clusters are permitted<br>- Node pools are limited to a maximum of three E2 Medium nodes<br>- Total disk size across nodes: 50 GB maximum Follow documented guidelines for cluster setup. SQL Databases - Only single-zone deployments<br>- Machine types: Standard or lightweight<br>- Maximum: 4 CPUs and 15 GB storage Use single-zone deployments for SQL instances. Additionally, for SQL databases: For a complete list of restrictions and permitted services, refer to the support documentation provided in your GCP Playground dashboard. Final Notes This concludes the walkthrough of the GCP Playground. By following this guide and adhering to the outlined restrictions, you can safely experiment with GCP services in a secure sandbox environment. For additional details on GCP services and best practices, consider visiting the following resources: Kubernetes Basics Google Cloud Documentation KodeKloud Support Happy learning and exploring! Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Overview Scaling compute with instance group and load balancers,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/GCP-Compute-Part-2/Overview-Scaling-compute-with-instance-group-and-load-balancers,"GCP Cloud Digital Leader Certification GCP Compute Part 2 Overview Scaling compute with instance group and load balancers Hello and welcome back! In this section, we dive into the essential topic of scaling compute instances in the Google Cloud Platform (GCP) environment. After understanding compute fundamentals and how GCP provides robust solutions to run your software, we now address the critical question: How can you scale these compute instances to handle increasing loads efficiently? Setting the Stage Imagine you're starting with a basic setup: A valid GCP account. A chosen GCP region. Applications accessed through VPC firewall routes. Software installed on a compute instance. While this configuration might work for a small deployment, it falls short when aiming for production-grade scalability. Key Point Scaling is not just about adding resources, but also ensuring that traffic is managed effectively and downtime is minimized during maintenance or updates. The Scalability Challenge Consider a very large pharmaceutical company scenario: Hundreds of thousands of users access websites or internal applications every minute. Relying on a single machine to accommodate such demand is unsustainable. Even a high-performance machine can become a bottleneck, especially during: Application Deployments: Introducing new features or updates. Maintenance Operations: Regular or emergency system updates. Unexpected Traffic Surges: Load spikes during peak usage times. Adding extra compute instances during high demand is only half the battle. You must also manage and distribute the incoming traffic seamlessly to prevent any instance from becoming overloaded. Critical Warning Without proper load balancing, one or more instances might experience heavy traffic while others remain underutilized. This imbalance can lead to performance degradation and potential downtime. Addressing the Two Main Challenges In this section, we focus on two pivotal challenges from a GCP perspective: Effective Compute Scaling: How do you horizontally scale your compute resources during traffic surges? What strategies and GCP services can be leveraged to handle growth efficiently? Traffic Distribution: How do you balance incoming traffic across multiple instances to prevent overload? Which load balancing techniques and tools provided by GCP can assist in ensuring smooth traffic flow? Our discussion encompasses the various GCP services and resources designed to tackle these challenges, turning the scaling of compute resources into a manageable and efficient process. Conclusion and Next Steps This article has explored the fundamental challenges of scaling compute instances and routing traffic effectively in a GCP environment. As you design and deploy your solutions, understanding these concepts is crucial to maintaining high availability and reliability. Stay tuned for our upcoming articles, where we will delve deeper into specific GCP services and best practices that further enhance your cloud architecture. Thank you for reading, and happy scaling! Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Demo Instance group,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/GCP-Compute-Part-2/Demo-Instance-group,"GCP Cloud Digital Leader Certification GCP Compute Part 2 Demo Instance group Welcome back! In this lesson, you will learn how to create an instance group in the Google Cloud Platform (GCP) console and explore the key parameters configured during its creation. Follow along to ensure your instance group is set up correctly and efficiently. Accessing the GCP Console Log into the Google Cloud Platform Console . Once logged in, verify that you have selected the correct project. For this demonstration, I have chosen the project I created specifically for this training. In the search bar, type “compute” and select Compute Engine . The Compute Engine page will display, showing that there are no active VMs running to help avoid unnecessary costs. If you do not see the left sidebar, click the toggle icon (the chevron icon) at the bottom of the screen. Creating an Instance Group Scroll down the left sidebar until you find the Instance groups section. If you haven’t created any instance groups yet, the interface will indicate that none exist. Click Create instance group to start the process. A configuration form appears with several parameters: Name and Description: Provide a clear name for your instance group and a brief description (for example, ""this is my test instance group""). A meaningful description is especially useful in collaborative environments. Instance Template: An instance template defines the configuration of the virtual machines within the group, including machine type, disk space, and other settings. If you haven’t created one yet, you can create an instance template on the fly. Within the instance template settings, you have the option to add labels (for example, ""NVIDIA development"") and adjust boot disk configurations. In this demonstration, the boot disk options are left as default and both HTTP and HTTPS traffic are enabled. Once the instance template is configured, click Save and continue . This saves the template for future instance groups, ensuring consistency across your deployments. Configure the instance group location (using the default option is acceptable) and adjust the autoscaling settings: Autoscaling Settings: Define the minimum and maximum number of instances. For this demo, the minimum is set to 1 and the maximum to 3. A scaling rule is configured to add an instance if CPU utilization exceeds 60%. Autoscaling Tip For a more conservative scaling policy, consider setting the CPU utilization threshold to 80% and defining an appropriate cooldown period to prevent frequent scaling events. After reviewing all settings, click Create . With these configurations, the instance group will initially deploy one virtual machine. If you specify a higher minimum instance count, the corresponding number of VMs will be created immediately. Verifying and Monitoring the Instance Group Navigate to VM instances in the left-hand menu. You should see that one virtual machine has been created and is managed by the instance group configuration, indicated by its labels and linked instance template. Click on the individual VM to inspect its configuration and confirm that it belongs to the instance group. Managed VMs are preferable to unmanaged ones, which might only be used for testing. Return to the instance group view and click on the Monitoring tab. This section displays aggregate metrics (such as average CPU and RAM utilization) for all VMs in the group, making centralized management of autoscaling and autohealing features easier. Testing Self-Healing Capability It is important to understand how instance groups handle failures: Select any instance in the group and click Delete instance . Confirm the deletion when prompted. Important With a minimum instance count set to 1, the instance group will automatically create a new virtual machine after deletion. This self-healing feature maintains service availability even if a VM is manually removed. Monitor the instance group to see a new VM being added. The newly created VM will have a modified identifier, confirming it as a replacement. Cleaning Up Resources When you have finished practicing, it is crucial to delete resources to prevent unnecessary charges: Return to the instance group page. Select the instance group and click Delete . Resource Cleanup Reminder Always delete unused resources to avoid exceeding your free quota and incurring additional costs. Although we created an instance template during this process, there is no need to delete it since it does not incur extra charges and can be reused in future projects. Conclusion This lesson demonstrated how to create, configure, and monitor an instance group in GCP. Follow each step diligently, and don’t forget to clean up your resources to avoid unnecessary charges. Thank you for participating, and see you in the next lesson! For further reading and more detailed guides, consider checking out the following resources: Google Cloud Documentation Compute Engine Documentation Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Instance group,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/GCP-Compute-Part-2/Instance-group,"GCP Cloud Digital Leader Certification GCP Compute Part 2 Instance group Welcome back. In our previous discussion, we explored the challenges of scalability and high availability on GCP and introduced the solution: instance groups. What Is an Instance Group? Instead of launching multiple virtual machines (VMs) independently, you can deploy them under a single instance group. This approach allows you to manage multiple VMs as one entity. When creating an instance group, you can configure essential parameters such as the desired number of VMs and set minimum and maximum limits for the group. Additionally, autoscaling rules can be defined to automatically add or remove VMs based on resource usage. For example, consider the following architecture: In this setup, the instance group consists of three VMs monitored as a single unit. You could configure the system to ensure that a minimum of 2 machines and a maximum of 10 machines are active. To manage scaling, you might define a rule: if the total CPU utilization exceeds 80%, add one more VM, wait two minutes, and recheck. This process continues until the group reaches the limit of 10 machines. These combined features provide a robust solution for achieving both scalability and high availability. Deployment Tip In application deployment, instance groups are invaluable. Deploy a new instance group, thoroughly test your software, and then seamlessly route traffic to the updated instances. High Availability and Self-Healing Relying on a single VM for high availability can be risky—if the VM fails, there’s no backup. Instance groups address this risk by automatically spinning up a replacement if a VM becomes unresponsive. Consider a scenario where an instance group comprises three VMs—VM01, VM02, and VM03. If VM01 fails, dropping the active count to two, the autoscaling mechanism detects this deficit and launches a replacement (e.g., VM04). The problematic VM is then marked as out of service, and the instance group self-heals to maintain the desired capacity. Self-Healing Benefits The ability of instance groups to self-heal minimizes downtime and reduces the need for manual intervention, ensuring continuous service availability. Types of Instance Groups An instance group is a collective of VMs managed as a single entity. There are two primary types: Managed Instance Groups: These groups consist of identical VMs that are automatically created and maintained. Features such as autoscaling, automated updates, self-healing, and high availability make Managed Instance Groups ideal for production environments. Unmanaged Instance Groups: In unmanaged instance groups, VMs are created independently and require separate management. Lacking built-in autoscaling and self-healing capabilities, they are less suited for scenarios where high availability and scalability are critical. Managing Incoming Traffic After establishing an instance group, the next step involves managing incoming traffic and implementing effective load balancing. How do you route traffic to the appropriate instance group when multiple groups are available? How do you ensure efficient load distribution? These questions will be addressed in upcoming discussions focused on load balancing and traffic management. That concludes this article. Thank you for reading! Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Load Balancer,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/GCP-Compute-Part-2/Load-Balancer,"GCP Cloud Digital Leader Certification GCP Compute Part 2 Load Balancer Welcome to this detailed lesson on load balancing within the Google Cloud Platform (GCP). In this article, we discuss the challenges of routing traffic between different instance groups and demonstrate how GCP’s load balancing solution can help distribute incoming network traffic efficiently and reliably. Routing Traffic Between Instance Groups Imagine a scenario where you manage two instance groups—one running software version 01 and the other running version 02. Initially, you might direct all incoming traffic exclusively to the instance group running version 01. Later, as you update your infrastructure or roll out new features, you can gradually shift traffic to the instance group running version 02. This strategy allows you to control traffic routing based on software versions or specific deployment requirements. Note This routing approach is applicable not just for managed instance groups but can also be adapted for unmanaged instance groups. You can effectively route traffic between individual compute instances, regardless of group management. The Role of Load Balancing Load balancing is the process of distributing incoming network traffic across multiple backend instances. This is essential for: Optimizing resource utilization Maximizing throughput Minimizing response times Preventing any single server from being overwhelmed In the context of GCP, load balancing is especially effective for managing TCP protocol traffic—ensuring that client requests are handled in an efficient and balanced manner. Below is a diagram that illustrates a typical GCP architecture. In this configuration, mobile devices connect to a cloud load balancer that distributes traffic to two instance groups hosting virtual machines (VMs) in the us-central-1 region: Next Steps In the following sections, you will learn more about: The intricacies of TCP protocols and their relevance to load balancing Various load balancing strategies available in GCP How to implement these concepts within your own cloud environment Note Keep an eye out for upcoming articles where we delve deeper into TCP protocols, load balancing strategies, and practical implementation tips. Thank you for reading this lesson. We look forward to exploring more technical topics with you in future articles. Additional Resources GCP Load Balancing Documentation Google Cloud Platform Overview Understanding TCP Protocols Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Sumary Understanding compute Persistant disk and firewall,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/GCP-Compute-Part-1/Sumary-Understanding-compute-Persistant-disk-and-firewall,"GCP Cloud Digital Leader Certification GCP Compute Part 1 Sumary Understanding compute Persistant disk and firewall Hello and welcome! In this lesson, we summarize the key concepts related to compute, persistent disk, and firewall components, emphasizing points that are especially relevant for certification. You may pause at any time to review these highlights before continuing to explore the details below. Compute Engine Compute Engine is our virtual machine (VM) solution where your applications run. It offers a selection of machine types to suit various workloads: Machine Type Description General Purpose Balanced performance for diverse tasks Memory Optimized Enhanced memory for intensive applications Compute Optimized Superior processing for compute-heavy tasks GPU Optimized Specialized for graphics and machine learning workloads One of the key advantages of Compute Engine is its per-second billing model, making it cost-effective for dynamic cloud deployments. In addition, you can choose from a range of operating systems, or even deploy your own custom image based on your specific requirements. Persistent Disk Persistent Disk provides durable network storage essential for the efficient operation of Compute Engine instances. It is used to host the application code and necessary software components. You can easily attach a persistent disk to any VM instance, and it offers the flexibility to scale storage size as needed. Note Persistent Disks are designed for high availability and durability, making them ideal for critical workloads. VPC Firewall Rules To securely access your Compute Engine instances—whether you're connecting via SSH or serving a deployed web application—it's crucial to configure VPC firewall rules. These rules control incoming and outgoing network traffic based on protocols, ports, and IP ranges, effectively acting as your security guardian. Additional Compute Options Beyond Compute Engine, Google Cloud Platform offers other computing solutions for various needs: Cloud Run for fully managed containerized applications. Cloud Functions for event-driven serverless computing. Google Kubernetes Engine (GKE) for orchestrating containerized applications using Kubernetes. These alternatives cater to different deployment models and will be discussed in more detail in upcoming lessons. Warning Ensure that you configure your VPC firewall rules correctly to avoid potential security risks. Always review and update rules as your network requirements change. Thank you for reading this summary. We look forward to exploring more advanced topics in the next lesson! Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Demo Load Balancer,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/GCP-Compute-Part-2/Demo-Load-Balancer,"GCP Cloud Digital Leader Certification GCP Compute Part 2 Demo Load Balancer Hello, and welcome back to this article. In this guide, we will walk you through configuring a load balancer on Google Cloud Platform (GCP) and explain the essential concepts behind each step. Setting Up the Instance Group Begin by logging into the GCP console and selecting the appropriate project. Navigate to the Compute section by entering ""Compute"" in the console search bar. On the Compute page, create an instance group as outlined below. Click on the Instance Groups section. Select the option to create a new instance group. Name the group to reflect its purpose (e.g., ""load balancing"") and choose an appropriate instance template. This template, which outlines the configuration for your instances, will be reused throughout this demonstration. Adjust the autoscaling settings: set the maximum number of instances to two instead of the default ten, since this demonstration does not require a high scale. Instance Group Interface Overview Review the instance group creation interface to familiarize yourself with the available options, including naming the group, providing a description, selecting an instance template, and choosing the target region. After confirming the settings, accept the default location and other configuration settings. Ensure that the autoscaling settings have been adjusted as described, then click ""Create"" to provision your instance group. Creating the Load Balancer With the instance group in place, the next step is to create a load balancer. While it is possible to attach multiple instance groups to a load balancer, this demonstration uses a single instance group for simplicity. To locate the load balancing service: Use the scroll menu on the left side of the GCP console. Alternatively, search for “load balancing” in the search bar. GCP organizes load balancing under Network Services, alongside features like VPC firewall rules. Once in the load balancing section, click on ""Create Load Balancer."" GCP will present three types of load balancers: HTTPS Load Balancing (Layer 7) TCP Load Balancing (Layer 4) UDP Load Balancing (Layer 4) Since HTTPS load balancing is the most common and operates at Layer 7 over TCP, click ""Start Configuration"" under the HTTP load balancer option. When prompted, specify the purpose of your load balancer: Choose between ""internet-facing"" (for external access) and ""internal-facing"" (for internal communication only). For this demonstration, select the internet-facing option. Then, choose between a global HTTPS load balancer, a classic load balancer, or a regional load balancer. For simplicity, proceed with the default configuration and click ""Continue."" Provide a name for your load balancer (e.g., ""HTTP Load Balancer"") and confirm that the frontend will listen on port 80. Configuring the Backend Service Proceed to the ""Backend"" section to define where the load balancer should route incoming traffic: Click on the drop-down menu and select ""Create a backend service."" Fill in the details: Assign a name to the backend service (e.g., ""load balancer backend service""). Select the instance group created earlier. Configure the port to 80. If you plan to use additional instance groups, you can click ""Add a backend."" For this demonstration, proceed with a single backend and click ""Cancel"" when prompted to add another. Note At this stage, no service is running on port 80 of your instance group, so any incoming traffic will not reach an active service. Further automation and software installation will be covered in subsequent articles. Setting Up the Health Check Scroll down to set up a health check, which is critical to monitor the status of your backend instances. Create a health check (for example, name it ""simple health check"") that will attempt to access port 80. Because there is currently no service running on port 80, the health check will fail. This is expected and will be rectified once the necessary application is deployed. Optionally, you can configure a security policy using Cloud Armor. For now, leave the security settings as default and click ""Create."" Confirm any prompts by clicking ""OK"" to finalize the backend service configuration. Configuring Routing Rules Next, define the routing rules that determine how incoming traffic is directed to your backend: Navigate to the routing rules dialog. Specify host and path rules if you need to direct specific URL patterns (e.g., traffic ending with ""/payment"" can be assigned to a dedicated backend). For this demonstration, use the default routing rule. Click ""Create"" to finalize the load balancer configuration. After creation, the load balancer generates a unique URL. However, if you access it immediately, you might see a ""no healthy upstream"" error because the backend instance is not yet serving content on port 80. Verifying Health Checks and Load Balancer Status After connecting the load balancer to your instance group, you may notice that the backend is flagged as unhealthy. To troubleshoot: Click on the health check settings to review the configuration. Verify that the health check is attempting to access port 80 on your instance group. Once the necessary application is deployed and listens on port 80, the health check will succeed, and the load balancer will start routing traffic appropriately. Clean-Up and Conclusion After completing this demonstration, it is important to clean up unused resources to avoid unnecessary charges. Follow these steps: Delete the load balancer along with its associated health check. Remove the instance group from the Compute section. This concludes our demo on setting up a load balancer on Google Cloud Platform. In future articles, we will explore automation processes to install and update software on your instance group, ensuring your applications become fully operational once deployed. Thank you for reading, and we look forward to guiding you in the next article. Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Summary of Load Balancer,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/GCP-Compute-Part-2/Summary-of-Load-Balancer,"GCP Cloud Digital Leader Certification GCP Compute Part 2 Summary of Load Balancer In this lesson, we explore key terminologies and concepts related to load balancers that are essential for managing and optimizing network traffic. TCP Protocol The Transmission Control Protocol (TCP) is a foundational element in networking. TCP ensures reliable, ordered, and error-checked delivery of data between servers and clients, forming a key part of the TCP/IP suite. Network Models: TCP/IP vs. OSI Understanding network communication begins with layered models. Two of the most common models include: TCP/IP Model : Consists of four layers—Application, Transport, Network, and Network Interface. OSI Model : Divides communication into seven layers—Application, Presentation, Session, Transport, Network, Data Link, and Physical. Each protocol operates at specific layers: HTTP functions at the Application layer. Ethernet operates at the Network Interface (or Data Link) layer. TCP works at the Transport layer. Load Balancing Protocol Options When setting up a load balancer, you have three primary protocol options to choose from: HTTP : Ideal for static websites serving simple content. TCP : Suitable for services requiring reliable transport, such as streaming platforms. UDP : Often used for streaming and applications where speed is critical over reliability. For example, streaming services like YouTube or Netflix might use TCP or UDP protocols depending on their specific requirements. Understanding these protocol options is crucial before diving deeper into TCP networking. Health Checks Health checks ensure the backend services behind a load balancer are operational and ready to handle traffic. They continuously monitor the status of instance groups and help direct traffic to healthy nodes. Note Health checks are integral in minimizing downtime by automatically rerouting traffic when an instance group fails the health check, thereby maintaining continuous service. Example Scenario Imagine a setup where two instance groups host a website through a load balancer. Health checks monitor these instance groups; if one group encounters an issue (such as returning a ""no upstream"" error), the load balancer automatically directs traffic to the operational group. This ensures that users continue to access the website seamlessly despite underlying failures. Core Concepts of Load Balancing Load balancers distribute incoming user traffic across multiple servers or groups to achieve several benefits: Reducing load on individual servers Enhancing overall performance and reliability Dynamically routing traffic to healthy instances to mitigate failures Load balancers operate primarily at two layers: Layer 4 : Handles TCP and UDP traffic. Layer 7 : Manages application-level traffic, such as HTTPS. They can be deployed as either external-facing or internal-facing systems, and the use of robust health checks is critical to ensure optimal performance. Conclusion This article has provided a concise overview of load balancer concepts, from TCP protocols and network models to protocol options and health checks. Understanding these elements is crucial for configuring and maintaining efficient, resilient networks. We look forward to elaborating on these topics in upcoming lessons as we explore service deployment and advanced network configurations. Thank you for reading, and stay tuned for more insights in our next lesson! Watch Video Watch video content"
GCP Cloud Digital Leader Certification,4 steps of handling big data in GCP,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Google-Clouds-solutions-for-machine-learning-and-AI/4-steps-of-handling-big-data-in-GCP,"GCP Cloud Digital Leader Certification Google Clouds solutions for machine learning and AI 4 steps of handling big data in GCP Welcome back! In this guide, we explore the four critical steps that organizations should follow for an effective Big Data and AI implementation in Google Cloud Platform (GCP). Building on the concepts from the 4V’s of Big Data, these steps will help you design a robust infrastructure to collect, process, analyze, and leverage data with advanced AI and machine learning techniques. 1. Data Collection Data collection is the foundation of any Big Data strategy. There are two main approaches: Batch Processing: Data is collected and sent periodically—such as monthly or daily with a 24-hour delay. Real-Time Processing: Also known as near real-time, where the data is collected with minimal delay (a few milliseconds to seconds). By understanding these methods, you can choose the right collection strategy to suit your business needs. 2. Data Processing Once data is collected, the next step is to store and transform the information. Given the high volume and variety of data, relying on specialized processing tools like Apache Spark is crucial. These tools efficiently handle large datasets, ensuring that data is properly cleaned and formatted for further analysis. 3. Data Analysis After processing, data analysis comes into play. This step involves extracting valuable insights from the processed data to drive informed decision-making. Effective analysis leads to better business strategies and allows organizations to uncover patterns and trends that might otherwise go unnoticed. 4. AI and Machine Learning The final step is to integrate AI and machine learning into the workflow. These advanced technologies can further refine insights by: Analyzing past data to predict future trends. Optimizing marketing strategies based on previous campaign performance. Customizing ad targeting across different social media platforms to reach the right audience. Note Incorporating AI and machine learning not only enhances your analytical capabilities but also transforms data into actionable business intelligence. Consider a scenario where an organization invests significantly in product marketing. Effective advertising demands that the right message reaches the targeted audience. By leveraging AI and machine learning, companies can assess past marketing performance, fine-tune future campaigns, and ensure optimal ad placement across various channels. These four steps—data collection, processing, analysis, and the application of AI/machine learning—form the pillars of a resilient Big Data setup. Next, we will dive into the specific GCP services that support each stage of this process, ensuring a seamless transition from data acquisition to intelligent insights. Thank you for reading, and stay tuned for more insights on building an efficient Big Data infrastructure with GCP! Watch Video Watch video content"
GCP Cloud Digital Leader Certification,SQL and NoSQL,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Database/SQL-and-NoSQL,"GCP Cloud Digital Leader Certification Database SQL and NoSQL Hello and welcome back. In this article, we explore the fundamental differences between SQL and NoSQL databases, discuss their unique use cases, and illustrate how various databases in Google Cloud Platform (GCP) fit into these categories. SQL Databases Relational (SQL) databases store data in a tabular format and adhere strictly to the ACID properties: Atomicity, Consistency, Isolation, and Durability. These properties ensure robust transaction management and data integrity—an essential requirement in online transaction processing (OLTP) systems. Common examples include ticketing, order processing, and authentication systems where transactional integrity is critical. In GCP, prominent SQL databases include: Cloud SQL Cloud Spanner These databases are optimized for managing structured data and ensure consistency across multiple transactions. NoSQL Databases NoSQL databases diverge from the traditional tabular structure by using storage models such as document-based or key-value pair systems. Each data entry is stored as a key and its corresponding value, which allows for a more flexible schema design. This flexibility makes NoSQL databases ideal for applications requiring rapid scalability and adaptable data structures. For instance, if you are analyzing which restaurants a customer ordered from in 2022, you might store the customer ID as the key and the associated restaurant IDs as values. This method is particularly effective for data science applications—such as tailoring personalized recommendations based on ordering patterns and taste preferences. In GCP, the key NoSQL databases include: Cloud Bigtable Memory Store Firestore Depending on your application’s requirements, you can choose between a SQL system for transactional reliability or a NoSQL system for flexible scalability. Choosing the Right Database When deciding on a database, it is crucial to evaluate your application needs. For data integrity and complex transactions, lean towards SQL databases. If your primary focus is on scalability and flexible data models, a NoSQL solution might be more appropriate. Categorizing Database Models A practical way to differentiate between SQL and NoSQL databases is by grouping them according to their design models—relational, non-relational, and in-memory databases. Each group is tailored to specific use cases: Database Type Use Case Description GCP Examples Relational (SQL) High data integrity and transaction management Cloud SQL, Cloud Spanner Non-relational (NoSQL) Flexible schema designs, scalable for analytics and recommendations Cloud Bigtable, Firestore, Memory Store In-memory Fast data retrieval, often combined with other models for enhanced performance (Various caching services) This classification not only helps in understanding the inherent differences between databases but also assists in selecting the right GCP service based on your project’s specific needs. Conclusion SQL and NoSQL databases each serve unique roles in modern application architectures. From handling structured transactional data to providing flexible, scalable solutions, understanding these models is key to optimizing your application performance on Google Cloud Platform. That is it for this article. We look forward to sharing more insights in our next discussion. Thank you for reading! Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Demo Database demo,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Database/Demo-Database-demo,"GCP Cloud Digital Leader Certification Database Demo Database demo Hello and welcome back to this demo lesson on launching and managing a cloud database using Google Cloud Platform (GCP). In this session, we will set up a Cloud SQL instance, connect to it using Cloud Shell, and explore essential configuration options. Cloud Database Architecture The architecture for this demo is straightforward: users access a URL on their smartphones or laptops, which routes through a cloud load balancer to an instance group. This group then connects to Cloud SQL for database services. Below is a diagram illustrating the typical Google Cloud architecture: Creating a Cloud SQL Instance In this section, you'll learn how to spin up a Cloud SQL instance and configure it for your needs. Note In production environments, always secure your database with a strong password. Follow these steps: Access the GCP Console: Start by logging into your GCP account and navigating to the Cloud SQL page. Create Instance: Click on Create Instance . Select MySQL: From the three options provided, choose MySQL . Name your instance ""main-db"". For this demo, the instance is set up without a password. (For production, always use password protection.) Configure Instance Settings: Choose your desired MySQL version. Additional configurations include: Automated Backups: GCP automatically backs up your database. High Availability: The database is spread across multiple zones to minimize downtime. Point-in-Time Recovery: Enables quick recovery from failures. Choose Environment: Optionally, select whether the database is for development or production. Leave the region at its default value for this demo. Create Instance: Once you have verified your settings, click on Create Instance . The process usually takes about 6 to 7 minutes. After the instance is created, click on it to open the dashboard, where you can monitor CPU utilization, active connections, and other key metrics. Connecting to the Cloud SQL Instance To interact with your MySQL database, use Cloud Shell available directly in the GCP console. Launch Cloud Shell: Click on Open Cloud Shell . Connect to the Database: Run the following command to initiate the connection: gcloud sql connect main-db --user=root --quiet Enable Cloud SQL Admin API: If the connection fails because the Cloud SQL Admin API isn’t enabled, a URL will be provided. Copy and open this URL in a new browser tab to enable the API. This process may take one to two minutes. Reconnect: After enabling the API, execute the command again: gcloud sql connect main-db --user=root --quiet MySQL Session: No password was set during creation, so simply press Enter when prompted. Within the MySQL session, you can run commands like: SHOW DATABASES; Exit MySQL: To exit the session, type: quit Tip In production applications, credentials should be securely stored and used rather than connecting manually through Cloud Shell. Disabling Delete Protection and Deleting the Instance When you're ready to clean up, you will disable delete protection before deleting the instance. Edit the Instance: Click on the Edit button for your database. Disable Delete Protection: Locate the delete protection option and disable it, then save your changes. Review Maintenance Settings (Optional): You may review and adjust maintenance windows by scrolling to the maintenance section. Configure a preferred maintenance window (e.g., Sunday) and set its duration. Delete the Instance: Return to the main database page, click Delete , confirm the database name, and proceed. Deletion typically takes about two to three minutes. Summary In this lesson, you learned how to: Spin up a Cloud SQL database on Google Cloud Platform. Connect to the database using Cloud Shell. Configure key features such as automated backups, high availability, and maintenance windows. Disable delete protection and safely delete the database instance. Thank you for following along with this demo lesson on managing cloud databases with GCP. For further reading, check out the Kubernetes Documentation and Docker Hub . Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Simple application deployment 01,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Usecase/Simple-application-deployment-01,"GCP Cloud Digital Leader Certification Usecase Simple application deployment 01 In this guide, we detail the process of deploying a simple, scalable, and highly available application on Google Cloud Platform (GCP). Throughout this tutorial, you'll learn about autoscaling, instance groups, load balancers, and strategies to ensure high availability while maintaining an easy, streamlined deployment process. Key Concepts and Best Practices The application should adhere to the following best practices: Scalability: The deployment must smoothly handle increasing load. High Availability: The system should continue running even when individual resources fail. Easy Deployment: The process of rolling out the application should be straightforward and automated. Application Deployment Requirements To build a resilient application on GCP, consider the following minimum requirements. This diagram illustrates the essential components necessary for successful deployment: Integrating GCP Components A robust application depends on the seamless integration of multiple GCP components. Some critical questions to address include: How do we connect an instance group to a load balancer? What methods are effective for scaling an instance group? How can we ensure the application code is deployed automatically to new instances? Important Note Keep in mind that when using an instance group, machines might be terminated and replaced frequently. It is therefore essential to automate the deployment process so that every new instance automatically includes the latest version of your application code. Practical Implementation In this demo, we integrate a load balancer, an autoscaling instance group, and an automated deployment procedure. The main objective is to guarantee that each new instance in the group has the required application code installed seamlessly. Let's dive into the solution and explore how these GCP components work together to deliver a scalable, highly available, and straightforward deployment process. For more detailed information on navigating GCP services and similar deployment strategies, please refer to Google Cloud Documentation . Watch Video Watch video content"
GCP Cloud Digital Leader Certification,What is object storage,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Object-Storage/What-is-object-storage,"GCP Cloud Digital Leader Certification Object Storage What is object storage Hello and welcome back! In this lesson, we explore the storage options available in Google Cloud Platform (GCP) with a focus on object storage. Previously, we covered volumes and databases—storage solutions designed for structured data. Today, our scenario is different. Imagine pharmacies around the world uploading diverse types of information to our system, including transactional receipts, bills, and other related data. This information must be persisted for auditing and compliance purposes. Since the data is unstructured—encompassing files, images, and more—it cannot be efficiently stored in traditional volumes or databases. Instead, a storage solution optimized for unstructured data is required. This is where object storage becomes the ideal choice. Object storage manages data as individual objects, making it highly efficient for storing and retrieving unstructured content. In GCP, Google Cloud Storage is the service that delivers this capability. Note Google Cloud Storage is often referred to interchangeably as ""buckets."" This terminology highlights the way data is organized and accessed within the service. Benefits of Google Cloud Storage for Object Storage Scalability: Efficiently handles vast amounts of unstructured data without performance degradation. Durability: Promises high durability and availability, ensuring your data remains safe and accessible. Cost Efficiency: Minimizes expenses and operational complexities compared to maintaining extensive on-premises data centers. By understanding the differences between storage options and recognizing the unique benefits of object storage, organizations can make informed decisions when migrating to GCP. Thank you for reading this lesson. Stay tuned for more insights in our upcoming articles. Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Different storage class GCP Storage,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Object-Storage/Different-storage-class-GCP-Storage,"GCP Cloud Digital Leader Certification Object Storage Different storage class GCP Storage Hello and welcome back! In this lesson, we will explore the concept of storage classes in Google Cloud Storage and understand their significance. After learning how to create and configure a Cloud Storage bucket, it’s crucial to know how to select the optimal storage class for your needs. When setting up a Cloud Storage bucket, one of the key configurations you encounter is the storage class selection. Consider the following scenario: multiple pharmacies around the world are uploading various multimedia data to Cloud Storage. After implementing this architecture, you might wonder what happens to older data—say, files uploaded a year ago. Even if this data is infrequently accessed, it may be necessary for auditing purposes, which means deletion is not an option. Understanding Storage Classes Storage classes allow you to optimize costs by balancing storage expenses with data access frequency and durability. They help ensure that you only pay the necessary cost for data that is seldom accessed. Below is a table summarizing the primary storage classes offered by Cloud Storage, including their minimum storage duration and typical monthly availability: Storage Class Details Standard Storage Standard Storage is the default option. With Standard Storage, there is no minimum storage duration requirement. You can delete files immediately after uploading, and you only pay for the duration that the file is stored. Nearline, Coldline, and Archive Storage The other storage classes, such as Nearline, Coldline, and Archive Storage, are designed for cost-effective storage of infrequently accessed data. For example, when you upload data to Nearline Storage, it must be retained for at least 30 days. Despite the lower storage costs compared to Standard Storage, retrieval fees can be higher. As you progress from Standard to Nearline, then to Coldline and Archive Storage, costs decrease. Archive Storage is particularly ideal for long-term retention of data that might not be accessed regularly—such as files from over a year ago that are retained solely for archival or auditing purposes. Important Consideration Remember that while Nearline, Coldline, and Archive Storage offer cost-effective solutions, the cost of retrieving data from these classes is higher. Ensure that you only move data that is not required for rapid, daily access into these storage classes. These storage classes provide flexible configurations to help you control costs and optimize performance based on how frequently you access your data. Thank you for reading this lesson! Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Understanding API,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Building-APIs-in-GCP/Understanding-API,"GCP Cloud Digital Leader Certification Building APIs in GCP Understanding API Hello and welcome back! In our previous lesson, we explored a problem statement whose solution has evolved into what we now call an API. What Is an API? An API (Application Programming Interface) is an endpoint that enables two systems or teams to communicate. It allows you to: Retrieve information using a GET request. Execute actions using a POST request. Update information using a PUT request. For instance, consider a situation where two teams—each focused on different microservice objectives—implement an API. If the production maintenance team decides to apply a change, they don’t execute the change directly. Instead, they notify the logistics team through the API, and the logistics team then updates their code accordingly. Key Insight This approach promotes team independence, enabling upgrades, downgrades, and bug fixes without creating direct interdependencies. The Benefits of API-Driven Design Even though the applications might be tightly coupled, the use of APIs simplifies communication, making the system easier to manage, maintain, and scale. This architectural pattern is now a standard in modern software design and development. Managing APIs in Google Cloud Platform (GCP) As organizations scale, the number of APIs deployed in the cloud can grow rapidly. Managing these APIs efficiently is critical. In GCP, developers have access to tools that help in creating, building, maintaining, and testing APIs effortlessly. A critical consideration for modern development is identifying the developer-friendly tool in GCP that assists in API creation and management. Looking Ahead In the next lesson, we will: Explore the specific GCP tools that simplify API creation. Discuss strategies for effectively managing a growing portfolio of APIs. Thank you for reading, and see you in the next lesson! Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Object storage and GCP Storage bucket,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Object-Storage/Object-storage-and-GCP-Storage-bucket,"GCP Cloud Digital Leader Certification Object Storage Object storage and GCP Storage bucket Welcome back! In this article, we explore the essential concept of object storage and dive into the features of Google Cloud Storage Buckets. Object storage is a type of computer architecture meticulously designed to manage large volumes of unstructured data. A common example is Google Drive, where data is uploaded once, infrequently modified, and efficiently stored without the constraints of traditional folder hierarchies. Google Cloud Storage leverages this modern architecture by organizing data into containers known as ""buckets."" Buckets simplify data management and ensure scalability, allowing you to store any type of file—from documents to multimedia files—with ease. What Is a Google Cloud Storage Bucket? A Google Cloud Storage Bucket serves as a highly scalable repository that accommodates every type of data including images, audio, video, and files of all sizes. Once created, you can upload files into your bucket while only incurring costs based on actual storage usage. For example, storing 20 gigabytes of data means you will only be billed for that amount, and your charges will increase as you add more data. Google Cloud Storage bucketing also features turbo replication, which ensures rapid synchronization across regions. When enabled, this feature replicates 100% of your data between different geographical areas in 15 minutes or less. Whether your data is replicated from the US to India or between any other supported regions, you benefit from quick and efficient data distribution. Data Durability Note Google Cloud Storage Buckets are engineered for exceptional reliability with durability ratings often described as nine nines. This high durability minimizes the risk of data loss, ensuring your information remains safe and intact. Key Use Cases Google Cloud Storage is designed to meet a wide range of modern data storage requirements. Its flexibility makes it ideal for several applications, such as: Rich Media Storage and Delivery: Host and deliver high-quality images, videos, and multimedia content seamlessly. Big Data Analytics: Utilize buckets as central repositories for analytical workloads and data warehousing. Internet of Things (IoT): Aggregate sensor data from numerous IoT devices for analysis and processing. Backup and Archival: Securely store backup and archival data for long-term retention and regulatory compliance. Exam questions and real-world scenarios frequently emphasize these use cases. When selecting a storage solution for backup or archival needs, Google Cloud Storage Buckets often emerge as the optimal choice. That concludes this article. Thank you for reading, and we look forward to seeing you in the next lesson. Watch Video Watch video content"
GCP Cloud Digital Leader Certification,How APIs can modernize legacy systems,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Building-APIs-in-GCP/How-APIs-can-modernize-legacy-systems,"GCP Cloud Digital Leader Certification Building APIs in GCP How APIs can modernize legacy systems Welcome to this lesson on modernizing legacy systems using APIs. In this session, we’ll explore the challenges presented by traditional legacy infrastructures and examine how APIs—complemented by the robust services of Google Cloud Platform (GCP) —can resolve these issues. In many organizations, teams such as production, maintenance, research & development (R&D), logistics, sales, and inventory operate both independently and interdependently. For example, in our pharmaceutical company scenario, while each team runs its own applications, they require seamless integration to share critical information. The logistics team, for instance, depends on data from production, maintenance, and R&D, whereas sales and inventory need real-time updates from logistics. Each department typically manages its own application, yet effective data exchange between these applications is crucial. For example, maintenance and R&D might develop an application for the logistics team to confirm that a package is ready for shipment. Similarly, when shipments take place, the sales and inventory systems must be updated in real time. This scenario underscores the challenge of ensuring efficient communication between independently developed systems operating in a tightly coupled environment. Integration Beyond Internal Systems In many cases, internal communications extend to external interactions. For instance, when the sales and inventory departments process orders, they may integrate with banking systems using credit or debit card APIs to securely handle transactions. This interdependency has given rise to what is known as microservice architecture. In this approach, independent software components—often developed by different teams—communicate efficiently using well-defined APIs. This structure not only streamlines internal processes but also offers the flexibility to interface with external systems, thereby establishing a robust and scalable microservice ecosystem. In the next section, we will delve into how these independent services interact and integrate within a microservice framework to form a cohesive and modern system architecture. Thank you for joining this lesson on how modern APIs can revitalize legacy systems. Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Demo Creating a GCP storage bucket,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Object-Storage/Demo-Creating-a-GCP-storage-bucket,"GCP Cloud Digital Leader Certification Object Storage Demo Creating a GCP storage bucket Welcome to this guide on creating a Google Cloud Platform (GCP) Storage Bucket. In this walkthrough, you'll learn how to set up a storage bucket, review its key configuration settings, and understand best practices when managing your cloud storage. Verify Your Project in GCP Console Start by confirming that you're working within the correct GCP project. In the console, you should see details similar to those in the image below: Access Cloud Storage In the GCP Console, search for “Cloud Storage” and select the corresponding option from the menu. This will navigate you to the storage buckets overview screen: Since there are no storage buckets listed yet, click on Create Bucket to get started. Create a New Storage Bucket When creating a bucket, you need to provide a name. Remember, the bucket name must be globally unique, not just unique within your organization. Naming Tip A helpful naming strategy is to start with your company prefix (e.g., ""pharma"") followed by a descriptor for its purpose (e.g., ""store-data-information""). Once the name is set, click Continue . If the chosen name isn’t globally unique, GCP will alert you with an error, prompting you to try a different name. Configure Bucket Location and Storage Class Choose a Region Select the region where your data will be stored. Depending on your compliance and data residency requirements, you might choose a specific region. Alternatively, you can opt for a multi-region setting if you want high availability by replicating data across multiple regions. For this demonstration, the multi-region option is selected. Click Continue after making your choice. Select a Storage Class At this step, you choose a storage class that best meets your needs. Although we won’t dive deep into the different storage classes in this article, the default setting is sufficient for this demonstration. Move forward with the default options. Enforce Public Access Prevention Before finalizing the creation, GCP displays an important warning regarding public access: Security Warning Exposing a bucket to the public internet can be risky because unauthorized users might gain access to your stored data. GCP recommends enabling the option ""Enforce public access prevention on this bucket"" to keep your data secure. Check that the public access prevention setting is enabled and then click Confirm . Upload Files to Your New Bucket After your bucket has been successfully created, you can start uploading files. The bucket supports various file types such as images, documents, and audio files. To upload, simply click on the Upload button and select either individual files or an entire folder from your laptop. For this demonstration, the process of clicking Upload Files and selecting a file is illustrated below: An important note is that the bucket and its contents are private by default. Only users with the necessary permissions can access the stored data. Next Steps Once your bucket is set up and files are uploaded, you can leverage it for various live activities within your projects. Stay tuned as future guides will cover more advanced configurations and best practices. Thank you for following along with this demo. For further information, check out: Google Cloud Storage Documentation GCP Security Best Practices Happy cloud computing! Watch Video Watch video content"
GCP Cloud Digital Leader Certification,4Vs of BigData,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Google-Clouds-solutions-for-machine-learning-and-AI/4Vs-of-BigData,"GCP Cloud Digital Leader Certification Google Clouds solutions for machine learning and AI 4Vs of BigData Hello and welcome back to our deep dive into Big Data and AI on Google Cloud Platform (GCP). In this lesson, we explore the four core characteristics—Volume, Velocity, Variety, and Veracity—that form the foundation of Big Data concepts. Volume of Data Throughout our day, every interaction with technology—from streaming videos and music to performing Google searches—generates enormous amounts of data. This continuous creation of data defines the first ""V"" — Volume. Understanding and managing this large quantity of data is crucial for leveraging it effectively. Velocity of Data Closely tied to volume is the speed at which data is generated. For example, when using a food ordering app, every interaction—scrolling through menus, selecting items, and confirming orders—is captured and transmitted in real time. This rapid pace of data generation and processing is what we refer to as Velocity. Real-Time Data Processing Real-time data processing enables companies to respond quickly to emerging trends and customer behaviors. Variety of Data The data we produce doesn't come in a single format. From images and voice recordings to text inputs and sensor data, our digital interactions yield a diverse range of data types. This diversity is captured by the term Variety, which emphasizes the need to adapt systems to handle multiple data formats effectively. Veracity of Data Even though massive volumes of data are generated at high speeds and in various formats, not all of it is reliable or accurate. Veracity refers to the quality and trustworthiness of data. Organizations must critically assess and validate data to ensure it is both accurate and useful. Data Quality Alert Ensuring data veracity is essential—poor-quality data can lead to misleading analyses and flawed decision-making. These four principles—Volume, Velocity, Variety, and Veracity—are the cornerstones of Big Data analytics. Often compared to a new form of oil, data is a strategic asset. To gain a competitive edge, improve your products, and build innovative solutions, it is essential to harness and manage your data effectively. Big Data and AI in GCP Google Cloud Platform provides a robust suite of services designed to tackle Big Data challenges while integrating powerful AI capabilities. In this lesson, we introduced the 4 V's as a framework for effective data management. In our next session, we will explore specific GCP services that utilize AI to address these big data challenges, complete with practical examples and detailed case studies. Thank you for joining us, and stay tuned for the next lesson! Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Apigee in GCP,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Building-APIs-in-GCP/Apigee-in-GCP,"GCP Cloud Digital Leader Certification Building APIs in GCP Apigee in GCP Hello, and welcome back! In our previous lesson, we explored the critical role that APIs play in modernizing applications. Today, we dive into Apigee—a robust API management service offered by Google Cloud Platform (GCP). Overview of Apigee Apigee delivers a comprehensive toolset for building, managing, and scaling APIs in the cloud. It also supports on-premises deployment, making it an excellent choice when migrating from legacy systems to a modern cloud infrastructure. This flexibility ensures that you can test and validate your applications in an on-premises environment before transitioning seamlessly to GCP. Key Features AI-Powered API Monitoring: Apigee leverages artificial intelligence to monitor API traffic, spot failures, and address scalability challenges. Detailed insights into API performance help maintain optimal operations. Developer-Friendly Tools: Apigee provides a wide range of tools that simplify API development, testing, and monitoring for both client applications and backend services. Microservices Support: Designed for modern architectures, Apigee excels at managing API interactions within microservices, ensuring smooth communication and integration. Note Understanding the key features of Apigee can significantly enhance your API management strategy. Whether operating on-premises or in the cloud, Apigee's flexibility is a valuable asset. Architectural Overview Apigee's architecture centralizes API management by serving as the single entry point for client applications. Here’s a high-level breakdown of its operation: Clients and Apigee Edge: Client applications route requests through Apigee Edge, which manages and authenticates API calls to backend services. AI-Powered Monitoring: Real-time monitoring services deliver actionable insights, ensuring high availability and scalability of your APIs. Developer Integration: An extensive suite of tools empowers developers to efficiently design, test, and deploy APIs. Modernizing with Apigee Adopting Apigee is a strategic step in transitioning from outdated on-premises systems to a dynamic, cloud-based architecture. By harnessing Apigee, you can enhance API management, streamline operations, and leverage advanced monitoring capabilities. Important For those preparing for GCP-related certifications or technical interviews, a deep understanding of Apigee’s capabilities—especially its support for microservices and migration strategies—is essential. For further insights on modernizing your API architecture, visit the Google Cloud Apigee documentation . That concludes this article. Thank you for reading, and stay tuned for our next lesson! Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Databases in GCP,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Database/Databases-in-GCP,"GCP Cloud Digital Leader Certification Database Databases in GCP In this article, we explore why leveraging databases on Google Cloud Platform (GCP) is a smart choice for modern applications, and we introduce the range of database options available. By choosing GCP's fully managed database solutions, you benefit from enhanced features, simplified management, and reduced infrastructure overhead. Why Choose a GCP Database? GCP offers a comprehensive suite of fully managed database solutions designed to meet diverse application needs. With built-in disaster recovery, scalability, and performance enhancements, these databases streamline your operations. For example, if your application relies on an Oracle database traditionally hosted in an in-house data center, migrating to GCP can reduce licensing fees, maintenance costs, and the need for specialized personnel—making it an attractive, cost-effective solution for cloud migration. Tip Migrating to managed cloud databases can significantly reduce your IT overhead while boosting application performance and reliability. Overview of GCP Database Options Cloud SQL Cloud SQL is a fully managed relational database service that supports MySQL, PostgreSQL, and SQL Server. It simplifies database administration by automating tasks such as backups, replication, and patch management, letting you focus on your application rather than maintenance. Cloud Spanner Cloud Spanner is built for high capacity and global scalability, capable of processing up to 2 billion requests per second during peak times. With its enterprise-grade performance and a 99.999% availability SLA, Cloud Spanner is the ideal choice for applications that demand both consistency and scalability without the complexity of manual management. AlloyDB for PostgreSQL AlloyDB offers a fully managed, PostgreSQL-compatible database solution engineered for superior performance and ease of use. This service allows PostgreSQL users to benefit from GCP's managed services while maintaining compatibility with familiar tools and applications. Cloud Bigtable Cloud Bigtable is a highly performant NoSQL database managed by GCP. Ideal for large-scale applications that require low latency and robust performance, it is comparable to other NoSQL solutions like Cassandra and HBase. SQL vs. NoSQL Databases When choosing between SQL and NoSQL databases on GCP, it is essential to understand the key differences. SQL databases like Cloud SQL, AlloyDB, and Cloud Spanner provide strong consistency and structured query capabilities, making them perfect for transactional applications. On the other hand, NoSQL databases like Cloud Bigtable offer flexible schema design and high throughput for large-scale, low-latency workloads. We will delve deeper into this comparison in our next article. Thank you for reading, and stay tuned for more insights in our upcoming lesson. For further reading, consider exploring additional resources like Kubernetes Basics and the Google Cloud Documentation . Happy cloud computing! Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Demo Lab Session,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Usecase/Demo-Lab-Session,"GCP Cloud Digital Leader Certification Usecase Demo Lab Session Welcome to this comprehensive lab article. In this guide, you will learn how to deploy a simple application on Google Cloud Platform (GCP), leveraging Compute Engine, Instance Groups, Instance Templates, and HTTP Load Balancers. This step-by-step tutorial covers creating and managing virtual machines and automating your application deployment. Setting Up the Instance Group Begin by logging into the GCP Console and confirming that you are in the correct project. Follow these steps to set up your instance group: Navigate to Compute Engine by searching for ""Compute"" and selecting Compute Engine . In the Compute Engine section, go to Instance Groups and create a new instance group. Name the instance group: simple instance group Add a description, for example, this is a simple application . You have the option to select an existing template; however, in this lab, you will create a new instance template. Creating an Instance Template When creating the instance template, follow these instructions: Name the template simple instance group (or simple instance for brevity). Add a metadata key-value pair where the key is ENV and the value is development . Keep the machine type as default. Scroll down to the boot script section, click Change , and select Ubuntu . At this moment, the latest version of Ubuntu is automatically selected. Note If you receive an error indicating that the selected compute instance is not compatible with the chosen Ubuntu version, switch to the Intel version (x86_64) and click Select . Understanding Instance Templates and Startup Scripts An instance template is a blueprint for your virtual machines within an instance group. Ensure the following within the template: In the firewall section, enable both the HTTP and HTTPS traffic options. Enabling HTTP is crucial for external accessibility to your web application. Configuring a Startup Script Under Advanced Options , navigate to Management where you will find the startup script section. A startup script automates the process of installing and configuring your application after the VM boots. Consider the following simple Bash startup script: #!/bin/bash
apt update
apt -y install apache2
echo 'Simple application running on $(hostname)' > /var/www/html/index.html Tip Even if the startup script encounters an error, the virtual machine will continue running. You can extend this script to install additional packages or configure other services as necessary. After entering your startup script, click on Save and Continue . Configure the instance group with these settings: Minimum instances: 2 Maximum instances: 5 The default setting starts with two instances. Leave all other settings as default and then click Create . Please note that it may take up to five minutes for the instance group to become fully operational. Verifying the Instance Group and Application After setting up the instance group, verify that everything is functioning as expected by checking the individual instances. Follow these steps: Confirm that the startup script ran successfully by inspecting each instance. Refresh the instances section to see the external IP addresses assigned. Access the web application by clicking on an external IP address. Remember to use the HTTP protocol (not HTTPS). If the page does not load immediately, allow a few minutes for the updates. Configuring the HTTP Load Balancer Next, set up an HTTP Load Balancer to distribute incoming traffic across your instances. In the GCP Console, search for Load Balancer and create a new HTTP load balancer (initially, leave all configurations as default). Name the load balancer simple application load balancer . Configuring the Backend Service For the backend configuration, perform the following actions: Create a backend service and choose the previously created instance group. Set the backend port to 80 . Enable the default settings. Create a health check titled Simple application to verify port 80 using HTTP. This health check ensures traffic is only directed to healthy instances. After configuring the backend, save your settings. You can leave the routing rules as default and then click Create to build the load balancer. After the configuration, the load balancer will deploy and connect to your instance group. Keep in mind that it might take 10–15 minutes for the load balancer's IP address to become fully available and for all health checks to pass. Test the load balancer by entering its IP address in your browser with HTTP on port 80. You might initially see a 404 error, but after refreshing, you should notice different instance names in the response—confirming that the traffic is being distributed across your virtual machines. Cleaning Up Resources Once you have verified that your deployment is working correctly, it is important to clean up your resources to avoid incurring additional charges. The cleanup process should be performed in the following order: Delete the load balancer (this will also remove the associated health check). Navigate to Compute Engine > Instance Groups and then delete the instance group. Warning Ensure that you delete the load balancer before deleting the instance group to avoid dependency errors. Conclusion This lab article demonstrated how to: Create and configure an instance group with a custom instance template. Automate application deployment using a startup script. Set up an HTTP load balancer to distribute traffic among virtual machines. Perform clean-up of resources to reduce unnecessary charges. We hope you found this lab informative and that it helps you build your expertise in cloud computing on GCP. Happy cloud computing! For additional resources and further reading, check out the following: Google Cloud Documentation Compute Engine Overview Introduction to Load Balancing Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Why do we need databases,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Database/Why-do-we-need-databases,"GCP Cloud Digital Leader Certification Database Why do we need databases Hello and welcome to this lesson on the importance of databases, especially within applications hosted on the Google Cloud Platform (GCP) . In today’s digital landscape, databases are not just storage solutions but are critical components in ensuring scalability, reliability, and efficiency for modern applications. Imagine an architecture where an instance group hosts an application behind a load balancer. Consider a pharmaceutical company that requires user authentication for its engineers and researchers. In this scenario, each user provides a username and password that could be stored locally on individual virtual machines. However, this approach introduces several challenges: Data stored on one virtual machine is not automatically shared with others. As the application scales to include more virtual machines or additional instance groups, isolated authentication details can lead to inconsistent behavior. In the event of a failure in one instance group, health checks may reroute traffic to another instance group that lacks the necessary authentication records. Data stored on local volumes may be lost when machines are replaced or scaled. Key Point Centralized data management is critical for applications that require high availability and seamless scaling. To overcome these challenges, adopting a centralized and robust method for storing and retrieving data is essential. Databases address these issues by: Centralized Storage: Consolidates data into a single, reliable location. Scalability: Efficiently manages large volumes of data—potentially terabytes—while supporting concurrent data access. Concurrent Access: Allows multiple users or systems to read and modify data simultaneously. Search and Sort Functionality: Offers built-in capabilities to quickly locate and organize data. Advanced Analytics: Enables powerful data analysis to extract insights that drive informed decision-making. This diagram effectively summarizes why databases are essential for modern applications. In the upcoming lesson, we will explore the various database options available in GCP and examine the different types of databases you can implement. That concludes this lesson. Thank you for joining us, and see you in the next session! Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Usecase for Big Data,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Google-Clouds-solutions-for-machine-learning-and-AI/Usecase-for-Big-Data,"GCP Cloud Digital Leader Certification Google Clouds solutions for machine learning and AI Usecase for Big Data Hello and welcome back! In this lesson, we explore how a pharmaceutical company leverages big data using Google Cloud Platform (GCP) to optimize its shipment and delivery processes. This real-time data processing workflow enhances logistics efficiency and improves customer experience. Business Process Overview In our scenario, the pharmaceutical company distributes various drugs, prescription tablets, and health-related products. The shipment process from the warehouse to pharmacies includes several critical steps: A notification is sent to the pharmacy when a shipment is initiated. Pharmacies can track the order’s location during transit. A confirmation notification is sent upon successful delivery. This end-to-end process demonstrates a real-time application where continuous monitoring and prompt notifications are essential for operational excellence. By capturing data across the entire shipment journey, the company collects a vast volume of diverse information. This data is then transformed into actionable insights that help refine shipment strategies and boost overall efficiency. Integrating Google Cloud Services GCP’s suite of services plays a central role in addressing the big data challenges in this scenario. Below is an overview of each service and its role in the data pipeline: Data Ingestion with Pub/Sub Immediately after a product leaves the warehouse, data is ingested via Internet of Things (IoT) devices into Cloud Pub/Sub . This service streams real-time events by publishing shipment data to a dedicated topic, ensuring that every event is captured as it occurs. Data Storage with Cloud Storage Once ingested, the data is securely stored in Cloud Storage . This cost-effective object storage solution holds vast amounts of shipment data, serving as a reliable repository for subsequent analysis and processing. Data Processing with Dataproc With large data volumes at play, processing is essential. Cloud Dataproc – a fully managed service supporting over 30 open-source tools (including Apache Spark and Apache Flink ) – cleans and transforms the raw data. This modern ETL process is both scalable and cost-efficient under a pay-as-you-go pricing model. Analytics with BigQuery For in-depth analysis, the processed data is analyzed using BigQuery . Whether queried directly from BigQuery or integrated from Cloud Storage, this powerful data warehouse allows for rapid and efficient exploration of large datasets, supporting data-driven decision-making. AI and Machine Learning with Vertex AI The final step involves leveraging artificial intelligence to derive predictive insights. Vertex AI offers an end-to-end platform for developing, training, testing, and deploying machine learning models. It supports multiple frameworks such as TensorFlow and scikit-learn , making it a versatile tool for data science engineers. Summary of the Data Pipeline Data Ingestion: Real-time events are streamed via Pub/Sub as soon as the shipment process starts. Data Storage: Cloud Storage holds the extensive shipment data securely. Data Processing: Dataproc processes and cleans the raw data for better analysis. Data Analysis: BigQuery allows for quick querying and deep insights. AI/ML Deployment: Vertex AI facilitates the development and deployment of machine learning models. Key Takeaway Google Cloud Platform services collectively establish a robust and scalable data pipeline that effectively addresses the big data challenges encountered during the pharmaceutical shipment process. This seamless integration not only supports real-time monitoring but also drives strategic decision-making through advanced analytics and AI. In summary, GCP provides a comprehensive suite of services that enable real-time data ingestion, efficient storage, streamlined processing, and advanced analytics. This integrated solution optimizes shipment tracking and lays the groundwork for deploying AI and machine learning models to further enhance operational decision-making. That concludes our lesson on leveraging GCP for big data applications in the pharmaceutical industry. Thank you for reading, and we look forward to exploring more topics in the next lesson. Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Demo PubSub,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Google-Clouds-solutions-for-machine-learning-and-AI/Demo-PubSub,"GCP Cloud Digital Leader Certification Google Clouds solutions for machine learning and AI Demo PubSub Welcome to this comprehensive lesson on integrating and managing Pub/Sub within Google Cloud Platform (GCP). In this guide, you will learn how to create a Pub/Sub topic, publish JSON messages to it, and understand the subscription process. We will also explore how to connect Pub/Sub with services like BigQuery and Cloud Storage for real-time analytics and batch data processing. Accessing the Pub/Sub Interface After logging into your GCP console and confirming you are in the desired project, follow these steps to access the Pub/Sub interface: Enter ""Pub/Sub"" in the search bar and select it. If prompted to enable the API, proceed with the activation (this demo assumes the API is already enabled). On the left-hand side of the Pub/Sub console, you will see several options: Topics: Temporary storage or streams where incoming data is held. Subscriptions: Connect to a topic to read the data and allow for further processing by other applications. Snapshots: Create backups of your topic. Schemas: (Not covered in this lesson) Define the structure for your messages. Creating a Pub/Sub Topic Follow these steps to create a Pub/Sub topic that will serve as your data pipeline: Click on Create Topic . Provide a unique topic name (e.g., ""data-ingestion""). Ensure the option ""Add a default subscription"" is selected. Once you click Create Topic , the topic along with its default subscription is established. This unique topic name becomes the ingestion endpoint for your applications that will push data. Exploring Topic Details and Subscriptions After creating the topic, click on it to view detailed settings and options: The topic details page provides insights on sending data to BigQuery or Cloud Storage. The default subscription is clearly visible and is configured to pull data from the topic. Next, switch to the Subscriptions section. Here, you can find options like ""Messages"" accompanied by a ""pull"" button to retrieve the messages from your topic. Publishing Messages to Your Topic To demonstrate publishing, follow these steps to push messages and validate the subscription retrieval: Return to your topic and click on Messages . Initiate message publishing by selecting the appropriate subscription. Enter your message using JSON format. For example, your initial message can be: {
  ""id"": 101
} Enhance the message with additional details: {
  ""shipment_id"": 101,
  ""dest"": ""sri_lanka""
} After entering the desired message, click Publish to push it into the topic. Then, navigate back to the Subscriptions section, select your subscription, and click Pull to retrieve the message from the topic. The message details will then be displayed, confirming that the subscription successfully processed the published content. Understanding Data Ingestion and Retention Options Pub/Sub is designed for real-time data streaming and offers flexible data ingestion capabilities: Data can be exported directly to BigQuery for instant analytics. If the data requires further processing (e.g., cleansing or computation), it can first be transferred to Cloud Storage, then loaded into BigQuery. By default, a topic retains data for seven days. You can extend the retention period up to a maximum of 31 days by adjusting the topic settings. Important Remember: A Pub/Sub topic is optimized for streaming real-time data rather than serving as a permanent data repository. Ensure that data is either consumed by a subscription or exported to long-term storage. Configuring and Editing Subscriptions To optimize how your data is processed, you can modify the configuration settings for your subscriptions. To do this: Navigate to the Subscriptions section. Click on Edit for the subscription you want to modify. Within the edit view, you can configure various options such as: Directly streaming data export to BigQuery (ensure that your BigQuery datasets and tables are set up in advance). Adjusting message retention settings. Changing delivery options for real-time analytics. These configurations facilitate near real-time data analytics by enabling a seamless data flow into BigQuery or further processing via Cloud Storage. Cleaning Up Resources Before proceeding to further lessons—where integration with Cloud Storage and BigQuery is explored—it's essential to clean up the resources created during this demo. To do so: Navigate back to the topic. Delete the subscription by clicking on the Delete button. Finally, delete the topic. Note that once a topic is deleted, its data is irretrievable. If the data is critical, consider enabling a snapshot before deletion. Warning Deleting topics and subscriptions is irreversible. Make sure to export or backup necessary data before performing deletion. Conclusion In this lesson, you have learned how to effectively create and manage Pub/Sub topics and subscriptions in GCP, publish JSON-based messages, and configure data export options to BigQuery and Cloud Storage. As you continue your journey, the next lesson will delve deeper into leveraging BigQuery for advanced data analysis. Thank you for following along and happy streaming! Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Demo BigQuery,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Google-Clouds-solutions-for-machine-learning-and-AI/Demo-BigQuery,"GCP Cloud Digital Leader Certification Google Clouds solutions for machine learning and AI Demo BigQuery Welcome to this lesson on exploring BigQuery using the Google Cloud Platform (GCP) Console. In this guide, you will learn how to navigate to BigQuery in GCP, create datasets and tables, upload data, and run SQL queries. Follow the step-by-step instructions below to get started. Accessing BigQuery in GCP Log in to your GCP Console , select the appropriate project, and use the search bar to find ""BigQuery"". You will see options under Data Warehouse and Analytics . Click on BigQuery . If prompted to activate the BigQuery API, please do so. Once activated, expand your project on the left-hand side to view its details. At this point, you may notice that no datasets exist in your project. Creating a Dataset Before you can store tables, you need to create a dataset within your project. Click on Create dataset . In the dataset creation form, enter a dataset name (for example, ""sample_data""). Select the location where your data will be stored. Choosing the correct location is critical to meeting your company's data protection regulations and aligning with your organization's geographical preferences. Note If you encounter an error related to naming conventions, consider modifying the dataset name (for example, using an underscore like ""sample_data"") and try again. Once you create the dataset, it will serve as a container for your tables. Creating a Table from a CSV File Now that you have your dataset, you can create a table to store your data. Within the dataset view, click on Create table . You will be presented with several table creation options: Create an Empty Table Create a Table from Google Cloud Storage Upload Data (choose this option for local file uploads) Click on Upload . Click on Browse to select a local CSV file that contains your sample data. BigQuery will automatically detect the file format. You can also upload other supported formats such as JSON, Avro, or Parquet. Choose your dataset (in this example, ""sample_data"") and specify a table name (for example, ""user_data""). Enable schema auto-detection so that BigQuery determines the table fields based on your CSV file. Finally, click on Create Table to finish the process. Your CSV data will be uploaded and the table will be created. Once the table is created, it will appear under your dataset. Expanding the table reveals the schema detected from your CSV file. Clicking on Details provides additional information such as the number of rows and the data size, while the Preview tab gives a glimpse of the table's entries. Running SQL Queries To analyze the table data, you can run SQL queries using the query editor: Click on the Query option to open a new query tab. Write your SQL query. For example, if you want to fetch 10 records from the table, use the following query: SELECT *
FROM `kodekloud-gcp-training.sample_data.user_data`
LIMIT 10; Query Cost Consideration Before executing the query, review the query plan to check that only 70 KB of data will be processed. Remember, BigQuery charges are calculated based on the volume of data processed, so it's important to optimize your queries for cost efficiency. Click Run to execute your query. The results will be displayed at the bottom of the screen. BigQuery also allows you to download query results in various formats. Additionally, review execution details such as query duration, read time, and compute time to further optimize query performance. Additional Features On the left side of the BigQuery interface, you will find the BI Engine option in the administrator tools. BI Engine caches frequently accessed query results, reducing the need to recompute them for subsequent queries. This feature can significantly lower query costs and improve performance for popular tables. Return to the SQL workspace, where you can: Review your query history (both personal and project-wide) Save or share queries for future use Schedule queries to run periodically BigQuery thus provides a robust, scalable analytics engine within GCP that supports both ad-hoc querying and routine data analysis. Conclusion This lesson covered how to: Access BigQuery through the GCP Console Create a dataset and upload a CSV file as a table Configure schema auto-detection Execute SQL queries and leverage BigQuery's cost management features Thank you for following along, and we look forward to seeing you in the next lesson. Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Container orchestration in GCP,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Container-orchestration-in-GCP/Container-orchestration-in-GCP,"GCP Cloud Digital Leader Certification Container orchestration in GCP Container orchestration in GCP Welcome to this lesson on containerization and orchestration in Google Cloud Platform (GCP). In this session, you'll learn the importance of containerization, understand the limitations of traditional monorepos, and explore how virtualization empowers containers to run uniformly across different machines. We'll also provide an overview of the GCP services that orchestrate containerized applications. Why Containerization? Imagine a scenario where a pharmaceutical company hosts its website (www.pharmaceutical.com) with various teams responsible for different sections. For example, the product team updates product details, the shipping team manages delivery information, and the sales team monitors performance. When one team (say, the product team) introduces a change that inadvertently breaks the website, the entire system can be adversely affected. This issue is common in monorepos, where all code shares the same repository and infrastructure, making scalability a challenge. Standardizing Environments and Deployment Deploying software across varied client environments can be problematic. Consider mobile apps, which are designed for specific operating systems like iOS or Android. The consistency of these operating systems ensures that an app will run reliably on any device using the same platform. Containerization extends this assurance to software development by packaging applications with all necessary dependencies into containers. This encapsulation guarantees that the code runs consistently regardless of the deployment environment, allowing developers to concentrate on enhancing application features rather than managing diverse system configurations. Virtualization with Docker Virtualization is the key technology that enables containerization. Docker, one of the leading tools in this space, creates containers (or images) that encapsulate an application along with all its dependencies. When an application is containerized, its image can be shared and executed on any system equipped with the necessary virtualization software. This process mirrors the way mobile operating systems, such as iOS or Android, standardize software behavior across devices, ensuring that containerized applications are both reliable and portable. Orchestrating Containers in GCP With a solid understanding of containerization and virtualization, we now shift our focus to container orchestration in GCP. GCP offers robust services designed specifically to deploy, manage, and scale containerized applications efficiently in the cloud. These services simplify complex deployment scenarios, enabling seamless management of container clusters. Explore GCP Orchestration Tools For further details on the orchestration services available in GCP, be sure to check out the Google Kubernetes Engine (GKE) documentation. Below is a table summarizing some key GCP container orchestration services: Service Type Description Example Usage Google Kubernetes Engine Managed Kubernetes clusters for container orchestration Deploy and scale containerized applications Cloud Run Serverless platform for running containers Run stateless containers without managing servers Compute Engine Scalable virtual machines that host containers Run custom container orchestration setups By leveraging these GCP services, organizations can streamline their deployment pipelines, enhance resource management, and ensure high availability of their containerized applications. Continue to explore how GCP’s container orchestration capabilities can transform your development and deployment strategies. Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Demo Cloud Run,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Container-orchestration-in-GCP/Demo-Cloud-Run,"GCP Cloud Digital Leader Certification Container orchestration in GCP Demo Cloud Run Welcome to this comprehensive lesson on deploying container images using Cloud Run on Google Cloud Platform (GCP). In this tutorial, you'll learn how to deploy a container image, configure autoscaling, monitor your service, and explore Kubernetes Engine (GKE) options for container orchestration. After logging into your GCP Console, start by verifying that you are working in the correct project. Once confirmed, search for ""Cloud Run."" Since no services are listed, click on ""Create Service."" If prompted to activate the Cloud Run API, proceed with the activation. On the service creation page, provide the container image you wish to deploy. A demo container image is available by default for all projects. Next, assign a service name and choose your desired region (the default option is used in this example). Configure the autoscaling settings by deciding whether to maintain a minimum of one container at all times or allow scaling down to zero when there is no traffic. You can also set a maximum limit; in this demonstration, the maximum is set to 200 containers—but feel free to adjust this as needed. For this demo, all traffic is allowed, and ""Allow unauthenticated invocation"" is selected. With all other settings left as default, click ""Create."" The deployment process is impressively fast—your application is up and running in less than ten minutes. Return to the Cloud Run dashboard to review the deployed service, which displays details such as the number of incoming requests (currently zero), the region, and the deployer's identity. Click on your service to access its monitoring dashboard. From the monitoring dashboard, you can track key performance metrics and view detailed information about your container. To access the live application, simply copy the provided URL and paste it into your web browser. This will load the demo website generated directly from the deployed Docker image. Note The Docker image you deployed contains the complete code for your web application, ensuring consistency across deployments. If you ever need to remove the application, you can do so easily from the Cloud Run dashboard by selecting the service and choosing the delete option. For those interested in container orchestration beyond Cloud Run, consider exploring Google Kubernetes Engine (GKE) . Unlike Cloud Run, GKE is not a fully serverless solution but provides robust container orchestration using Kubernetes. To get started with GKE, search for ""Kubernetes Engine"" in your GCP Console. If required, activate the GKE API—which may take up to five minutes before it becomes active. Once the API is enabled, click on ""Create a cluster."" You will be presented with two primary options: Autopilot and Standard. Autopilot mode manages most configuration settings for you, requiring only basic details like cluster name and network. Standard mode offers extensive customization options for more advanced deployment configurations. Note If you're new to GKE or GCP, Autopilot mode is recommended as it simplifies configuration and maintenance, allowing you to focus on developing your application. When preparing for your GCP certification, keep in mind that the two primary container orchestration services to master are Cloud Run and GKE. This concludes our lesson on deploying an application using Cloud Run and provides an overview of GKE cluster options. Thank you for following along, and see you in the next lesson! Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Cloud Run,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Container-orchestration-in-GCP/Cloud-Run,"GCP Cloud Digital Leader Certification Container orchestration in GCP Cloud Run Cloud Run provides an efficient alternative for testing and running containerized applications without the overhead and expense of setting up a full Google Kubernetes Engine (GKE) cluster. This serverless service allows you to deploy applications quickly, test their performance, and pay only for the resources you use—making it ideal for development, testing, and scaling workloads. Benefits of Cloud Run include: Quick deployment of containerized applications built using various programming languages. A pay-as-you-go billing model where charges apply only during active request handling. Elimination of infrastructure management, as the underlying resources are fully abstracted. Easy integration with other Google Cloud services such as load balancing and logging. Note Cloud Run is especially useful for evaluating application performance without incurring the costs of running idle resources, which is often a risk with GKE clusters. How Does Cloud Run Work? Cloud Run functions as a scalable, serverless platform that dynamically provisions container instances in response to incoming requests. While its user interface may resemble that of GKE, its internal architecture simplifies operations by automatically handling scalability and resource allocation. When a request arrives, Cloud Run promptly spins up the required container instances. For instance, if you set a container's concurrency parameter to 1, each container processes one request at a time. Conversely, increasing the concurrency value to 80 allows a single container to process up to 80 simultaneous requests. If the volume of incoming requests exceeds the established limit per container, additional instances are automatically created to manage the load. This scalable approach ensures that your applications can efficiently respond to traffic spikes without manual intervention. It also fits perfectly into a microservices architecture, where individual containerized services can be deployed and managed independently. To better illustrate how Cloud Run manages request handling and scalability, consider this diagram that contrasts different concurrency settings: Warning Before migrating to Cloud Run for production workloads, ensure that the serverless model fits your application's performance requirements, as the abstraction could limit certain advanced configuration options available in GKE. In summary, Cloud Run offers a streamlined path to deploy container images, evaluate application performance, and handle dynamic scaling—all while minimizing ongoing resource costs. This makes it an excellent choice for developers seeking a flexible, cost-effective deployment platform on Google Cloud Platform. Thank you for reading. Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Shared responsibility model,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Security-in-GCP/Shared-responsibility-model,"GCP Cloud Digital Leader Certification Security in GCP Shared responsibility model Hello and welcome back! In our previous discussion, we explored various GCP services that enable robust security best practices. Today, we'll dive deeper into a key security principle known as the Shared Responsibility Model. What Is the Shared Responsibility Model? Imagine a circle representing your entire cloud infrastructure, encompassing all GCP services. In this model, two primary roles emerge: Security of the Cloud: Managed entirely by Google Cloud Platform (GCP). Security Inside the Cloud: Managed by you, the cloud user or organization. GCP's Responsibilities GCP takes care of the foundational elements, which include: Physical Data Center Security: GCP secures their data centers against unauthorized access. Global Networks: Management of internet connectivity, network configuration, and cybersecurity for data centers. System Maintenance: Regular system upgrades, patches, and licensing. Compliance and Regulation: Ensuring operating system compliance with regional regulations and addressing taxation matters. Note GCP's comprehensive management of these aspects allows organizations to focus on securing their own data and applications. Your Responsibilities As an organization leveraging GCP’s infrastructure, you are responsible for the security of the data and applications you deploy in the cloud. This includes: Data Collection: Ensuring you only collect necessary data from your users. Compliance Adherence: Maintaining compliance with regional data sovereignty laws and relevant application standards. Configuration Best Practices: Following security best practices when configuring your applications to prevent misconfigurations that could expose resources. Proactive Security Measures: Implementing proactive security measures and addressing any potential threats promptly. Visual Representation of the Model Any exam or certification referring to the ""security of the cloud"" specifically points to the areas managed by GCP. For a clear visual breakdown, review the diagram below: Conversely, actions that you take as a GCP user relate to ""security inside the cloud."" This distinction reinforces the notion that while GCP ensures a secure infrastructure, the safety of your data and applications is your responsibility. Security Reminder Always review your application configurations and security policies regularly to ensure that any vulnerabilities are addressed swiftly. Conclusion Understanding the Shared Responsibility Model is critical to effectively managing your cloud infrastructure’s security. By clearly distinguishing between ""security of the cloud"" (handled by GCP) and ""security inside the cloud"" (your responsibility), you can focus your efforts and resources appropriately. Stay tuned for more insights in our upcoming articles. Thank you for reading! Watch Video Watch video content"
GCP Cloud Digital Leader Certification,GCP Security Privacy and Cloud Compliance,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Security-in-GCP/GCP-Security-Privacy-and-Cloud-Compliance,"GCP Cloud Digital Leader Certification Security in GCP GCP Security Privacy and Cloud Compliance Welcome to our in-depth look at security in the cloud. In this lesson, we explore how to maintain robust security measures while leveraging the flexibility and scalability of Google Cloud Platform (GCP). When managing on-premises systems, you have full control over the security measures around your data centers. However, this level of control often comes at a high cost, as substantial resources must be invested in ensuring best security practices. A common question arises: How can we meet compliance requirements and uphold these best practices when migrating to the cloud? This lesson addresses that very concern. Key Components of Cloud Security Cloud security is built on several critical pillars. Below, we detail the essential components that ensure comprehensive security, privacy, and compliance in the GCP environment: Threat Detection and Investigation Rapid detection of malicious activities is critical. Leveraging the right tools and services allows you to identify threats early, conduct in-depth investigations, and respond promptly to minimize damage. Tip Implementing automated alerts and continuous monitoring can significantly reduce response times during security incidents. Application Protection Against Fraud and Web Attacks Public-facing applications are prone to various fraud attempts and web attacks. It is important to deploy robust defenses to safeguard your core business applications from these threats. Security Alert Be sure to keep your applications updated and to regularly test their defenses against common attack vectors. Digital/Data Sovereignty Unlike on-premises data centers, which are often confined to a single geographical location, the cloud allows data to be distributed across multiple regions. It is critical to ensure that data storage and processing comply with local data sovereignty regulations, regardless of where the data resides. This is crucial for meeting both legal standards and your application-specific requirements. Secure Access for Engineers and Developers Providing controlled and secure access to cloud resources is fundamental. Proper access management ensures that only authorized personnel can interact with sensitive systems and data, reducing the risk of insider threats and security breaches. These components collectively form the backbone of security in the GCP cloud, ensuring that you can achieve high standards of privacy and compliance. What's Next? Are you curious about which GCP services best support these security pillars? In the next lesson, we will dive deeper into specific GCP offerings designed to strengthen your cloud security posture. Feel free to review these concepts and revisit this article as needed. Join us in the upcoming lesson for a more detailed exploration of GCP's security services and how they can help you maintain a secure and compliant cloud environment. For additional insights and resources on cloud security, visit the GCP Security Documentation . Watch Video Watch video content"
GCP Cloud Digital Leader Certification,GCP services for securing our cloud setup,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Security-in-GCP/GCP-services-for-securing-our-cloud-setup,"GCP Cloud Digital Leader Certification Security in GCP GCP services for securing our cloud setup Hello, and welcome back. In our previous lesson, we discussed the importance of adopting security best practices in GCP. In this lesson, we will explore various GCP services designed to help secure your cloud setup and protect your valuable data and applications. Data Replication When your application—or even your entire company—is operating in a specific region (for example, London in Europe), it is crucial to consider the impact of localized issues. For instance, if a network failure or another unexpected event occurs in that region, your data (such as database entries and customer information) must be quickly replicated to another region to ensure continuity. Data replication configurations depend on your database setup and data storage architecture. In our previous demonstration, we illustrated how to configure disaster recovery and data replication across multiple regions. Reminder Replicating data across regions can increase storage costs because you are maintaining duplicate copies. Always evaluate the cost implications when designing your replication strategy. Single Sign-On (SSO) Adaptation GCP offers seamless integration with Single Sign-On (SSO) services, simplifying access to various applications without the need for separate accounts. For example, users can leverage a single Gmail account to access multiple services. This centralized authentication process not only streamlines user management but also enhances security by consolidating access control. Implementing SSO in GCP, especially when combined with multi-factor authentication, is essential for ensuring that both engineers and end users access services securely. This functionality is primarily managed through Identity and Access Management (IAM). Identity and Access Management (IAM) IAM is a fundamental service in GCP that facilitates precise control over user permissions. By configuring IAM, administrators can define what actions each user is permitted to perform, ensuring that all operations within your GCP environment are carried out by authorized personnel only. Proper IAM configuration is critical to maintaining a secure cloud infrastructure. Cloud Armor Cloud Armor acts as a robust shield for your deployed systems, protecting them from various external threats and attacks. Typically integrated with load balancing policies, Cloud Armor authenticates incoming traffic and blocks malicious activities—such as unauthorized URL access. This makes it a vital component in defending your applications against external cyber threats. Threat Detection Tools GCP features comprehensive threat detection tools that continuously monitor your environment for potential misconfigurations or security breaches. With multiple users and engineers making changes, accidental misconfigurations of security policies or load balancer settings can occur. GCP’s threat detection services are designed to alert you immediately so corrective action can be taken to mitigate risks. Shared Responsibility Model It is important to note that none of the aforementioned services are enabled by default. This is where the shared responsibility model comes into play. While GCP provides the necessary tools and services for securing your environment, it is your responsibility to configure these services appropriately to ensure robust security. Next Steps In our next lesson, we will delve deeper into the shared responsibility model and explore its implications for your cloud setup. Thank you for joining us, and stay tuned for more insights on securing your cloud environment with GCP. Watch Video Watch video content"
GCP Cloud Digital Leader Certification,Google Kubernetes Engine GKE,https://notes.kodekloud.com/docs/GCP-Cloud-Digital-Leader-Certification/Container-orchestration-in-GCP/Google-Kubernetes-Engine-GKE,"GCP Cloud Digital Leader Certification Container orchestration in GCP Google Kubernetes Engine GKE Hello and welcome back! In our previous lesson, we explored the rising popularity of containerization and its adoption by organizations worldwide. Today, we'll dive into Google Kubernetes Engine (GKE) and understand how it streamlines containerized application deployment and management. Google Cloud Platform (GCP) offers GKE, a managed service built on Kubernetes—an open-source container orchestration system. Kubernetes automates the deployment, scaling, and management of containerized applications. While Kubernetes gives you the core functionality to deploy scalable services, GKE enhances it with a fully managed environment. Notably, Google is one of the original contributors to the Kubernetes project, ensuring tight integration and continuous innovation. GKE Modes of Operation GKE supports two primary modes: Autopilot Mode : Ideal for developers who want to focus solely on deploying applications. Google manages underlying tasks like scaling, updates, patches, and bug fixes. Standard Mode : Offers full control over the Kubernetes cluster for those who wish to customize configurations to meet specific requirements. For straightforward applications, Autopilot mode often proves more efficient. Its seamless integration with other GCP services makes GKE a favorite among developers, especially when migrating on-premises systems to the cloud. GKE Architecture Overview Understanding the architecture of GKE is essential for leveraging its full potential. Within a GKE cluster, the smallest deployable unit is called a pod. A pod contains one or more containers that execute your application processes. When configured in Autopilot mode, GKE automatically handles key cluster management tasks—such as scaling, updates, minor upgrades, patches, and bug fixes. This automation ensures that as your application scales, the system seamlessly adapts by managing additional pods and containers without manual intervention. Key Takeaways GKE is a managed Kubernetes service on Google Cloud Platform that simplifies containerized application deployment. Autopilot mode automates cluster management, making it ideal for rapid deployment with minimal operational overhead. Standard mode provides users with granular control over their Kubernetes clusters for custom configurations. GKE's tight integration with other GCP services and Kubernetes' inherent scalability make it a top choice for modern cloud-native applications. Summary In summary, Google Kubernetes Engine provides a robust, automated platform for container orchestration. Whether using Autopilot for ease of management or Standard mode for granular control, GKE leverages Kubernetes to meet diverse deployment needs. That concludes this lesson on Google Kubernetes Engine. Thank you for reading, and see you in the next article! Watch Video Watch video content"
GitOps with ArgoCD,What is GitOps,https://notes.kodekloud.com/docs/GitOps-with-ArgoCD/Introduction/What-is-GitOps,"GitOps with ArgoCD Introduction What is GitOps GitOps is an operational framework that leverages Git as the single source of truth for managing both infrastructure and application code. It extends the principles of Infrastructure as Code, enabling automated deployments and rollbacks by controlling the entire code delivery pipeline through Git version control. GitOps Workflow Developers begin by committing their changes to a centralized Git repository. Typically, they work in feature branches created as copies of the main codebase. These branches allow teams to develop new features in isolation until they are deemed ready. A Continuous Integration (CI) service automatically builds the application and runs unit tests on the new code. Once tests pass, the changes undergo a review and approval process by relevant team members before being merged into the central repository. The final step in the pipeline is Continuous Deployment (CD), where changes from the repository are automatically released to Kubernetes clusters. At the heart of GitOps is the concept of a declaratively defined state. This involves maintaining your infrastructure, application configurations, and related components within one or more Git repositories. An automated process continuously verifies that the state stored in Git matches the actual state in the production environment. This synchronization is managed by a GitOps operator running within a Kubernetes cluster. The operator monitors the repository for updates and applies the desired changes to the cluster—or even to other clusters as needed. When a developer merges new code into the application repository, a series of automated steps is triggered: unit tests are run, the application is built, a Docker image is created and pushed to a container registry, and finally, the Kubernetes manifests in another Git repository are updated. The GitOps operator continuously compares the desired state (as defined in Git) with the actual state in the Kubernetes cluster. If discrepancies are found, the operator pulls the necessary changes to ensure that the production environment remains aligned with the desired configuration. Ease of Rollbacks One of the key benefits of GitOps is the seamless rollback process. Since the entire configuration is maintained in Git, reverting to a previous state is as simple as executing a git revert command. The GitOps operator detects this change and automatically rolls back the production environment to match the desired state. Watch Video Watch video content"
GitOps with ArgoCD,Push vs,https://notes.kodekloud.com/docs/GitOps-with-ArgoCD/GitOps-Introduction/Push-vs,"GitOps with ArgoCD GitOps Introduction Push vs In this article, we explore the differences between push-based and pull-based deployment strategies for Kubernetes clusters. We will examine their benefits, challenges, and use cases, helping you determine the best approach for your environment. Push-Based Deployment Push-based deployment is commonly used in CI/CD pipelines. With this approach, the application code goes through various stages within the CI pipeline before updates are pushed directly to the Kubernetes cluster. Key Characteristics The CI system requires read-write access to the Kubernetes cluster, which means Kubernetes credentials are stored in the CI system outside the cluster. This arrangement may introduce potential security risks. Typically, the CI system has read-only access to the Git repository and read-write access to the container registry, while the Kubernetes cluster itself only has read-only access to the registry. Deployments can leverage a variety of plugins and tools. For instance, Jenkins can use multiple plugins or approaches, and Helm plugins further simplify the deployment of Helm charts. Security Consideration Storing Kubernetes credentials in the CI system exposes a potential security risk, as these credentials grant read-write access to the cluster. Challenges The deployment configuration is tightly coupled with the CI system. Migrating from one CI platform to another (for example, switching from Jenkins to a different platform) often requires reworking many deployment configurations. Embedding cluster credentials in the CI system increases the risk of unauthorized access if the CI system is compromised. Pull-Based Deployment Pull-based deployment, frequently associated with GitOps, employs an operator running within the Kubernetes cluster. This operator monitors for changes—either in a container registry for new images or in a Git repository for updated manifests—and then autonomously deploys those changes. Key Characteristics The CI/CD system only needs read-write access to the container registry, without requiring direct access to the Kubernetes cluster. Deployments are executed internally from within the cluster, enhancing security by minimizing external access. GitOps operators are particularly supportive of multi-tenant environments, allowing teams to manage multiple repositories and namespaces. For example, different teams can maintain separate Git repositories and corresponding namespaces for their deployments. Secrets can be securely managed by encrypting them using tools like HashiCorp Vault or Bitnami Sealed Secrets. These encrypted secrets are stored in Git or decrypted during the deployment process. GitOps operators can monitor container registries for newer image versions and automatically trigger deployments of the latest images. Secret Management While GitOps encourages declarative management—including secrets—in Git, the process often requires additional tools and steps (e.g., encryption and decryption) to ensure security, especially with Helm chart deployments. Challenges Managing secrets and configurations can be more complex compared to the push-based model. Although GitOps principles promote a declarative approach, handling encrypted credentials adds an extra layer of complexity. Visual Comparison Summary Deployment Strategy Pros Cons Push-Based - Direct integration with CI/CD pipelines<br>- Flexible deployment configurations using various tools and plugins - Requires CI system to have cluster credentials<br>- Tightly coupled to specific CI systems, making migrations challenging Pull-Based (GitOps) - Enhanced security by limiting external access<br>- Supports multi-tenant environments and automated image updates - More complex secret management<br>- Additional tools required for encrypting and decrypting configurations In summary, push-based deployment strategies simplify certain aspects of automation but may lead to inflexibility and potential security issues. In contrast, pull-based (GitOps) deployments enhance internal management and security at the cost of added complexity in handling secrets and configuration management. Explore more about these methodologies in the Kubernetes Documentation and learn how GitOps can revolutionize your deployment pipeline. Watch Video Watch video content"
GitOps with ArgoCD,GitOps Principles,https://notes.kodekloud.com/docs/GitOps-with-ArgoCD/Introduction/GitOps-Principles,"GitOps with ArgoCD Introduction GitOps Principles In this lesson, we will explore the core principles of GitOps—an approach to continuous deployment that leverages Git as the single source of truth for infrastructure and application state. The GitOps methodology is built upon four foundational principles. Remember GitOps ensures system consistency and reduces human error by enforcing a declarative model of infrastructure management. 1. Declarative vs. Imperative Approach The first principle stresses a declarative methodology over an imperative one. In the declarative model, the entire system—including both infrastructure and application manifests—is described in a desired state. This contrasts with the imperative approach, where specific commands are executed sequentially to change the system state. Relying on the imperative style can complicate reconciliation since it does not maintain a comprehensive record of the system's intended state. 2. Storing the Desired State in Git The second principle mandates that all declarative files, which represent the desired state of the system, be stored in a Git repository. Git not only offers powerful version control capabilities but also preserves immutability. Storing the desired state in Git makes it the definitive source of truth for system configuration. Any changes pushed to Git are automatically recognized and applied across the system. 3. Automated Application of the Desired State via GitOps Operators The third principle involves using GitOps operators—software agents that continuously monitor Git for updates. Once they detect changes, these operators automatically retrieve the desired state from the repository and apply it across one or more clusters or environments. Consider the following deployment manifest example that a GitOps operator might manage: $ cat deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx This operator can run in a single cluster and propagate configuration changes to other clusters as necessary, ensuring uniformity and scalability. 4. Reconciliation and Self-Healing The final principle centers on continuous reconciliation. GitOps operators maintain a self-healing system by constantly checking for discrepancies between the actual state of the system and the desired state stored in Git. They execute this process through three key steps: Observe: Monitor the Git repository for updates. Diff: Compare the desired state from Git with the current state of the cluster. Act: Reconcile any differences by updating the system to reflect the declared desired state. This ongoing reconciliation loop minimizes the risk of configuration drift and helps maintain a robust, error-resistant system. By understanding and applying these GitOps principles, you can ensure your infrastructure remains consistent, scalable, and resilient to changes. Thank you. Watch Video Watch video content"
GitOps with ArgoCD,GitOps Benefits Drawbacks,https://notes.kodekloud.com/docs/GitOps-with-ArgoCD/GitOps-Introduction/GitOps-Benefits-Drawbacks,"GitOps with ArgoCD GitOps Introduction GitOps Benefits Drawbacks This article reviews the key advantages and challenges associated with GitOps, providing insights for managing Kubernetes application deployments effectively. Benefits of GitOps GitOps offers several compelling advantages: It is lightweight and vendor-neutral, leveraging the open-source Git protocol to work seamlessly across diverse platforms. GitOps enables faster and safer deployments by ensuring immutable and reproducible environments. In team setups where environmental changes might occur unexpectedly, GitOps prevents unintended modifications. The GitOps operator enforces consistency by disallowing manual updates, thus eliminating configuration drift. In the event of a manual update, the GitOps operator automatically restores the desired state from Git. Developers enjoy the familiarity of using Git and CI/CD tools. The workflow remains straightforward: push the code to the repository, and a CI/CD pipeline handles testing and deployment. Git’s history tracking allows for easy comparison between declarative file revisions, making it simple to correlate changes with specific change requests. Note For more details on CI/CD integrations with GitOps, refer to the official GitOps Documentation . Challenges of GitOps Despite its advantages, GitOps introduces a few challenges that need to be addressed: Centralized Secret Management: GitOps does not secure secrets by default. Although it recommends storing secrets declaratively in Git repositories, operations teams must integrate additional tools to manage secrets securely. Repository Organization: As the number of microservices and environments grows, organizing Git repositories becomes complex. Decisions need to be made about whether to store source code and manifests in a single repository or use multiple repositories/branches. There is no one-size-fits-all solution—each organization must tailor this approach to fit its specific application requirements. Update Conflicts: Frequent application updates in continuous delivery environments can trigger simultaneous CI processes, leading to multiple pull requests. This may result in conflicts when several processes attempt to update the GitOps repository concurrently, often necessitating manual resolution. Governance and Policy Enforcement: Relying on pull requests (PRs) for approval can reduce the effectiveness of enforcing strict company policies after a PR is approved. Configuration Validation: Malformed YAML files or configuration errors can occur. External validation tools are essential for ensuring that manifest files meet the required standards. Warning Ensure that you integrate robust secret management and repository organization strategies when implementing GitOps to mitigate these challenges effectively. Watch Video Watch video content"
GitOps with ArgoCD,GitOps Projects Tools,https://notes.kodekloud.com/docs/GitOps-with-ArgoCD/GitOps-Introduction/GitOps-Projects-Tools,"GitOps with ArgoCD GitOps Introduction GitOps Projects Tools In this article, we explore a diverse range of GitOps projects and tools available as of this recording. These solutions have been designed to streamline the management of Kubernetes applications by leveraging GitOps practices through various controllers and automation tools. Overview This guide provides insights into both GitOps controllers and complementary tools that enhance Kubernetes application deployment and management. GitOps Controller: ArgoCD ArgoCD is our primary GitOps controller. It is a declarative continuous deployment tool for Kubernetes that simplifies application management while ensuring your deployment process remains automated and consistent. Additional GitOps Tools Enhance your Kubernetes GitOps workflows with these additional tools: Atlantis: Automates Terraform workflows by integrating directly with pull requests. AutoApply: Automatically applies configuration changes from a Git repository to your Kubernetes cluster, saving manual intervention. CloudRollout: Provides advanced feature flagging, enabling teams to deploy and iterate rapidly without sacrificing safety. GitOps with FluxCD : Offers continuous and progressive delivery solutions optimized for Kubernetes environments. Helm Operator: Automates the release of Helm charts following GitOps principles. Flagger: A Kubernetes operator focused on progressive delivery. It supports canary releases, A/B testing, and blue-green deployments. Ignite: Functions as a virtual machine manager with a container-like user experience, incorporating built-in GitOps capabilities. Faros: A GitOps controller that utilizes Custom Resource Definitions (CRDs) for streamlined operations. GitKube: Facilitates Docker image building and deployment to Kubernetes clusters through a Git push workflow. Jenkins X: Tailored for Kubernetes, this CI/CD platform provides pipeline automation with integrated GitOps and preview environments. KubeStack: Leverages Terraform to provide a GitOps framework for cloud Kubernetes distributions such as AKS, GKE, and EKS, complete with CI/CD examples. Weave Cloud: An automation and management platform designed to support both development and DevOps teams. PipeCD: A continuous delivery solution built for declarative Kubernetes, serverless, and infrastructure applications. Further Learning For additional insights into Kubernetes and GitOps practices, consider exploring resources like Kubernetes Documentation and Docker Hub . Thank you for reading this comprehensive overview of GitOps projects and tools. Happy deploying! Watch Video Watch video content"
GitOps with ArgoCD,Meeting with Task Dash DevOps Team,https://notes.kodekloud.com/docs/GitOps-with-ArgoCD/Introduction/Meeting-with-Task-Dash-DevOps-Team,"GitOps with ArgoCD Introduction Meeting with Task Dash DevOps Team In this article, we explore the challenges faced by the TaskDash DevOps team and how adopting GitOps practices can streamline their processes and enhance security within their infrastructure management. Dasher is a software vendor offering a platform that connects data, applications, and devices across on-premises environments. Recently, their R&D team began investigating cloud migration to leverage container technologies. The DevOps Journey at TaskDash The TaskDash DevOps team is building their project from the ground up by following industry best practices. Their multi-cloud infrastructure employs Docker for containerization and Kubernetes for orchestrating containerized applications. The first step in their approach is Infrastructure as Code (IaC), which automates the provisioning and management of infrastructure using code instead of manual processes. In addition to IaC, they implement several ""X as Code"" strategies, including: Policy as Code Configuration as Code Network as Code Rather than setting up infrastructure, networks, and deploying applications manually to a Kubernetes cluster, the team utilizes automation tools like Terraform and Ansible. These tools leverage YAML and configuration files to define the desired state of the infrastructure and applications. Currently, the configuration files reside on the master branch of a Git repository, and each team member applies changes manually. This workflow bypasses code reviews and automated testing, as every change is instantly pushed to the master branch. Warning Manual execution of configuration changes without code reviews or automated tests can lead to discrepancies and potential security vulnerabilities. The CI/CD Workflow and Challenges Due to the lack of automation for updating the infrastructure, team members often resort to manually executing commands to apply modifications. This practice makes it difficult to track changes and identify who performed specific modifications. To mitigate these challenges, the team has established a CI/CD pipeline: The Continuous Integration (CI) pipeline automates building, testing, and containerizing the applications. The Continuous Deployment (CD) pipeline deploys these applications to the Kubernetes cluster using a push-based model: kubectl apply v2.8.4 Additionally, the team sometimes applies updates manually through the Kubernetes command-line interface (CLI). This hybrid approach introduces several issues: Exposed credentials outside the cluster, increasing the risk of security breaches. A higher likelihood of configuration drift, whereby the deployed state diverges from what is defined in Git. Note Manual CLI alterations are generally discouraged due to potential security risks and the possibility of configuration inconsistencies. Disaster Recovery Concerns Cloud computing environments are susceptible to disasters caused by natural events (e.g., earthquakes or floods), technical failures (e.g., power outages or network interruptions), or human errors (e.g., misconfigurations). In such events, having a robust disaster recovery plan is critical to restoring both the infrastructure and application states. While the Git repository maintains the desired state, manual interventions often lead to discrepancies. For example, over time, the following commands have been executed, each representing a different version applied to the Kubernetes cluster: kubectl apply v2.9.5
kubectl apply v3.0.1
kubectl apply v2.8.4 After a disaster recovery, the team must painstakingly identify which manual changes were applied in order to reconcile the current state with the desired state in Git—a process that is error-prone and time-consuming. The GitOps Advantage Adopting the GitOps methodology helps address the above challenges by ensuring all changes—whether to infrastructure or applications—are tracked and applied through the Git repository. This unified approach promotes consistency, enhances security, and simplifies the disaster recovery process. For additional reading on modern cloud deployment strategies, consider exploring these resources: Kubernetes Basics Kubernetes Documentation Docker Hub Terraform Registry Thank you for reading. Watch Video Watch video content"
GitOps with ArgoCD,GitOps Feature Set,https://notes.kodekloud.com/docs/GitOps-with-ArgoCD/GitOps-Introduction/GitOps-Feature-Set,"GitOps with ArgoCD GitOps Introduction GitOps Feature Set This article provides an in-depth overview of GitOps features and their associated use cases, demonstrating how storing every configuration declaratively in a Git repository can transform your deployment workflows. Every configuration is stored declaratively in Git, which serves as the single source of truth containing the full desired state of the system. This approach not only simplifies application rollbacks—enabling a quick recovery with a simple git revert—but also ensures that audit trails are automatically available through pull requests and commit histories. Key Benefit Storing configurations in Git allows teams to effortlessly rollback to a previous state and maintain a complete audit trail for all changes. Automated CI/CD and Continuous Deployment CI/CD automation is a cornerstone of GitOps. By leveraging automation: Building, testing, and deployment tasks are triggered automatically based on the desired state stored in Git. Continuous deployment becomes seamless and consistent, as applications are deployed automatically to clusters without manual intervention. Extending GitOps to Infrastructure and Cluster Resources Once GitOps is established for application deployment, extend these practices to manage both cluster resources and Infrastructure as Code. For instance, in Kubernetes environments, you can manage various resources including: Secrets management Networking agents and service mesh configurations Database provisioning Prometheus monitoring The core principle here is automatic reconciliation: the system continuously compares the desired state in Git with the actual state in the cluster. If any unintended changes occur, the system automatically reverts them, ensuring consistency. Automatic Reconciliation GitOps continuously compares Git’s desired state against the actual runtime state and reverts any drift, maintaining alignment across your infrastructure. Detecting and Preventing Configuration Drift Early detection of configuration drift is a fundamental aspect of GitOps. Identifying drift as soon as it happens allows teams to resolve inconsistencies before they evolve into significant issues. This proactive stance distinguishes GitOps from other deployment methodologies. Multi-Cluster Deployment Made Easy Managing multiple clusters, especially across different geographical locations, can be challenging. GitOps simplifies this process by centralizing cluster state within Git. This means: A single operator can deploy applications across multiple clusters. There is no need to install or set up the operator individually on each cluster. The deployment process is streamlined and significantly more efficient. For more details on deploying and managing resources with GitOps, explore additional resources such as: Kubernetes Documentation GitOps Tools Overview Modern CI/CD Practices By leveraging GitOps, teams can achieve high levels of deployment efficiency, improved management across diverse environments, and robust recovery mechanisms, making it an essential strategy for modern infrastructure management. Watch Video Watch video content"
GitOps with ArgoCD,DevOps vs GitOps,https://notes.kodekloud.com/docs/GitOps-with-ArgoCD/GitOps-Introduction/DevOps-vs-GitOps,"GitOps with ArgoCD GitOps Introduction DevOps vs GitOps This lesson dives into the contrasting approaches of DevOps and GitOps—two methodologies that share common goals but differ significantly in execution and toolsets. GitOps leverages containerization technologies such as OpenShift and Kubernetes. It uses Git as the single source of truth for both infrastructure and deployments. In comparison, DevOps is a broader methodology that can be applied to diverse application environments and workflows. DevOps Pipeline A typical DevOps pipeline operates as follows: A developer writes code in an Integrated Development Environment (IDE) and commits it to a source code management system. A Continuous Integration (CI) process detects the commit, runs tests, and builds the necessary artifacts. The pipeline then creates a container image and publishes it to a container repository. Finally, the Continuous Deployment (CD) process connects to a Kubernetes cluster and uses a command-line tool such as kubectl (with imperative commands) to push updates directly to the cluster. Key Point In DevOps, the deployment is initiated by pushing changes directly into the cluster. GitOps Pipeline While the CI processes in a GitOps pipeline mirror those of DevOps up to the point of publishing the container image, the deployment process is distinct: Two separate Git repositories are maintained: one dedicated to application code and another for Kubernetes manifests. Once the image is published, the manifest repository is cloned and updated—typically the new image name is specified. These changes are then committed and pushed. The pipeline automatically raises a pull request for the manifest repository. A team member reviews the pull request, suggests adjustments if necessary, and merges the changes upon approval. A GitOps operator, running within the Kubernetes cluster, continuously monitors the repository. When changes are detected, it synchronizes the cluster state to match the repository configuration. Takeaway In a GitOps pipeline, the deployment operator pulls changes from the repository and applies them to the cluster, contrasting with the push-based approach in traditional DevOps workflows. The image below illustrates a side-by-side comparison of the CI/CD pipelines in both DevOps and GitOps. It highlights the key steps—ranging from code development to deployment—and emphasizes the differences in update management and application. Conclusion This article compared DevOps and GitOps pipelines by detailing the key stages of each process. Understanding these differences is essential for choosing the right methodology for your projects. In the next lesson, we will explore the advantages and challenges associated with each approach. Thank you for reading. Watch Video Watch video content"
HashiCorp Certified Vault Associate Certification,Vault Policies Capabilities,https://notes.kodekloud.com/docs/HashiCorp-Certified-Vault-Associate-Certification/Create-Vault-Policies/Vault-Policies-Capabilities,"HashiCorp Certified: Vault Associate Certification Create Vault Policies Vault Policies Capabilities When defining Vault policies, you specify both the path you’re granting privileges to and the capabilities allowed on that path. Capabilities control what actions can be performed—such as creating or reading secrets, listing entries, or even explicitly denying access. They map closely to HTTP verbs and are always defined as a list of strings under the capabilities field. Capability Matrix Capability HTTP Method Description create POST or PUT (new key) Write a brand-new secret read GET Retrieve a secret or configuration update POST or PUT (existing key) Overwrite or change an existing secret delete DELETE Remove a secret or configuration list LIST Enumerate keys under a path (no values) sudo N/A Perform operations on root-protected paths deny N/A Explicitly block access (overrides others) Note Vault does not support a write capability. Instead, policies use separate create and update actions. Create vs. Update create : Required for writing a brand-new key (it must not exist yet). update : Required to modify an existing key’s value. If you grant only create , users cannot modify existing keys. If you grant only update , users cannot create new keys. Other Capabilities read : Retrieve secrets or configurations. delete : Remove secrets or configurations. list : View which keys exist under a path (without reading their values). sudo : Perform operations on root-protected paths. deny : Block access regardless of any other capability rules. Example Policy: Database Credentials & App Secrets Suppose you need: read access to database credentials at database/creds/dev-db01 . create , read , update , and delete permissions for secrets under kv/apps/dev-app01 . You can combine these into a single policy with two path blocks: path ""database/creds/dev-db01"" {
  capabilities = [""read""]
}

path ""kv/apps/dev-app01"" {
  capabilities = [""create"", ""read"", ""update"", ""delete""]
} Once applied, your application can generate dynamic credentials for dev-db01 and fully manage secrets under kv/apps/dev-app01 . Example Policy with Wildcards & Deny Imagine a KV secrets engine mounted at kv/ with this directory structure: kv/
└─ apps/
   ├─ webapp/
   │  ├─ super_secret
   │  ├─ API_token
   │  └─ hostname
   ├─ mid-tier/
   └─ database/ Requirements: Grant read access to everything under kv/apps/webapp/ . Deny access to kv/apps/webapp/super_secret . Use a wildcard for the first rule and an explicit deny for the second: path ""kv/apps/webapp/*"" {
  capabilities = [""read""]
}

path ""kv/apps/webapp/super_secret"" {
  capabilities = [""deny""]
} Users can read any current or future secrets under webapp/ (e.g., API_token , hostname ). The explicit deny ensures super_secret remains inaccessible. Pop Quiz #1 Does the above policy permit reading the path kv/apps/webapp itself? Answer: No. The pattern kv/apps/webapp/* matches entries beneath webapp/ , not the directory itself. Pop Quiz #2 Can a user with only the above policy browse to kv/apps/webapp in the UI or list its contents via CLI? Answer: No. Browsing or listing requires the list capability on each parent path. To enable navigation, extend the policy: path ""kv"" {
  capabilities = [""list""]
}

path ""kv/apps"" {
  capabilities = [""list""]
}

path ""kv/apps/webapp"" {
  capabilities = [""list""]
}

path ""kv/apps/webapp/*"" {
  capabilities = [""read"", ""list""]
}

path ""kv/apps/webapp/super_secret"" {
  capabilities = [""deny""]
} Warning Without list on each parent path, users cannot navigate the directory structure in the Vault UI or via the CLI. References Vault Policies Documentation Vault HTTP API — System Policies KV Secrets Engine (v2) Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Pagers and VI,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/Pagers-and-VI,"Linux Foundation Certified System Administrator (LFCS) Essential Commands Pagers and VI In this guide, we explore the usage of pagers and the powerful Vim text editor. First, we discuss two popular pagers—Less and More—detailing how to view and navigate through text files in the terminal. Then, we introduce Vim, covering its core functionality, essential commands, and editing techniques. Using the Less Pager Pagers like Less allow you to view and navigate through large text files directly in the terminal. Less provides advanced features such as scrolling and searching, making it a popular choice for system log inspections and file viewing. To open a file using Less, enter the following command (here, we inspect the system log): sudo less /var/log/syslog When the file is open, you will typically see the filename highlighted in the lower left corner. Navigation is easy with the arrow keys for scrolling up and down. Below is an example snippet from a syslog as displayed by Less: 2024-05-07T00:34:30.893981+00:00 kodekloud kernel: Linux version 6.8.0-31-generic (buildd@lcy02-amd64-0080) (x86_64-linux-gnu-gcc-13 (Ubuntu 13.2.0-23ubuntu4) 13.2.0, GNU ld (GNU Binutils for Ubuntu) 2.42) # 31-Ubuntu SMP PREEMPT_DYNAMIC Sat Apr 20 00:40:06 UTC 2024 (Ubuntu 6.8.0-31.31-generic 6.8.1)
2024-05-07T00:34:30.894695+00:00 kodekloud kernel: Command line: BOOT_IMAGE=/vmlinuz-6.8.0-31-generic root=/dev/mapper/ubuntu--vg-ubuntu--lv ro
2024-05-07T00:34:30.894698+00:00 kodekloud kernel: KERNEL supported cpus:
2024-05-07T00:34:30.894699+00:00 kodekloud kernel:  Intel GenuineIntel
2024-05-07T00:34:30.894699+00:00 kodekloud kernel:  AMD AuthenticAMD
2024-05-07T00:34:30.894699+00:00 kodekloud kernel:  Hygon HygonGenuine
2024-05-07T00:34:30.894699+00:00 kodekloud kernel:  Centaur CentaurHauls
2024-05-07T00:34:30.894699+00:00 kodekloud kernel:  zhaoxin Shanghai
2024-05-07T00:34:30.894700+00:00 kodekloud kernel: BIOS-provided physical RAM map:
[...]
/var/log/syslog You can scroll further using the arrow keys. For example, this snippet demonstrates additional navigation: 2024-05-07T00:34:30.894695+00:00 kodekloud kernel: Command line: BOOT_IMAGE=/vmlinuz-6.8.0-31-generic ro
2024-05-07T00:34:30.894698+00:00 kodekloud kernel: KERNEL supported cpus:
2024-05-07T00:34:30.894699+00:00 kodekloud kernel: Intel GenuineIntel
2024-05-07T00:34:30.894700+00:00 kodekloud kernel: AMD AuthenticAMD
2024-05-07T00:34:30.894699+00:00 kodekloud kernel: Hygon HygonGenuine
2024-05-07T00:34:30.894699+00:00 kodekloud kernel: Centaur CentaurHauls
2024-05-07T00:34:30.894701+00:00 kodekloud kernel: zhaoxin Shanghai
2024-05-07T00:34:30.894703+00:00 kodekloud kernel: BIOS-provided physical RAM map:
2024-05-07T00:34:30.894704+00:00 kodekloud kernel: BIOS-e820: [mem 0x0000000000000000-0x000000000009fbf]
2024-05-07T00:34:30.894706+00:00 kodekloud kernel: BIOS-e820: [mem 0x000000000009fc00-0x000000000009fff]
2024-05-07T00:34:30.894709+00:00 kodekloud kernel: BIOS-e820: [mem 0x00000000000e0000-0x0000000000effff]
2024-05-07T00:34:30.894709+00:00 kodekloud kernel: BIOS-e820: [mem 0x0000000000100000-0x000000026a6fffff] Tip Remember to use the arrow keys in Less for smooth navigation through your log files. For more extensive documents, Less is highly recommended over other simpler pagers. Searching Within Less Less includes powerful search functionality. To search within a file: Press the slash (/) key. Type the desired search term (e.g., ""system"") and press Enter. Less highlights the matching terms, and you can navigate through occurrences by pressing N for the next match or Shift + N for the previous match. Below is a sample of search results highlighting various system messages: 2024-05-07T00:34:30.894818+00:00 kodekloud systemd[1]: Finished lvm2-monitor.service - Monitoring of LV M2 mirrors, snapshots etc. using dmeventd or progress polling.
2024-05-07T00:34:30.894822+00:00 kodekloud systemd[1]: Finished systemd-remount-fs.service - Remount Root and Kernel File Systems.
2024-05-07T00:34:30.894825+00:00 kodekloud systemd[1]: Mounted sys-fs-fuse-connections.mount - FUSE Control File System.
2024-05-07T00:34:30.894830+00:00 kodekloud systemd[1]: Activating swap swap.img.swap - /swap.img...
2024-05-07T00:34:30.894836+00:00 kodekloud systemd[1]: Starting multipathd.service - Device-Mapper Multipath Device Controller...
2024-05-07T00:34:30.894843+00:00 kodekloud systemd[1]: systemd-hwdb-update.service - Rebuild Hardware Database was skipped because no trigger condition checks were met.
2024-05-07T00:34:30.894847+00:00 kodekloud systemd-modules-load[379]: Inserted module 'dm_multipath'
2024-05-07T00:34:30.894855+00:00 kodekloud systemd[1]: Starting systemd-journal-flush.service - Flush Journal to Persistent Storage...
2024-05-07T00:34:30.894862+00:00 kodekloud systemd[1]: systemd-pstore.service - Platform Persistent Storage Archival was skipped because of an unmet condition check (ConditionDirectoryNotEmpty=/sys/fs/pstore).
2024-05-07T00:34:30.894867+00:00 kodekloud systemd[1]: Starting systemd-random-seed.service - Load/Save Random Seed.
2024-05-07T00:34:30.894862+00:00 kodekloud systemd[1]: systemd-tpm2-setup.service - TPM2 SRK Setup was skipped because of an unmet condition check (ConditionSecurity-measured-uki).
2024-05-07T00:34:30.894867+00:00 kodekloud systemd[1]: Activated swap swap.img.swap - /swap.img.
2024-05-07T00:34:30.894871+00:00 kodekloud systemd[1]: Finished systemd-modules-load.service - Load Kernel Modules.
2024-05-07T00:34:30.894874+00:00 kodekloud systemd[1]: Mounted sys-kernel-config.mount - Kernel Configuration File System.
2024-05-07T00:34:30.894878+00:00 kodekloud systemd[1]: Reached target swap.target - Swaps. By default, searches are case sensitive. To perform a case-insensitive search, append ""\c"" to your search term (e.g., /example\c ): 2024-05-07T00:34:30.894818+00:00 kodekloud systemd[1]: Finished lvm2-monitor.service - Monitoring of LV M2 mirrors, snapshots etc. using dmeventd or progress polling.
2024-05-07T00:34:30.894822+00:00 kodekloud systemd[1]: Finished systemd-remount-fs.service - Remount Root and Kernel File Systems.
2024-05-07T00:34:30.894825+00:00 kodekloud systemd[1]: Mounted sys-fs-fuse-connections.mount - FUSE Control File System.
2024-05-07T00:34:30.894830+00:00 kodekloud systemd[1]: Activating swap swap.img.swap - /swap.img...
2024-05-07T00:34:30.894836+00:00 kodekloud systemd[1]: Starting multipathd.service - Device-Mapper Multipath Device Controller...
2024-05-07T00:34:30.894843+00:00 kodekloud systemd[1]: systemd-hwdb-update.service - Rebuild Hardware Database was skipped because no trigger condition checks were met.
2024-05-07T00:34:30.894847+00:00 kodekloud systemd-modules-load[379]: Inserted module 'msr'
2024-05-07T00:34:30.894855+00:00 kodekloud systemd[1]: systemd-journal-flush.service - Flush Journal to Persistent Storage...
2024-05-07T00:34:30.894861+00:00 kodekloud systemd[1]: systemd-pstore.service - Platform Persistent Storage Archival was skipped because of an unmet condition check (ConditionDirectoryNotEmpty=/sys/fs/pstore).
2024-05-07T00:34:30.894867+00:00 kodekloud systemd[1]: Starting systemd-random-seed.service - Load/Save Random Seed...
2024-05-07T00:34:30.894872+00:00 kodekloud systemd[1]: systemd-tpm2-setup.service - TPM2 SRK Setup was skipped because of an unmet condition check (ConditionSecurity=measured-uki).
2024-05-07T00:34:30.894878+00:00 kodekloud systemd[1]: Reached target swap.target - Swaps. To navigate backwards through matches, press uppercase N . When you are ready to exit Less, simply press Q . Using the More Pager While Less offers many features, the More pager is a simpler alternative with a more straightforward interface. More is effective for quick file views. To open a file with More, run the following command: more /path/to/your/file When you view a file with More, a progress indicator (like ""More (2%)"") appears at the bottom of the screen. Use the spacebar to advance through the file page by page. Below is a sample output from More: 2024-05-07T00:34:30.894971+00:00 kodekloud systemd[1]: plymouth-start.service - Show Plymouth Boot Screen
2024-05-07T00:34:30.894974+00:00 kodekloud systemd[1]: Started systemd-ask-password-console.path - Dispatch Password Requests to Console Directory Watch.
2024-05-07T00:34:30.894980+00:00 kodekloud systemd[1]: systemd-ask-password-plymouth.path - Forward Password Requests to Plymouth Directory Watch was skipped because of an unmet condition check (ConditionPathExists=/run/plymouth/pid).
2024-05-07T00:34:30.894983+00:00 kodekloud systemd[1]: Reached target cryptsetup.target - Local Encrypted Volumes.
2024-05-07T00:34:30.894987+00:00 kodekloud (udev-worker)[456]: dm-0: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/dm-0' failed with exit code 1.
2024-05-07T00:34:30.894992+00:00 kodekloud (udev-worker)[451]: sda: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/sda' failed with exit code 1.
2024-05-07T00:34:30.894995+00:00 kodekloud lvm[522]: PV /dev/sda3 online, VG ubuntu-vg is complete.
2024-05-07T00:34:30.894996+00:00 kodekloud lvm[522]: VG ubuntu-vg finished
2024-05-07T00:34:30.895000+00:00 kodekloud (udev-worker)[467]: sda2: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/sda2' failed with exit code 1.
2024-05-07T00:34:30.895008+00:00 kodekloud (udev-worker)[451]: sda1: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/sda1' failed with exit code 1.
2024-05-07T00:34:30.895012+00:00 kodekloud (udev-worker)[473]: sr0: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/sr0' failed with exit code 1.
2024-05-07T00:34:30.895019+00:00 kodekloud (udev-worker)[450]: sda3: Process '/usr/bin/unshare -m /usr/bin/snap auto-import --mount=/dev/sda3' failed with exit code 1.
2024-05-07T00:34:30.895020+00:00 kodekloud systemd[1]: Found device dev-disk-by\x2duuid-18157e24\x2d3de7\x24d2b2\x2d9dc0\x2d21d01162db644.service - File System Check on /dev/disk/by-uuid/18157e24-3d
--More-- (2%) To exit More, press the Q key. Introduction to Vim Vim, short for ""Vi Improved,"" is a powerful text editor designed for efficiency. When you start Vim, a welcome screen appears, providing basic information and usage tips: VIM - Vi IMproved
version 9.1.16
by Bram Moolenaar et al.
Modified by [email protected] Vim is open source and freely distributable

Become a registered Vim user!
type :help register<Enter> for information

type :q<Enter> to exit
type :help<Enter> or <F1> for on-line help
type :help version9<Enter> for version info Basic Vim Modes Vim is a mode-based editor, meaning it operates in different modes for specific tasks: Insert Mode: Press the i key to enter Insert mode and start editing your file. Command Mode: Press the Esc key to switch back to Command mode. In this mode, you can run commands for saving, quitting, or navigating within the file. Remember Always press Esc to ensure you are in Command mode before executing commands. Creating, Saving, and Quitting Files in Vim To create or edit a file, run: vim test.txt While in Command mode, use the following commands: To save your changes, type :w and press Enter. To save and exit, type :wq after pressing Esc if necessary. To exit without saving, type :q! . To open an existing file, simply enter: vim example.txt Navigating and Searching in Vim Movement in Vim is efficient with both arrow keys and command shortcuts: Use h , j , k , and l for left, down, up, and right navigation respectively. To search for text, press / followed by your search query (e.g., /example ) and press Enter. For a case-insensitive search, append \c to the query (e.g., /example\c ). To jump to a specific line, type :10 (where 10 is the line number). Copying, Pasting, and Cutting Text Vim offers quick commands for text manipulation: Copy (yank) a line: Press yy . Paste the copied line: Press p (repeat to paste multiple times). Cut (delete) a line: Press dd , and then paste where appropriate using p . These simple commands help streamline your workflow without reliance on a mouse. Summary In this guide, we reviewed the Less and More pagers for file navigation in the terminal, and provided an introduction to Vim with practical commands for editing, searching, and navigating files. Below is an additional text snippet as a closing note: OK - This is another line of text
OK - This is another line of text
Fusce cursus efficitur hendrerit. Duis aliquam nibh diam, sed fermentum justo ultrices ac. Nunc eu dui tempus, accumsan odio ut, bibendum lacus. Pellentesque sed congue nibh. Donec eu egestas erat. Etiam vita dolor turpis. Sed eget velit consectetur, scelerisque ex at, fermentum lectus. Integer porttitor nisl orci, ut porttitor libero sollicitudin id. Duis sagittis, quam ut bibendum pretium, purus nisl eleifend quam, finibus semper mi diam ut urna. Fusce volutpat mauris ut efficitur vulputate. Vestibulum et iaculis mi. Aenean eget magna sollicitudin, convallis tortor finibus, cursus est. That concludes our overview of terminal pagers and Vim. Happy editing, and see you in the next lesson! Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,List Set and Change Standard File Permissions,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/List-Set-and-Change-Standard-File-Permissions,"Linux Foundation Certified System Administrator (LFCS) Essential Commands List Set and Change Standard File Permissions In this article, we explore how to list, set, and modify standard file permissions in Linux. Mastering file permissions is crucial for managing file and directory ownership effectively. When you run the following command: $ ls -l you may see output similar to this, which shows that each file or directory is owned by a particular user: -rw-r----- 1 aaron family 49 Oct 27 14:41 family_dog.jpg In the example above, the file ""family_dog.jpg"" is owned by the user ""aaron"" and associated with the group ""family"". Only the file owner or the superuser (root) can change its permissions. Changing the Group Owner with chgrp To change the group of a file or directory, use the chgrp command. The syntax is as follows: $ chgrp group_name file/directory For example, to change the file's group to ""sudo"", execute: $ chgrp sudo family_dog.jpg After running this command and listing the file details using ls -l , you will see the group updated to ""sudo"". Note that you can only change the group to one that you are a member of. To display your current groups, run: $ groups
aaron sudo family Note Only the root user can change the file group to any group available on the system. Changing the User Owner with chown To change the user owner of a file or directory, use the chown command with the syntax below: $ sudo chown new_owner file/directory For example, to change the ownership of ""family_dog.jpg"" to ""jane"", use: $ sudo chown jane family_dog.jpg After executing ls -l , you will observe that the file's owner is now ""jane"". Only the root user has the privileges to change the file owner. You can also modify both the owner and group simultaneously using: $ sudo chown aaron:family family_dog.jpg This command resets the owner to ""aaron"" and the group to ""family"". Here is a sequence of commands demonstrating changing ownership and group: $ ls -l
-rw-r----- 1 aaron family 49 Oct 27 14:41 family_dog.jpg
$ chgrp sudo family_dog.jpg
$ ls -l
-rw-r----- 1 aaron sudo 49 Oct 27 14:41 family_dog.jpg
$ sudo chown jane family_dog.jpg
$ ls -l
-rw-r----- 1 jane sudo 49 Oct 27 14:41 family_dog.jpg
$ sudo chown aaron:family family_dog.jpg
$ ls -l
-rw-r----- 1 aaron family 49 Oct 27 14:41 family_dog.jpg Understanding ls -l Output and Permissions The ls -l command output provides detailed file information, including permissions: The first character indicates the entry type: A dash (-) represents a regular file. A ""d"" signifies a directory. An ""l"" denotes a symbolic link. The next nine characters are divided into three groups of three: The first trio pertains to the user (owner). The second trio is for the group. The third trio applies to others. For example: $ ls -l
-rwxrwxrwx. 1 aaron family 49 Oct 27 14:41 family_dog.jpg In this listing: ""rwx"" for the owner means the owner can read, write, and execute. ""rwx"" for the group grants identical permissions. ""rwx"" for others provides full access to all users. Permissions for Files vs. Directories For files: ""r"" (read) allows the content to be viewed. ""w"" (write) permits modifications. ""x"" (execute) enables running the file as a program or script. For directories: ""r"" allows listing the directory’s contents. ""w"" permits creating or deleting files within. ""x"" allows entering the directory using the cd command. For instance, to list files in the ""Pictures"" directory or create a new subdirectory: $ ls Pictures/
$ mkdir Pictures/Family How Permissions Are Evaluated When accessing a file, Linux evaluates permissions in the following order: If the user is the file owner, user permissions apply. If not, and the user is a member of the file’s group, group permissions apply. Otherwise, the ""others"" permissions are enforced. Consider the following output: (aaron)$ ls -l
-r--rw---- 1 aaron family 49 family_dog.jpg Even though ""aaron"" is in the ""family"" group (which has read and write permissions), the file displays the owner’s permissions (r--), meaning Aaron can only read the file. Attempting to append text as Aaron results in: (aaron)$ echo ""Add this content to file"" >> family_dog.jpg
bash: family_dog.jpg: Permission denied However, if another user, such as ""jane"" (a member of the ""family"" group), accesses the file: (aaron)$ su jane
(jane)$ echo ""Add this content to file"" >> family_dog.jpg
(jane)$ cat family_dog.jpg
Picture of Milo the dog If the user is neither the owner nor a member of the file’s group, the ""others"" permissions are applied. Changing File Permissions with chmod To modify file or directory permissions, use the chmod command: $ chmod permissions file/directory There are two primary methods to specify permissions: Using the Plus (+) and Minus (–) Signs You can add permissions with + and remove them with - . To add write permission for the owner: $ chmod u+w family_dog.jpg Suppose the file initially shows: $ ls -l
-r--rw----. 1 aaron family 49 Oct 27 14:41 family_dog.jpg After applying the command: $ ls -l
-rw-rw----. 1 aaron family 49 Oct 27 14:41 family_dog.jpg To remove permissions, for example, to remove read permission for others: $ chmod o-r family_dog.jpg This command ensures that only the owner and group have read access to the file. Using the Equal (=) Operator You can set permissions to exact values using the equal sign. For instance, to set group permissions to read-only: $ chmod g=r family_dog.jpg This command sets group permissions to exactly ""r--"", even if write or execute permissions were previously set. To remove all permissions for a group, use: $ chmod g= family_dog.jpg Combining Permission Changes You can combine changes for the user (u), group (g), and others (o) in a single command. For example, to grant the owner read and write permissions, set the group to read-only, and remove all permissions for others: $ chmod u+rw,g=r,o= family_dog.jpg Setting Permissions with Octal Notation The chmod command also accepts octal values for specifying permissions. First, view the file’s current permissions with the stat command: $ stat family_dog.jpg
File: family_dog.jpg
Size: 49            Blocks: 8          IO Block: 4096   regular file
Device: fd00h/64768d Inode: 52946177    Links: 1
Access: (0640/-rw-r-----)  Uid: ( 1000/ aaron)   Gid: (  27/ sudo) In the output above, ""0640"" represents the file permissions: Owner (6): read (4) + write (2) Group (4): read (4) Others (0): no permissions To set the permissions to 640, run: $ chmod 640 family_dog.jpg Understanding the Octal Calculation Permissions can be visualized in binary: For the owner, ""rw-"" translates to 110 in binary (6 in octal). For the group, ""r--"" translates to 100 in binary (4 in octal). For others, ""---"" translates to 000 in binary (0 in octal). A more common octal permission setting is 755, which means: Owner: 7 (rwx, or 111 in binary) Group: 5 (r-x, or 101 in binary) Others: 5 (r-x, or 101 in binary) Similarly, 777 means full permissions (read, write, and execute) for all. Below is an image that illustrates the conversion of binary file permissions to octal values: Another image further explains the octal permission notation used in Unix-like systems: Summary In this article, we covered the following key topics: Viewing file ownership and permissions using ls -l Changing file group ownership with chgrp Modifying file user ownership with chown Understanding the structure and significance of file and directory permissions Using chmod to modify permissions both with symbolic operators and octal notation With this detailed guide, you now have the knowledge to effectively manage file permissions on Linux systems, ensuring both security and proper access control. Happy learning and see you in the next article! Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Analyze Text Using Basic Regular Expressions,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/Analyze-Text-Using-Basic-Regular-Expressions,"Linux Foundation Certified System Administrator (LFCS) Essential Commands Analyze Text Using Basic Regular Expressions In this lesson, we explore regular expressions—commonly known as regex—and demonstrate how they can be used for advanced text searching. In earlier lessons, we relied on simple search patterns for precise text pieces (such as passwords). But as search conditions become more complex, regex helps to refine those queries. For instance, if you need to extract all IP addresses (e.g., 203.102.3.5) from hundreds of application files, a naive pattern that only looks for numbers separated by periods might accidentally capture values like 5.23 that don't represent valid IP addresses. Just like in mathematics where you can define conditions for an integer (for example, when x is greater than 3 and less than 8 so that x is 4, 5, 6, or 7), regex lets you specify and combine conditions to form patterns that match only the text meeting those criteria. We'll begin with simple examples and gradually progress to more complex expressions. Regular expressions are built using various operators such as the caret (^), dollar sign ($), period (.), asterisk (*), plus sign (+), braces ({}), question mark (?), vertical pipe (|), brackets ([]), and parentheses (()). Each operator has a distinct function that helps tailor your search. Matching Commented Lines in Linux Configuration Files In Linux configuration files, lines starting with a pound sign (#) are interpreted as comments. Although these lines are ignored by the system, they provide valuable context and documentation for humans. To search for these commented lines, you can build a regex that matches lines beginning with a pound sign by placing the caret operator (^) at the start of the pattern. Tip Use the caret operator (^) to ensure that your search starts at the beginning of the line. For example, to list all lines that start with a pound sign: $ grep '^#' /etc/login.defs If you want to display only non-commented lines, combine this with grep's invert option (-v): $ grep -v '^#' /etc/login.defs The output might look like this: MAIL_DIR           /var/mail
FAILLOG_ENAB       yes
LOG_UNKFAIL_ENAB   no
LOG_OK_LOGINS      no
SYSLOG_SU_ENAB     yes
SYSLOG_SG_ENAB     yes
FTMP_FILE          /var/log/btmp
SU_NAME            su
HUSHLOGIN_FILE     .hushlogin This approach is very effective for filtering out cluttered comments in large files. Similarly, to search for lines that start exactly with the letters ""PASS,"" use: $ grep '^PASS' /etc/login.defs The Caret (^) and Dollar Sign ($) Operators Sometimes you may need to change a setting that's currently set to a specific value (for example, seven days). A naive search using: $ grep '7' /etc/login.defs might inadvertently match other instances of the digit 7. To refine the search, anchor the pattern to the end of the line using the dollar sign ($) if you know the variable value occurs last on the line: $ grep -w '7$' /etc/login.defs This ensures that only lines ending with the digit 7 are captured. Additionally, to target lines ending with the word ""mail,"" you can use: $ grep 'mail$' /etc/login.defs Remember, the caret (^) specifies the start, while the dollar sign ($) specifies the end of the line. The Dot (.) Operator and Wildcards The period (.) is a wildcard character in regex that matches any single character. For example, the pattern C.T would match strings such as ""cat,"" ""cut,"" ""CRT,"" ""C1T,"" or ""C#T,"" whereas it wouldn't match ""CT"" because there must be exactly one character between C and T. Similarly, C..T ensures there are exactly two characters between C and T. To match whole words rather than sub-strings within larger words, leverage grep’s -w option: $ grep -r 'c.t' /etc/ For a recursive search with whole word matching, use: $ grep -wr 'c.t' /etc/ The Asterisk (*) and Plus (+) Operators The asterisk (*) specifies that the preceding element can occur zero, one, or many times. For example, consider the pattern that matches ""let"" followed by zero or more ""T"" characters using let* . This pattern can match ""LE,"" ""LET,"" ""LETT,"" and so on. A recursive search example with the asterisk: $ grep -r 'let*' /etc/ Be aware that the asterisk can also make the preceding character optional. For instance, the pattern 0* will match lines regardless of whether the digit 0 is present: $ grep -r '0*' /etc/ To search for lines where the digit 0 appears one or more times, use the plus operator (+). Note that grep’s Basic Regular Expressions (BRE) require you to escape the plus sign: $ grep -r '0\+' /etc/ Using Extended Regular Expressions with grep's -E option eliminates the need for escaping the plus sign. Remember When working with grep, consider using the -E option for Extended Regular Expressions to simplify your patterns and avoid confusion with escaped characters. Conclusion Understanding how to strategically place operators like ^ and $ for anchoring searches, . for matching any character, and * or + for repetitions is key to building efficient regular expressions. Mastering these basics lays the groundwork for more advanced techniques, which we will explore in the next lesson on Extended Regular Expressions. For more detailed documentation and examples, check out the Kubernetes Documentation and other related technical resources . Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Extended Regular Expressions,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/Extended-Regular-Expressions,"Linux Foundation Certified System Administrator (LFCS) Essential Commands Extended Regular Expressions Extended regular expressions (ERE) simplify pattern matching by reducing the need for escaping special characters. When using ERE with grep (via the -E option or its alias, egrep), most special characters are interpreted as regex operators by default. You only need to escape them when you want them treated as literal characters. Tip Use ERE with grep by using the uppercase -E flag or the egrep command to simplify your regular expressions and avoid common pitfalls with escaping characters. Basic Usage with grep Consider a command that searches for one or more occurrences of the digit zero in files under the /etc/ directory: $ grep -Er '0+' /etc/ This command uses the + operator to match one or more zeros. Equivalently, you could use: $ egrep -r '0+' /etc/ In both cases, the command highlights lines in various /etc/ files where the pattern is found. Matching Specific Repetitions To find strings containing at least three consecutive zeros, you can use the curly bracket syntax: $ egrep -r '0{3,}' /etc/ Here, {3,} specifies a minimum repetition of three, with no upper limit. If you want to search for a string beginning with 1 followed by up to three zeros, the pattern is: $ egrep -r '10{,3}' /etc/ This regex also matches the case where no zero follows the digit 1 . To match exactly three zeros, omit the comma and second number: $ egrep -r '0{3}' /etc/ Optional Characters with the Question Mark The question mark operator ( ? ) makes the preceding element optional (i.e., it can appear once or not at all). For example, to find lines containing either ""disable"" or ""disabled,"" you might use: $ egrep -r 'disable?d?' /etc/ Be cautious—this expression may also match portions of longer words (like ""disables""). To match whole words exactly, consider using the -w option with grep or an alternation operator: $ egrep -r 'enabled|disabled' /etc/ For broader matching that handles case variations, add the -i option for a case-insensitive search. Ranges and Sets Ranges allow you to specify a set of characters between two endpoints. For example: [a-z] matches any lowercase letter. [0-9] matches any digit. Sets allow matching one character from a list. To search for either ""cat"" or ""cut"": $ egrep -r 'c[au]t' /etc/ This pattern checks for the letters a or u in the middle of ""c?t,"" effectively matching both ""cat"" and ""cut."" Combining Regex Patterns: Matching Device Files When matching configuration entries for device files (e.g., /dev/sda1 or /dev/twa0 ), the naive use of .* might be too broad: $ egrep -r '/dev/.*' /etc/ Instead, you can be more specific by matching a forward slash followed by any number of lowercase letters: $ egrep -r '/dev/[a-z]*' /etc/ To include trailing digits, adjust the pattern: $ egrep -r '/dev/[a-z]*[0-9]' /etc/ This matches only device names ending in a digit. To accommodate both cases (with or without a trailing digit): $ egrep -r '/dev/[a-z]*[0-9]?' /etc/ For multi-segment device names (like /dev/tty0p0 ), group the pattern for letters and an optional digit, then allow repetition: $ egrep -r '/dev/([a-z]*[0-9]?)+' If uppercase letters are also possible in device names, extend the character class: $ egrep -r '/dev/(([a-zA-Z])*[0-9]?)+' /etc/ This pattern effectively matches various formats, including /dev/ttyS0 . Using the Negation Operator Inside square brackets, the caret (^) negates a set. For example, to search for the string ""https"" that is not immediately followed by a colon: $ egrep -r 'https[^:]' /etc/ Similarly, you can refine your pattern to match ""http"" not followed by certain characters by excluding them in the character set. For example, to find lines where a forward slash is immediately followed by a character that is not a lowercase letter: $ egrep -r '/[^a-z]' /etc/ This command will return lines where the character following / does not fall within the lowercase alphabet. Practical Considerations and Further Resources Regular expressions provide a powerful, precise method for text searching and manipulation. By mastering regex operators, ranges, sets, and grouping, you can craft expressions tailored to your needs. Further Study Explore online tools such as regexr.com to experiment with and validate your regular expressions. Additionally, refer to the grep documentation for more detailed information. Happy grepping! Watch Video Watch video content Practice Lab Practice lab"
Linux Foundation Certified System Administrator LFCS,Archive Back Up Compress Unpack and Uncompress Files Optional,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/Archive-Back-Up-Compress-Unpack-and-Uncompress-Files-Optional,"Linux Foundation Certified System Administrator (LFCS) Essential Commands Archive Back Up Compress Unpack and Uncompress Files Optional In this article, we explore efficient methods for archiving files in Linux, compressing them, and backing them up to a remote location. This tutorial is ideal if you manage a website or any system with thousands of files and directories and want to streamline your backup process. Instead of handling individual files, you can pack them into a single archive—often referred to as a tarball—and compress that archive to save space. When you archive files, you combine all files and directories into one file (e.g., backup.tar). This process is called archiving. Once created, the archive can be compressed (for example, to backup.tar.gz) to reduce the storage space needed. Finally, copying the compressed file to a remote location adds an extra layer of protection to your data. In the sections that follow, we first discuss archiving, then move on to compressing the archive, and finally, we review methods to back up your compressed files to a remote location. Archiving Files Using Tar Tar (tape archive) was originally developed for backing up files to magnetic tapes. Although magnetic tapes are less common now, tar remains a critical tool because of its efficient way of packing and unpacking files. Tar works by combining multiple files and directories into a single file, commonly known as a tarball. This technique simplifies file transfers, uploads, or downloads, as you are working with a single file instead of many. Consider an existing archive file named archive.tar on your system. You can view its contents using any of these commands: $ tar --list --file archive.tar
file1
file2
file3 $ tar -tf archive.tar
file1
file2
file3 $ tar tf archive.tar
file1
file2
file3 While the shorthand version (tar tf archive.tar) is quick to type, using the longer options like --list can be more intuitive for beginners. Best Practice Always include the -f option immediately before specifying the tar file name. This practice ensures that tar correctly identifies the subsequent argument as the archive file, preventing potential misinterpretations of your options. Common Tar Commands Below are some frequently used tar commands: Archive a Single File To archive a single file (file1) into archive.tar: $ tar --create --file archive.tar file1 This command can be shortened to: $ tar cf archive.tar file1 Append a File to an Existing Archive To add another file (file2) to your existing archive: $ tar --append --file archive.tar file2 Archive an Entire Directory To archive a directory such as Pictures/ along with its contents: $ tar --create --file archive.tar Pictures/ When using a relative path (e.g., Pictures/), the archive retains the same folder structure. Alternatively, using an absolute path: $ tar --create --file archive.tar /home/aaron/Pictures/ will store the absolute path in the archive. Before extracting files from an archive, it's recommended to list its contents to review the directory structure. For example: $ tar --list --file archive.tar
Pictures/
Pictures/family_dog.jpg Extraction recreates the archived paths relative to your current directory: $ tar --extract --file archive.tar If you are in the /home/aaron/work directory, the extraction will produce: /home/aaron/work/Pictures/
/home/aaron/work/Pictures/family_dog.jpg To extract files into a different directory, use the -C option. For instance, if you’re in /home/errand and want to extract archive.tar's contents to /tmp , run: $ tar --extract --file archive.tar --directory /tmp/ Or using the shorthand version: $ tar xf archive.tar -C /tmp/ File Permissions Tar archives store file permissions and ownership information. If you extract files archived with a different user, you might not preserve the original ownership unless you run the command with elevated privileges (using sudo ). Next Steps: Compressing and Remote Backups In upcoming sections, we will cover how to compress your tar archives to save space and review best practices for backing up data to remote locations. These techniques are essential for ensuring high data availability and safeguarding against data loss. For more detailed information on related topics, refer to the following resources: Kubernetes Documentation Docker Hub Terraform Registry With these skills in your toolkit, you can efficiently manage and secure your Linux file systems. Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Create and Manage Soft Links,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/Create-and-Manage-Soft-Links,"Linux Foundation Certified System Administrator (LFCS) Essential Commands Create and Manage Soft Links In this article, we explore how Linux handles soft links (also known as symbolic links). Soft links in Linux work similarly to the shortcuts you might find on a Windows desktop. For instance, when you install a program on Windows, a shortcut is added to your desktop that points to the actual executable stored elsewhere (such as ""C:\Program Files\MyCoolApp\application.exe""). When you double-click the shortcut, it launches the program even though the executable is not directly stored on your desktop. C:\Program Files\MyCoolApp\application.exe Unlike hard links, which directly reference an inode, soft links are files that contain a path to another file or directory. Essentially, they are text files holding the address where the target file or directory is located. To create a soft link, you add the -s option to the ln command. The basic syntax is: # ln -s path_to_target_file path_to_link_file ""path_to_target_file"" is the location of the file or directory that the soft link will reference. ""path_to_link_file"" is the name (and optionally, the location) of the new soft link. For example, to create a symbolic link for a family dog picture, use the following commands: $ ln -s /home/aaron/Pictures/family_dog.jpg family_dog_shortcut.jpg
$ ls -l
lrwxrwxrwx. 1 aaron aaron family_dog_shortcut.jpg -> /home/aaron/Pictures/family_dog.jpg In the output of ls -l , the leading ""l"" indicates that the file is a soft link. It also displays the path that the soft link points to. If the target path is lengthy, ls -l might not show the entire path. In these cases, you can use the readlink command to view the complete link destination: $ readlink family_dog_shortcut.jpg
/home/aaron/Pictures/family_dog.jpg Note Although the soft link appears to have full permission bits (rwx), these permissions are not actually enforced. Instead, the permissions of the destination file or directory determine access rights. For example, if you attempt to redirect output to a soft link that points to a protected file (such as /etc/fstab ), the operation will be denied: $ ln -s /home/aaron/Pictures/family_dog.jpg family_dog_shortcut.jpg
$ ls -l
lrwxrwxrwx. 1 aaron aaron family_dog_shortcut.jpg -> /home/aaron/Pictures/family_dog.jpg
$ readlink family_dog_shortcut.jpg
/home/aaron/Pictures/family_dog.jpg
$ echo ""Test"" >> fstab_shortcut
bash: fstab_shortcut: Permission denied Using an absolute path in a soft link (e.g., /home/aaron/Pictures/family_dog.jpg ) means that if the directory name (like ""aaron"") changes in the future, the link will break. A broken link is typically displayed in red when you use ls -l . To prevent this issue, consider creating a soft link with a relative path if you are working within the same directory structure. This method ensures that when the soft link is accessed, it correctly redirects to the intended file relative to the current directory. Soft links can also be created for directories or for files and directories located on different file systems. Watch Video Watch video content Practice Lab Practice lab"
Linux Foundation Certified System Administrator LFCS,SUID SGID and Sticky Bit,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/SUID-SGID-and-Sticky-Bit,"Linux Foundation Certified System Administrator (LFCS) Essential Commands SUID SGID and Sticky Bit In Unix/Linux systems, managing permissions is critical to maintaining security and efficient resource access. In this article, we explore three special permissions—SUID, SGID, and the Sticky Bit—that allow controlled elevation of privileges and help manage collaborative environments. Overview Understanding these permissions ensures that applications can safely operate with elevated privileges without compromising system integrity. SUID (Set User ID) SUID is a permission that, when applied to an executable file, enables the process to run with the file owner's privileges instead of those of the user who launched it. This feature is particularly useful when an application requires access to restricted resources. For example, if Emily develops a reports application that needs to access files under /usr/local/reports , she can allow John to run the application without granting him unfettered access to her directory. Demonstration of SUID Below is a step-by-step demonstration of setting and verifying the SUID bit: # Create the test file
touch suidfile

# Check default permissions
ls -l suidfile
# Output: -rw-rw-r--  1 jeremy jeremy 0 May 8 01:22 suidfile To set the SUID bit, which is represented by a leading digit of 4 in the permission mode, execute: chmod 4664 suidfile
ls -l suidfile Notice that the execute bit for the owner may be displayed as a capital ""S"" when it is not enabled. Including the execute permission (for example, using 4764 ) will show a lowercase ""s"" instead. SGID (Set Group ID) SGID works similarly to SUID but applies to the group ownership of an executable or directory. For executables, SGID allows any user running the file to do so with the file's group privileges. When applied to a directory, any new file or directory created inherits the group's ownership, which is invaluable for collaborative work environments. For instance, if a reports application is associated with the reports group, both Emily and John can access executable files, and newly created files inside the directory will automatically inherit the reports group. Demonstration of SGID Follow these simple steps to set the SGID bit on a file: # Create the file
touch sgidfile

# Check the default file permissions
ls -l sgidfile
# Expected output: -rw-rw-r-- 1 jeremy jeremy 0 May 8 01:25 sgidfile To set SGID without granting execute permissions for the group: chmod 2664 sgidfile
ls -l sgidfile If the execute permission is also required (thus displaying a lowercase ""s""), use: chmod 2674 sgidfile
ls -l sgidfile Combining SUID and SGID Combining SUID and SGID on a single file is straightforward. Since SUID is represented by 4 and SGID by 2, the combined digit is 6. For example, to apply both on a file called both : touch both
chmod 6664 both
ls -l both To efficiently locate files using these special permissions, use the find command: # Find files with the SUID bit set
find . -perm /4000

# Find files with the SGID bit set
find . -perm /2000

# Find files with either SUID or SGID (or both) set
find . -perm /6000 Sticky Bit The Sticky Bit is a special permission applied primarily to directories to control file deletion. When set, it restricts file deletion within the directory so that only the file owner, the directory owner, or the superuser can delete or rename files. This is especially beneficial for shared directories where multiple users have write access but should not be able to remove files created by others. Demonstration of the Sticky Bit Creating a directory with a Sticky Bit is illustrated below: # Set the Sticky Bit using a shorthand command
chmod 1777 stickydir/
ls -ld stickydir/
# Expected output: drwxrwxrwt 2 jeremy jeremy 4096 May 8 01:29 stickydir/ In the permission output, a lowercase ""t"" signifies that the Sticky Bit is active along with the execute permission. If the execute permission is revoked (for example, by setting mode 1666 ), the indicator changes to an uppercase ""T"": chmod 1666 stickydir/
ls -ld stickydir/ Interpreting the Sticky Bit A lowercase ""t"" denotes that the Sticky Bit is set and execute permission is enabled, while an uppercase ""T"" indicates that only the Sticky Bit is set. Conclusion Understanding and properly configuring SUID, SGID, and the Sticky Bit is crucial for managing permissions in Unix/Linux environments. With SUID, programs can execute with the file owner's privileges; SGID facilitates group-controlled execution and inheritance; and the Sticky Bit secures shared directories against unauthorized file deletions. By leveraging these permissions, system administrators can implement controlled privilege escalations while ensuring robust security. For further reading, check out the Kubernetes Documentation and explore how secure permissions integrate within broader system management practices. Happy experimenting, and enjoy the power of controlled permission management! Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Read and Use System Documentation,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/Read-and-Use-System-Documentation,"Linux Foundation Certified System Administrator (LFCS) Essential Commands Read and Use System Documentation Linux commands offer a plethora of command-line switches, making it challenging to remember every option. As you use a command regularly, you naturally learn the available options. However, during your initial attempts, it’s common to forget them after only a few tries. That’s why Linux offers several ways to access built-in help manuals and documentation directly from the command line. For example, when using the long listing format with the ls command, you might not recall if the correct option is -P or something else. Instead of guessing, you can instantly refer to the help text by typing: $ ls --help
Usage: ls [OPTION]... [FILE]...
List information about the FILEs (the current directory by default).
Sort entries alphabetically if none of -cftuvSUX nor --sort is specified.

Mandatory arguments to long options are mandatory for short options too.
  -a, --all               do not ignore entries starting with .
  -A, --almost-all        do not list implied . and ..
  -B, --ignore-backups    do not list implied entries ending with ~
  -I, --ignore=PATTERN    do not list implied entries matching shell PATTERN
  -k, --kibibytes         default to 1024-byte blocks for disk usage
  -l                     use a long listing format
  -c                     with -lt: sort by, and show, ctime (time of last
                         modification of file status information); Scrolling through the output, you can quickly locate the specific option you need (for example, the -L flag). Notice how all the command-line options are neatly sorted alphabetically and come with concise descriptions. Viewing Help and Listing Directories Below is an example workflow where you use the help option and then list the directory contents: $ ls --help
Usage: ls [OPTION]... [FILE]...
List information about the FILEs (the current directory by default).
Sort entries alphabetically if none of -cftuvSUX nor --sort is specified.

Mandatory arguments to long options are mandatory for short options too.
  -a, --all              do not ignore entries starting with .
  -A, --almost-all       do not list implied . and ..
  -B, --ignore-backups   do not list implied entries ending with ~
  -I, --ignore=PATTERN   do not list implied entries matching shell PATTERN
  -k, --kibibytes        default to 1024-byte blocks for disk usage
  -l                    use a long listing format
  -c                     with -lt: sort by, and show, ctime (time of last
                        modification of file status information);
                        with -l: show ctime and sort by name;
                        otherwise: sort by ctime, newest first

$ ls -l
bin/    libexec/    sbin/
lib/    local/      share/ Using the --help flag is especially useful when you momentarily forget an option—even for commands featuring numerous switches. Detailed Command Help with journalctl For more complex commands, you may need a comprehensive explanation. Take journalctl for example, a command used to read system logs. Entering journalctl --help will provide detailed information: $ journalctl --help
journalctl [OPTIONS...] [MATCHES...]

Query the journal.

Options:
  --system                Show the system journal
  --user                  Show the user journal for the current user
  -M --machine=CONTAINER  Operate on local container
  -S --since=DATE         Show entries not older than the specified date
  -U --until=DATE         Show entries not newer than the specified date
  -C --cursor=CURSOR      Show entries starting at the specified cursor
      --after-cursor=CURSOR  Show entries after the specified cursor
      --show-cursor       Print the cursor after all the entries
  -b --boot[=ID]         Show current boot or the specified boot
      --list-boots       Show terse information about recorded boots When you run this command, the output is presented in a pager, allowing you to scroll through the text with the arrow keys or page up/page down, and by pressing Q, you can exit the viewer. $ journalctl --help
journalctl [OPTIONS...] [MATCHES...]
Query the journal.
Options:
  --system              Show the system journal
  --user                Show the user journal for the current user
  -M --machine=CONTAINER Operate on local container
  -S --since=DATE       Show entries not older than the specified date
  -U --until=DATE       Show entries not newer than the specified date
  -c --cursor=CURSOR    Show entries starting at the specified cursor
  --after-cursor=CURSOR Show entries after the specified cursor
  --show-cursor         Print the cursor after all the entries
  -b --boot[=ID]       Show current boot or the specified boot
  --list-boots          Show terse information about recorded boots Accessing Manual Pages (man) Every significant command in Linux comes with its own manual (""man"") page. To access a command's manual, type: $ man journalctl This displays a brief description, a synopsis outlining the command syntax, and a detailed explanation of how the command works. For instance: $ man journalctl

JOURNALCTL(1)

NAME
    journalctl - Query the systemd journal

SYNOPSIS
    journalctl [OPTIONS...] [MATCHES...]

DESCRIPTION
    journalctl may be used to query the contents of the systemd(1) journal as written by systemd-journald.service(8).

    If called without parameters, it will show the full contents of the journal, starting with the oldest entry collected.

    If one or more match arguments are passed, the output is filtered accordingly. A match is in the format ""FIELD=VALUE"", e.g. ""_SYSTEMD_UNIT=httpd.service"", referring to structured journal entry components. See systemd.journal-fields(7) for a list of well-known fields. When multiple matches cover different fields, the log entries are filtered by all; if two matches apply to the same field, they function as alternatives (logical OR). The character ""+"" can also be used as a separator to combine matches in a disjunction. Many manual pages also conclude with practical examples. For example, the journalctl man page provides examples such as: man journalctl

EXAMPLES
Without arguments, all collected logs are shown unfiltered:

journalctl

With one specified match, only the log entries matching the expression are shown:

journalctl _SYSTEMD_UNIT=avahi-daemon.service

When matching two different fields, only entries that satisfy both conditions are displayed simultaneously:

journalctl _SYSTEMD_UNIT=avahi-daemon.service _PID=28097

If two matches refer to the same field, entries matching any of the specified values are shown:

journalctl _SYSTEMD_UNIT=avahi-daemon.service _SYSTEMD_UNIT=dbus.service

Using the separator ""+"", you can combine expressions with a logical OR. For example, the command below displays all messages from the Avahi service process with PID 28097 along with all messages from the D-Bus service regardless of its process:

journalctl _SYSTEMD_UNIT=avahi-daemon.service _PID=28097 + _SYSTEMD_UNIT=dbus.service Tip Use manual pages to deepen your understanding of each command’s options and usage. This practice is especially beneficial during exams or when you need a quick refresher. Distinguishing Similar Man Pages Sometimes, two man pages may share the same command name. For example, there is both a command and a programming function for printf . The manual categorizes these pages into sections. You can observe this by checking the man page for man itself: To view the manual page for the printf command itself, specify section 1: $ man 1 printf For the printf function (used in programming), specify section 3: $ man 3 printf During online exams, the Linux Foundation allows the use of both man and the --help option, so remember to utilize them if you forget a command option. $ man man
$ man 1 printf
$ man 3 printf Delving into a manual page might take some time, but it is invaluable in understanding the inner workings of a command. Using apropos to Find Commands What if you can’t remember the name of the command you need? The apropos command searches the man page database for keywords in their short descriptions. For instance, to search for man pages related to directories, you might enter: $ apropos director
directory        directories
$ apropos director
director: nothing appropriate If you encounter an error, it might be because the mandb database hasn’t been created or updated. You can generate or update it manually with: $ sudo mandb After updating the database, running: $ apropos director will list relevant entries, such as mkdir . Keep in mind that apropos displays entries from all sections—including system calls from section 2—which may be overly advanced for everyday use. Since common commands are usually located in sections 1 and 8, you can filter the results using the -s option: $ apropos -s 1,8 director
ls (1)                    - list directory contents
ls (1p)                   - list directory contents
mcd (1)                   - change MSDOS directory
mdeltree (1)              - recursively delete an MSDOS directory and its contents
mdir (1)                  - display an MSDOS directory
mdu (1)                   - display the amount of space occupied by an MSDOS direc...
mkdir (1)                 - make directories
mkdir (1p)                - make directories
mkdir (2)                 - create a directory
mkdir (3p)                - make a directory relative to directory file descriptor
mkdirat (2)               - create a directory This filtering can help you quickly locate the specific command you need. Command Auto-Completion Another useful feature in Linux is command auto-completion. When you type a command like systemctl and press TAB, the terminal automatically completes the command. For example: $ systemctl Many commands provide auto-completion suggestions for additional arguments. For instance, after entering systemctl followed by a space, pressing TAB twice might display options such as: $ systemctl 
add-requires           emergency           isolate              poweroff            show
add-wants             enable              is-system-running     preset              show-environment
cancel                exit                kexec                reboot              start
cat                   force-reload        kill                 reenable            status
condreload            get-default         link                 reload             stop
condrestart           halt                help                 reload-or-restart   suspend
condstop              list-dependencies   list-jobs           rescue              switch-root Note that auto-completion may not list every single option. For example, if you type: $ systemctl list-dependencies and then press TAB, the command might auto-complete the remaining text. Auto-completion isn’t limited to commands—it also applies to file and directory names. For example: $ ls /usr/
bin/
lib/
libexec/
local/
sbin/
share/ For long filenames like wordpress_archive.tgz , you might only have to type the first few characters (e.g., wor ) and press TAB to complete the name. Hands-On Practice with man and --help While help pages and manuals are invaluable, the initial learning curve might be steep. A good exercise is to choose a command you’re less familiar with and learn it exclusively by using man and the --help option. This method is especially useful during exams when theoretical questions may probe knowledge you may have forgotten. Proficiency in quick look-ups using manual pages can save you significant time. Consider additional command options often found in manual pages, such as those for grep: -I        equivalent to --binary-files=without-match
-d, --directories=ACTION      how to handle directories;
                                ACTION is 'read', 'recurse', or 'skip'
-D, --devices=ACTION          how to handle devices, FIFOs and sockets;
                                ACTION is 'read' or 'skip'
-r, --recursive                recurse like --directories=recurse
-R, --dereference-recursive    likewise, but follow all symlinks
--include=GLOB                search only files that match GLOB (a file pattern)
--exclude=GLOB                skip files that match GLOB
--exclude-from=FILE           skip files that match any file pattern from FILE
--exclude-dir=GLOB            skip directories that match GLOB
-L, --files-without-match      print only names of FILEs with no selected lines
-l, --files-with-matches       print only names of FILEs with selected lines
-c, --count                    print only a count of selected lines per FILE
-T, --initial-tab              make tabs line up (if needed)
-Z, --null                     print 0 byte after FILE name

Context control:
-B, --before-context=NUM       print NUM lines of leading context
-A, --after-context=NUM        print NUM lines of trailing context
-C, --context=NUM              print NUM lines of output context
-NUM                           print NUM lines between matches with context
--group-separator=SEP         print SEP on line between matches with context
--no-group-separator          do not print separator for matches with context
--color=[WHEN],               use markers to highlight the matching strings;
    --colour=[WHEN]           WHEN is 'always', 'never', or 'auto'
-U, --binary                   do not strip CR characters at EOL (MSDOS/Windows)

When FILE is '-', read standard input. With no FILE, read '.' if recursive, '!' otherwise. With fewer than two FILEs, assume -h.
Exit status is 0 if any line is selected, 1 otherwise; if any error occurs and -q is not given, the exit status is 2.

Report bugs to: [email protected] .
GNU grep home page: <https://www.gnu.org/software/grep/>
General help using GNU software: <https://www.gnu.org/gethelp/>
aaron@kodekould:~$ By practicing with these manual pages, you’ll develop the skills to quickly look up details and gain a deep understanding of options—an essential ability for effective Linux system administration and for successfully passing exams. Final Tip Regularly using the man and help commands can significantly improve your proficiency, ensuring you’re well-prepared for any Linux system task or exam challenge. Watch Video Watch video content Practice Lab Practice lab"
Linux Foundation Certified System Administrator LFCS,Search for Files,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/Search-for-Files,"Linux Foundation Certified System Administrator (LFCS) Essential Commands Search for Files In this article, we explore various strategies to search for files in Linux using the versatile find command. Linux systems typically organize files in a structured manner—for example, SSH configuration files are usually found in /etc/ssh , while log files are maintained in /var/log . Even if you often know the file location, search commands can be invaluable for unexpected cases and complex queries. Let's dive into some common scenarios. Finding Specific File Types Imagine you have a website and need to find all JPEG images stored in /usr/share . The find command can easily list all files ending in .jpg : $ find /usr/share/ -name '*.jpg'
1.jpg 2.jpg 3.jpg Searching by File Size In another scenario, if your server hosting virtual machines is nearly out of disk space, you might need to identify unusually large files. Although this example does not contain files larger than 20 GB, you can modify the search to find files over a specified size. For example, to search for files larger than 10 megabytes in /lib64 : $ find /lib64/ -size +10M
large-file.txt Searching by Modification Time When you update an application, you might want to identify which files were modified in the last minute. You can use the -mmin flag to achieve this: $ find /dev/ -mmin -1
abc.txt Using the find command in these examples shows how powerful it is for locating files based on a variety of parameters. Basic Usage To search for a file by name within a specific directory, use the -name parameter. For example: $ find /bin -name file1.txt If no path is specified, find defaults to searching in the current directory. Always place the search path before the search parameters. An incorrect command such as: $ find -name file1.txt /bin may yield unexpected results. Think of it like entering your room before looking for your keys. Tip Always specify the search path before your parameters to ensure accurate search results. Detailed Search Parameters Case Sensitivity The -name option is case sensitive. For example, searching for ""felix"" will not match a file named ""Felix"". Use -iname for a case-insensitive search: $ find -name felix
$ find -iname felix Wildcard Patterns Wildcards let you search for files matching a particular pattern. To find all files that begin with a lowercase ""f"": $ find -name ""f*"" The asterisk (*) acts as a wildcard matching zero or more characters. Time-Based Searches Search for files based on their modification time using the -mmin option, which is measured in minutes. Examples include: -mmin 5 : Finds files modified exactly 5 minutes ago. -mmin -5 : Finds files modified in the last 5 minutes. -mmin +5 : Finds files modified more than 5 minutes ago. Be aware that if it is currently 12:05, -mmin 5 targets files modified at 12:01. To include files modified at 12:00, use -mmin +5 . For searches based on days, the -mtime option is used. For instance: $ find -mtime 2 -mtime 0 matches files modified in the last 24 hours. -mtime 1 matches files modified between 24 and 48 hours ago. Important File modification time measures content changes, not the file creation time. Change Time Versus Modification Time Linux differentiates between modification time (when file contents change) and change time (when file metadata, such as permissions, are altered). The find command can help locate files that recently had their metadata changed—a useful feature if you're troubleshooting permission issues. Size-Based Searches To search for files based on file size, the -size option is used. For example, to find files that are exactly 512 KB: $ find -size 512k Here, suffixes indicate the following: No suffix: bytes K: kilobytes M: megabytes G: gigabytes (note that M and G must be uppercase) To search for files greater than or less than a specific size, prefix the size with a + or - sign respectively: $ find -size +512k  # Files greater than 512 KB
$ find -size -512k  # Files less than 512 KB Combining Multiple Parameters You can combine multiple parameters to narrow your search. For example, to find files that start with the letter ""f"" and are exactly 512 KB: $ find -name ""f*"" -size 512k The parameters are combined using an implicit AND operator. To use an OR operator, you can insert -o between conditions. Negation in Searches To exclude files that meet a certain condition, use the NOT operator ( -not or \! ). For instance, to find all files that do not start with the letter ""f"": $ find -not -name ""f*""
$ find \! -name ""f*"" Attention Remember that in the Bash shell, the exclamation mark (!) has a special meaning. Escaping it with a backslash (i.e., \! ) ensures it is interpreted literally. Permission-Based Searches The -perm option lets you search for files based on their file permissions. For example, to find files with exactly 664 permissions (user: read/write, group: read/write, others: read): $ find -perm 664  # Exact match for 664 permissions To find files that have at least these permissions, precede the value with a hyphen: $ find -perm -664 Alternatively, use a slash ( / ) to match any of the listed permissions: $ find -perm /664 Here are a few more examples: To find files that only the owner can read and write (600): $ find -perm 600 To locate files where the owner has at least execute permission: $ find -perm -100 To find files that others are not allowed to read: $ find \! -perm -o=r Files that cannot be read by anyone will be excluded from the search results. Summary The find command is a powerful tool for locating files based on various criteria such as name, size, modification time, and permissions. By understanding and combining these search parameters, you can efficiently manage and troubleshoot your Linux system. For more in-depth information, refer to the following documentation: Linux find Command Documentation Kubernetes Documentation Docker Hub Watch Video Watch video content Practice Lab Practice lab"
Linux Foundation Certified System Administrator LFCS,Create Delete Copy and Move Files and Directories,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/Create-Delete-Copy-and-Move-Files-and-Directories,"Linux Foundation Certified System Administrator (LFCS) Essential Commands Create Delete Copy and Move Files and Directories This guide demonstrates how to manage files and directories on Linux by creating, deleting, copying, and moving them. Before diving into the commands, ensure you understand these key concepts: File system tree structure Absolute paths Relative paths Listing Files and Directories To list files and directories—including hidden files (those starting with a dot)—use the ls -la command. The -a flag stands for ""all"" and the -l flag provides a detailed, long listing format. Here are some examples: $ ls -la
$ ls -a To list files for a specific directory, include the directory path: $ ls /var/log
$ ls -l /var/log The command ls -l /var/log displays detailed information such as file permissions, ownership, and modification dates. Combining options is also possible: $ ls -ahl
total 76K
drwx------ 16 aaron aaron  4.0K Nov  1 17:57 .
drwxr-xr-x  7 root  root   70 Oct 26 16:54 ..
-rw-------  1 aaron aaron  5.0K Nov  1 17:56 .bash_history
-rw-r--r--  1 aaron aaron   18 Jul 27 09:21 .bash_logout
-rw-r--r--  1 aaron aaron  141 Jul 27 09:21 .bash_profile
-rw-r--r--  1 aaron aaron  376 Jul 27 09:21 .bashrc
drwxr-xr-x  2 aaron aaron    6 Oct 19 00:11 Desktop
drwxr-xr-x  3 aaron aaron   25 Oct 23 18:15 Documents
drwxr-xr-x  2 aaron aaron    6 Oct 19 00:11 Downloads
drwxr-xr-x  2 aaron aaron    6 Oct 19 00:11 Music
drwxr-xr-x  2 aaron aaron    2 Oct 19 00:11 Pictures
-rw-rw-r--  1 aaron aaron   36 Oct 28 20:06 testfile Tip Using combined options like ls -ahl makes it quicker to view comprehensive file information. Understanding the File System Tree Linux organizes files and directories in what’s known as a file system tree. In this inverted hierarchy, the root is at the top and branches (subdirectories) and leaves (files) extend downward. The root directory is represented by a forward slash ( / ), and it forms the base for other essential directories such as home , var , and etc . Absolute and Relative Paths Absolute Paths Absolute paths start from the root directory ( / ) and specify the complete location of a file or directory. For example, consider the following absolute path leading to a file named Invoice.pdf : /home/aaron/Documents/Invoice.pdf Relative Paths Relative paths are defined in relation to your current working directory. To display your current directory, use the pwd command: $ pwd By default, you usually start in your home directory (e.g., /home/Aaron or /home/Jane ). Changing directories can be done using the cd command: $ cd /var/log   # Uses an absolute path
$ cd ..         # Moves up one directory level Here, .. refers to the parent directory. For example, if you are in /home/Aaron , executing cd .. will take you to /home . Relative Path Examples Assume your current directory is /home/Aaron : To access a file inside the Documents subdirectory: $ Documents/Invoice.pdf To access a file in the current directory: $ Invoice.pdf To access a file located one directory above: $ ../Invoice.pdf You can chain .. to move up multiple levels (e.g., ../../Invoice.pdf moves up two levels). Additional directory commands: Switch to the root directory: $ cd / Return to your previous directory: $ cd - Return to your home directory: $ cd Creating Files and Directories To create a new file, use the touch command. For example, to create receipt.pdf in the current directory: $ touch receipt.pdf You can create a file in another location using an absolute or relative path: $ touch /home/Jane/receipt.pdf
$ touch ../Jane_receipt.pdf To create a new directory, use the mkdir command: $ mkdir receipts Copying Files and Directories The cp command copies files. You provide a source file and specify a destination. For instance, to copy receipt.pdf to the receipts directory: $ cp receipt.pdf receipts/ Adding a trailing slash (i.e., receipts/ ) specifies that the destination is a directory. To copy and rename a file simultaneously: $ cp receipt.pdf receipts/receipt_copy.pdf To copy an entire directory with its contents, use the -r (recursive) flag: $ cp -r Receipts/ BackupOfReceipts/ Important Ensure the destination directory (e.g., BackupOfReceipts/ ) does not already exist when using recursive copying, as behavior may vary across systems. Moving and Renaming Files and Directories The mv command is used for moving files between directories or renaming them. Here are some examples: To move a file into another directory: $ mv Receipt.pdf Receipts/ To rename a file: $ mv receipt.pdf old_receipt.pdf To rename a directory: $ mv receipts old_receipts Note that you don't need a recursive flag when moving directories; mv handles them by default. Deleting Files and Directories To delete files, use the rm command. For example, to remove Invoice.pdf : $ rm Invoice.pdf To remove a directory and all its contents, apply the -r flag: $ rm -r invoices Caution Always double-check the command before running rm -r to avoid accidental deletion of important files or directories. With these commands and explanations, you are now equipped with a solid foundation for managing files and directories in Linux. For additional Linux command cheatsheets and best practices, consider exploring more guides and tutorials online. Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Prerequisites Exam Objectives and Details,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Introduction/Prerequisites-Exam-Objectives-and-Details,"Linux Foundation Certified System Administrator (LFCS) Introduction Prerequisites Exam Objectives and Details In this article, we review the essential information needed to prepare for the Linux Foundation Certified System Administrator (LFCS) exam. Whether you're new to the certification process or looking to validate your existing skills, this guide is here to help you succeed. Exam Overview No prerequisites are required for the LFCS exam. Candidates with relevant Linux administration skills are welcome to apply. The exam focuses on practical, real-world tasks rather than multiple-choice questions. Exam Objectives The LFCS exam is structured around five key sections, each covering vital aspects of system administration. Below is an overview of the objectives along with their respective weight in the overall score: Exam Objective Weightage (%) Essential Commands 20% Operation Deployment 25% Users and Groups 10% Networking 25% Storage 20% The course content is organized to reflect these objectives, ensuring that you gain comprehensive knowledge across all essential areas. Exam Details Duration: 2 hours Format: Performance-based, simulating real on-the-job tasks Question Style: No multiple-choice or true/false questions Cost: $395 Certification Validity: 2 years Proctoring: Online, allowing you to take the exam from home Next Steps More details about the registration process and further exam preparation will be discussed later in this article. Make sure to review these guidelines carefully to maximize your exam readiness. Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Compare and Manipulate File Content,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/Compare-and-Manipulate-File-Content,"Linux Foundation Certified System Administrator (LFCS) Essential Commands Compare and Manipulate File Content In this article, we explore essential Linux command-line tools for comparing and manipulating file content. Because Linux relies heavily on text—whether for SSH sessions, configuration files, or log files—mastering these commands will improve your efficiency in managing and troubleshooting your system. Viewing File Content To inspect a small file quickly, use the cat command with the filename. For example: $ cat /home/users.txt
user1
user2
user3
user4
user5
user6 If you prefer to see the contents in reverse order (from bottom to top), the tac command is available: $ tac /home/users.txt
user6
user5
user4
user3
user2
user1 For longer files, such as logs, it is often more practical to view only a portion of the file. The tail command displays the last 10 lines by default, which is useful for checking the most recent log entries. Conversely, the head command shows the beginning of a file. For instance, consider a log file with 10 lines (keep in mind that empty lines might also count): $ head /var/log/apt/term.log
Log started: 2024-03-11 04:41:37
Selecting previously unselected package libyaml2:amd64.
(Readi ng database ... 118768 files and directories currently installed.)
Preparing to unpack .../libyaml2_2.1.0-3ubuntu0.22.04.1_amd64.deb ...
Unpacking libyaml2:amd64 (2.1.0-3ubuntu0.22.04.1) ...
Selecting previously unselected package libvirt0:amd64.
Preparing to unpack .../libvirt0_8.0.0-1ubuntu7.8_amd64.deb ...
Unpacking libvirt0:amd64 (8.0.0-1ubuntu7.8) ...
Selecting previously unselected package libvirt-clients. You can also control the number of lines displayed by using the -n option with both tail and head . Automating Text Replacement with SED Editing multiple instances manually in large files can be error-prone and time-consuming. The Stream Editor (SED) automates search and replace tasks efficiently. For example, if a file listing user details has the country ""Canada"" misspelled as ""canda"", you can preview the correction with: $ sed 's/canda/canada/g' userinfo.txt Let's break down the command: s/canda/canada/g : The substitute command where canda is replaced with canada globally on each line. Single quotes ensure Bash does not interpret special characters. The -g flag replaces all occurrences in each line. Once you're satisfied with the preview, apply the change in-place: $ sed -i 's/canda/canada/g' userinfo.txt Note Always back up your files before performing in-place edits with sed -i . It is important to quote the expression correctly to prevent Bash from misinterpreting special characters such as the asterisk. Both single and double quotes can be used: $ sed ""s/canda/canada/g"" userinfo.txt Extracting Data with Cut The cut command is ideal for extracting specific columns from a file. For example, to extract the first column—which often contains names—from a space-separated file, use: $ cut -d ' ' -f 1 userinfo.txt Here, -d ' ' sets the delimiter to a space, while -f 1 specifies that the first field should be extracted. If the file is comma-separated, simply adjust the delimiter. For instance, to extract the third field (which could represent country names) and save the output to countries.txt , run: $ cut -d ',' -f 3 userinfo.txt > countries.txt In this command, the redirection operator ( > ) saves the extracted output to a new file. Removing Duplicate Entries After extracting data—like a list of countries—you might encounter duplicate entries. The uniq command removes duplicates from adjacent lines. For example: $ uniq countries.txt
usa
canada
usa
canada To remove duplicates effectively, sort the file first so that similar lines are adjacent, then pipe the output to uniq : $ sort countries.txt | uniq
canada
usa Piping ( | ) is a powerful technique that allows you to pass the output from one command directly into another for further processing. Comparing Files with Diff When system upgrades or configuration changes modify files, comparing the old and new versions is crucial. The diff command highlights these differences. Consider the following example: $ diff file1 file2
1c1
< only exists in file 1
---
> only exists in file 2
4c4
< only exists in file 1
---
> only exists in file 2 In this output, the notation 1c1 indicates that line 1 of file1 differs from line 1 of file2 . The < symbol shows content from file1 , while > represents content from file2 . For more context, use the -c option: $ diff -c file1 file2
*** file1	2021-10-28 20:39:43.083264406 -0500
--- file2	2021-10-28 20:40:02.900262846 -0500
**************
** 1,4 ****
! only exists in file 1
  identical line 2
  identical line 3
! only exists in file 1
--- 1,4 ----
! only exists in file 2
  identical line 2
  identical line 3
! only exists in file 2 For a side-by-side visual comparison, use the -y option: $ diff -y file1 file2
only exists in file 1        | only exists in file 2
identical line 2            | identical line 2
identical line 3            | identical line 3
only exists in file 1 Alternatively, you can use sdiff for a similar side-by-side comparison: $ sdiff file1 file2
only exists in file 2
identical line 2
identical line 3
exists in file 2 Tip Using the diff command with different options ( -c , -y , or sdiff ) can help you pinpoint changes more easily during system upgrades or when troubleshooting configuration issues. Summary This guide introduced a variety of Linux commands— cat , tac , head , sed , cut , sort , uniq , and diff —that are invaluable for viewing, editing, and comparing file content. Mastery of these tools not only streamlines your workflow but also enhances your ability to manage and debug files in any Linux environment. For more detailed explanations and advanced use cases, consider exploring additional Linux command-line resources . Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Create and Manage Hard Links,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/Create-and-Manage-Hard-Links,"Linux Foundation Certified System Administrator (LFCS) Essential Commands Create and Manage Hard Links In this article, we'll explore how Linux manages hard links and why they are beneficial for efficient file management. Understanding hard links requires a basic knowledge of file systems and inodes. File System Basics Imagine a Linux system used by two distinct users, Aaron and Jane. Each user logs in with their own credentials, which provide personalized desktops, settings, and file directories. Suppose Aaron takes a picture of the family dog and saves it as: /home/aaron/pictures/family_dog.jpg To simulate the file creation, we use the following command, which writes a description (acting as the file's content) into the file: $ echo ""Picture of Milo the dog"" > Pictures/family_dog.jpg When you inspect the file with the stat command, it shows details that include the inode number: $ stat Pictures/family_dog.jpg
  File: Pictures/family_dog.jpg
  Size: 49              Blocks: 8          IO Block: 4096   regular file
Device: fd00h/64768d Inode: 52946177   Links: 1
Access: (0640/-rw-r-----)  Uid: ( 1000/ aaron)   Gid: ( 1005/ family)
Access: 2021-10-27 16:33:18.949749912 -0500
Modify: 2021-10-27 14:41:19.202778881 -0500
Change: 2021-10-27 16:33:18.851749919 -0500
Birth: 2021-10-26 13:37:17.980969655 -0500 In Linux, every file is represented by an inode—a data structure that stores metadata (like permissions, modification times, and data block locations). While the inode number is the technical reference, we use the file name (in this case, family_dog.jpg ) to map to that inode. Notice the output indicates ""Links: 1"", meaning there is a single hard link (the original file name) associated with the inode. Creating and Using Hard Links Hard links allow you to reference the same data from different locations without duplicating file content. This is especially useful if you want to share data without unnecessarily consuming additional disk space. Consider Jane, who has her own pictures directory at /home/jane/pictures. Instead of copying family_dog.jpg from Aaron's directory, which duplicates the file data, you can create a hard link. This avoids the overhead of duplicating thousands of high-resolution images. While the typical copy command might be: $ cp -r /home/aaron/Pictures/ /home/jane/Pictures/ you can create a hard link using the following syntax: $ ln /home/aaron/Pictures/family_dog.jpg /home/jane/Pictures/family_dog.jpg After creating the hard link, both file paths reference the same inode. Running the stat command now will show the file has two hard links: $ stat Pictures/family_dog.jpg
File: Pictures/family_dog.jpg
Size: 49         Blocks: 8          IO Block: 4096   regular file
Device: fd00h/64768d Inode: 52946177  Links: 2
Access: (0640/-rw-r-----)  Uid: ( 1000/ aaron)   Gid: ( 1005/ family)
Access: 2021-10-27 16:33:18.949749912 -0500
Modify: 2021-10-27 14:41:19.20278881 -0500
Change: 2021-10-27 16:33:18.851749919 -0500
Birth: 2021-10-26 13:37:17.980969655 -0500 Persistence of Data If one user deletes their reference (hard link) to the file, the data remains accessible through the other link. The file data is only removed when the last hard link is deleted. Deleting Hard Links For example, if Aaron removes his file: $ rm /home/aaron/Pictures/family_dog.jpg Jane still has access via her hard link. However, if Jane then deletes the file as well: $ rm /home/jane/Pictures/family_dog.jpg the filesystem marks the data blocks as free, and from the user's perspective, the file is gone. Managing Permissions with Hard Links Since hard links share the same inode, any permission changes on one link are reflected on all links. To ensure both Aaron and Jane have correct access to the file, you might add them to the same group (for example, ""family"") and adjust the file's permissions accordingly: $ usermod -a -G family aaron
$ usermod -a -G family jane
$ chmod 660 /home/aaron/Pictures/family_dog.jpg These permission changes apply to all hard links referencing the inode, ensuring consistent access. Limitations of Hard Links There are a few limitations to be aware of when working with hard links: Hard links can only be created for files, not directories. Hard links must reside on the same file system; you cannot create a hard link from one file system to another. Limitation Reminder Remember, attempting to create a hard link across different file systems or for directories will result in an error. Summary Hard links allow multiple directory entries to reference the same file data without duplicating storage capacity. This method provides an efficient way to share files between users while ensuring that the data remains accessible until all links are removed. For further reading on Linux file systems and inode management, consider visiting Linux File System Hierarchy . Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Course Structure,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Introduction/Course-Structure,"Linux Foundation Certified System Administrator (LFCS) Introduction Course Structure Welcome to the Linux Foundation Certified System Administrator Preparation Course. In this lesson, Jeremy Morgan and Alexandru Andrei will guide you through the essential concepts and skills needed to become a proficient Linux system administrator. The Linux Foundation developed this certification to address the increasing demand for skilled Linux administrators. Earning this certification not only validates your expertise in system administration but also helps you stand out in a competitive job market. Course Domains This course is organized into five main domains designed to cover every critical aspect of Linux system administration: Essential Commands Master basic commands to log into Linux systems and work efficiently with files and directories. Operations Deployment Delve into the system boot process, automate tasks, and manage critical processes and resources. Users and Groups Develop skills to create and manage user and group accounts, set resource quotas, and configure advanced authentication options. Networking Learn about network services, routing, and packet filtering to effectively manage and troubleshoot network configurations. Storage Management Explore logical volume management (LVM), RAID configurations, encrypted storage options, and advanced file system permissions. Hands-On Learning Approach This course emphasizes a practical approach to learning, featuring engaging videos, interactive labs, and mock exams to ensure you gain the hands-on experience necessary for success. Keep in mind that the practical assessments require extensive hands-on practice to build both confidence and proficiency. Note Practical experience is essential. Regularly revisiting the labs will help reinforce key concepts and build your expertise. Course Format The course is structured as follows: 30% Videos – Engaging video lessons to explain and demonstrate fundamental concepts. 60% Labs – Real-world lab exercises designed to give you hands-on experience. 10% Mock Exams – Practice exams to test your knowledge and readiness. In total, you will have access to approximately 30 hours of lab sessions, which can be revisited as often as needed to build and maintain your confidence and technical skills. This comprehensive structure ensures that you are well-prepared for both the Linux Foundation Certified System Administrator exam and real-world system administration challenges. Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Log in to Local Remote Graphical and Text Mode Consoles,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/Log-in-to-Local-Remote-Graphical-and-Text-Mode-Consoles,"Linux Foundation Certified System Administrator (LFCS) Essential Commands Log in to Local Remote Graphical and Text Mode Consoles In this lesson, we explore four methods to log into a Linux system—similar to signing in to websites or apps with a username and password. The four methods covered include: Local Text Mode Console Local Graphical Mode Console Remote Text Mode Login Remote Graphical Mode Login You may encounter terms such as console, virtual terminal, and terminal emulator. Despite their historical origins, today's explanation is straightforward: A console is the display screen where Linux outputs text and accepts commands. A terminal emulator is a graphical application that replicates console functionality within a window. Historically, when computing resources were scarce and expensive, a powerful central computer was connected to multiple remote terminals, allowing simultaneous use by multiple users. Today, both consoles and terminal emulators are implemented in software. For instance, during the Linux boot process, you will see vital system events displayed as text: [  OK  ] Reached target sound.target - Sound Card.
[  OK  ] Finished systemd-binfmt.service - Set Up Additional Binary Formats.
[  OK  ] Finished apparmor.service - Load AppArmor profiles.
         Starting snapd.apparmor.service - Load AppArmor profiles managed internally by snapd...
[  OK  ] Started systemd-timesyncd.service - Network Time Synchronization.
[  OK  ] Reached target time-set.target - System Time Set.
[  OK  ] Started systemd-resolved.service - Network Name Resolution.
[  OK  ] Reached target nss-lookup.target - Host and Network Name Lookups.
[  OK  ] Finished snapd.apparmor.service - Load AppArmor profiles managed internally by snapd.
[  OK  ] Reached target sysinit.target - System Initialization.
[  OK  ] Started apt-daily.timer - Daily apt download activities.
[  OK  ] Started apt-daily-upgrade.timer - Daily apt upgrade and clean activities.
[  OK  ] Started dpkg-db-backup.timer - Daily dpkg database backup timer.
[  OK  ] Started e2scrub_all.timer - Periodic ext4 Online Metadata Check for All Filesystems.
[  OK  ] Started fstrim.timer - Discard unused filesystem blocks once a week.
[  OK  ] Started fwupd-refresh.timer - Refresh fwupd metadata regularly.
[  OK  ] Started logrotate.timer - Daily rotation of log files.
[  OK  ] Started man-db.timer - Daily man-db regeneration.
[  OK  ] Started motd-news.timer - Message of the Day.
[  OK  ] Started systemd-tmpfiles-clean.timer - Daily Cleanup of Temporary Directories.
[  OK  ] Reached target paths.target - Path Units.
[  OK  ] Listening on dbus.socket - D-Bus System Message Bus Socket.
[  OK  ] Listening on iscsiuio.socket - Open-iSCSI iscsid Socket.
[  OK  ] Listening on snap.lxd.daemon.unix.socket - Socket unix for snap application lxd.daemon.
[  OK  ] Listening on snap.lxd.user-daemon.unix.socket - Socket unix for snap application lxd.user-daemon.
[  OK  ] Listening on ssh.socket - OpenBSD Secure Shell server Socket.
[  OK  ] Listening on uuidd.socket - UUID daemon activation socket.
[  OK  ] Listening on snapd.socket - Socket activation for snappy daemon.
[  OK  ] Reached target sockets.target - Socket Units.
[  OK  ] Reached target basic.target - Basic System.
[  OK  ] Starting dbus.service - D-Bus System Message Bus...
[  OK  ] Started dmesg.service - Save initial kernel messages after boot.
         Starting e2scrub_reap.service - Remove Stale Online ext4 Metadata Check Snapshots... After booting, you can access a virtual terminal by pressing Control + Alt + F2 on your keyboard. This action brings up a text-based interface for login. If you are using a Linux desktop with a graphical user interface (GUI), you will typically use a terminal emulator to issue commands. The login interface changes based on whether a GUI is available: For a GUI system, you will see a graphical login screen displaying a list of users. For systems without a GUI (like many servers), a text console login prompt will request your username and password (the password is hidden as you type). To log out of these sessions, simply type exit . Remote graphical connections are available, though set up differently. Administrators may configure VNC (Virtual Network Computing) or RDP (Remote Desktop Protocol) for remote GUI access. For VNC, use a compatible client like VNC Viewer or RealVNC. For RDP, open the Remote Desktop Connection application on Windows and log in with your credentials. Remote text-based sessions use OpenSSH, which runs the SSH daemon (sshd) to provide secure, encrypted remote login—making it a preferred alternative to older, unsecured methods like Telnet. Quick Tip Remember: SSH encrypts all login data, ensuring that your session and credentials remain safe. If you're following along using a virtual machine, first log in locally. Once logged in, open your terminal emulator and run the following command to view your current IP configuration: $ ip a
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: enp0s3: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc fq_codel state UP group default qlen 1000
    link/ether 08:00:27:6b:d7:87 brd ff:ff:ff:ff:ff:ff
    inet 192.168.0.17/24 brd 192.168.0.255 scope global dynamic noprefixroute enp0s3
       valid_lft 1966sec preferred_lft 1966sec
    inet6 fe80::a00:27ff:fe6b:d787/64 scope link noprefixroute In this output, locate your IP address (e.g., 192.168.0.17). This address is used to simulate connecting to a remote server that is running the SSH daemon. Most Linux distributions (and macOS) include an SSH client by default. For Windows 10 or 11 users, an SSH client is pre-installed. To connect to the remote system via SSH, run the command below—replace ""aaron"" with your username: $ ssh [email protected] [email protected] 's password:
Last login: Tue Oct 19 20:27:15 2021 from 192.168.0.3
[aaron@kodekloud ~]$ Once connected, your SSH session remains active, allowing you to execute commands and manage the remote system. Next Step Join the upcoming demonstration lesson to see these login methods in action! For more detailed information on secure shell protocols and remote system management, consider reviewing the Kubernetes Documentation or Docker Hub . Happy learning! Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Search File Using Grep,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/Search-File-Using-Grep,"Linux Foundation Certified System Administrator (LFCS) Essential Commands Search File Using Grep In this lesson, you'll learn how to use the grep utility to search for specific text within files. Grep is particularly useful when working with large files containing thousands of lines, as it helps you quickly filter and locate the information you need. Basic grep Usage The general syntax for the grep command is: grep [options] [search pattern] [file] For example, to search for the term ""password"" inside the SSH daemon configuration file, run: $ grep 'password' /etc/ssh/sshd_config
#PermitRootLogin prohibit-password
# To disable tunneled clear text passwords, change to no here!
# Change to yes to enable challenge-response passwords (beware issues with
# the setting of ""PermitRootLogin without-password"". Tip Always enclose your search pattern in single quotes to prevent Bash from misinterpreting special characters, such as asterisks used in more complex search patterns. Case Sensitivity By default, grep performs a case-sensitive search. This means that searching for 'password' will only match lowercase occurrences. Searching for 'Password' (with an uppercase P) produces different results. Consider the following examples: $ grep 'password' /etc/ssh/sshd_config
#PermitRootLogin prohibit-password
# To disable tunneled clear text passwords, change to no here!
# Change to yes to enable challenge-response passwords (beware issues with
# PasswordAuthentication. Depending on your PAM configuration,
# PAM authentication, then enable this but set PasswordAuthentication $ grep 'Password' /etc/ssh/sshd_config
#PasswordAuthentication yes
#PermitEmptyPasswords no
# PasswordAuthentication. Depending on your PAM configuration,
# PAM authentication, then enable this but set PasswordAuthentication To perform a case-insensitive search, use the -i option: $ grep -i 'password' /etc/ssh/sshd_config
#PermitRootLogin prohibit-password
# To disable tunneled clear text passwords, change to no here!
#PasswordAuthentication yes
#PermitEmptyPasswords no
# Change to yes to enable challenge-response passwords (beware issues with
# PasswordAuthentication. Depending on your PAM configuration,
# the setting of ""PermitRootLogin without-password"".
# PAM authentication, then enable this but set PasswordAuthentication Recursive Search in Directories Grep allows you to search through all files within a directory and its subdirectories using the -r (recursive) option. For example: $ grep -r 'password' /etc/
/etc/fwupd/redfish.conf:# The username and password to the Redfish service
grep: /etc/sudoers.d/README: Permission denied
grep: /etc/ssl/private: Permission denied
/etc/ssl/openssl.cnf:# input_password = secret
/etc/ssl/openssl.cnf:# output_password = secret
/etc/ssl/openssl.cnf:challengePassword        = A challenge password
/etc/cloud/cloud.cfg: - set-passwords The matched files are highlighted by default (using color) to help you quickly identify the search term. You might also see ""Permission denied"" messages for files your user does not have permission to read. Combine recursive search with case-insensitivity using both -r and -i options: $ grep -ri 'password' /etc/
/etc/fwupd/redfish.conf:# The username and password to the Redfish service
/etc/fwupd/redfish.conf:#Password=
/etc/fwupd/remotes.d/lvsf-testing.conf:#Password=
grep: /etc/sudoers.d/README: Permission denied
grep: /etc/ssl/private: Permission denied
/etc/ssl/openssl.cnf:# Passwords for private keys if not present they will be prompted for
/etc/ssl/openssl.cnf:  input_password = secret
/etc/ssl/openssl.cnf:  output_password = secret
/etc/ssl/openssl.cnf:challengePassword    = A challenge password
/etc/ssl/openssl.cnf:challengePassword_min  = 4
/etc/ssl/openssl.cnf:challengePassword_max  = 20
/etc/ssl/openssl.cnf:[pbm] # Password-based protection for Insta CA
/etc/cloud/cloud.cfg: - set-passwords Using sudo for Restricted Files If you need to search through files that are only accessible by the administrator (root), prepend the grep command with sudo. Note that this might disable colored output, so the --color option can be used to force it: $ sudo grep -ri 'password' --color /etc/
/etc/fwupd/redfish.conf:# The username and password to the Redfish service
/etc/fwupd/redfish.conf:#Password=
/etc/fwupd/remotes.d/lvfs-testing.conf:#Password=
/etc/ssl/openssl.cnf:# Passwords for private keys if not present they will be prompted for
/etc/ssl/openssl.cnf:# input_password = secret
/etc/ssl/openssl.cnf:# output_password = secret
/etc/ssl/openssl.cnf:challengePassword        = A challenge password
/etc/ssl/openssl.cnf:challengePassword_min    = 4
/etc/ssl/openssl.cnf:challengePassword_max    = 20
/etc/ssl/openssl.cnf:[pbm] # Password-based protection for Insta CA
/etc/cloud/cloud.cfg:  - set-passwords Important When using sudo with grep, be aware that forcing colored output is necessary if you want to visualize matches in color. Inverting the Search If you need to display lines that do not contain a specific pattern, use the -v option. This inverts the search results to exclude the matched pattern. Matching Whole Words Sometimes it's important to match only the exact word ""password"" rather than substrings (like ""PasswordAuthentication"" or ""passwords""). The -w option restricts grep to whole word matches. Compare the outputs below: $ grep -i 'password' /etc/ssh/sshd_config
#PermitRootLogin prohibit-password
# To disable tunneled clear text passwords, change to no here!
#PasswordAuthentication yes
#PermitEmptyPasswords no
# Change to yes to enable challenge-response passwords (beware issues with
# PasswordAuthentication. Depending on your PAM configuration,
# the setting of ""PermitRootLogin without-password"". $ grep -wi 'password' /etc/ssh/sshd_config
#PermitRootLogin prohibit-password
# the setting of ""PermitRootLogin without-password"". Notice that using the -w option excludes lines where ""password"" is part of a longer word. Displaying Only the Matched Portions By default, grep prints the entire line containing a match. If you need to extract just the text that matches your pattern, use the -o option. This is useful when you want to isolate specific data. The following example compares two approaches: $ grep -i 'password' /etc/ssh/sshd_config
#PermitRootLogin prohibit-password
# To disable tunneled clear text passwords, change to no here!
#PasswordAuthentication yes
#PermitEmptyPasswords no
# Change to yes to enable challenge-response passwords (beware issues with
# PasswordAuthentication. Depending on your PAM configuration,
# the setting of ""PermitRootLogin without-password"".
# PAM authentication, then enable this but set PasswordAuthentication $ grep -oi 'password' /etc/ssh/sshd_config
password
password
Password
Password
password
Password
password
Password Using the -o option outputs only the matching portions of each line, simplifying further data processing. Conclusion Grep is a versatile and powerful tool for searching text within files. Its many options—such as case insensitivity, recursive searching, whole word matching, and output customization—make it an essential utility for efficiently filtering and extracting information. Happy grepping! For more technical guides and tips, check out our Comprehensive Unix/Linux Tutorials . Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Compress and Uncompress Files Optional,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/Compress-and-Uncompress-Files-Optional,"Linux Foundation Certified System Administrator (LFCS) Essential Commands Compress and Uncompress Files Optional In this article, you will learn how to compress and uncompress files in Linux. File compression reduces storage space and speeds up file transfers between systems. Most Linux distributions come with at least three built-in compression utilities: gzip, bzip2, and xz. Basic Compression with gzip and bzip2 Below is a simple example of compressing files with gzip and bzip2: $ gzip file1
$ bzip2 file2 When these commands are executed, each file is compressed into a new file (e.g., file1 becomes file1.gz and file2 becomes file2.bz2) and the original file is automatically deleted. Decompressing Files To restore the original files, use the corresponding decompression commands for gzip, bzip2, and xz: $ gunzip file1.gz
$ bunzip2 file2.bz2
$ unxz file3.xz Retaining the Original File Sometimes you may want to compress a file without deleting the original. Most Linux compression utilities offer an option to keep the original file. Use the --help flag to check available options: $ gzip --help
Usage: gzip [OPTION]... [FILE]...
Compress or uncompress FILEs (by default, compress FILES in-place).

Mandatory arguments to long options are mandatory for short options too.
  -c, --stdout       write on standard output, keep original files unchanged
  -d, --decompress   decompress
  -f, --force        force overwrite of output file and compress links
  -h, --help         give this help
  -k, --keep         keep (don't delete) input files
  -l, --list         list compressed file contents
  -L, --license      display software license
  -N, --no-name      do not save or restore the original name and timestamp
  -n, --name         save or restore the original name and timestamp
  -q, --quiet        suppress all warnings
  -r, --recursive    operate recursively on directories
  --rsyncable        make rsync-friendly archive
  -S, --suffix=SUF   use suffix SUF on compressed files
  --synchronous      synchronous output
  -t, --test         test compressed file integrity
  -v, --verbose      verbose mode
  -V, --version      display version number Note Using the --keep (or -k ) option retains the original file when compressing. Here are examples of using the --keep option: $ gzip --keep file1
$ bzip2 --keep file2
$ xz --keep file3 Inspecting Compressed File Contents If you need to view the contents of a compressed file, you can use the --list option. For example: $ gzip --list file1
compressed        uncompressed      ratio   uncompressed_name
71                78               39.7%   file1 Working with the zip Utility While gzip, bzip2, and xz are popular for compressing individual files, the zip utility can compress an entire directory or multiple files into a single archive. Archiving a Single File with zip $ zip archive.zip file1
adding: file1 (deflated 40%) Compressing an Entire Directory $ zip -r archive.zip Pictures/
adding: Pictures/ (stored 0%)
adding: Pictures/family_dog.jpg (stored 0%) To extract a zip archive, use the following command: $ unzip archive.zip
Archive:  archive.zip
replace file1? [y]es, [n]o, [A]ll, [N]one, [r]ename: N Using tar for Archiving and Compression Unlike zip, utilities like gzip do not support compressing multiple files into a single archive. Instead, tar is commonly used to bundle files together before compression. Creating a Tar Archive and Compressing it $ tar --create --file archive.tar file1
$ gzip archive.tar
# This creates archive.tar.gz If you want to keep the uncompressed tar archive along with the compressed file, use the --keep option: $ gzip --keep archive.tar
# This results in both archive.tar and archive.tar.gz Modern versions of tar allow you to create and compress an archive in a single step. The compression utility is automatically selected based on the file extension: $ tar --create --gzip --file archive.tar.gz file1
$ tar --create --bzip2 --file archive.tar.bz2 file1
$ tar --create --xz --file archive.tar.xz file1
$ tar --create --autocompress --file archive.tar.gz file1 During extraction, tar automatically detects the compression type, eliminating the need to specify a decompression method. Summary This article provided an overview of various techniques for compressing and uncompressing files in Linux using utilities such as gzip, bzip2, xz, and zip. You also learned how to use tar for archiving and compressing multiple files. These methods help efficiently manage file sizes and streamline file transfers. For more detailed information on Linux file management and compression techniques, explore additional resources like Kubernetes Basics or visit Linux Documentation . Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Back Up Files to a Remote System Optional,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/Back-Up-Files-to-a-Remote-System-Optional,"Linux Foundation Certified System Administrator (LFCS) Essential Commands Back Up Files to a Remote System Optional In this guide, we'll explain how to back up files on Linux using built-in tools. Although advanced backup utilities exist, this article focuses on a straightforward, efficient method to copy files from one system to another's backup location. A widely used tool for this process is rsync. The name ""rsync"" is derived from its core functionality—remote synchronization. It allows you to synchronize a directory from one system with a corresponding directory on another system over a network connection. For instance, you might sync ""/some/directory"" on server 1 with ""/some/other/directory"" on server 2. Remember, the remote server must have an SSH daemon running so that rsync can transfer data securely. The basic syntax for rsync is as follows: The -a (archive) option ensures that subdirectories, file permissions, modification times, and other attributes are synchronized. Always add a trailing forward slash (""/"") at the end of directory paths to explicitly indicate that you are syncing the contents of the directory rather than the directory itself. For example, to synchronize a local pictures directory with a pictures directory on a remote server, use the following command: rsync -a /path/to/local/pictures/ [email protected] :/path/to/remote/pictures/ In the command above: • ""Aaron"" is the username on the remote system. • ""9.9.9.9"" is the IP address of the remote server. • The directory after the colon specifies the destination directory on the remote system. An advantage of using rsync is that on subsequent runs, it only transfers changed data. This incremental approach can significantly speed up future backups by skipping files that haven't been modified. You can also reverse the source and destination to copy data from a remote directory back to a local directory. Additionally, rsync works for synchronizing between two local directories. Note If you're looking to create a bit-for-bit copy of an entire disk or partition, consider using the dd utility instead of rsync. The dd command creates an exact clone (or image) of a disk, making it ideal for tasks such as creating full disk backups. To ensure data consistency, always unmount the disk or partition before running dd, so that no data is altered during the imaging process. Below is an example of using dd to create a disk image. Since accessing disk data requires elevated permissions, the command is prefixed with sudo: sudo dd if=/dev/vda of=diskimage.raw bs=1M status=progress In this command: • if=/dev/vda specifies the input file (i.e., the source disk or partition). • of=diskimage.raw specifies the output file where the disk image is saved. • bs=1M sets the block size to 1 megabyte, which can improve read-write speeds compared to default, smaller sizes. • status=progress displays real-time progress information during the operation. To restore a disk image from a file back to a disk, simply reverse the input and output options as shown below: sudo dd if=diskimage.raw of=/dev/vda bs=1M status=progress Warning Avoid running these dd commands on a virtual machine unless you are absolutely sure you want to overwrite the virtual disk, as this operation can result in data loss. This guide demonstrated how to use native Linux tools—rsync for file synchronization and dd for full disk imaging—to perform backup operations in a simple and efficient way. Related Resources Tool Use Case Example Command rsync Incremental backup of files/directories rsync -a /source/directory/ username@remote:/destination/directory/ dd Full disk or partition imaging sudo dd if=/dev/sda of=diskimage.raw bs=1M status=progress For further reading, consider visiting Linux Backup Strategies and dd Command Tutorial . Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Install Software by Compiling Source Code,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Operations-Deployment/Install-Software-by-Compiling-Source-Code,"Linux Foundation Certified System Administrator (LFCS) Operations Deployment Install Software by Compiling Source Code In this lesson, we will explore how to install software by compiling its source code. Unlike installations from Ubuntu's repositories or third-party packages, compiling from source allows you to run the absolute latest version of an application available directly from its development repository. When you compile software, you transform human-readable source code into an optimized binary executable that your computer can run efficiently. Clone the Repository Before getting started, ensure that Git is installed on your system. Most modern versions of Ubuntu and Linux come with Git pre-installed. In this example, we will clone the Git repository for an application called htop. Run the command below to clone the repository: git clone <GitHub-URL-of-htop-repository> After cloning, switch to the project's directory and review its contents. You'll notice a README file that includes built-in instructions for building the project, along with other key files. Install Dependencies According to the README file, several libraries and compilation tools are required. One essential package is libncursesw5-dev , which is a development library, indicated by the -dev suffix. Additionally, the build-essential package installs the necessary compilation tools. Install the required packages by running: sudo apt install libncursesw5-dev autotools-dev autoconf automake build-essential Tip If you encounter dependency errors, verify that your software repositories are updated and consider running sudo apt update before installing. Run the Autogen Script After installing the dependencies, locate the executable script named autogen.sh (often highlighted in green). This script generates the configure file, which prepares the build configuration options for your project. To run the autogen script, execute: bash ./autogen.sh The execution output should look similar to: autoreconf: export WARNINGS=all
autoreconf: Entering directory '.'
autoreconf: configure.ac: not using Gettext
autoreconf: running: aclocal --force
autoreconf: configure.ac: tracing
autoreconf: configure.ac: creating directory build-aux
autoreconf: configure.ac: not using Libtool
autoreconf: configure.ac: not using Intltool
autoreconf: configure.ac: not using Gtkdoc
autoreconf: running: /usr/bin/autoconf --force
autoreconf: running: /usr/bin/autoheader --force
autoreconf: running: automake --add-missing --copy --force-missing
configure.ac:69: installing 'build-aux/compile'
configure.ac:23: installing 'build-aux/config.guess'
configure.ac:23: installing 'build-aux/config.sub'
configure.ac:24: installing 'build-aux/install-sh'
configure.ac:24: installing 'build-aux/missing'
Makefile.am: installing './INSTALL'
Makefile.am: installing 'build-aux/depcomp'
autoreconf: Leaving directory '.' This generated configure script allows you to set various build options. To view all available configuration options, run: bash
./configure --help Running ./configure without arguments will configure the project using default settings. Verify Configuration Requirements Before building the project, the configuration script checks for the presence of necessary libraries and compiler support. Below is an example snippet of these checks: checking for stdlib.h... (cached) yes
checking for string.h... (cached) yes
checking for strings.h... (cached) yes
checking for sys/param.h... yes
checking for sys/time.h... yes
checking for sys/utsname.h... yes
checking for unistd.h... (cached) yes
checking for sys/mkdev.h... no
checking for sys/sysmacros.h... yes
checking for execinfo.h... yes
checking for mbstate_t... yes
checking for mode_t... yes
checking for off_t... yes
checking for pid_t... yes
checking for size_t... yes
checking for ssize_t... yes
checking how to run the C preprocessor... gcc -E
checking for grep that handles long lines and -e... /usr/bin/grep
checking for egrep... /usr/bin/grep -E
checking for uid_t in sys/types.h... yes
checking for uint8_t... yes
checking for uint16_t... yes
checking for uint32_t... yes
checking for uint64_t... yes
checking for alloc_size... yes
checking for access... yes
checking for nonnull... yes
checking for NaN support... yes
checking for library containing ceil... | Once all checks pass, you are ready to compile the application using make . Compile the Application The make command compiles the source code into an executable binary. To start the compilation process, run: make If you wish to see a list of all available make targets (such as clean , install , and uninstall ), type make followed by a space and press the Tab key twice. These common targets help in managing build artifacts and installation processes. For example, if the build process fails or you need to start from a clean state, you can run: make clean After successfully running make , the htop binary is compiled in the current directory. Run the Compiled Application After compilation, you can run the newly created htop executable directly from the current directory: ./htop Upon running htop , you should see an interface similar to the following: Main          I/O
PID     USER      PR NI    VIRT    RES  SHR S  CPU% MEM% TIME+     Command
13535   jeremy    20  0  15120   6956  4992 S   2.0  0.1  0:06.53   sshd: jeremy@pts/0
19840   jeremy    20  0   8000   4224  3456 R   1.3  0.1  0:00.04   /htop
24200   root      20  0  58560   4200  23304 S   0.7  0.3  0:13.96   /usr/lib/systemd/systemd-journald
... Press Q to quit the application interface. Install the Binary System-wide Typing the full path to run the executable each time can be cumbersome. To streamline the process, you can install the application system-wide. This action moves the htop binary into a directory included in your system's PATH (typically /usr/local/bin ), allowing you to run it from any location. Install htop system-wide by executing: sudo make install Now, you can simply type: htop For example, navigate to your home directory and run: cd ~
htop Because /usr/local/bin is in the default PATH for Ubuntu, you no longer need to specify the full path to launch the application. SEO Tip Compiling software from source can provide enhanced performance and access to the latest features. Always refer to the project's README for specific build instructions and troubleshooting tips. Conclusion In this lesson, we covered the process of compiling and installing software from source code. The steps included: Cloning the repository Installing necessary dependencies Generating configuration scripts via autogen.sh Verifying configuration requirements Compiling the project using make Running and installing the binary system-wide By compiling from source, you ensure that you are running the most recent version of the application. Happy compiling, and see you in the next lesson! htop Watch Video Watch video content Practice Lab Practice lab"
Linux Foundation Certified System Administrator LFCS,Verify Integrity and Availability of Resources and Processes,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Operations-Deployment/Verify-Integrity-and-Availability-of-Resources-and-Processes,"Linux Foundation Certified System Administrator (LFCS) Operations Deployment Verify Integrity and Availability of Resources and Processes In this lesson, we explain how to verify the integrity and availability of key system resources and processes. Over time, servers tend to use more resources—storage space may fill up as databases grow or users store more files. In cloud environments, adding storage devices is relatively straightforward, but knowing when your storage is near capacity is essential. Below, we provide detailed steps to monitor disk space, RAM, CPU load, file system integrity, and essential services. ───────────────────────────────────────────── Checking Disk Space with df The ""df"" (disk free) utility reports on disk usage. By default, sizes are shown in 1-kilobyte blocks, which can be difficult to interpret. Use the ""-h"" option to display sizes in a human-readable format (MB, GB, TB). Example output of ""df"": $ df
Filesystem                                   1K-blocks      Used Available Use% Mounted on
tmpfs                                         400588      1112    399476   1% /run
/dev/mapper/ubuntu--vg-ubuntu--lv         10218772   4070292   5607808  43% /
tmpfs                                        2002940         0   2002940   0% /dev/shm
tmpfs                                          5120         0      5120   0% /run/lock
/dev/vda2                                   1790136    256868   1424068  16% /boot
tmpfs                                         400588         4    400584   1% /run/user/1000 Using the human-readable option: $ df -h
Filesystem                                   Size  Used Avail Use% Mounted on
tmpfs                                        392M  1.1M  391M   1% /run
/dev/mapper/ubuntu--vg-ubuntu--lv             9.8G  3.9G  5.4G  43% /
tmpfs                                        2.0G  0.0G  2.0G   0% /dev/shm
tmpfs                                        5.0M  0.5M  0.5M   0% /run/lock
/dev/vda2                                    1.8G  251M  1.4G  16% /boot
tmpfs                                        392M  4.0K  392M   1% /run/user/1000 In the output above, filesystems labeled as ""tmpfs"" are virtual filesystems that only reside in memory. In this case, only two actual filesystems are in use: the root filesystem (mounted on ""/"") where the Linux operating system is installed, and the smaller ""/boot"" filesystem that holds boot files. To view the disk space used by a specific directory, use the ""du"" (disk usage) utility. By default, ""du"" lists the space used by the directory and all of its subdirectories. The ""-s"" (summarize) option provides output for the specified directory only, while ""-h"" displays sizes in a human-friendly format. $ du -sh /usr/
3.0G    /usr/ ───────────────────────────────────────────── Checking Memory Usage with free Monitoring RAM is as important as disk monitoring. The ""free"" command displays how much RAM is used and available, while the ""-h"" option renders sizes in a human-readable format (using mebibytes and gibibytes). $ free -h
              total        used        free      shared  buff/cache   available
Mem:          3.6Gi       1.0Gi       1.5Gi        15Mi       1.1Gi       2.4Gi
Swap:         2.0Gi          0B       2.0Gi In this example, the ""used"" value might appear high, but the ""available"" column indicates that 2.4 GiB of memory can still be reclaimed if necessary. Temporary memory used for caching large files does not prevent the memory from being available to applications. Tip Use the ""free -h"" command as a quick reference to ensure your system has adequate memory, especially when running high-load applications. ───────────────────────────────────────────── Analyzing CPU Load with uptime The ""uptime"" command provides key data about system load and uptime. The output shows three load average numbers representing the average system load over the last 1, 5, and 15 minutes. $ uptime
17:24:55 up 32 min,  1 user,  load average: 0.05, 0.05, 0.01 A load average of 1.0 over the last minute indicates that one CPU core has been fully utilized on average. For systems with multiple cores, a load average higher than the number of cores suggests some processes are waiting for CPU time. Consistently high load averages may signal the need to upgrade hardware or optimize running processes. ───────────────────────────────────────────── Checking File System Integrity Before checking a file system for errors, ensure it is unmounted. File system checks differ depending on whether your system uses XFS or ext4. For more detailed information on file systems, partitions, mounting, and unmounting, refer to our upcoming storage sections. File System Information Operating systems in the Red Hat family typically use the XFS file system by default, while Ubuntu systems use ext4. To verify an XFS file system, run: $ sudo xfs_repair -v /dev/vdb1
Phase 1 - find and verify superblock...
        - block cache size set to 175968 entries
Phase 2 - using internal log
        zero log...
zero_log: head block 103 tail block 103
        scan filesystem freespac and inode maps...
        found root inode chunk
Phase 3 - for each AG...
        scan and clear agi unlinked lists...
        process known inodes and perform inode discovery...
        agno = 0
        agno = 1
        agno = 2
        agno = 3
Phase 4 - check for duplicate blocks...
        setting up duplicate extent list...
        check for inodes claiming duplicate blocks...
        agno = 0
        agno = 1
        agno = 2
        agno = 3
Phase 5 - rebuild AG headers and trees...
        agno = 0
        agno = 1
        agno = 2
        agno = 3
Phase 6 - check inode connectivity...
        resetting contents of realtime bitmap and summary inodes
        traversing filesystem
        agno = 0
        agno = 1
        agno = 2
        agno = 3
Phase 7 - scan finished ...
        moving disconnected inodes to lost+found ...
done Here, ""/dev/vdb1"" is the partition containing the file system. Depending on your configuration, the device name might differ (for example, ""/dev/vda2"" or ""/dev/sdc3""). For an ext4 file system, use the fsck utility with ext4 options. The ""-v"" flag increases verbosity, ""-f"" forces a check even on healthy filesystems, and ""-p"" (preen mode) fixes simple issues automatically. $ sudo fsck.ext4 -v -f -p /dev/vdb2
11 inodes used (0.00%, out of 262144)
0 non-contiguous files (0.0%)
0 non-contiguous directories (0.0%)
# of inodes with ind/dind/tind blocks: 0/0/0
Extent depth histogram: 3
36942 blocks used (3.52%, out of 1048576)
0 bad blocks
1 large file

0 regular files
2 directories
0 character device files
0 block device files
0 fifos
0 links
0 symbolic links (0 fast symbolic links)
0 sockets
__________
2 files Hint Using the ""-p"" option with fsck.ext4 is especially useful when the file system has many errors, as it automates the repair process. ───────────────────────────────────────────── Verifying Key Processes and Services Ensuring that critical services are running is vital for system stability. The following command displays systemd unit dependencies in a tree-like structure, helping you visualize service relationships. $ systemctl list-dependencies
default.target
└─ apport.service
   ├─ display-manager.service
   ├─ systemd-update-utmp-runlevel.service
   ├─ udisks2.service
   └─ multi-user.target
       └─ anacron.service In the dependency tree, a green (active) circle indicates the service is running, while a white (inactive) circle means it isn’t. Some services only run briefly at boot (and then exit), but others—like ssh.service, cron.service, and atd.service—should remain active. To simulate an issue, let’s terminate the atd daemon: $ sudo pkill atd After terminating, check the dependencies to see that the atd service is now inactive: $ systemctl list-dependencies
multi-user.target
├─ atd.service
├─ console-setup.service
├─ cron.service
└─ dbus.service View the status of the terminated service for more details: $ systemctl status atd.service
○ atd.service - Deferred execution scheduler
   Loaded: loaded (/lib/systemd/system/atd.service; enabled; vendor preset: enabled)
   Active: inactive (dead) since Fri 2024-03-08 03:18:09 EET; 4min 10s ago
Mar 08 03:45:30 kodekloud systemd[1]: atd.service: Deactivated successfully. Since atd is configured to start on boot, restarting it should resolve the issue: $ sudo systemctl start atd.service If ""systemctl status"" logs do not clearly explain the issue, review the service logs with journalctl: $ journalctl -u atd.service This command helps you pinpoint the root cause if the service is failing to start. ───────────────────────────────────────────── Summary By using tools such as df, du, free, uptime, and file system check utilities like xfs_repair and fsck.ext4, alongside service monitoring commands like systemctl and journalctl, you can proactively verify the integrity and availability of your server’s resources and processes. This systematic monitoring helps ensure your systems run smoothly and alerts you early to potential issues. Happy monitoring! Watch Video Watch video content Practice Lab Practice lab"
Linux Foundation Certified System Administrator LFCS,Change Kernel Runtime Parameters Persistent and Non Persistent,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Operations-Deployment/Change-Kernel-Runtime-Parameters-Persistent-and-Non-Persistent,"Linux Foundation Certified System Administrator (LFCS) Operations Deployment Change Kernel Runtime Parameters Persistent and Non Persistent In this article, you will learn how to modify Linux kernel runtime parameters both temporarily (non-persistent) and permanently (persistent). Kernel runtime parameters control essential aspects of the Linux kernel, such as memory management, networking, and filesystem behavior. Viewing Current Kernel Parameters Kernel parameters influence how the system operates. You can see all active settings using: $ sysctl -a
fs.pipe-user-pages-hard = 0
fs.pipe-user-pages-soft = 16384
sysctl: permission denied on key 'fs.protected_fifos'
sysctl: permission denied on key 'fs.protected_hardlinks'
sysctl: permission denied on key 'fs.protected_regular' If you encounter permission issues, run the command with root privileges: $ sudo sysctl -a
net.ipv6.conf.default.addr_gen_mode = 0
net.ipv6.conf.default.autoconf = 1
net.ipv6.conf.default.dad_transmits = 1
net.ipv6.conf.default.disable_ipv6 = 0
net.ipv6.conf.default.disable_policy = 0
vm.admin_reserve_kbytes = 8192 Notice that naming conventions give clues about parameters’ purposes: Parameters beginning with net. relate to networking. Parameters starting with vm. pertain to virtual memory. Filesystem settings use the fs. prefix. Adjusting a Specific Kernel Parameter (Non-Persistent) Let’s consider the parameter net.ipv6.conf.default.disable_ipv6 . A value of 0 indicates that IPv6 is enabled. To disable IPv6 temporarily, change its value to 1: $ sudo sysctl -w net.ipv6.conf.default.disable_ipv6=1
net.ipv6.conf.default.disable_ipv6 = 1 You can verify the updated setting with: $ sudo sysctl net.ipv6.conf.default.disable_ipv6
net.ipv6.conf.default.disable_ipv6 = 1 Note Non-persistent changes will revert upon reboot, reverting to the default values. To check a specific parameter without listing all parameters, append its name to the sysctl command. Use sudo if permissions are insufficient. Making Changes Persistent Persistent adjustments require adding a configuration file in the /etc/sysctl.d directory. These files must have a .conf extension, ensuring that your custom settings are applied automatically at system boot. For more details, refer to the manual page: $ man sysctl.d Listing the directory contents with ls /etc/sysctl.d will also display sample configuration files that serve as formatting examples. Filtering Specific Parameters To view only memory-related settings (prefixed with vm. ), use the following command: $ sysctl -a | grep vm
vm.panic_on_oom = 0
vm.percpu_pagelist_fraction = 0
vm.stat_interval = 1
vm.swappiness = 60 The parameter vm.swappiness , currently set to 60, controls swap behavior. A higher value increases swapping, while a lower value reduces swap usage. Making vm.swappiness Persistent To permanently change vm.swappiness to 20, follow these steps: Create a configuration file in /etc/sysctl.d . For example, name it swap-less.conf to indicate reduced swapping: $ sudo vim /etc/sysctl.d/swap-less.conf Add the following line to set the swappiness value: vm.swappiness=20 Save the file. Although the setting will be applied on boot, the current session continues using the old value until reloading the settings. To immediately apply your change, run: $ sudo sysctl -p /etc/sysctl.d/swap-less.conf Note Editing kernel parameters in /etc/sysctl.conf is an alternative, though this file may be overwritten during system upgrades. It is recommended to use /etc/sysctl.d for persistent customizations. Summary Table Configuration Scope Change Method Example Command Non-Persistent Temporary setting change sudo sysctl -w net.ipv6.conf.default.disable_ipv6=1 Persistent Create conf file in /etc/sysctl.d sudo vim /etc/sysctl.d/swap-less.conf <br> (add vm.swappiness=20) With these techniques, you can modify kernel runtime parameters for your Linux system effectively—using both non-persistent methods for immediate changes and persistent methods for settings that survive reboots. For further details on Linux kernel parameters, consider browsing related documentation provided by your Linux distribution or the official Linux Kernel Documentation . Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,List and Identify SELinux File and Process Contexts,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Operations-Deployment/List-and-Identify-SELinux-File-and-Process-Contexts,"Linux Foundation Certified System Administrator (LFCS) Operations Deployment List and Identify SELinux File and Process Contexts In this lesson, we explore how SELinux applies security contexts to both files and processes. Traditional Linux access controls such as file permissions and directory settings are often too coarse to handle sophisticated cyber threats. Although these basic permissions allow read, write, or execute actions, they typically do not enforce fine-grained restrictions. SELinux bridges this gap by introducing advanced security contexts that enable highly granular access control. Note SELinux is enabled by default on Red Hat-based operating systems, whereas it requires manual configuration on Ubuntu. We will discuss Ubuntu SELinux configuration in a future lesson. How SELinux Works at a High Level At its core, SELinux uses security contexts—extended metadata labels on files and processes—to determine which actions are permitted or denied. A typical SELinux file label includes four components in the following order: SELinux User (e.g., unconfined_u ) Role (e.g., object_r ) Type (e.g., user_home_t ) Level (e.g., s0 ) For example, the context label: unconfined_u:object_r:user_home_t:s0 indicates that the file is associated with the SELinux user unconfined_u , the role object_r , the type or domain user_home_t , and the security level s0 . SELinux’s decision-making process follows these steps: User Mapping : The system first checks the SELinux user, which may be different from the Linux login username. Each login is mapped to an SELinux user as specified by the policy. Role Verification : The system then determines if the user has the necessary permissions to assume a specific role. For example, a developer may be confined to using roles such as developer_r or docker_r , while roles like sysadmin_r remain off limits. Type-Based Access Control : The type component enforces the most granular security. When processes or files are assigned a specific type, they are confined to a strict set of permitted actions. Security Level : Although rarely used in standard configurations, the level field enables multi-level security controls for organizations with multiple clearance layers. Viewing SELinux File Labels While the ls -l command displays standard file permissions, SELinux adds an extra layer of metadata referred to as a security context or label. To view these additional details, use the -Z option: $ ls -lZ
-rw-rw-r--. 1 aaron aaron 160 Dec  1 18:19 archive.tar.gz In this output, the extra field represents the SELinux context. This label at the filesystem level enables SELinux to determine which actions are permitted or denied on the file. SELinux Process Contexts Just as files receive security contexts, processes are also assigned SELinux contexts. To view process contexts, use the ps command with the -Z option: $ ps axZ
system_u:system_r:accountsd_t:s0         995 ?    Ssl    0:00 /usr/libexec/accounts-daemon
system_u:system_r:NetworkManager_t:s0    1024 ?    Ssl    0:00 /usr/sbin/NetworkManager
system_u:system_r:sshd_t:s0-s0:c0.c1023   1030 ?    Ss     0:00 /usr/sbin/sshd -D -
system_u:system_r:tuned_t:s0             1032 ?    Ssl    0:00 /usr/libexec/platform-tuned
system_u:system_r:cupsd_t:s0-s0:c0.c1023   1033 ?    Ss     0:00 /usr/sbin/cupsd -l In the example above, notice that the SSH daemon operates within the sshd_t domain. Only executables labeled with sshd_exec_t (the type for the SSH daemon file) can run in this domain. To verify the SSH daemon file’s SELinux context, run: $ ls -Z /usr/sbin/sshd
system_u:object_r:sshd_exec_t:s0 /usr/sbin/sshd This strict relationship between file types and process domains ensures that if a process becomes compromised, it remains confined within its limited security domain, thereby minimizing potential damage. SELinux User Context and Role Mapping SELinux assigns a security context to each user session. Upon login, a Linux user is mapped to an SELinux user. You can view your current SELinux user context with: $ id -Z
unconfined_u:unconfined_r:unconfined_t:s0-s0:c0.c1023 Here, the logged-in user is mapped to unconfined_u along with an associated role and type that impose minimal restrictions by default. Most access control decisions are enforced by the role and type components. For further insight into user-to-role mappings, you can list the allowed logins and SELinux user configurations: $ sudo semanage login -l
Login Name      SELinux User       MLS/MCS Range     Service
__default__     unconfined_u       s0-s0:c0.c1023   *
root            unconfined_u       s0-s0:c0.c1023   * $ sudo semanage user -l
Labeling         MLS/Prefix         MLS/MCS Level     MCS Range
root             sysadm             s0                s0-s0:c0.c1023
staff_u          staff              s0                s0-s0:c0.c1023
sysadm_u         sysadm             s0                s0-s0:c0.c1023
system_u         user               s0                s0-s0:c0.c1023
unconfined_u     unconfined         s0                s0-s0:c0.c1023
user_u           user               s0                s0-s0:c0.c1023 These commands provide a clear view of how Linux logins are mapped to SELinux users along with their permitted roles and clearance levels. Checking SELinux Enforcement Status To determine whether SELinux is actively enforcing policies, use the following command: $ getenforce
Enforcing If the output is ""Enforcing,"" SELinux is actively blocking unauthorized actions. If the output shows ""Permissive,"" SELinux logs actions that would be denied but does not block them. A status of ""Disabled"" indicates that SELinux is currently inactive. Summary In summary, SELinux uses multi-layered security contexts—composed of user, role, type, and level—to enforce strict access controls on both files and processes. Even if a process is compromised, these stringent policies confine it to a safe security domain, significantly reducing potential system damage. Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Create and Enforce MAC Using SELinux,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Operations-Deployment/Create-and-Enforce-MAC-Using-SELinux,"Linux Foundation Certified System Administrator (LFCS) Operations Deployment Create and Enforce MAC Using SELinux In this lesson, we'll explore how to enforce Mandatory Access Control (MAC) using the SELinux security module. SELinux is enabled by default on Red Hat and CentOS systems, while Ubuntu uses AppArmor as its default security module. Before installing or configuring SELinux on a system set up by others, always verify which security module is active. In this guide, we assume that your Ubuntu system does not have SELinux enabled. We'll walk you through disabling AppArmor, installing SELinux tools, and configuring SELinux from scratch. Disabling AppArmor Since AppArmor is enabled by default on Ubuntu systems, you must stop its service to avoid conflicts with SELinux: sudo systemctl stop apparmor.service After stopping the service, ensure that AppArmor does not automatically start on boot. Installing SELinux and AuditD To get started with SELinux, install the SELinux utilities along with the AuditD package. The SELinux package includes essential tools and default security policies, and AuditD logs system events—a critical resource when building custom SELinux policies. Below is a sample installation output. Your output may vary: Selecting previously unselected package libgfortran5:amd64.
Preparing to unpack .../04-libgfortran5_14-20240412-0ubuntu1_amd64.deb ...
Unpacking libgfortran5:amd64 (14-20240412-0ubuntu1) ...
...
Progress: [ 28%] [##############################.....................] Similarly, when AuditD is configured, you might observe: Setting up python3-semanage (3.5-1build5) ...
...
Progress: [ 78%] [#########################.........................] Verifying and Activating SELinux To verify whether SELinux is enabled on your system, run: sestatus If SELinux is disabled, the output will resemble: jeremy@kodekLOUD:~$ sestatus
SELinux status:             disabled
jeremy@kodekLOUD:~$ You can also verify the current SELinux context assignments in the root directory: jeremy@kodekLOUD:~$ ls -Z /
?      bin              ?   lib64           ?   mnt             ?   run
?      boot             ?   home            ?   lost+found      ?   proc
... To enable SELinux, execute: sudo selinux-activate This command modifies the GRUB bootloader to include the parameter necessary to load the SELinux module at boot. The output will inform you about generating the GRUB configuration and activate SELinux—after which a system reboot is recommended. Configuring the Bootloader After activating SELinux, verify the bootloader settings. In the /etc/default/grub file, you should see the following line instructing the kernel to load SELinux: GRUB_CMDLINE_LINUX=""security=selinux"" Once you confirm the configuration, note the presence of an autorelabel file in the root filesystem. This file tells SELinux to relabel every file upon reboot. For example, listing the root directory including hidden files shows: jeremy@kodekLOUD:~$ ls -a /
.              bin.usr-is-merged  etc               lib.usr-is-merged  opt   sbin    swap.img  var
..             boot                home              lost+found        proc  sbin.usr-is-merged  root  snap
.autolabel    cdrom               lib               media             mnt   run     srv       tmp    usr Now, reboot your system to allow SELinux to relabel the filesystem. The relabeling process might take some time depending on the number of files: sudo reboot During the first boot after enabling SELinux, the bootloader pauses for 30 seconds to allow intervention if necessary. Once complete, subsequent boots will operate normally. After rebooting, verify SELinux status by running: sestatus Expected output: SELinux status:                 enabled
SELinuxfs mount:                /sys/fs/selinux
SELinux root directory:         /etc/selinux
Loaded policy name:             default
Current mode:                   permissive
Mode from config file:          permissive
... Understanding SELinux Modes SELinux operates in two primary modes: Permissive: In this mode, SELinux logs policy violations without enforcing them. It is ideal for ""training"" your policies. Enforcing: Here, SELinux actively enforces policies and may deny actions that do not comply with the defined rules. To check the current mode, run: getenforce When operating in permissive mode, actions that should be blocked are only logged. For instance, accessing via SSH may trigger AVC (Access Vector Cache) messages in the audit log. To inspect these messages, use: sudo audit2why --all | less The output might include entries like: type=AVC msg=audit(1716410944.847:111): avc: denied { execute } for pid=922 comm=""run-parts"" name=""1-landscape-sysinfo.wrapper"" dev=""dm-0"" ino=284986 scontext=system_u:system_r:sshd_t:s0 tcontext=system_u:object_r:usr_t:s0 tclass=file permissive=1
        Was caused by:
                Missing type enforcement (TE) allow rule.
        You can use audit2allow to generate a loadable module to allow this access. Note Although similar audit entries might be repeated, a single occurrence generally provides enough insight into the issue. Examining Process and File Contexts You can verify SELinux contexts for running processes and files. For example, to inspect the contexts of SSH daemons, run: ps -eZ | grep sshd_t Sample output: system_u:system_r:sshd_t:s0       905 ?        00:00:00 sshd
system_u:system_r:sshd_t:s0       906 ?        00:00:00 sshd
system_u:system_r:sshd_t:s0       1020 ?       00:00:02 sshd To view the context for the SSH daemon executable: ls -Z /usr/sbin/sshd Expected output: system_u:object_r:sshd_exec_t:s0 /usr/sbin/sshd When a file labeled with sshd_exec_t is executed, SELinux transitions the process into the sshd_t domain, where its type enforcement rules become active. Generating and Loading a Custom SELinux Policy After operating in permissive mode and gathering necessary logs, you can generate a custom SELinux policy module from the audit logs. Run: sudo audit2allow --all -M mymodule The command output will include a message similar to: ********************* IMPORTANT *********************
To make this policy package active, execute:
semodule -i mymodule.pp To load the custom policy module, execute: sudo semodule -i mymodule.pp Note If you see a warning such as ""libsemanage.add_user: user sddm not in password file"", it is a known issue. You can safely ignore it if you are not using sddm. Switching to Enforcing Mode To temporarily switch SELinux from permissive to enforcing mode, execute: sudo setenforce 1
getenforce Expected output: Enforcing For initial testing, it is advisable to use temporary enforcement. Once you are sure that your custom policies cover all the necessary actions, make the change permanent by modifying /etc/selinux/config . Open the configuration file with: sudo vim /etc/selinux/config Then change: SELINUX=permissive to: SELINUX=enforcing Save the file and reboot the system to apply the changes. Reviewing the Custom Policy Module After loading the custom module, two files are created: A binary package ( mymodule.pp ) A human-readable policy file ( mymodule.te ) The .te file contains the type enforcement rules. An excerpt might look like: module mymodule 1.0;
require {
    type sshd_t;
    type tmp_t;
    type getty_t;
    type systemd_journal_init_t;
    type policykit_t;
    type mount_exec_t;
    type kmod_t;
    type var_t;
    type systemd_journal_t;
    type systemd_runtime_notify_t;
    type xdg_cache_t;
    type fixed_disk_device_t;
    type fsadm_exec_t;
    type mount_runtime_t;
    type var_lib_t;
    type mount_t;
    type systemd_hostnamed_t;
    type var_log_t;
    type etc_runtime_t;
    type lib_t;
    type system_dbusd_t;
    type usr_t;
};
class dir { add_name search watch write };
class dbus send_msg; Review this file carefully before deploying the policies in production. Understanding and Reviewing SELinux Type Enforcement The custom module includes specific rules for the sshd_t domain. For instance, it allows the SSH daemon to access files labeled with var_log_t : allow sshd_t var_log_t:file { append create getattr ioctl open }; This rule ensures that processes running in the sshd_t domain, such as the SSH daemon, can write to log files like /var/log/auth.log , which are labeled as var_log_t . You can verify these contexts with the following commands: ps -eZ | grep sshd_t
ls -Z /var/log/auth.log
grep var_log_t:file mymodule.te Expected outputs: system_u:system_r:sshd_t:s0       905 ?        00:00:00 sshd
...
system_u:object_r:var_log_t:file /var/log/auth.log
...
allow sshd_t var_log_t:file { append create getattr ioctl open }; This level of fine-grained control helps restrict processes to specific files, reducing potential risks if a process (such as an NGINX web server) is compromised. Changing SELinux File Contexts with chcon and restorecon A file's SELinux context is composed of three parts: user, role, and type. To manually change a file’s context, use the chcon command. For example, to view and modify the context of /var/log/auth.log : ls -Z /var/log/auth.log
sudo chcon -u unconfined_u /var/log/auth.log To change the role and type: sudo chcon -r object_r /var/log/auth.log
sudo chcon -t user_home_t /var/log/auth.log Warning Manual changes made with chcon may be overridden during a complete filesystem relabel. To restore the default context, use the restorecon command. For example, to fix the context of /var/log/auth.log based on /var/log/dmesg as a reference: sudo chcon --reference=/var/log/dmesg /var/log/auth.log Fixing Contexts for Web Content If you create a new directory for your website under /var/www and add files, they may initially have an incorrect SELinux type (such as var_t ). To correct this recursively: sudo mkdir /var/www
sudo touch /var/www/{1..10}
ls -Z /var/www/
# Files may initially be labeled as:
# unconfined_u:object_r:var_t:s0
sudo restorecon -R /var/www/
ls -Z /var/www/ After running restorecon -R /var/www/ , the files should be relabeled correctly (for example, as httpd_sys_content_t ), which is appropriate for web content. Note that restorecon by default only restores the type, not the user or role. If needed, use the force option to restore all parts of the label. To permanently assign a default label to a specific file (e.g., /var/www/10 ), add a rule with semanage : sudo semanage fcontext --add --type var_log_t /var/www/10
# You might encounter:
# libsemanage.add_user: user sddm not in password file
sudo restorecon /var/www/10 For directories and their contents, remember to wrap the path in quotes if it contains special characters. Refer to the SELinux documentation for additional examples. SELinux Booleans and Port Bindings SELinux booleans act as switches to enable or disable related security policies. To list supported booleans and their current settings, run: getsebool -a | grep virt_use_nfs To set the virt_use_nfs boolean, execute: sudo setsebool virt_use_nfs 1 SELinux also manages which ports a daemon is allowed to bind to. To list all allowed port bindings: sudo semanage port --list For instance, SSH is typically restricted to port 22. To allow SSH to bind to an additional port (such as 2222), run: sudo semanage port --add --type ssh_port_t --proto tcp 2222 To later remove this port binding: sudo semanage port --delete --type ssh_port_t --proto tcp 2222 Note Any warnings regarding the user sddm may be safely ignored if you are not using sddm. Conclusion SELinux offers a robust mechanism for enforcing security policies by confining processes and controlling file access through detailed security contexts. While the setup and testing process can seem complex initially, running in permissive mode allows you to identify and adjust potential issues before switching to enforcing mode for full protection. With continuous study and practice, you can develop tailored SELinux policies that significantly reduce the risk of common attacks. Enjoy your journey to a more secure system! Watch Video Watch video content Practice Lab Practice lab"
Linux Foundation Certified System Administrator LFCS,Work With SSL Certificates,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/Work-With-SSL-Certificates,"Linux Foundation Certified System Administrator (LFCS) Essential Commands Work With SSL Certificates In this lesson, we explore SSL certificates—what they are for, how to create them, and how to inspect them. Although many still refer to these as SSL certificates, the correct term nowadays is TLS (Transport Layer Security). TLS is essentially an enhanced version of SSL, addressing many of its security vulnerabilities. Purpose of SSL/TLS Certificates SSL/TLS certificates play a critical role in securing website communication. They serve two primary functions: Authentication: Verifies you are communicating with the legitimate website rather than an impostor. For example, it ensures you are connecting to KodeKloud.com and not a fraudulent clone. Encryption: Safeguards the data exchanged between a user and the website by encrypting network traffic. Note The certificate, combined with its corresponding private key, secures the connection by encrypting all exchanged data. Creating Certificates with OpenSSL On Linux, OpenSSL is the most common utility for creating and managing TLS certificates. Even though its name still mentions “SSL,” it is fully compatible with creating modern TLS certificates. Consulting the manual with the command man openssl reveals that OpenSSL can perform an array of cryptographic operations beyond certificate generation. In this lesson we focus on creating X.509 certificates, which are essential for authenticating and encrypting website traffic. OpenSSL supports a broad range of subcommands and options. Here are some tips to get started. Discovering OpenSSL Subcommands To list the available subcommands, run one of the following commands in your terminal: openssl help or simply: openssl Look for the ""Standard Commands"" section. Of particular interest are: req for generating certificate requests. x509 for handling X.509 certificates. Because OpenSSL provides a separate manual for each subcommand, view the manual page for a specific subcommand by using: man openssl req Alternatively, you might try: man openssl-req While browsing the manual, press the slash key (/) and search for ""EXAMPLE"" to locate sections that show how to generate certificates. For example, these two commands generate a certificate signing request (CSR) with slight variations: openssl req -newkey rsa:2048 -keyout key.pem -out req.pem
openssl req -x509 -newkey rsa:2048 -keyout key.pem -out req.pem Press Q to exit the manual once finished. Certificate Signing Requests (CSR) The req subcommand is primarily used to generate certificate signing requests (CSRs). While certificates secure the connection, browsers must also verify that the certificate was signed by a trusted Certificate Authority (CA). When you generate a CSR, you send it to a CA (for example, Google) that will verify your details and sign your certificate. This digital signature allows browsers to confirm the certificate’s legitimacy. To generate a CSR along with a new private key using OpenSSL, use a command like: openssl req -newkey rsa:2048 -keyout key.pem -out req.pem Here’s what the command options do: -newkey rsa:2048 generates a new 2048-bit RSA private key. -keyout key.pem specifies the file to save your private key. -out req.pem designates the file for the CSR. After executing this command, OpenSSL will prompt you for a password for the private key and request additional information such as country, state, locality, organization, and more. You can then verify the contents with: cat key.pem and cat req.pem In real-world scenarios, the CSR is sent to a CA that verifies the details and, upon successful validation, returns a signed certificate. Below is an example CSR display: jeremy@kodekloud:~$ cat req.pem
-----BEGIN CERTIFICATE REQUEST-----
MIICjCCAWCAQAwTElMakG1UBEBhMVhMCzAJBgNVBAgM5ZREwDwYDVQQH
DHaOZxcgw9YazESMBAgA1UECgws29zKzB3VGYgAOCQ8AMIBCgKQEAom6zqRUBsPk75fJ7keM671dM0GvSGWsCvIuJaoZJwfjmPkD2v2O+etwKxrVczidIab9mdJ3h3ywZoleOUzU/iQrSv6hSrEw37HEmzvNzL+xkIxue/eaYBjoBUm9FQbOoMCm5tM5tpzA7o67mC+IDABAoABM3dvJzK5IUhvcN
AQELBQADgEBADx7Msu0fEu1Gy618kkiG1KJclDArsvLgtknzbFMKS1TczGPgKjqIAvAeS359wUcrGm5wEJAsCWGEwx/1z+bWvJWBOAx6usgz6caiYBXJ6Cpt
GVyyjLn+vEUw6J2RcY9M84JoQi22g68NxRwZUl6CUSHLCXTEFSF15h6jl/Q0l
wUD2QoNOx8SWJ/xZCdo0fWQ9mUHHrS3XdJxJaXjplAcotDr7d351JUptIIBZvJ
+tjGx44pi25CyYE/O12PbrRFnjMZ3xSW63nwVflaDpkDwaiy8rHDUr3KdAGx+bRjUvFn2oJ/Nm8DFVNhvQpK3eWm5T/I=
-----END CERTIFICATE REQUEST-----
jeremy@kodekloud:~$ Generating a Self-Signed Certificate In some cases—particularly on internal networks—you might prefer to generate a self-signed certificate rather than involve a Certificate Authority. Self-signed certificates can be appropriate where you have control over trusted certificates. To generate a self-signed TLS certificate valid for 365 days with a 4096-bit RSA key, use the following command: openssl req -x509 -noenc -newkey rsa:4096 -days 365 -keyout myprivate.key -out mycertificate.crt The above command’s options are explained below: -x509 : Instructs OpenSSL to generate an X.509 certificate directly rather than just a CSR. -noenc : Specifies that the private key should not be encrypted. (Some distributions use -nodes instead.) Warning In production environments, it is recommended to encrypt the private key. -newkey rsa:4096 : Generates a new 4096-bit RSA private key. -days 365 : Sets the validity period of the certificate to 365 days. -keyout myprivate.key : File to save the generated private key. -out mycertificate.crt : File to save the self-signed certificate. During execution, OpenSSL will prompt for certificate details such as Country Name, State or Province, Locality, Organization, and Common Name. For example: Country Name (2 letter code) [AU]:US
State or Province Name (full name) [Some-State]:Oregon
Locality Name (eg, city) []:Gaston
Organization Name (eg, company) [Internet Widgits Pty Ltd]:KodeKloud You can verify your certificate by inspecting its contents with: cat mycertificate.crt This file will include base64-encoded data with header and footer lines indicating the certificate's boundaries. Below is an example display of a CSR similar to what you might see when generating a self-signed certificate: jeremy@kodekloud:~$ cat req.pem
-----BEGIN CERTIFICATE REQUEST-----
MIICjCCAW4CAQAwCTELMAkGA1UEBhMCVVMxCzAJBgNVBAMMAktFVm1xMTAeFw0y
MTA2MTAwMDAwMDBaFw0yMjA2MTAwMDAwMDBaDAQBg8AT0z9QdeG7DIAgAB
AOCAPgM7FhMPOlW6N6Epvld5OTNXD4Ggkem67TIdM5vGScVuiJoaoZJwfEmpKDv2
f8QgBzqgYUX1eOuzMKV8d2kQOuYOSCRjn3rTmT7kHC2O2+etKxrVczidab9mdJ3
3hYzloeiUo/iyCaXS4HUIqXBP2pV80upt5sJ872iQrSv6hSrEw37HEmVzulXkUxe
/fAY8bJO/Um9FybOboMkpIQRgZ3AtO6r++IDAQABYJK0EihVnCVhcnlOqT0incV
AQEBLQADggEBADx/tMsu0FEuf1gv618kkiG1KVJcJdArsvLgtknzbFMKSlTczGP
gKjqIAvAeS359WUkcrGm5wEJAsCWGEwx/1z+bWvJWBYoAx6usgz6caiYBXJ6Cpt
GVvyj1Zn+vEUw6J2RcY9M84JoQi22l68NxRwZUl6CUSHLCXTEFSF15h6jl/Oql
wUD2qoNOx8SwJ+/xZCdo0w9tUHrXSxdxJaXJpLaacotDPr7d351JUptIIBSzVJ
+tjCgx44pi25CcyYE/O12PbrRFTJmZ3xSW63nwVflaDpkDwaiy8rHDUr3KdAG
+x+bRjUvfn2oJ/Nm8DFVNhvQpK3eWfm5TI/E=
-----END CERTIFICATE REQUEST-----
jeremy@kodekloud:~$ To view the details of your self-signed certificate in a human-readable format, use the x509 subcommand: openssl x509 -in mycertificate.crt -text This command outputs details such as issuer and subject information, validity period, and public key details: jeremy@kodekloud:~$ openssl x509 -in mycertificate.crt -text
Certificate:
    Data:
        Version: 3 (0x2)
        Serial Number:
            43:f0:d9:9b:fe:36:34:3d:f2:3d:64:ef:91:c2:30:3a:fe:d8:f9:cb
        Signature Algorithm: sha256WithRSAEncryption
        Issuer: C = US, ST = Oregon, L = Gaston, O = KodeKloud, CN = www.kodekloud.com
        Validity
            Not Before: Jun 13 02:38:25 2024 GMT
            Not After : Jun 13 02:38:25 2025 GMT
        Subject: C = US, ST = Oregon, L = Gaston, O = KodeKloud, CN = www.kodekloud.com
    Subject Public Key Info:
        Public Key Algorithm: rsaEncryption
        Public-Key: (4096 bit)
        ... Note that abbreviated field names (e.g., CN for Common Name, O for Organization) and details such as expiration date under ""Validity"" are shown. Recap and Further Exploration To summarize: Start by generating a Certificate Signing Request (CSR) using the req subcommand. If you wish to bypass a Certificate Authority, generate a self-signed certificate with the -x509 flag. Here is a quick reference command for generating a self-signed certificate: openssl req -x509 -noenc -newkey rsa:4096 -days 365 -keyout myprivate.key -out mycertificate.crt For more detailed examples and options, consult the OpenSSL manuals: OpenSSL req Manual OpenSSL x509 Manual We hope these explanations and examples help you confidently work with OpenSSL in your future projects. Now, let's move on to the next lesson. Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Diagnose and Manage Processes,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Operations-Deployment/Diagnose-and-Manage-Processes,"Linux Foundation Certified System Administrator (LFCS) Operations Deployment Diagnose and Manage Processes In this guide, you will learn how Linux handles processes and discover various techniques to diagnose and manage them effectively. Every command you execute creates a process that runs until it completes its task or is manually terminated. For instance, running a command like ""ls"" creates a short-lived process that displays the directory contents and then exits, while services like the SSH daemon run continuously in the background. Below is an example that lists all files—including hidden ones—using the ls command: jeremy@kodekloud:~$ ls -a
.  ..  .bash_logout  .bashrc  .cache  .lesshst  .profile  .ssh  .sudo_as_admin_successful
jeremy@kodekloud:~$ Using the ps Command The ps command is a fundamental utility for inspecting active processes. It has several option syntaxes, reflecting both Unix and BSD styles. For example, ps -a uses Unix-style options, while ps -A (uppercase A) follows BSD syntax, leading to different outputs. Running ps without any options only displays processes associated with the current terminal session. To see all processes running on the system, combine the options a , x , and u : jeremy@kodekloud:~$ ps aux In this command: The ax options ensure that processes from all controlling terminals are listed. The u option presents a user-oriented format, adding columns for memory and CPU usage as well as the process owner. This is why ps aux is widely used for obtaining a complete snapshot of system processes. Analyzing the ps aux Output Review the typical columns shown in the ps aux output: Column Description %CPU Percentage of one CPU core’s capacity used by the process. A value of 150 may indicate one full core plus 50% of a second core. %MEM Percentage of the system's total memory used by the process. START The time or date when the process was initiated. TIME Total CPU time consumed by the process. Processes with long lifespans often spend the majority of their time sleeping. COMMAND The command that initiated the process, along with its parameters. Processes displayed within square brackets (e.g., [kthreadd] ) are kernel processes operating in privileged areas and usually do not require user interaction. Only processes outside these brackets represent user-space applications. Monitoring Processes in Real Time with top While ps gives you a static snapshot, the top command is ideal for real-time process monitoring. It continuously updates its display and reorders the processes by CPU usage, making it easier to identify resource-intensive processes. Use the arrow keys to navigate the list, and press ""Q"" to exit. Below is an example output of the top command: top - 18:13:36 up 3:05, 2 users, load average: 0.05, 0.04, 0.00
Tasks: 215 total, 1 running, 214 sleeping, 0 stopped, 0 zombie
%Cpu(s):  0.0 us,  0.0 sy,  0.0 ni, 99.3 id,  0.0 wa,  0.0 hi,  0.7 si,  0.0 st
MiB Mem :  7939.4 total, 7303.2 free,  546.3 used,  325.0 buff/cache
MiB Swap:  4096.0 total, 4096.0 free,    0.0 used. 7393.1 avail Mem
PID USER      PR  NI    VIRT    RES    SHR S %CPU %MEM     TIME+ COMMAND       
11110 jeremy    20   0  11912  5632  3456 R 14.3  0.1   00:00.05 top           
1526  jeremy    20   0  15124  7092  5120 S  7.1  0.1   00:15.84 sshd          
   1  root      20   0  22560 13412  9316 S  0.0  0.2   00:23.06 systemd       
   2  root      20   0      0     0     0 S  0.0  0.0   00:00.05 kthreadd      
   3  root      20   0      0     0     0 S  0.0  0.0   00:00.00 pool_workqueue_release
   4  root       0 -20      0     0     0 S  0.0  0.0   00:00.00 kworker/R-rcu_g
   5  root       0 -20      0     0     0 S  0.0  0.0   00:00.00 kworker/R-rcu_p You can filter process listings by specific criteria such as PID or user. For example, to view details for process 1: jeremy@kodekloud:~$ ps 1
    PID TTY      STAT   TIME COMMAND
      1 ?        Ss     0:23 /sbin/init
jeremy@kodekloud:~$ For detailed user-oriented information about process 1, run: jeremy@kodekloud:~$ ps u 1
USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
root         1  0.2  0.1  22560 13412 ?        Ss     15:07   0:23 /sbin/init
jeremy@kodekloud:~$ To list processes started by a specific user (e.g., jeremy), use the uppercase -U option: jeremy@kodekloud:~$ ps u -U jeremy
USER     PID  %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
jeremy  1398  0.0  0.1 20324 11392 ?        Ss    15:33   0:00 /usr/lib/systemd/systemd --user
jeremy  1401  0.0  0.0 21148  3520 ?        S     15:33   0:00 (sd-pam)
jeremy  1412  0.0  0.0  8656  5632 tty1     S+    15:33   0:00 -bash
jeremy  1526  0.1  0.0 15124  7092 ?        Ss    15:37   0:16 sshd: jeremy@pts/0
jeremy  1527  0.0  0.0 10300  6400 pts/0    S     15:37   0:00 -bash
jeremy 11326  0.0  0.0 10884  4480 pts/0    R+    18:15   0:00 ps u -U jeremy Additionally, you can search for processes by name using the pgrep command with the -a option to display the complete command line: jeremy@kodekloud:~$ pgrep -a syslog
1084 /usr/sbin/rsyslogd -n -iNONE
jeremy@kodekloud:~$ Adjusting Process Priority Using Niceness and renice Linux processes have a ""niceness"" value that determines their scheduling priority. This value ranges from -20 (highest priority) to 19 (lowest priority). When launching a process, you can assign a niceness value. For example, launching a bash shell with a niceness value of 11: nice -n 11 bash Note that the default ps aux output does not include niceness values. To view these, use the BSD long format with ps l , which displays the NI column: jeremy@kodekloud:~$ nice -n 11 bash
jeremy@kodekloud:~$ ps l
F UID   PID  PPID PRI NI  VSZ  RSS WCHAN   STAT TTY   TIME COMMAND
4 1000  1412 1227 20  0  8656 5632 do_sel S+   tty1  0:00 -bash
0 1000  1527 1526 20  0 10300 6400 do_wai Ss   pts/0 0:00 -bash
0 1000 11657 1527 31 11 8652 5504 do_wai SN   pts/0 0:00 bash
0 1000 11702 11657 31 11 10916 4224 - RN+ pts/0 0:00 ps l Child processes inherit the niceness value from their parent process. To modify the niceness of an already running process, use the renice command. For example, to change the nice value of a process with PID 12238 from 0 to 7: jeremy@kodekloud:~$ renice 7 12238
12238 (process ID) old priority 0, new priority 7
jeremy@kodekloud:~$ Superuser Privileges Only superusers can lower the nice value (i.e., increase priority) of a process. Regular users can only increase the niceness value. Sending Signals to Processes Linux signals prompt processes to perform actions such as stopping, pausing, or terminating. Although some signals can be handled gracefully by applications, signals like SIGSTOP and SIGKILL are non-catchable. SIGSTOP pauses a process until it receives a SIGCONT, while SIGKILL immediately terminates it. List all available signals using: jeremy@kodekloud:~$ kill -l
 1) SIGHUP  2) SIGINT  3) SIGQUIT  4) SIGILL  5) SIGTRAP 
 6) SIGABRT 7) SIGBUS  8) SIGFPE  9) SIGKILL 10) SIGUSR1 
11) SIGSEGV 12) SIGUSR2 13) SIGPIPE 14) SIGALRM 15) SIGTERM 
16) SIGSTKFLT 17) SIGCHLD 18) SIGCONT 19) SIGSTOP 20) SIGXCPU 
... (continues) For example, to send a SIGHUP signal (which typically instructs a process to reload its configuration) to the SSH daemon with PID 1457, use: jeremy@kodekloud:~$ kill -s SIGHUP 1457 Since SSHD runs as root, a normal user must prepend sudo : jeremy@kodekloud:~$ sudo kill -s SIGHUP 1457 When no signal is specified, the kill command sends SIGTERM by default for a graceful shutdown. To forcefully terminate a process, you can use SIGKILL in any of these ways: kill -s SIGKILL PID
kill -KILL PID
kill -9 PID The pkill command lets you send signals based on process names. For example, to send SIGKILL to all processes with ""bash"" in their name, first verify the target processes: jeremy@kodekloud:~$ pgrep -a bash
1412 -bash
1527 -bash
12066 bash If the list is correct, use: jeremy@kodekloud:~$ pkill -KILL bash Caution Be very careful when terminating processes with signals such as SIGKILL. Stopping critical processes may lead to system instability or disconnect you from your session. Managing Background and Foreground Processes Long-running tasks can be executed in the background, allowing you to use your terminal for other commands. For example, to run a sleep command for 180 seconds: jeremy@kodekloud:~$ sleep 180 If you need to interrupt a process, press CTRL-C. In applications like Vim, you can suspend the process temporarily by pressing CTRL-Z: jeremy@kodekloud:~$ vim /etc/hostname
[1]+  Stopped                 vim /etc/hostname
jeremy@kodekloud:~$ To bring a suspended process back to the foreground, execute: jeremy@kodekloud:~$ fg
vim /etc/hostname For processes started in the background using an ampersand (&): jeremy@kodekloud:~$ sleep 300 &
[1] 13868
jeremy@kodekloud:~$ jobs
[1]+  Running                 sleep 300 & To resume work on a backgrounded process, you can bring it to the foreground with fg or continue running it in the background using bg . Checking Open Files with lsof The lsof command lets you inspect which files or directories are in use by a process. First, obtain the PID of your bash shell: jeremy@kodekloud:~$ pgrep -a bash
13536 bash Then, list all open files for that process: jeremy@kodekloud:~$ lsof -p 13536
COMMAND   PID USER     FD   TYPE DEVICE SIZE/OFF NODE NAME
bash    13536 jeremy    cwd    DIR  252,0     4096 786475 /home/jeremy
bash    13536 jeremy    rtd    DIR  252,0     4096     2 /
bash    13536 jeremy    txt    REG  252,0 1446024 262614 /usr/bin/bash
... If you need to check files for processes requiring elevated privileges (or if you suspect missing information), prepend sudo: jeremy@kodekloud:~$ sudo lsof /var/log/auth.log
COMMAND    PID   USER   FD   TYPE DEVICE SIZE/OFF NODE NAME
rsyslogd  1084 syslog   9w  REG 252,0  16701  1299 /var/log/auth.log Conclusion This guide has demonstrated multiple techniques for diagnosing and managing processes in Linux. You learned to inspect processes with ps and top , adjust process priorities using nice and renice , send signals with kill and pkill , manage background and foreground tasks, and inspect open files with lsof . Mastering these tools is essential for effective system administration and troubleshooting on Linux. For more in-depth knowledge, explore additional resources such as: Linux Process Management Linux Signal Documentation Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Boot or Change System Into Different Operating Modes Optional,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Operations-Deployment/Boot-or-Change-System-Into-Different-Operating-Modes-Optional,"Linux Foundation Certified System Administrator (LFCS) Operations Deployment Boot or Change System Into Different Operating Modes Optional In this article, we explore how Linux leverages systemd targets to control the boot process and how you can modify the default boot target to suit different system environments. By understanding systemd targets, you gain flexibility in managing resource usage and operational modes. Currently, the Linux system boots to a graphical login screen. At startup, the operating system loads various programs and services in a specific order using instructions defined in systemd target files. To see which boot target is active by default, run: systemctl get-default If the output is ""graphical.target,"" it means the system is configured to boot into a full graphical environment. The corresponding graphical.target file contains the necessary instructions to start services and programs required for a graphical login. Booting into the graphical target requires additional system resources due to the loading of the graphical user interface. If you do not need the graphical environment, you can change the default boot target to a more resource-efficient option, such as the multi-user target. This target provides essential services—like daemons and network services—running in a text-based mode. To switch the default boot target, execute: sudo systemctl set-default multi-user.target Note Changing the default boot target means that on the next reboot, the system will operate in text mode rather than launching a graphical interface. Below is an example session showing how to check and change the default boot target: # Check the current default target
jeremy@kodekloud:~$ systemctl get-default
graphical.target

# Set the default target to multi-user
jeremy@kodekloud:~$ sudo systemctl set-default multi-user.target
[sudo] password for jeremy:
Removed ""/etc/systemd/system/default.target"".
Created symlink /etc/systemd/system/default.target → /lib/systemd/system/multi-user.target.
jeremy@kodekloud:~$ The multi-user target is named so because it supports simultaneous logins by multiple users, while still keeping network services active to ensure continuous connectivity. After changing the default target, reboot the system. Instead of the usual graphical login screen, you will encounter a text-based login console, similar to the following: Ubuntu 23.10 kodekloud tty1
kodekloud: jeremy
Password:
Welcome to Ubuntu 23.10 (GNU/Linux 6.5.0-27-generic x86_64)
 * Documentation: https://help.ubuntu.com
 * Management: https://landscape.canonical.com
 * Support: https://ubuntu.com/advantage

68 updates can be applied immediately.
To see these additional updates run: apt list --upgradeable

The list of available updates is more than a week old.
To check for new updates, run: sudo apt update
Last login: Tue May 28 12:25:21 PDT 2024 on tty1
jeremy@kodekloud:~$ If you temporarily need a graphical interface—perhaps to work with a 3D modeling application—you don't have to permanently switch the boot target. Instead, you can start the graphical environment immediately using: sudo systemctl isolate graphical.target This command activates the graphical interface on demand without altering the system's default text-based boot mode. Additional systemd targets include emergency.target and rescue.target. The table below summarizes the most commonly used targets and their purposes: Target Description Use Case Command Example graphical.target Boots into a full graphical desktop environment Standard desktop usage systemctl get-default multi-user.target Boots into a text-based environment with network services Server or low-resource environments sudo systemctl set-default multi-user.target emergency.target Boots with minimal system services; root FS is read-only Critical troubleshooting when other services cause issues (Invoked automatically when selected) rescue.target Loads essential services with a root shell access Administrative tasks in a minimal environment (Invoked automatically when selected) Warning When booting into emergency.target or rescue.target, ensure that the root account has a password set. Without a root password, these modes will not be accessible. This concludes the demonstration on changing systemd targets and boot modes. For further details on system management and troubleshooting, be sure to refer to the official systemd documentation . Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Manage Software with the Package Manager,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Operations-Deployment/Manage-Software-with-the-Package-Manager,"Linux Foundation Certified System Administrator (LFCS) Operations Deployment Manage Software with the Package Manager This article explains how to install, update, upgrade, and remove software on Ubuntu using the apt package manager. Ubuntu simplifies software management by handling installation, upgrades, and dependency resolution with apt. Follow the examples below for practical guidance on managing your system's packages. Updating the Package Database Before installing or upgrading any software, it is essential to refresh the local package database. Running the following command downloads the latest package information from the official repositories, ensuring that you work with up-to-date data. sudo apt update Example output: sudo apt update
[sudo] password for jeremy:
Get:1 http://us.archive.ubuntu.com/ubuntu noble InRelease [97.5 kB]
Get:2 http://us.archive.ubuntu.com/ubuntu noble-updates InRelease [89.7 kB]
Get:3 http://us.archive.ubuntu.com/ubuntu noble-backports InRelease [89.7 kB]
Get:4 http://security.ubuntu.com/ubuntu noble-security InRelease [89.7 kB]
Fetched 179 kB in 2s (77.4 kB/s)
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
4 packages can be upgraded. Run 'apt list --upgradable' to see them.
jeremy@kodekloud:~$ Note If the package information was last refreshed a while ago (for example, one week ago), running sudo apt update ensures you get the latest package listings. Upgrading Installed Packages After updating the package database, you can upgrade the installed packages with: sudo apt upgrade When prompted, type ""y"" to confirm the upgrade. For added efficiency, you can chain the update and upgrade commands. Using the && operator ensures that the upgrade command runs only if the update is successful: sudo apt update && sudo apt upgrade Installing New Applications To install a new application such as Nginx, run: sudo apt install nginx For a smoother process that uses the latest package data, chain the update and install commands together: sudo apt update && sudo apt install nginx During the installation, additional dependencies, including necessary libraries and components, may be automatically installed to ensure Nginx runs correctly. Example output when installing Nginx: sudo apt update && sudo apt install nginx
Hit:1 http://us.archive.ubuntu.com/ubuntu noble InRelease
Hit:2 http://security.ubuntu.com/ubuntu noble-security InRelease
Hit:3 http://us.archive.ubuntu.com/ubuntu noble-updates InRelease
Hit:4 http://us.archive.ubuntu.com/ubuntu noble-backports InRelease
Reading package lists... Done
Building dependency tree       
Reading state information... Done
The following additional packages will be installed:
  nginx-common
Suggested packages:
  fcgiwrap nginx-doc ssl-cert
The following NEW packages will be installed:
  nginx nginx-common
0 upgraded, 2 newly installed, 0 to remove and 0 not upgraded.
Need to get 552 kB of archives.
After this operation, 1,596 kB of additional disk space will be used.
Do you want to continue? [Y/n] Understanding Packages A package is an archive containing everything required by a piece of software, including binaries, configuration files, and documentation. Once a package is installed, you can inspect its contents with the dpkg tool. For example, to list all files provided by the Nginx package: dpkg --listfiles nginx Example output: .
usr
usr/sbin
usr/sbin/nginx
usr/share
usr/share/doc
usr/share/doc/nginx
usr/share/doc/nginx/changelog.Debian.gz
usr/share/doc/nginx/copyright
usr/share/man
usr/share/man/man8
usr/share/man/man8/nginx.8.gz This output confirms that the Nginx executable is located at /usr/sbin/nginx . To determine which package provides a specific file, use the dpkg --search command: dpkg --search /usr/sbin/nginx Example output: jeremy@kodekloud:~$ dpkg --search /usr/sbin/nginx
nginx: /usr/sbin/nginx
jeremy@kodekloud:~$ To gather more details about a specific package, such as its dependencies or description, you can use the apt show command. For instance: apt show libnginx-mod-stream Example output: jeremy@kodekloud:~$ apt show libnginx-mod-stream
Package: libnginx-mod-stream
Version: 1.24.0-2ubuntu7
Priority: optional
Section: httpd
Source: nginx
Origin: Ubuntu
Maintainer: Ubuntu Developers < [email protected] >
Original-Maintainer: Debian Nginx Maintainers < [email protected] >
Bugs: https://bugs.launchpad.net/ubuntu/+filebug
Installed-Size: 224 kB
Depends: nginx-abi-1.24.0-1, libc6 (>= 2.33)
Recommends: nginx
Homepage: https://nginx.org
Download-Size: 84.2 kB
APT-Sources: http://us.archive.ubuntu.com/ubuntu noble/main amd64 Packages
Description: Stream module for Nginx
 The nginx_stream module adds stream proxy support to nginx.
 .
 Stream module supports loadbalancing & proxying to TCP servers. The module
 also supports ACLs/connection limiting and configuring multiple operational
 parameters.
jeremy@kodekloud:~$ Searching for Packages If you are unsure which package contains the software you need, you can search for it using apt search . For example, to find packages related to ""nginx"": apt search nginx This command looks through both package names and descriptions and may return multiple results. To narrow down the search to package names only, use the --names-only option: apt search --names-only nginx You can also search using multiple keywords. For example, to find a package that includes ""nginx"", ""module"", and ""image"", run: apt search nginx-module-image This ensures that all specified search terms are included in the results. Removing Packages When a package is no longer necessary, you can remove it with the following command. For example, to remove Nginx: sudo apt remove nginx Example output: jeremy@kodekloud:~$ sudo apt remove nginx
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following packages will be REMOVED:
  nginx nginx-common
0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.
After this operation, 1,596 kB disk space will be freed.
Do you want to continue? [Y/n] y Note This command removes only the main package. Residual dependencies that are no longer needed remain on the system. To clean up those extra dependencies, use: sudo apt autoremove nginx Example output: sudo apt autoremove nginx
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
The following packages will be REMOVED:
  nginx nginx-common
0 upgraded, 0 newly installed, 2 to remove and 0 not upgraded.
After this operation, 1,596 kB disk space will be freed.
Do you want to continue? [Y/n] y
(Reading database ... 83359 files and directories currently installed.)
Removing nginx-common (1.24.0-2ubuntu7) ...
Removing nginx (1.24.0-2ubuntu7) ...
Processing triggers for man-db (2.12.0-4build2) ... If needed, you can always reinstall Nginx using: sudo apt install nginx Using apt autoremove is a convenient way to remove both the main package and any leftover dependencies in a single operation. Conclusion In this article, we covered the essentials of managing software on Ubuntu using its apt package manager. You learned how to update the package database, upgrade installed packages, install new applications like Nginx, inspect package contents, search for specific packages, and remove unwanted software along with any residual dependencies. By mastering these commands, you can efficiently maintain and secure your Ubuntu system. Let's move on to the next lesson. Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Locate and Analyze System Log Files,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Operations-Deployment/Locate-and-Analyze-System-Log-Files,"Linux Foundation Certified System Administrator (LFCS) Operations Deployment Locate and Analyze System Log Files In this guide, you will learn how to locate and analyze system log files on a Linux system. Logging is a critical part of managing any Linux server since logs offer detailed insights into system activities—covering events such as user actions, system errors, and service operations. These logs are written as text messages, making them easy to search, read, and troubleshoot. Logging Basics The Linux kernel and many applications generate status messages, errors, and warnings, which are stored in log files. Because multiple programs continuously generate these messages, logging daemons are used to collect and organize information into centralized log files. The most common logging daemon on Linux is rsyslog (Rocket Fast System for Log Processing). By default, rsyslog stores log files in the /var/log directory. These plain text files can be easily explored using text-search tools like grep . Below is an example of listing the files in the /var/log directory: $ ls /var/log/
alternatives.log     landscape           nginx
apt                  lastlog             private
auth.log             dmesg.0             syslog
auth.log.1           dmesg.1.gz         syslog.1
auth.log.2.gz        dmesg.2.gz         syslog.2.gz
bootstrap.log        dpkg.log           ubuntu-advantage.log
btmp                 faillog            ubuntu-advantage.log.1
btmp.1               installer          unattended-upgrades
cloud-init.log       journal             wtmp
cloud-init-output.log kern.log           kern.log.1
dist-upgrade         kern.log.2.gz      dmesg Note Most log files under /var/log are restricted to the root user. If you need to view these logs as a non-root user, consider switching to root using commands like su --login or sudo --login . For example: $ su --login
Password: or $ sudo --login
[sudo] password for aaron: Identifying Specific Log Files If you are unsure where specific logs are stored—such as SSH logs that give details about login attempts—you can search all log files for entries related to the SSH daemon ( sshd ): $ grep -r 'ssh' /var/log/ The output might look like this: /var/log/auth.log:Mar  3 03:32:37 kodekloud sshd[1653]: Connection closed by authenticating user aaron 10.11.12.1 port 57660
/var/log/auth.log:Mar  3 03:32:39 kodekloud sshd[1655]: Accepted password for aaron from 10.11.12.1 port 52560 ssh2
/var/log/auth.log:Mar  3 03:32:39 kodekloud sshd[1655]: pam_unix(sshd:session): session opened for user aaron(uid=1000) by
grep: /var/log/private: Permission denied
/var/log/installer/installer-journal.txt:Jun 30 12:18:56 ubuntu-server sshd[1409]: Server listening on 0.0.0.0 port 22.
... This indicates that SSH logs are primarily found in /var/log/auth.log . You can open this file using an editor like Vim or a pager such as less to search for additional SSH-specific details. Example output from /var/log/auth.log : $ less /var/log/auth.log
Mar  3 03:21:24 kodekloud sshd[1501]: Accepted password for aaron from 10.11.12.1 port 56862 ssh2
Mar  3 03:32:34 kodekloud sshd[1653]: Failed password for aaron from 10.11.12.1 port 57660 ssh2
Mar  3 03:32:53 kodekloud sudo:     aaron : TTY=pts/0 ; PWD=/home/aaron; USER=root ; COMMAND=/usr/bin/apt update
Mar  3 03:37:30 kodekloud passwd[2129]: pam_unix(passwd:chauthtok): password changed for aaron Another key log file is /var/log/syslog , which includes general system messages: $ less /var/log/syslog
Mar  3 00:00:14 kodekloud systemd[1]: Finished Daily dpkg database backup service.
Mar  3 00:10:14 kodekloud rsyslogd: [origin software=""rsyslogd"" swVersion=""8.2112.0"" x-pid=""638"" x-info=""https://www.rsyslog.com""] rsyslog was HUPed
Mar  3 00:17:01 kodekloud CRON[1357]: (root) CMD   (cd / && run-parts --report /etc/cron.hourly)
Mar  3 00:18:52 kodekloud systemd-timesyncd[521]: Network configuration changed, trying to establish connection.
Mar  3 00:33:14 kodekloud systemd[1]: Starting Refresh fwupd metadata and update motd... Older log files often have suffixes such as .1 or are compressed with .gz , while the uncompressed file (for example, auth.log ) contains the latest entries. Monitoring Logs in Real Time When you need to debug an application or monitor system changes as they happen, you can use the tail command with the -f option to follow a log file in real time. For example: $ tail -F /var/log/auth.log
Mar  3 03:32:53 kodekloud sudo:     aaron : TTY=pts/0 ; PWD=/home/aaron ; USER=root ; COMMAND=/usr/bin/apt update
Mar  3 03:32:53 kodekloud sudo: pam_unix(sudo:session): session opened for user root (uid=0) by aaron(uid=1000)
Mar  3 03:32:58 kodekloud sudo: pam_unix(sudo:session): session closed for user root
Mar  3 03:37:30 kodekloud passwd[2129]: pam_unix(passwd:chauthtok): password changed for aaron
... Press Ctrl+C to exit the follow mode. To filter the live output for specific entries, such as those related to sudo , you can pipe the output through grep : $ tail -F /var/log/auth.log | grep ""sudo"" Advanced Log Analysis Using journalctl Modern Linux systems that use systemd employ the journal daemon for structured log management. The journalctl command provides powerful options to filter and analyze logs. Viewing Logs by Command To view logs for a specific command, such as sudo , first determine its full path: $ which sudo
/usr/bin/sudo Then filter the journal logs associated with it: $ journalctl /usr/bin/sudo This command opens the log output in your default pager (typically less ), allowing you to navigate and search through the logs. Press q to exit the pager. Viewing All Journal Logs To view all entries collected by the journal daemon, run: $ journalctl Jump to the end of the log output by pressing > within the pager, or run: $ journalctl -e Following Journal Logs Live Just like tail -f , you can follow journal logs in real time by using: $ journalctl -f
Mar 03 23:24:43 kodekloud sudo[1077]: pam_unix(sudo:session): session closed for user root
Mar 03 23:28:07 kodekloud systemd[1]: Starting Cleanup of Temporary Directories...
... Press Ctrl+C to exit follow mode. Filtering Logs by Priority Use the -p option with journalctl to filter log output based on priority levels. Available priorities include: emerg, alert, crit, err, warning, notice, info, and debug. For example, to display only error messages: $ journalctl -p err
Feb 08 21:09:19 kodekloud systemd[1]: multipathd.socket: Socket service already active, refusing.
Feb 08 21:09:19 kodekloud systemd[1]: Failed to listen on multipathd control socket.
-- Boot 35a9a34be95e43cb85c097ecdd0afa4d --
Mar 03 00:33:14 kodekloud systemd[1]: Failed to start Refresh fwupd metadata and update motd. Tip If you forget the priority names, type journalctl -p (with a trailing space) and press Tab twice to list all available options. Combining Filters with Regex You can combine filters with regular expressions. For example, to display only log entries with info priority where messages start with the letter “b”, use: $ journalctl -p info -g '^b'
Jun 30 15:34:11 kodekloud kernel: Booting paravirtualized kernel on KVM
Jun 30 15:34:11 kodekloud kernel: Built 1 zonelists, mobility grouping on.  Total pages: 1031896
Jun 30 15:34:11 kodekloud kernel: Block layer SCSI generic (bsg) driver version 0.4 loaded (major 243)
Jun 30 15:34:11 kodekloud kernel: blacklist: Loading compiled-in revocation X.509 certificates
Jun 30 15:34:11 kodekloud kernel: btrfs loaded, crc32c=crc32c-intel, zoned=yes, fsverity=yes Filtering Logs by Time Range To filter journal logs by time, use the -S (since) and -U (until) options. For instance, to view logs between 1 a.m. and 2 a.m.: $ journalctl -S 01:00 -U 02:00
Mar 04 01:17:01 kodekloud CRON[1417]: pam_unix(cron:session): session opened for user root(uid=0) by (uid=0)
Mar 04 01:17:01 kodekloud CRON[1418]: (root) CMD (cd / && run-parts --report /etc/cron.hourly)
Mar 04 01:17:01 kodekloud CRON[1417]: pam_unix(cron:session): session closed for user root
Mar 04 01:35:24 kodekloud systemd[1]: Starting Discard unused blocks on filesystems from /etc/fstab... For full date and time filtering, enclose the datetime string in single quotes: $ journalctl -S '2024-03-03 01:00:30'
Mar 03 01:13:06 kodekloud systemd-timesyncd[521]: Network configuration changed, trying to establish connection.
Mar 03 01:13:06 kodekloud systemd-timesyncd[521]: Initial synchronization to time server 185.125.190.56:123 (ntp.ubuntu.com).
... Viewing Logs by Boot Session The journal organizes log entries by boot session. To view logs from the current boot, run: $ journalctl -b 0
Mar 03 23:12:59 kodekLOUD kernel: Linux version 5.15.0-97-generic (buildd@lcy02-amd64-033) ...
Mar 03 23:12:59 kodekLOUD kernel: Command line: BOOT_IMAGE=/vmlinuz-5.15.0-97-generic root=...
... For previous boot sessions, use a negative offset (e.g., -b -2 for two boots ago). Note that some systems might only store logs in memory, so persistent logging may not be available unless enabled. To enable persistent logging, create the directory: $ sudo mkdir /var/log/journal/ If you run a journalctl command and see no output, try using sudo or check that your user has the appropriate permissions. Viewing User Login History Reviewing user login history is straightforward with the last command, which displays recent logins in reverse chronological order: $ last
aaron     pts/0        10.11.12.1    Sun Mar  3 23:15 - 23:15  still logged in
reboot    system boot  5.15.0-97-generic  Sun Mar  3 23:12 - 23:12  still running
aaron     tty1         Sun Mar  3 04:14 - 04:22  (00:08)
... The lastlog command provides a summary of the last login times for each user and can include the originating IP address for remote logins (such as SSH): $ lastlog
Username      Port     From             Latest
root          pts/0    **Never logged in**
daemon        pts/0    **Never logged in**
bin           pts/0    **Never logged in**
sys           pts/0    **Never logged in**
tss           pts/0    **Never logged in**
landscape     pts/0    **Never logged in**
fwupd-refresh pts/0    **Never logged in**
usbmux       pts/0    **Never logged in**
aaron        pts/0    10.11.12.1      Sun Mar  3 23:15:19 +0200 2024
lxd          pts/0    **Never logged in** Summary In this article, we explored how to locate and analyze system log files in Linux. We examined where logs are stored (mostly under /var/log ), how to view and filter logs using tools like grep , tail , and journalctl , and how to review user login history with commands such as last and lastlog . By mastering these tools, you can efficiently diagnose issues, monitor system activities, and ensure that your Linux server is running smoothly. Happy logging! Watch Video Watch video content Practice Lab Practice lab"
Linux Foundation Certified System Administrator LFCS,Schedule Tasks to Run at a Set Date and Time,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Operations-Deployment/Schedule-Tasks-to-Run-at-a-Set-Date-and-Time,"Linux Foundation Certified System Administrator (LFCS) Operations Deployment Schedule Tasks to Run at a Set Date and Time In this article, we explore how to schedule tasks to run at specified times on Linux systems. Automating tasks such as database backups every Sunday at 3:00 AM is crucial for consistent system maintenance. There are three primary tools available for task scheduling: Cron Utility Anacron At Utility Below, we detail how each tool works and how to configure them for your needs. Cron Utility Cron is best suited for repetitive tasks that run at regular intervals—whether every few minutes, specific hours, days, or even months. The basic syntax for a cron job consists of five time-and-date fields followed by the command to be executed. When editing the system-wide cron table (found at /etc/crontab ), a username field is included. The time fields are as follows: Minute (0–59) Hour (0–23) Day of the month (1–31) Month (1–12) Day of the week (0–6, where 0 or 7 denotes Sunday) You can use special characters in these fields: Asterisk (*) denotes every possible value. Comma (,) separates multiple values. For example, ""15,45"" in the minute field runs the job at minute 15 and 45. Dash (-) specifies a range (e.g., ""2-4"" in the hour field). Slash (/) defines steps. For example, ""*/4"" in the hour field indicates every 4 hours, and ""0-8/4"" represents 0 AM, 4 AM, and 8 AM. The default system-wide cron table, located at /etc/crontab , usually includes explanatory comments. Below is a sample excerpt: $ cat /etc/crontab
SHELL=/bin/sh
# You can also override PATH; by default, newer versions inherit it.
# Example of job definition:
# ----------- minute (0 - 59)
# ----------- hour (0 - 23)
# ----------- day of month (1 - 31)
# ----------- month (1 - 12) OR jan,feb,mar,apr ...
# ----------- day of week (0 - 6) (Sunday=0 or 7) OR
#               sun,mon,tue,wed,thu,fri,sat
# * * * * * user-name command to be executed
35 6 * * * root /bin/some_command --some_options Note When modifying the system-wide cron file, be aware that package updates may overwrite your changes. It is generally safer to add cron jobs using the user's personal cron table. To edit your personal cron table, run: $ crontab -e In your personal cron table, the username is not required because the job inherits your current privileges. For example, if you want to create a file named test_path every day at 6:35 AM using the touch command, first determine its full path: $ which touch
/usr/bin/touch Then, add the following line using crontab -e : 35 6 * * * /usr/bin/touch test_path Save and exit the editor to schedule the job. Below are additional examples for scheduling tasks: $ which touch
/usr/bin/touch

$ crontab -e
35 6 * * * /usr/bin/touch test_passed            # Every day at 6:35 AM
0 3 * * 0 /usr/bin/touch test_passed              # Every Sunday at 3:00 AM
0 3 * * 7 /usr/bin/touch test_passed              # Alternative notation for Sunday
0 3 15 * * /usr/bin/touch test_passed             # On the 15th of every month at 3:00 AM To run a command every day at 3:00 AM or at the top of every hour (when the minute is 00), adjust the timing fields as needed. For further testing of your cron expressions, visit crontab.guru . To list your current cron jobs, use: $ crontab -l
35 6 * * * /usr/bin/touch aaron_test To view or edit other users' cron jobs (e.g., root or another user), use sudo : $ sudo crontab -l
0 * * * * /usr/bin/touch root_test

$ sudo crontab -e -u jane
30 * * * * /usr/bin/touch jane_test If you need to remove a user's cron jobs entirely, use the -r option: $ crontab -r
$ sudo crontab -r -u jane An alternative to adding jobs to the cron table directly is by placing scripts into specific directories, such as /etc/cron.daily , /etc/cron.hourly , /etc/cron.weekly , or /etc/cron.monthly . For instance, to schedule a shell script named shellscript to run hourly: $ touch shellscript
$ sudo cp shellscript /etc/cron.hourly/ Make sure the script is both readable and executable. To remove it, simply delete the file from the directory. Anacron Anacron is designed for tasks that need to run periodically (daily, weekly, monthly) on systems that are not continuously powered on. If a scheduled cron task is missed because the system is off, anacron will run the task once the system is available. On many systems, anacron is pre-installed. If not, install it with: $ sudo apt install anacron Anacron's configuration is in the /etc/anacrontab file, which includes helpful comments and examples. Here’s an excerpt: $ sudo vim /etc/anacrontab
# See anacron(8) and anacrontab(5) for details.
# These entries replace cron's job entries.
1       5       cron.daily      run-parts --report /etc/cron.daily
7       10      cron.weekly     run-parts --report /etc/cron.weekly
@monthly 15      cron.monthly    run-parts --report /etc/cron.monthly The syntax in the anacrontab file consists of four fields: The period in days (e.g., 1 for daily, 7 for weekly). A delay (in minutes) after system startup. A unique job identifier. The command to execute. For example, to run a job every three days with a 10-minute delay: 3 10 test_job /usr/bin/touch /root/anacron_created_this Other examples include: 7 10 test_job /usr/bin/touch /root/anacron_created_this
@monthly 10 test_job /usr/bin/touch /root/anacron_created_this After editing, verify your anacrontab syntax with: $ anacron -T
anacron: /etc/anacrontab: Unknown named period on line 13, skipping If no errors are reported, your anacron entries are correctly formatted. Documentation Tip For more detailed information on anacron, refer to the man pages by running: $ man 5 anacrontab At Utility The at utility is ideal for one-time task scheduling, rather than recurring jobs. If at is not already installed on your Ubuntu system, install it with: $ sudo apt install at To schedule a job with at, specify the desired run time. For example, to run a job at 3:00 PM: $ at '15:00'
warning: commands will be executed using /bin/sh
at> /usr/bin/touch file_created_by_at
at> <CTRL-D> You can also schedule jobs for specific dates or relative times. Some examples include: $ at 'August 20 2024'
$ at '2:30 August 20 2024'
$ at 'now + 30 minutes'
$ at 'now + 3 hours'
$ at 'now + 3 days'
$ at 'now + 3 weeks'
$ at 'now + 3 months' To view scheduled at jobs, use: $ atq
1 Wed Mar  6 15:00:00 2024 a aaron Here, the first number is the job ID. To inspect a job's details, use the -c option, similar to using cat . To remove a job, execute: $ atrm 1 Quick Reference For more information on the at utility, refer to the at documentation . Each of these tools—cron, anacron, and at—has its unique strengths. Cron excels for regular, recurring tasks; anacron ensures tasks are completed even if scheduled times are missed due to system downtime; and at is perfect for one-off commands. By mastering these scheduling utilities, you can automate and manage tasks efficiently on your Linux system, ensuring a more resilient and well-maintained environment. Watch Video Watch video content Practice Lab Practice lab"
Linux Foundation Certified System Administrator LFCS,Git Branches and Remote Repositories,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/Git-Branches-and-Remote-Repositories,"Linux Foundation Certified System Administrator (LFCS) Essential Commands Git Branches and Remote Repositories In this lesson, you will learn how to leverage Git branches and remote repositories to manage different versions of your code, track modifications, merge changes, and effectively collaborate with your team. ────────────────────────────── Understanding Branches Running the ""git status"" command might produce output similar to the following: aaron@kodekloud:~/project$ git status
On branch master
Changes to be committed:
  (use ""git restore --staged <file>..."" to unstage)
        deleted:    file3

aaron@kodekloud:~/project$ git commit -m ""Removed buggy feature""
[master 44b9e16] Remove buggy feature
 1 file changed, 1 deletion(-)
 delete mode 100644 file3 Git branches allow you to develop different versions of your project simultaneously. For example, if version 1.0 is already in production and requires bug fixes while new features are developed for version 1.1, you can create separate branches for each. The master branch is the default branch that typically holds your stable, released version. Imagine your master branch is currently at version 1.0 with file1 and file2. The diagram below illustrates this concept by presenting the ""Master branch"" as the stable codebase accessed by users. To experiment or develop new features without affecting the master branch, create a new branch. In this case, the branch is named ""1.1-testing"" to indicate that it is under development. ────────────────────────────── Working with Branches in the Terminal You can easily create and switch to a new branch using the following commands: jeremy@kodekloud:~/project$ git branch 1.1-testing
jeremy@kodekloud:~/project$ git branch --list
* master
  1.1-testing
jeremy@kodekloud:~/project$ git checkout 1.1-testing
Switched to branch '1.1-testing' Now that you are on the 1.1-testing branch, open ""file2"" in your preferred editor and make your modifications (for example, update a line to ""This is the IMPROVED line of code in file2""). Then, run ""git status"" to see that your changes have been detected: jeremy@kodekloud:~/project$ vim file2
jeremy@kodekloud:~/project$ git status
On branch 1.1-testing
Changes not staged for commit:
  (use ""git add <file>..."" to update what will be committed)
  (use ""git restore <file>..."" to discard changes in working directory)
    modified:   file2

no changes added to commit (use ""git add"" and/or ""git commit -a"") Staging Changes Before committing, always add your changes to the staging area using ""git add"" to ensure that only the intended modifications are committed. Proceed to add and commit your changes: jeremy@kodekloud:~/project$ git add file2
jeremy@kodekloud:~/project$ git commit -m ""Improved code in file2""
[1.1-testing 34a562] Improved code in file2
 1 file changed, 1 insertion(+), 1 deletion(-) At this point, the improvements exist only in the 1.1-testing branch, leaving the master branch unchanged. ────────────────────────────── Viewing Commit History To review the history of your Git commits, use the Git log. Executing the following command provides detailed commit entries: jeremy@kodekloud:~/project$ git log --raw In the log output, symbols indicate file actions: A for added, D for deleted, and M for modified files. Each commit is identified by a unique hash value. To inspect the changes in a specific commit, use the ""git show"" command along with part of the commit hash. For example, a diff might resemble: +++ b/file2
@@ -1 +1 @@
-This is the ORIGINAL line of code in file2
+This is the IMPROVED line of code in file2 If you switch back to the master branch, you'll notice that it remains one commit behind. For example: jeremy@kodekloud:~/project$ git checkout master
Switched to branch 'master'
jeremy@kodekloud:~/project$ cat file2
This is the ORIGINAL line of code in file2 This demonstrates that the HEAD pointer reflects the state of the current branch. ────────────────────────────── Merging Branches Once you have verified the improvements on the 1.1-testing branch, you can merge these changes back into the master branch. First, switch to the master branch: jeremy@kodekloud:~/project$ git checkout master
Already on 'master' Then, merge the 1.1-testing branch: jeremy@kodekloud:~/project$ git merge 1.1-testing
Updating 44e4de1..344e562
Fast-forward
 file2 | 2 +-
 1 file changed, 1 insertion(+), 1 deletion(-) After the merge, the master branch now includes the improvements made in file2: jeremy@kodekloud:~/project$ cat file2
This is the IMPROVED line of code in file2 ────────────────────────────── Working with Remote Repositories Up until now, you have been working with your local repository. In collaborative environments, it's vital to synchronize your changes with a remote repository hosted on platforms like GitHub or GitLab . Pushing Local Changes Pushing your local commits to a remote repository ensures that your collaborators can access the latest version of your project. Start by checking the current remote configuration: jeremy@kodekloud:~/project$ git remote -v If there's no remote configured, add one using the connection string provided by your hosting platform: jeremy@kodekloud:~/project$ git remote add origin [email protected] :jeremykodekloud/kkproject.git
jeremy@kodekloud:~/project$ git remote -v
origin [email protected] :jeremykodekloud/kkproject.git (fetch)
origin [email protected] :jeremykodekloud/kkproject.git (push) Push the changes on the master branch to your remote repository: jeremy@kodekloud:~/project$ git push origin master Follow any prompts regarding host authenticity if this is your first connection. ────────────────────────────── Configuring SSH Keys For secure communication with Git remote repositories, it's recommended to use SSH keys. To generate an SSH key pair, run: jeremy@kodekloud:~/project$ ssh-keygen
Generating public/private ed25519 key pair.
Enter file in which to save the key (/home/jeremy/.ssh/id_ed25519):
Enter passphrase (empty for no passphrase): Once your keys have been generated, view your public key: jeremy@kodekloud:~/project$ cat ~/.ssh/id_ed25519.pub
ssh-ed25519 AAAAC3NzaC1lZDII1NTE5AAAAIBwgR4rMy6jBVr3GUM2OejEiLRHTaRrxgGpgzYWzwtto jeremy@kodekloud Copy the displayed key and add it to your GitHub account under Settings → SSH and GPG keys. After adding your SSH key, you can securely push your changes. ────────────────────────────── Cloning a Remote Repository When a new team member joins, they can easily clone the remote repository to get the entire project history. For example: git clone [email protected] :jeremykodekloud/kkproject.git This command creates a local directory named ""kkproject"" along with a hidden .git folder containing all commit history. ────────────────────────────── Getting Additional Help Git is a powerful tool with a wide range of features. If you ever need assistance or a reminder of a command's options, simply type ""git"" and press the tab key twice to see a list of commands. For in-depth information on any command, you can consult the manual pages: man git-add A sample Git log entry may look like this: commit 136b3a701d98fcc5c5e17ec3bc92272e4a810be
Author: jeremy < [email protected] >
Date:   Thu Jun 13 20:52:23 2024 +0000

removed buggy feature

commit e354e75b4bc7e97bfdaadc45243693218eab83d0e
Author: jeremy < [email protected] >
Date:   Thu Jun 13 20:47:33 2024 +0000

Added new feature

Added our first two files to get the project started For further reading and advanced Git techniques, please visit the KodeKloud Git Documentation . ────────────────────────────── This lesson has covered the fundamental techniques for working with Git branches and remote repositories. Continue with our next lesson to further enhance your Git skills. Watch Video Watch video content Practice Lab Practice lab"
Linux Foundation Certified System Administrator LFCS,Boot Reboot and Shutdown a System Safely,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Operations-Deployment/Boot-Reboot-and-Shutdown-a-System-Safely,"Linux Foundation Certified System Administrator (LFCS) Operations Deployment Boot Reboot and Shutdown a System Safely In this lesson, you will learn several methods to boot, reboot, and shut down a Linux system safely using system commands. Linux uses the systemctl command (short for ""system control"") to manage system states, and many of these commands require administrative privileges. The root account inherently has these privileges; however, regular users can execute them by prefixing commands with sudo. Root vs. Sudo If you are logged in as root, you do not need the sudo prefix for any of these commands. Below, we present a summary of the fundamental commands for rebooting and shutting down your system. Rebooting the System As the Root User When you are logged in as the root user, simply execute: $ systemctl reboot As a Regular User with Elevated Privileges If you are a regular user, use sudo to obtain temporary root privileges: $ sudo systemctl reboot
[sudo] password for aaron: Summary of Reboot Commands # When logged in as root:
$ systemctl reboot

# When using sudo as a regular user:
$ sudo systemctl reboot
[sudo] password for aaron: Shutting Down the System To safely shut down the system, whether as root or using sudo, execute: $ sudo systemctl power off
[sudo] password for aaron: In certain cases, an unresponsive or misbehaving program may cause the system to refuse a normal reboot or shutdown. In such instances, you can force the operation by appending the --force flag. Use this option only when absolutely necessary. $ sudo systemctl reboot --force or $ sudo systemctl power off --force If a single force does not succeed, you may specify the force flag twice. This method functions similar to pressing the physical reset button, immediately rebooting the system without allowing programs to close properly or save their data. Caution Forcing a reboot or shutdown can result in data loss. Use the force option only as a last resort. Scheduling Reboots and Shutdowns In managed server environments, you might need to perform scheduled reboots or shutdowns without manual intervention, often during off-peak hours. The shutdown command is ideal for this purpose. Schedule a Shutdown at a Specific Time Use the 24-hour format when specifying a shutdown time: $ sudo shutdown 02:00
[sudo] password for aaron: Schedule a Shutdown after a Delay To schedule a shutdown a certain number of minutes in the future, replace the time with a plus sign (+) followed by the delay in minutes. For example, to shut down after 15 minutes: $ sudo shutdown +15
[sudo] password for aaron: Schedule a Reboot To schedule a reboot, simply include the -r option. For instance: # Schedule a reboot at 2 a.m.
$ sudo shutdown -r 02:00
[sudo] password for aaron:

# Schedule a reboot after 15 minutes
$ sudo shutdown -r +15
[sudo] password for aaron: Utilizing the Wall Message Feature The shutdown command supports a wall message feature that notifies all logged-in users about an impending reboot or shutdown. This feature gives users time to save their work or prepare for disconnection. For example, to schedule a reboot in one minute while displaying a message about a scheduled Linux kernel upgrade, use: $ sudo shutdown -r +1 'Scheduled restart to upgrade our Linux kernel'
[sudo] password for aaron: Conclusion By following these system commands and scheduling techniques, you can manage the boot, reboot, and shutdown processes of a Linux system safely and efficiently. Make sure to use forced options only when absolutely necessary to prevent potential data loss, and always provide clear notifications to your users when scheduling system maintenance. For additional Linux administration tips, please refer to the official Linux documentation . Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Create systemd Services,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Operations-Deployment/Create-systemd-Services,"Linux Foundation Certified System Administrator (LFCS) Operations Deployment Create systemd Services In this lesson, we'll learn how to create a systemd service file through a practical exercise. Imagine you have an application that must always be running on your servers. Since applications can occasionally crash, a mechanism is required to monitor and automatically restart them. Additionally, you may want the application to start automatically when the system boots. Systemd services offer these features and more by defining the instructions to manage your application's lifecycle. Building the Sample Application Let's begin by creating a sample application. Open your preferred text editor with the following command: jeremy@kodekloud:~$ sudo vim /usr/local/bin/myapp.sh
[sudo] password for jeremy: Add the following content to the file. The first echo statement logs an informational message that ""MyApp"" has started, while the second statement logs an error after a five-second delay to simulate a crash: #!/bin/sh
echo ""MyApp Started"" | systemd-cat -t MyApp -p info
sleep 5
echo ""MyApp Crashed"" | systemd-cat -t MyApp -p err Explanation • systemd-cat sends messages directly to the system log. • The -t option attaches a tag (MyApp) to every log message. • The -p option sets the logging priority (using info for normal status and err for error). • The sleep 5 command simulates a delay of five seconds before logging a crash. Save the file and make it executable: jeremy@kodekloud:~$ sudo chmod +x /usr/local/bin/myapp.sh Understanding Systemd Service Options Detailed explanations for service unit files and configuration options are available in the systemd manual pages. For example, you can learn more by using: man systemd.service The manual explains useful configuration options. For instance, the Restart option specifies under which conditions systemd will restart your application. Even if your application always returns a zero exit code, configuring it to restart every time it exits can be beneficial. Additionally, the RestartSec option introduces a delay before restarting, which helps avoid creating a rapid restart loop that can overwhelm the system. Other parameters, such as RestartSec , RestartSteps , RestartMaxDelaySec , and TimeoutStartSec , offer fine-tuned control over the timing and behavior of service restarts in different scenarios. Creating a Service Template Instead of starting from scratch, you can use an existing service file as a template. Pre-existing service files are typically stored in the /lib/systemd/system directory. List the contents of this directory with: jeremy@kodekloud:~$ ls /lib/systemd/system As a starting point, copy the SSH service file to create your application service file: sudo cp /lib/systemd/system/ssh.service /etc/systemd/system/myapp.service After copying, open the file for editing. A standard service file is divided into three sections: [Unit] , [Service] , and [Install] . Modifying the Unit Section In the [Unit] section, update the description to better reflect your application. Additionally, adjust dependencies so the service starts only after critical services like networking and audit are available: [Unit]
Description=My Application
After=network.target auditd.service Tip Some examples might include conditions like ConditionPathExists=!/etc/ssh/sshd_not_to_be_run . This condition prevents the service from starting if the file exists. For our simple application, you can omit such conditions. Configuring the Service Section In the [Service] section, define how systemd should manage your application's lifecycle. Remove any settings that are irrelevant to a simple script, and insert a pre-start command that logs when systemd is about to start MyApp. Then specify your application script as the start command: [Service]
ExecStartPre=/bin/echo ""Systemd is preparing to start MyApp""
ExecStart=/usr/local/bin/myapp.sh
KillMode=process
Restart=always
RestartSec=1
Type=simple Key configuration options include: • KillMode=process instructs systemd to terminate only the main process. • Restart=always ensures that the service is restarted regardless of its exit status. • RestartSec=1 sets a one-second delay before each restart. • Type=simple is appropriate since the application does not send a readiness notification to systemd. Setting Up the Install Section The [Install] section controls how the service integrates with the system startup process. Typically, you would include the service in the multi-user.target to ensure it runs in a multi-user environment: [Install]
WantedBy=multi-user.target Your final service unit file should resemble the following: [Unit]
Description=My Application
After=network.target auditd.service

[Service]
ExecStartPre=/bin/echo ""Systemd is preparing to start MyApp""
ExecStart=/usr/local/bin/myapp.sh
KillMode=process
Restart=always
RestartSec=1
Type=simple

[Install]
WantedBy=multi-user.target Activating the Service After saving your service file in /etc/systemd/system/myapp.service , reload the systemd daemon so that it recognizes your new service: jeremy@kodekloud:~$ sudo systemctl daemon-reload Next, start the service with the following command: jeremy@kodekloud:~$ sudo systemctl start myapp.service Monitoring the Service Logs To view your service's logs in real time, use the journalctl command with the follow option: sudo journalctl -f A typical log output might look like this: May 22 16:46:55 kodekloud MyApp[1838]: MyApp Started
May 22 16:47:00 kodekloud MyApp[1842]: MyApp Crashed
May 22 16:47:00 kodekloud systemd[1]: myapp.service: Deactivated successfully.
May 22 16:47:00 kodekloud systemd[1]: myapp.service: Scheduled restart job, restart counter is at 4.
May 22 16:47:01 kodekloud systemd[1]: Starting myapp.service - My Application...
May 22 16:47:03 kodekloud systemd[1]: Started myapp.service - My Application.
May 22 16:47:04 kodekloud sudo[1852]:     jeremy : TTY=pts/0 ; PWD=/home/jeremy ; USER=root ; COMMAND=/usr/local/bin/myapp.sh
May 22 16:47:07 kodekloud MyApp[1856]: MyApp Crashed
May 22 16:47:07 kodekloud systemd[1]: myapp.service: Deactivated successfully.
May 22 16:47:07 kodekloud systemd[1]: myapp.service: Scheduled restart job, restart counter is at 5.
May 22 16:47:08 kodekloud systemd[1]: Starting myapp.service - My Application...
May 22 16:47:08 kodekloud systemd[1]: Started myapp.service - My Application.
May 22 16:47:08 kodekloud MyApp[1863]: MyApp Started When you have finished reviewing the logs, press Ctrl-C to exit. Further Exploration To learn more about the systemd options we used in this lesson, consult the following manual pages: man systemd.service
man systemd.unit
man systemd.exec
man systemd.kill Additionally, you can explore existing service files in the /lib/systemd/system directory to better understand various service configurations: jeremy@kodekloud:~$ ls /lib/systemd/system This command will list several service and target files that showcase different configurations used in practice. Conclusion This lesson has guided you through the process of creating a systemd service file, explaining each configuration option and demonstrating how to monitor and manage a service. Although there are many configuration options available, understanding the basics of setting up a service file makes managing your applications much more straightforward. As you continue working with systemd, you'll discover even more advanced configurations and options. Happy coding, and see you in the next lesson! Watch Video Watch video content Practice Lab Practice lab"
Linux Foundation Certified System Administrator LFCS,Configure the Repositories of Package Manager,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Operations-Deployment/Configure-the-Repositories-of-Package-Manager,"Linux Foundation Certified System Administrator (LFCS) Operations Deployment Configure the Repositories of Package Manager Most of the software you need is available in Ubuntu's official repositories. You can check the default repositories configured on your system by examining the appropriate file location for your release. In Ubuntu 22.04—the latest release at the time of this article—the repository configuration file may differ from previous versions. Older Ubuntu versions stored the file in a different path and even included comments referencing a new filename. Let’s examine the repository configuration in detail. Below is an excerpt from the file ""/etc/apt/sources.list.d/ubuntu.sources"": Types: deb
URIs: http://us.archive.ubuntu.com/ubuntu/
Suites: noble noble-updates noble-backports
Components: main restricted universe multiverse
Signed-By: /usr/share/keyrings/ubuntu-archive-keyring.gpg
Types: deb
URIs: http://security.ubuntu.com/ubuntu/
Suites: noble-security
Components: main restricted universe multiverse
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
~       
""/etc/apt/sources.list.d/ubuntu.sources"" 111L, 386B Breaking Down the Repository Configuration Note Types : The first line identifies the type of repository. Here, deb indicates a Debian-style repository, signifying that the package manager (e.g., apt) will work with Debian package files (with the .deb extension) containing executable programs, configuration files, documentation, and scripts. URIs : The next line specifies the Uniform Resource Identifier (URI) of the repository. In this instance, it points to the Ubuntu archive mirror in the United States ( us.archive.ubuntu.com ), with /ubuntu indicating the repository's content. This URI directs the package manager to the correct source for downloading packages. Suites : This field outlines the suites or release components provided in the repository. In Ubuntu, a suite represents a set of packages tied to a specific release. For the current release codenamed ""noble,"" three suites are defined: Suite Description noble The primary suite containing core packages for the release. noble-updates Contains bug fixes, security patches, and minor enhancements. noble-backports Includes packages backported from newer Ubuntu releases. Components : This line categorizes packages based on licensing and functionality: Component Description Main Contains free and open-source software that is officially supported by Ubuntu. Restricted Contains free and open-source software that has certain usage or redistribution restrictions. Universe Contains free and open-source software that is not officially supported by Ubuntu. Multiverse Contains packages that are non-free or have additional licensing restrictions. On servers, you typically primarily use packages from ""Main"" (and possibly ""Universe"" if needed). By default, Ubuntu enables all four components. Working with Additional Repositories Sometimes, the required software may not be available in the official Ubuntu repositories or may be outdated. In these situations, you can add third-party repositories—maintained by external teams or companies—that remain compatible with your system. For instance, if you need the latest stable version of Docker, begin by downloading the public key for Docker’s repository. Docker signs its packages with a private key; using the public key ensures the integrity of the packages by verifying their signatures. Downloading the Docker Public Key Execute the following command to download the Docker public key and save it as docker.key : jeremy@kodekloud:~$ curl ""https://download.docker.com/linux/ubuntu/gpg"" -o docker.key
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100 3817  100 3817    0     0  9651      0 --:--:-- --:--:-- --:--:-- 9663
jeremy@kodekloud:~$ After downloading, verify the file by listing the directory contents: jeremy@kodekloud:~$ ls -la
total 32
drwxr-x---  4 jeremy jeremy 4096 Jun  5 01:56 .
drwxr-xr-x  3 root   root   4096 Jun  5 01:47 ..
-rw-r--r--  1 jeremy jeremy  220 Mar 31 08:41 .bash_logout
-rw-r--r--  1 jeremy jeremy 3771 Mar 31 08:41 .bashrc
drwx------  2 jeremy jeremy 4096 Jun  5 01:48 .cache
-rw-r--r--  1 jeremy jeremy  807 Mar 31 08:41 .profile
drwxr-xr-x  2 jeremy jeremy 4096 Jun  5 01:47 .ssh
-rw-r--r--  1 jeremy jeremy    0 Jun  5 01:50 .sudo_as_admin_successful
jeremy@kodekloud:~$ Next, convert the key to a binary format using the gpg --dearmor command: jeremy@kodekloud:~$ gpg --dearmor docker.key
jeremy@kodekloud:~$ ls
docker.key  docker.key.gpg
jeremy@kodekloud:~$ Then, move the dearmored key into the apt keyrings directory: jeremy@kodekloud:~$ sudo mv docker.key.gpg /etc/apt/keyrings/
jeremy@kodekloud:~$ ls /etc/apt
apt.conf.d  keyrings  preferences.d  sources.list.d
jeremy@kodekloud:~$ ls /etc/apt/keyrings/
auth.conf.d  preferences.d  sources.list This method organizes third-party keys separately, making them easier to manage. If you ever need to disable a repository, simply remove its configuration file. Adding the Docker Repository Rather than altering the main sources file, create a new configuration file in the sources.list.d directory. For Docker, create a file named docker.list and insert the following configuration: deb [signed-by=/etc/apt/keyrings/docker.key.gpg] https://download.docker.com/linux/ubuntu noble stable This configuration line includes: deb : Specifies a Debian-style repository. The Docker repository URL. The distribution codename ( noble ). The component ( stable ) indicating the stable Docker software version. The [signed-by=...] option that points to the trusted public key. After saving the file, update the package manager's database with: jeremy@kodekLOUD:~$ sudo apt update The output should indicate that the Docker repository is being queried: jeremy@kodekloud:~$ sudo apt update
Get:1 https://download.docker.com/linux/ubuntu noble InRelease [48.8 kB]
Get:2 https://download.docker.com/linux/ubuntu noble/stable amd64 Packages [6,952 B]
Hit:3 http://us.archive.ubuntu.com/ubuntu noble InRelease
Get:4 http://security.ubuntu.com/ubuntu noble-security InRelease [126 kB]
...
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done Warning If you encounter errors related to package signatures, ensure that the dearmored public key is correctly located in /etc/apt/keyrings/docker.key.gpg . Using Personal Package Archives (PPAs) Ubuntu supports Personal Package Archives (PPAs) as a convenient method for adding third-party repositories. PPAs simplify the process and even allow you to create your own. For example, to add a PPA for the latest graphics drivers, use: sudo add-apt-repository ppa:graphics-drivers/ppa This command structure is simple: The ppa: prefix indicates a Personal Package Archive. It is followed by the username (e.g., graphics-drivers ) and the repository name ( ppa ). After executing the command, confirm the change by pressing Enter when prompted. This utility will handle key management, update repository configuration files, and refresh the package database automatically. To list all enabled PPAs, run: sudo add-apt-repository --list To remove a PPA, use: sudo add-apt-repository --remove ppa:graphics-drivers/ppa A prompt will appear asking for confirmation before removal. Final Steps After configuring repositories—whether they are official, third-party, or PPAs—it is important to update the package database one final time: sudo apt update This ensures that your system is aware of all available packages and updates. With the repositories correctly configured and verified, you are now ready to install software and manage your Ubuntu system effectively. Now, let’s move on to the next lesson. Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Git Staging and Committing Changes,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/Git-Staging-and-Committing-Changes,"Linux Foundation Certified System Administrator (LFCS) Essential Commands Git Staging and Committing Changes In this lesson, we explore how to effectively stage and commit changes using Git. The process involves three key steps: modifying files in your working directory, adding desired changes to the staging area, and finally committing these changes. Let's walk through each step in detail. 1. Modifying Files in the Working Area Your working area (or working directory) is the local project directory on your computer. Every modification—whether it's updating existing files or creating new ones—takes place here. For example, if you add a new file or change content within an existing file, you are modifying the working area. 2. Staging Changes The staging area gives you the flexibility to select which changes you want to include in your next commit. Instead of automatically tracking every change, Git allows you to only add the modifications relevant to a specific feature or bug fix. This selective process is useful when a file is still in progress or when grouping logically related changes together. For instance, if you add 50 lines of code today and plan to add another 50 lines tomorrow, you might not want to commit the half-finished feature immediately. Similarly, if you alter 10 different files to introduce a new feature, you can combine these into a single, cohesive commit. Demo Scenario Consider a project with two files. Start by creating these files with the following commands: echo ""This is the ORIGINAL line of code in file1"" > file1
echo ""This is the ORIGINAL line of code in file2"" > file2 To review what changes have been made, use the git status command. This command displays untracked files and changes in your working directory: jeremy@kodekloud:~/project$ git status
On branch master

No commits yet

Untracked files:
  (use ""git add <file>..."" to include in what will be committed)
    file1
    file2

nothing added to commit but untracked files present (use ""git add"" to track) Add the files to the staging area: jeremy@kodekloud:~/project$ git add file1 file2 After staging the files, run git status again to confirm that the files are ready for commit: jeremy@kodekloud:~/project$ git status
On branch master

No commits yet

Changes to be committed:
  (use ""git rm --cached <file>..."" to unstage)
    new file:   file1
    new file:   file2 If you need to unstage a file, run: git reset file2 This command removes file2 from the staging area but keeps it in your working directory. In this demo, we are satisfied with the staging configuration. Staging Multiple Files with Shortcuts In larger projects, staging or unstaging multiple files can be streamlined with patterns: Add HTML Files from the Current Directory git add ""*.html"" Remember to enclose the pattern in quotes to prevent shell expansion of the asterisk. Stage an Entire Subdirectory git add products/ Stage Only HTML Files from a Subdirectory git add ""products/*.html"" Unstage Multiple Files with a Pattern git reset ""products/*.html"" Note Staging with patterns can save time and ensure that only specific files are included in your commit. Always verify your changes using git status before committing. 3. Committing Changes Committing creates a snapshot of your project based on the files in the staging area. It’s best practice to include a concise commit message describing why the changes were made. For example, to commit the two files with a message, run: git commit -m ""Added our first two files to get the project started"" The output might look like this: [master (root-commit) e354e75] Added our first two files to get the project started
 2 files changed, 2 insertions(+)
 create mode 100644 file1
 create mode 100644 file2 When you commit, Git saves the current state of the staged files. Later, you can use tools like git diff to compare snapshots. The diff output highlights removed lines in red and added lines in green. Consider the following excerpt from a diff (from the Linux kernel repository): @@ -317,7 +317,7 @@ static int nfs_readdir_folio_array_append(struct folio *folio,
 name = nfs_readdir_copy_name(entry->name, entry->len);
 
-       array = kmap_atomic(folio_page(folio, 0));
+       array = kmap_local_folio(folio, 0);
 if (!name)
         goto out;
 ret = nfs_readdir_array_can_expand(array);
@@ -340,7 +340,7 @@ static int nfs_readdir_folio_array_append(struct folio *folio,
 out:
-       *cookie = array->last_cookie;
-       kunmap_atomic(array);
+       *cookie = array->last_cookie;
+       kunmap_local(array);
 return ret;
 } This diff shows how a function was replaced by a new one, helping team members clearly understand the changes. Tip Clear commit messages and detailed diffs are invaluable for collaborative environments where tracking changes is critical. Reviewing Project History Each commit in Git forms part of your project's history, allowing you to track all changes over time. A well-written commit message helps team members understand the purpose behind each update. Git not only detects additions but also modifications and deletions. Example: Removing a File Suppose you added a feature via a new file (file3) but later discovered a bug. First, create and commit the file: echo ""this feature is buggy"" > file3
git add file3
git commit -m ""Added new feature"" To remove file3 from both the working and staging areas, use: git rm file3 List the directory contents to verify that file3 has been removed: ls
# Output: file1  file2 Finally, commit the change to record the file removal: git commit -m ""Removed buggy feature file3"" Recap To summarize, Git change tracking consists of three main steps: Modify your working area (project directory). Stage the changes you want to capture. Commit these changes to save a snapshot of your project's state. Here’s a quick recap of staging and committing in a typical scenario: git add file1 file2
git commit -m ""Added our first two files to get the project started"" By following these steps, you ensure that each commit is deliberate and meaningful, aiding in both individual development and team collaboration. Advanced concepts such as branching and merging build on these foundations and will be covered in future lessons. Happy committing! Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Use Input Output Redirection e,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/Use-Input-Output-Redirection-e,"Linux Foundation Certified System Administrator (LFCS) Essential Commands Use Input Output Redirection e In this article, you'll learn how to effectively redirect input and output in Linux using simple utilities like sort, grep, and others. Understanding these concepts can help you streamline your workflow and automate routine tasks by controlling where commands read input from and where they send their output. Most Linux programs read input from a file or the keyboard (standard input) and write output to the terminal (standard output). By using redirection operators, you can change these default behaviors. Sorting File Content Assume you have a file named file.txt with the following numbers: $ cat file.txt
6
5
1
3
4
2 When you run the command: $ sort file.txt the sort utility reads file.txt, sorts the numbers, and displays the sorted output on the screen: 1
2
3
4
5
6 Many Linux utilities follow this pattern: reading input from a file, processing it, and then writing output to the terminal. Redirecting Output Sometimes, you may prefer to save the output of a command to a file rather than simply displaying it. Output redirection allows you to achieve that. For instance, to save the sorted result to a file, use the greater-than sign ( > ): $ sort file.txt > sorted_file.txt If sorted_file.txt does not exist, it will be created automatically. Note that using > will overwrite an existing file. Consider this example using the date command: $ date
Mon Nov  8 18:50:25 CST 2021
$ date > file.txt
$ date > file.txt
$ date > file.txt
$ date > file.txt Every time the date output is redirected, any previous content in file.txt is overwritten. Appending Output To append output to a file instead of overwriting it, use the double greater-than sign ( >> ): $ date >> file.txt Alternatively, you can explicitly specify standard output with a prefix: $ date > file.txt
$ date 1> file.txt Both commands have the same effect of redirecting standard output to file.txt. Standard Input, Standard Output, and Standard Error Linux commands use three default data streams: STDIN (Standard Input): The default source for input data. STDOUT (Standard Output): The default destination for regular output. STDERR (Standard Error): The default destination for error messages and warnings. Input redirection uses the less-than sign ( < ), and output redirection employs the greater-than sign ( > ). Because there are two output streams, you may specify which one to redirect by prefixing the operator with a digit: 1 refers to STDOUT and 2 refers to STDERR. To redirect error messages to a file, use: $ grep -r '^The' /etc/ 2>errors.txt This command sends error messages (such as ""Permission denied"") to errors.txt and prevents them from cluttering the terminal. A special file, /dev/null , can be used to discard output completely. For example: $ grep -r '^The' /etc/ 2>/dev/null Redirecting error messages to /dev/null effectively silences them. Redirecting Both STDOUT and STDERR There are times when you want to capture both regular output and error messages in a file. There are two common methods: Redirecting to Separate Files You can redirect standard output and error messages to separate files: $ grep -r '^The' /etc/ 1>output.txt 2>errors.txt To append to existing files rather than overwriting them, use: $ grep -r '^The' /etc/ 1>>output.txt 2>>errors.txt Merging STDOUT and STDERR into One File Use the syntax 2>&1 to merge STDERR with STDOUT so both are redirected to the same file: $ grep -r '^The' /etc/ >all_output.txt 2>&1 It is essential to place the redirection operators in the correct order. For example, writing: $ grep -r '^The' /etc/ 2>&1 1>all_output.txt will not work as intended because STDERR gets redirected before STDOUT is updated. Input Redirection Some commands do not allow you to specify an input file directly because they expect input from the keyboard. Input redirection solves this problem. Consider a hypothetical command, sendemail , that expects you to type an email message: $ sendemail [email protected] Hi Someone,
How are you today?
...
Talk to you soon
Bye You can simulate this input by redirecting the contents of a file (for example, emailcontent.txt) to the command: $ sendemail [email protected] < emailcontent.txt This approach eliminates the need for manual input while still sending the email content. Here Documents and Here Strings A here document (or ""here doc"") allows you to pass multiple lines of text as input to a command. You denote the beginning and end of the text block with a delimiter (commonly EOF): $ sort <<EOF
6
3
2
5
1
4
EOF
1
2
3
4
5
6 This method encapsulates both the command and its input in a single self-contained block. A here string is a simplified variant that provides a single line of input. Everything following the redirection operator ( <<< ) is treated as input: $ bc <<< 1+2+3+4
10 This is especially useful for commands like bc where you want to quickly evaluate a mathematical expression without entering an interactive session. Piping Output Between Programs Individual Linux utilities often perform a single task very well. However, by combining them with pipes, you can create powerful command chains to handle more complex operations. For example, suppose you want to extract configuration settings from /etc/login.defs , ignore comments, sort the settings alphabetically, and neatly format the output into columns. You can accomplish this by chaining commands together: $ grep -v '^#' /etc/login.defs | sort | column -t Here's what each command does: grep -v '^#' /etc/login.defs removes commented lines. sort arranges the remaining lines in alphabetical order. column -t aligns the output into well-formatted columns. Benefits of Piping Piping is a powerful technique that lets you pass the output of one command directly as input to another. This not only simplifies complex tasks but also allows you to create efficient workflows. By mastering these techniques for redirecting input, output, and error streams, as well as leveraging pipes, you can customize the behavior of Linux commands to suit a wide range of tasks. These methods not only simplify command-line operations but also enable you to automate processing within scripts effectively. For further learning, consider reviewing the Linux Command Line Basics and related technical guides. Watch Video Watch video content Practice Lab Practice lab"
Linux Foundation Certified System Administrator LFCS,Manage Startup Process and Services In Services Configuration,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Operations-Deployment/Manage-Startup-Process-and-Services-In-Services-Configuration,"Linux Foundation Certified System Administrator (LFCS) Operations Deployment Manage Startup Process and Services In Services Configuration This article explains how to manage startup processes and services in Linux. During the boot sequence, Linux automatically launches several critical applications in a defined order. For example, if Application 2 depends on Application 1, Application 1 will load first. Additionally, if a critical application crashes, the system is configured to automatically restart it to ensure uninterrupted operation. The Role of the Init System and Systemd Units The startup and management of services in Linux are controlled by the init system. This system uses configuration files called systemd units to determine how applications should be started, what actions to take when an application fails, and other necessary operations. The term systemd refers both to the suite of tools that manage Linux systems and the primary program that acts as the init system. Systemd ensures smooth system operation by initializing and monitoring various system components. There are several types of systemd units such as service, socket, device, and timer units. For example, timer units can schedule tasks like weekly file cleanups or database verifications. In this guide, the focus is on service units. A service unit provides systemd with all the details required to manage an application’s lifecycle. This includes the command to start the application, what to do if it crashes, how to reload configurations, and more. To explore the various options available in a service unit file, run: man systemd.service Example: Managing the SSH Daemon Many Linux servers run an SSH daemon to enable remote connections. Systemd manages this daemon using a specific service unit. You can display the SSH service unit file by executing: $ systemctl cat ssh.service The output might resemble the following: # /lib/systemd/system/ssh.service
[Unit]
Description=OpenBSD Secure Shell server
Documentation=man:sshd(8) man:sshd_config(5)
After=network.target auditd.service
ConditionPathExists=!/etc/ssh/sshd_not_to_be_run

[Service]
EnvironmentFile=/etc/default/ssh
ExecStartPre=/usr/sbin/sshd -t
ExecStart=/usr/sbin/sshd -D $SSHD_OPTS
ExecReload=/usr/sbin/sshd -t
ExecReload=/bin/kill -HUP $MAINPID
KillMode=process
Restart=on-failure
RestartPreventExitStatus=255
Type=notify
RuntimeDirectory=sshd
RuntimeDirectoryMode=0755 In this file: ExecStart specifies the command used to launch the SSH daemon. ExecReload defines the commands to reload the SSH configuration. Restart=on-failure ensures that systemd automatically restarts the service if it crashes. If the SSH daemon fails, systemd will restart it to maintain remote connectivity. Checking Service Status To verify the status of the SSH service, run: $ sudo systemctl status ssh.service A typical output may look like this: ● ssh.service - OpenBSD Secure Shell server
   Loaded: loaded (/etc/systemd/system/ssh.service; enabled; vendor preset: enabled)
   Active: active (running) since Wed 2024-02-28 18:32:18 UTC; 2h 29min ago
     Docs: man:sshd(8)
           man:sshd_config(5)
 Main PID: 688 (sshd)
    Tasks: 1 (limit: 4558)
   Memory: 7.6M
      CPU: 88ms
   CGroup: /system.slice/ssh.service
           └─688 ""sshd: /usr/sbin/sshd -D [listener] 0 of 10-100 startups""

Feb 28 18:32:18 kodekloud systemd[1]: Starting OpenBSD Secure Shell server...
Feb 28 18:32:18 kodekloud sshd[688]: Server listening on 0.0.0.0 port 22.
Feb 28 18:32:18 kodekloud sshd[688]: Server listening on :: port 22.
Feb 28 18:32:18 kodekloud systemd[1]: Started OpenBSD Secure Shell server. The output indicates whether the service is enabled to start at boot, confirms that the process is running, and displays the process identifier (PID) along with log messages for troubleshooting. Starting, Stopping, Restarting, and Reloading Services You can manually manage services using various systemctl commands: Stop a service: $ sudo systemctl stop ssh.service Start a service: $ sudo systemctl start ssh.service Restart a service: This command stops and then starts the service to apply new configurations. $ sudo systemctl restart ssh.service Reload a service: This command reloads the service’s configuration without interrupting active sessions, which is particularly useful when users are connected. $ sudo systemctl reload ssh.service After modifying the SSH configuration file located at /etc/ssh/sshd_config , you can enforce the new settings with either of the following commands: $ sudo systemctl restart ssh.service   # Fully stops and starts the service.
$ sudo systemctl reload ssh.service    # Gracefully reloads the configuration. You can review the status again to confirm the service behavior: $ systemctl status ssh.service
Feb 28 21:42:16 kodekloud systemd[1]: Stopped OpenBSD Secure Shell server.
Feb 28 21:42:16 kodekloud systemd[1]: Starting OpenBSD Secure Shell server...
Feb 28 21:42:48 kodekloud systemd[1]: Reloading OpenBSD Secure Shell server...
Feb 28 21:42:48 kodekloud sshd[2413]: Received SIGHUP; restarting.
Feb 28 21:42:48 kodekloud systemd[1]: Reloaded OpenBSD Secure Shell server. Note Not all applications support configuration reloads. When in doubt, systemd will first attempt a graceful reload and then perform a full restart if necessary. Enabling and Disabling Services To prevent a service from starting automatically at boot, use the disable command: $ sudo systemctl disable ssh.service
$ systemctl status ssh.service
# Output shows ""disabled"" in the Loaded line. Verify the service's enablement status with: $ systemctl is-enabled ssh.service To enable the service for automatic startup at boot, run: $ sudo systemctl enable ssh.service If you want a daemon to start immediately and at boot, you can use the --now option: $ sudo systemctl enable --now ssh.service Likewise, to stop a service immediately and disable it for future boots, run: $ sudo systemctl disable --now ssh.service Warning Be cautious when disabling critical services such as SSH, particularly on remote systems. Masking Services Some services might restart automatically even after being stopped or disabled because other services trigger them. In such cases, you can mask the service to completely prevent it from starting. For example, to prevent the at daemon from being activated: $ sudo systemctl mask atd.service A masked service cannot be enabled or started. Any attempt to do so will generate an error: Failed to enable unit: Unit file /etc/systemd/system/atd.service is masked.
Failed to start atd.service: Unit atd.service is masked. To reverse the masking and allow the service to operate normally, run: $ sudo systemctl unmask atd.service Listing Service Units Sometimes the service name for an installed application may not be obvious (for instance, Apache might be listed as apache.service or httpd.service depending on your distribution). To display all service units regardless of their state, use: $ sudo systemctl list-units --type service --all This command lists service units in various states—active, inactive, enabled, or disabled—ensuring you have a complete overview. An example output may look like: UNIT LOAD ACTIVE SUB DESCRIPTION accounts-daemon.service loaded active running Accounts Service alsa-restore.service loaded inactive dead Save/Restore Sound Card alsa-state.service loaded active running Manage Sound Card State apparmor.service not-found inactive dead apparmor.service atd.service loaded active running Job spooling tools auditd.service loaded active running Security Auditing Service auth-rpcgss-module.service loaded inactive dead Kernel Module support In addition to service units, systemd also manages other types of units such as sockets and timers. In summary, this article demonstrated how systemd manages Linux services through service units—from starting and stopping to restarting and reloading configurations. Mastering these commands is essential for efficient system management and ensuring continuous system reliability. Additional Resources Systemd Documentation Understanding Linux Systemd Services Happy managing! Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Use Scripting to Automate System Maintenance Tasks,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Operations-Deployment/Use-Scripting-to-Automate-System-Maintenance-Tasks,"Linux Foundation Certified System Administrator (LFCS) Operations Deployment Use Scripting to Automate System Maintenance Tasks In this article, we explore how to leverage scripting to automate essential system maintenance tasks on Linux. By utilizing bash scripting, you can log system information, archive important data, and streamline routine maintenance operations. When you log into a Linux system, you are immediately presented with a command-line interface. After a successful login, the bash shell starts, providing a text-based environment. Bash interprets every command you type, which is why it is also known as a command interpreter or shell. An interactive bash session typically looks like this: [aaron@kodekcloud]$ date
Mon Dec  6 16:28:09 CST 2021 In interactive mode, you type commands, press Enter, and view the results instantly. However, bash can also execute scripts—files containing multiple commands that are executed sequentially. ────────────────────────────── Creating a Simple Script Let's start by creating a basic script called script.sh. Although the "".sh"" extension is optional, it helps identify the file as a script when browsing directories. First, create and open the file for editing: $ touch script.sh
$ vim script.sh Begin your script with the shebang line, which must be the very first line in the file without any leading spaces: #!/bin/bash The shebang (#! followed by the interpreter's full path) specifies that bash should be used to execute the script. Next, add a comment to document the purpose of the script. Comments start with a ""#"" and are ignored during execution: #!/bin/bash
# Log the date and time when the script was executed
date >> /tmp/script.log In this snippet, the date command appends the current date and time to /tmp/script.log. This technique can be extended to redirect other outputs and errors. Now, include another command to log the Linux kernel version from the /proc/version file: #!/bin/bash
# Log the date and time when the script was executed
date >> /tmp/script.log
# Append the current Linux kernel version to the log
cat /proc/version >> /tmp/script.log After saving your changes (for example, by typing :wq in vim), make the script executable: $ chmod +x script.sh To run the script from the current directory, execute: $ ./script.sh Verify the output by checking the log file: $ cat /tmp/script.log
Linux version 5.15.0-94-generic (buildd@lcy02-amd64-096) (gcc …) This script now logs both the date/time and the kernel version, which can be very helpful for maintaining system records. Later, you might consider scheduling this script to run automatically at regular intervals. ────────────────────────────── Enhancing Scripts with Bash Built-ins Bash includes a variety of built-in commands that can make your scripts more intelligent and efficient—these include conditional statements, loops, and more. To see a list of available built-in commands, simply execute: $ help This command displays built-ins such as if, test, alias, and others. ────────────────────────────── Archiving Application Data Next, let’s create a script that archives the contents of the /etc/apt directory. Even if the file does not exist, your editor (like vim) will create it once you save. Open a new file for editing: $ vim archive-apt.sh Enter the following content to create a tar archive: #!/bin/bash
tar acf /tmp/archive.tar.gz /etc/apt/ This script uses tar to create an archive at /tmp/archive.tar.gz that contains the entire /etc/apt directory. After making the script executable and running it, you can inspect the archive contents using: tar -tf /tmp/archive.tar.gz Note Re-running this script will overwrite the existing archive. Adding backup logic can preserve previous archives. ────────────────────────────── Archive Script with Backup Rotation To avoid unintended data loss, create a script that implements backup rotation for the archive file: $ vim archive-apt-2.sh Add the following content: #!/bin/bash
if test -f /tmp/archive.tar.gz; then
    mv /tmp/archive.tar.gz /tmp/archive.tar.gz.OLD
    tar acf /tmp/archive.tar.gz /etc/apt/
else
    tar acf /tmp/archive.tar.gz /etc/apt/
fi This script checks if the file /tmp/archive.tar.gz exists. If it does, the script renames it to /tmp/archive.tar.gz.OLD before creating a new archive. Otherwise, it simply creates the archive. Make the script executable and run it: $ chmod +x archive-apt-2.sh
$ ./archive-apt-2.sh
$ ls /tmp
archive.tar.gz
archive.tar.gz.OLD
script.log Now you have both the new archive and a backup available for reference. ────────────────────────────── Understanding Exit Status Codes and Conditional Execution In bash, every command returns an exit status. A zero value usually indicates success, while a non-zero value signals an error or a condition that was not met. For example, the grep command returns zero if a match is found and one if no match is found. Consider the following script, which checks if the file /etc/default/grub contains the digit '5': #!/bin/bash
if grep -q '5' /etc/default/grub; then
    echo 'Grub has a timeout of 5 seconds.'
else
    echo 'Grub DOES NOT have a timeout of 5 seconds.'
fi Here, the -q option makes grep work in quiet mode, suppressing output and relying solely on its exit status for the conditional check. ────────────────────────────── Reviewing Common Script Structures Many essential system scripts are located in directories such as /etc, /cron.daily, /cron.weekly, and /cron.monthly. These scripts typically follow common conventions: starting with the shebang, using descriptive comments, and employing structured if/else statements. Below is an example shell script that checks whether anacron was run today and confirms if the system is running on battery power: #!/bin/sh
# Check whether anacron was run today already
if test -r /var/spool/anacron/cron.daily; then
    day=`cat /var/spool/anacron/cron.daily`
fi
if [ `date +%Y%m%d` = ""$day"" ]; then
    exit 0
fi

# Do not run jobs when on battery power
online=1
for psupply in AC ADP0 ; do
    sysfile=""/sys/class/power_supply/$psupply/online""
    if [ -f $sysfile ] ; then
        if [ `cat $sysfile 2>/dev/null` = 1 ]; then
            online=1
            break
        else
            online=0
        fi
    fi
done
if [ $online = 0 ] ; then
    exit 0
fi This script exemplifies standard practices in shell scripting, including proper use of the shebang, commenting, and managing conditionals with if/else statements and loops. ────────────────────────────── Further Learning To further enhance your bash scripting skills, consider exploring advanced courses. The Shell Scripts for Beginners course on KodeKloud is an excellent resource that provides hands-on labs and practical exercises along with clear theoretical explanations. This concludes our guide on using scripting to automate system maintenance tasks. By mastering these techniques, you can ensure efficient system performance and streamline routine administrative operations. Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Git Basic Operations,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Essential-Commands/Git-Basic-Operations,"Linux Foundation Certified System Administrator (LFCS) Essential Commands Git Basic Operations In today's fast-paced software development environment, large teams often consist of 10 or more developers collaborating simultaneously. With numerous people adding, deleting, and modifying code concurrently, it can be challenging to keep track of every change made in just 24 hours. For instance, while the testing team may report a bug, one developer could be fixing it across multiple files, another adding new features by creating new files and modifying existing ones, and yet another removing outdated features. Manual tracking through messages or chats is quickly overwhelming. When you log into your computer as part of this dynamic team, you need an efficient way to quickly see: What files have been added, modified, or removed Which lines of code have been changed Who made those changes Why the changes were made This comprehensive overview not only helps you understand your teammates’ contributions but also effectively communicates your own modifications. One robust solution is using a distributed version control system like Git. Git allows many developers to work on the same project concurrently while automatically keeping track of every change. Instead of relying on manual updates or informal messaging, Git provides a structured method to both update and review changes across your team. With Git, every change is recorded along with metadata—such as the author’s name, email, and a descriptive message—providing a clear history of modifications from your last visit. Git Repositories: Local and Remote At the core of Git is the repository. Each developer typically works with two repositories: A local repository on their own machine. A remote repository hosted on platforms like GitHub that the entire team accesses. When you work on your local repository, you later upload (or ""push"") your changes to the remote repository. Similarly, at the start of your work session, you download (or ""pull"") the latest changes made by your colleagues. Every project on platforms like GitHub is essentially a remote Git repository. By pushing your changes, you integrate your work with the team, and by pulling updates, you ensure you always have the most recent project version. When you're ready to share your latest code, you perform a push operation: To update your local repository with changes made by others, you execute a pull: The best way to grasp Git's functionality is to experience it firsthand. In this lesson, we will walk through practical Git exercises via the command line. Setting Up Git Most modern Linux distributions come with Git pre-installed. However, if you need to install Git, use the following command: sudo apt install git You might see output similar to this: Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
git is already the newest version (1:2.43.0-1ubuntu7).
0 upgraded, 0 newly installed, 0 to remove and 4 not upgraded. After installing Git, the first crucial step is to set your username and email address. These details are embedded in each commit to identify the contributor. Configuring Git User Details To set your identity globally, run the following commands: git config --global user.name ""jeremy""
git config --global user.email "" [email protected] "" If you want to specify a unique identity for a particular repository, simply omit the --global flag and run the commands inside that repository. Now, let's create a directory for our project and initialize a Git repository: mkdir project
cd project/
git init After executing git init , you should see a message indicating that an empty Git repository has been created. For example: hint: Using 'master' as the name for the initial branch. This default branch name
hint: is subject to change. To configure the initial branch name to use in all
hint: of your new repositories, which will suppress this warning, call:
hint:
hint:   git config --global init.defaultBranch <name>
Initialized empty Git repository in /home/jeremy/project/.git/ Listing the contents of the hidden .git directory will reveal various files and folders where Git stores repository data: ls .git A sample output might look like this: branches  config  description  HEAD  hooks  info  objects  refs To review your repository configuration (including your user details), check the config file: cat .git/config This should output something similar to: [core]
        repositoryformatversion = 0
        filemode = true
        bare = false
        logallrefupdates = true
[user]
        name = jeremy
        email = [email protected] Tracking File Changes Imagine you're just beginning your journey in the project directory. Let's create two files— file1 and file2 —and add an initial line of code to each: echo ""This is the ORIGINAL line of code in file1"" > file1
echo ""This is the ORIGINAL line of code in file2"" > file2 It's important to note that Git does not automatically track every file change continuously. Instead, you must explicitly instruct Git when changes occur using commands like git add and git commit . This process creates a snapshot of your project at that moment in time. This initial setup demonstrates how Git monitors code modifications and manages repository settings within the .git directory. In our next lesson, we will explore how to stage changes and commit them, integrating your work seamlessly with that of your teammates. Up Next Stay tuned for the next lesson where we dive into staging and committing your changes with Git! Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Manage and Configure Virtual Machines,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Operations-Deployment/Manage-and-Configure-Virtual-Machines,"Linux Foundation Certified System Administrator (LFCS) Operations Deployment Manage and Configure Virtual Machines In modern software, virtualization allows you to create a virtual computer—or virtual machine (VM)—within your actual computer. This capability is particularly valuable in server environments because it lets a single physical machine serve multiple clients simultaneously. For instance, imagine a powerful server equipped with 64 CPU cores and 1024 GB of RAM. By creating 32 virtual machines, each VM can be allocated 2 virtual CPUs (vCPUs) and 32 GB of RAM. Rather than renting out one enormous server to a single client, you can offer 32 smaller virtual servers. This is the foundation of cloud compute services provided by DigitalOcean , Amazon Web Services , and Google Cloud . Among many available tools for virtualization on Linux, QEMU-KVM has become the most popular. QEMU (Quick Emulator) emulates virtual computers, while KVM (Kernel-based Virtual Machine) integrates into the Linux kernel to leverage hardware acceleration for enhanced performance. In this guide, we focus on using a command-line tool called virsh (or VIRSH) to manage virtual machines. If you’ve used VirtualBox before, you might recall its graphical interface for VM creation, configuration, and management. Virsh, however, uses terminal commands to achieve similar tasks, making it a powerful choice for administrators. Getting Started To quickly begin, install the virt-manager package. Although virt-manager is designed for systems with a GUI, installing it will bring in many useful dependencies for headless or text-based environments. Run the following command: sudo apt install virt-manager When installing virt-manager, your package manager will fetch several dependency packages. The output may look similar to this: Get:95 http://us.archive.ubuntu.com/ubuntu noble/universe amd64 gir1.2-gtksource-4 amd64
Get:96 http://us.archive.ubuntu.com/ubuntu noble/universe amd64 spice-client-glib-usb-acl
Get:97 http://us.archive.ubuntu.com/ubuntu noble/main amd64 libpcsclite1 amd64 2.0.3-1build1
Get:98 http://us.archive.ubuntu.com/ubuntu noble/main amd64 libcaca0 amd64 1:2.8.0-3build1
Get:99 http://us.archive.ubuntu.com/ubuntu noble/main amd64 liborc-0.4-0t6 amd64 1:0.4.3-1
Get:100 http://us.archive.ubuntu.com/ubuntu noble/main amd64 libgstreamer-plugins-base1.0
Get:101 http://us.archive.ubuntu.com/ubuntu noble/universe amd64 libphodav-3.0-common
Get:102 http://us.archive.ubuntu.com/ubuntu noble/main amd64 libproxy1v5 amd64 0.5.4-4build1
Get:103 http://us.archive.ubuntu.com/ubuntu noble/main amd64 glib-networking-common
Get:104 http://us.archive.ubuntu.com/ubuntu noble/main amd64 glib-networking-services
Get:105 http://us.archive.ubuntu.com/ubuntu noble/main amd64 glib-networking amd64 2.80.0
Get:106 http://us.archive.ubuntu.com/ubuntu noble/main amd64 libsoup-3.0-common
Get:107 http://us.archive.ubuntu.com/ubuntu noble/main amd64 libsoup-3.0-0
18% [107 libsoup-3.0-0 194 B/289 kB 0%] Even though you may not use the virt-manager graphical interface, its installation provides several utilities essential for virtual machine management. Below is a snippet of further installation output: Get:251 http://us.archive.ubuntu.com/ubuntu noble/main amd64 ovmf all 2024.02-2 [4,571 kB]
Fetched 130 MB in 10s (13.2 MB/s)
Extracting templates from packages: 100%
Selecting previously unselected package acl.
(Read database ... 83,334 files and directories currently installed.)
Preparing to unpack .../000-acl_2.3.2-1build1_amd64.deb ...
Unpacking acl (2.3.2-1build1) ...
Selecting previously unselected package libgdk-pixbuf2.0-common.
Preparing to unpack .../001-libgdk-pixbuf2.0-common_2.42.10+dfsg-3ubuntu3_all.deb ...
Unpacking libgdk-pixbuf2.0-common (2.42.10+dfsg-3ubuntu3) ...
Selecting previously unselected package libgdk-pixbuf-2.0-0:amd64.
Preparing to unpack .../002-libgdk-pixbuf-2.0-0_2.42.10+dfsg-3ubuntu3_amd64.deb ...
Unpacking libgdk-pixbuf-2.0-0:amd64 (2.42.10+dfsg-3ubuntu3) ...
Selecting previously unselected package gtk-update-icon-cache.
Preparing to unpack .../003-gtk-update-icon-cache_3.24.41-4ubuntu1_amd64.deb ...
Unpacking gtk-update-icon-cache (3.24.41-4ubuntu1) ...
Selecting previously unselected package hicolor-icon-theme.
Preparing to unpack .../004-hicolor-icon-theme_0.17-2_all.deb ...
Unpacking hicolor-icon-theme (0.17-2) ...
Selecting previously unselected package humanity-icon-theme.
Preparing to unpack .../005-humanity-icon-theme_0.6.16_all.deb ...
Unpacking humanity-icon-theme (0.6.16) ... Creating a Virtual Machine Configuration Let's proceed with creating a configuration file for a virtual machine. Begin by creating a directory to store your VM definitions and then create an XML file using your preferred text editor: jeremy@kodekloud:~$ mkdir machines
jeremy@kodekloud:~$ cd machines/
jeremy@kodekloud:~/machines$ vim testmachine.xml The XML file below defines a virtual machine named ""TestMachine"" running under QEMU. It allocates 1 GiB of RAM, 1 vCPU, and uses a 64-bit (x86_64) architecture with hardware-assisted virtualization (HVM): <domain type=""qemu"">
  <name>TestMachine</name>
  <memory unit=""GiB"">1</memory>
  <vcpu>1</vcpu>
  <os>
    <type arch=""x86_64"">hvm</type>
  </os>
</domain> In a production setting, a complete VM configuration will include additional parameters such as storage, network interfaces, and the operating system. For demonstration purposes, this basic setup is sufficient. Define the virtual machine using the following command: virsh define testmachine.xml You should see an output similar to: jeremy@kodekloud:~/machines$ virsh define testmachine.xml
Domain 'TestMachine' defined from testmachine.xml
jeremy@kodekloud:~/machines$ Managing Virtual Machines with virsh The virsh tool has an extensive help page. To display it, use: virsh help By default, only active domains are listed. To view all defined domains (including inactive ones), run: virsh list --all You should see ""TestMachine"" listed, albeit with the state ""shut off"". Starting and Managing VM States To start your virtual machine, execute: virsh start TestMachine If your VM name includes spaces, enclose it in double quotes. Once started, verify its state with: virsh list To reboot the virtual machine gracefully – allowing software applications to exit properly – run: virsh reboot TestMachine If the machine becomes unresponsive, you can force a reset, which is analogous to pressing a hardware reset button: jeremy@kodekloud:~/machines$ virsh list
 Id    Name         State
--------------------------------
 1     TestMachine  running

jeremy@kodekloud:~/machines$ virsh reset TestMachine
Domain 'TestMachine' was reset
jeremy@kodekloud:~/machines$ Shutting Down and Destroying VMs To shut down the virtual machine gracefully (if the guest OS is present and active), use: virsh shutdown TestMachine For instance: jeremy@kodekloud:~/machines$ virsh shutdown TestMachine
Domain 'TestMachine' is being shutdown Since this test VM may not have an operating system installed, the shutdown command might not work as expected. However, in real scenarios, it allows applications to close properly. If the VM is unresponsive, you can force a hard power off (equivalent to unplugging the machine) as follows: virsh destroy TestMachine Sample workflow: jeremy@kodekloud:~/machines$ virsh list
 Id    Name             State
----------------------------------
 1     TestMachine      running

jeremy@kodekloud:~/machines$ virsh reset TestMachine
Domain 'TestMachine' was reset

jeremy@kodekloud:~/machines$ virsh shutdown TestMachine
Domain 'TestMachine' is being shutdown

jeremy@kodekloud:~/machines$ virsh list
 Id    Name             State
----------------------------------
 1     TestMachine      running

jeremy@kodekloud:~/machines$ virsh destroy TestMachine
Domain 'TestMachine' destroyed

jeremy@kodekloud:~/machines$ Note The destroy command only powers off the VM abruptly—it does not remove the VM's definition. To completely remove the VM, you must undefine it. To remove the VM's configuration, execute: virsh undefine TestMachine If you also want to remove any associated storage files, use: virsh undefine --remove-all-storage TestMachine After undefining, verify removal with: virsh list --all If needed, you can recreate the VM by redefining the XML file: jeremy@kodekloud:~/machines$ virsh define testmachine.xml
Domain 'TestMachine' defined from testmachine.xml
jeremy@kodekloud:~/machines$ Enabling Autostart To ensure that your virtual machine automatically starts when the host system boots, enable autostart: virsh autostart TestMachine Conversely, disable autostart with: virsh autostart --disable TestMachine Modifying VM Resources To inspect the resources assigned to a VM, use: virsh dominfo TestMachine Changing vCPU Count Suppose you wish to increase the number of vCPUs from one to two. First, check the current allocation: virsh dominfo TestMachine Use the setvcpus command to change the configuration. You can see command options by viewing: virsh help setvcpus The help output shows the syntax and options: NAME
   setvcpus - change number of virtual CPUs

SYNOPSIS
   setvcpus <domain> <count> [--maximum] [--config] [--live] [--current] [--guest] [--hotpluggable]

DESCRIPTION
   Change the number of virtual CPUs in the guest domain.

OPTIONS
   [--domain] <string>    domain name, id or uuid
   [--count] <number>     number of virtual CPUs
   --maximum              set maximum limit on next boot
   --config               affect next boot
   --live                 affect running domain
   --current              affect current domain
   --guest                modify cpu state in the guest
   --hotpluggable         make added vcpus hot(un)pluggable To permanently change the vCPU count (affecting the next boot), run: jeremy@kodekloud:~/machines$ virsh setvcpus TestMachine 2 --config If you encounter an error indicating that the requested vCPUs exceed the current maximum (e.g., ""2 > 1""), update the maximum allowed vCPUs with: jeremy@kodekloud:~/machines$ virsh setvcpus TestMachine 2 --config --maximum Remember that modifying the vCPU count does not change a running VM. You must destroy (power off) the VM first: virsh destroy TestMachine Then, start it again: virsh start TestMachine Verify the changes with: virsh dominfo TestMachine Changing Memory Allocation To adjust memory allocation, first set the maximum allowed memory and then modify the allocation. For example, to change the memory to 2048 MB, run: virsh setmem TestMachine 2048M --config Then, shut down the machine gracefully (or force shutdown if necessary) and restart it: jeremy@kodekloud:~/machines$ virsh shutdown TestMachine
Domain 'TestMachine' is being shutdown
jeremy@kodekloud:~/machines$ virsh destroy TestMachine
Domain 'TestMachine' destroyed
jeremy@kodekloud:~/machines$ virsh start TestMachine
Domain 'TestMachine' started Finally, confirm the configuration update: virsh dominfo TestMachine A typical output might be: Id:             3
Name:           TestMachine
UUID:           a10f764d-f147-4878-97d0-d89e7918f34
OS Type:        hvm
State:          running
CPU(s):         2
CPU time:       1.1s
Max memory:     2097152 KiB
Used memory:    2097152 KiB
Persistent:     yes
Autostart:      enable
Managed save:   no
Security model: none
Security DOI:   0 This confirms that your resource modifications have taken effect. Conclusion This guide walked you through creating and managing virtual machines using virsh. By following these steps, you can leverage virtualization to efficiently allocate server resources and adapt configurations to meet your specific needs. For further details, consider exploring additional Kubernetes Documentation or the Terraform Registry for advanced deployment scenarios. Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Create and Manage Containers,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Operations-Deployment/Create-and-Manage-Containers,"Linux Foundation Certified System Administrator (LFCS) Operations Deployment Create and Manage Containers Docker containers have become immensely popular due to their portability and ease of management. To illustrate their usefulness, let’s compare a traditional MariaDB database setup with one running inside a Docker container. Traditionally, to install MariaDB on a Linux system, you would run: $ sudo apt install mariadb-server After the installation, you would need to adjust configurations in the /etc/mysql directory, create databases, configure users, assign privileges, and manage log files. When it comes time to migrate this multi-component setup to a cloud server, the scattered pieces across various directories can make the process cumbersome. Key Insight By containerizing your application, such as setting up your MariaDB inside a Docker container, every component—including the daemon, configuration files, logs, and databases—resides within a single container. This makes migration as simple as copying one file. Imagine moving your entire database within a single container to a cloud server or scaling it across multiple servers effortlessly. Containers are designed to encapsulate applications, making them highly portable and scalable. Essentially, they are like copying and pasting an entire application setup in one go. In this article, we will explore practical examples of how to work with Docker by running an Nginx web server within a container. Using Docker Commands Before diving into running containers, note that you can often execute Docker commands without sudo if your Linux user is added to the docker group. If you encounter permission errors, you can either prepend sudo to your commands or add your user to the Docker group and then log out and back in. For example, you might see an error like this: aaron@kodekloud:~$ docker images
Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get ""http://%2Fvar%2Frun%2Fdocker.sock/1.24/images/json"": dial unix /var/run/docker.sock: connect: permission denied By using sudo , the command runs successfully: aaron@kodekloud:~$ sudo docker images

[sudo] password for aaron:
REPOSITORY         TAG            IMAGE ID        CREATED        SIZE
john/custom nginx   1.0           ed97200c0c18    3 hours ago    187MB
customnginx         1.0           ed97200c0c18    3 hours ago    187MB
nginx               latest        021283c8eb95    3 weeks ago    187MB Exploring Docker Commands If you're new to Docker, you can display all available commands by running: $ docker --help This command provides the following overview: Usage:  docker [OPTIONS] COMMAND

A self-sufficient runtime for containers

Common Commands:
  run        Create and run a new container from an image
  exec       Execute a command in a running container
  ps         List containers
  build      Build an image from a Dockerfile
  pull       Download an image from a registry
  push       Upload an image to a registry
  images     List images
  login      Log in to a registry
  logout     Log out from a registry
  search     Search Docker Hub for images
  version    Show the Docker version information
  info       Display system-wide information

Management Commands:
  builder    Manage builds
  container  Manage containers
  context    Manage contexts
  image      Manage images
  manifest   Manage Docker image manifests and manifest lists
  network    Manage networks
  plugin     Manage plugins
  system     Manage Docker One useful subcommand is search which allows you to find images like Nginx: $ docker search nginx The output lists available images, and the top result is usually the official image: NAME                                     DESCRIPTION                                                       STARS
nginx                                    Official build of Nginx.                                       19922
unit                                     Official build of NGINX Unit: Universal Web …                  31
nginx/nginx-ingress                      NGINX and NGINX Plus Ingress Controllers fo…                   92
nginxinc/nginx-unprivileged              Unprivileged NGINX Dockerfiles                                   152
nginxinc/nginx-prometheus-exporter       NGINX Prometheus Exporter for NGINX and NGIN…                   41
... (additional entries) You can pull the official Nginx image by running: $ docker pull nginx
Using default tag: latest
latest: Pulling from library/nginx
2cc3ae149d28: Pull complete
a97d90a43bc9: Pull complete
9571e65a55a3: Pull complete
0b432cb2d95e: Pull complete
24436767f2de: Pull complete
928c92caef0: Pull complete
ca6f48c6db4: Pull complete
Digest: sha256:0acab7c2237e052dc5adf1694ebce0b374063a62b2a1b7f2b3bc9cd3fb8c1ff
Status: Downloaded newer image for nginx:latest
docker.io/library/nginx:latest The term latest at the end of the image name is known as the image tag—it functions similarly to a version label. For instance, latest corresponds to the newest version, while a tag like 1.24.0 indicates a specific version. To pull an older version, run: $ docker pull nginx:1.22.1
1.22.1: Pulling from library/nginx
f1f26f570256: Pull complete
fd03b214774: Pull complete
ef2f2c869944: Pull complete
ac713a9ef2cc: Pull complete
fd071922d543: Pull complete
2a9f38700bb5: Pull complete
Status: Downloaded newer image for nginx:1.22.1
docker.io/library/nginx:1.22.1 Managing Docker Images After pulling images, it’s a good practice to remove any that are no longer needed. To list available images, run: $ docker images
REPOSITORY          TAG       IMAGE ID       CREATED          SIZE
ubuntu/nginx        latest    cd238b5026e    3 days ago       128MB
nginx               latest    dde0cca083bc   2 weeks ago      188MB
nginx               1.22.1    0f8498f13f3a   14 months ago    142MB To delete a specific image, such as the nginx:1.22.1 image, use the rmi command. When multiple images share the same repository name, remember to specify the tag: $ docker rmi nginx:1.22.1
Untagged: nginx:1.22.1
Untagged: nginx@sha256:fc5f5b747575c306aaf884566ebfe0b006240a184d52b923d2f0197108f6b7
Deleted: sha256:0f8498f13fadef3c82cdf069ecc880b01189be63491631d44eeaa5fb29... After deletion, verifying your images again should show only the remaining ones: $ docker images
REPOSITORY          TAG       IMAGE ID       CREATED         SIZE
nginx               latest    dde0cca083bc   2 weeks ago     188MB Running Containers Running a container is as easy as issuing the docker run command. For instance, to run an Nginx container: $ docker run nginx This command creates a new container and starts the Nginx daemon. However, running the container this way attaches you to its output, making it challenging to run additional commands simultaneously. The output may look similar to: $ docker run nginx
/docker-entrypoint.sh: /docker-entrypoint.d/ is not empty, will attempt to perform configuration
/docker-entrypoint.sh: looking for shell scripts in /docker-entrypoint.d/
/docker-entrypoint.sh: Launching /docker-entrypoint.d/10-listen-on-ipv6-by-default.sh
...
2024/06/14 02:40:53 [notice] 1#1: using the ""epoll"" event method
2024/06/14 02:40:53 [notice] 1#1: nginx/1.27.0
... Pressing Ctrl-C will detach you from the container but will also stop it. To run the container in the background, publish ports, and assign it a descriptive name, use the following command: $ docker run --detach --publish 8080:80 --name mywebserver nginx This command does the following: --detach : Runs the container in the background. --publish 8080:80 : Maps port 8080 on the host to port 80 in the container. --name mywebserver : Assigns the name ""mywebserver"" to the container. To view currently running containers, use: $ docker ps You might see output like: CONTAINER ID   IMAGE     COMMAND                  NAMES
34ef97c0c6a5   nginx     ""/docker-entrypoint...""  mywebserver Tip Remember that docker ps only shows running containers. Use docker ps --all to list both active and stopped containers. To start a stopped container, reference its container ID or name: $ docker start 833adf46106a
833adf46106a And validate its status with: $ docker ps If you need to stop a running container, simply run: $ docker stop mywebserver Testing the Published Port After running your container with port mapping, it’s important to verify that it’s accessible. You can do this by connecting to the published port on your host using a tool like netcat ( nc ): $ nc localhost 8080 Type the HTTP request: GET / You should receive the HTML response of the Nginx index page: <!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
<style>
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
</style>
</head>
<body>
<h1>Welcome to nginx!</h1>
<p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p>
<p>For online documentation and support please refer to
<a href=""http://nginx.org/"">nginx.org</a>.<br/>
Commercial support is available at
<a href=""http://nginx.com/"">nginx.com</a>.</p>
<p><em>Thank you for using nginx.</em></p>
</body>
</html> Press Ctrl-D to terminate the netcat session. Understanding Docker Run vs. Docker Start It’s essential to understand the difference between docker run and docker start : docker run: Creates a new container from an image and starts it. docker start: Simply starts an existing, previously created container. Use docker run when the container does not already exist and docker start to resume a stopped container. Removing Containers and Images Managing system resources involves cleaning up unused containers and images. To list all containers, run: $ docker ps --all To remove a stopped container (for example, one named determined_perlman ), execute: $ docker rm determined_perlman Important Remember: the docker rm command removes containers, while docker rmi is used to remove images. If an image is still in use, such as by the ""mywebserver"" container, attempting to remove it will result in an error: $ docker rmi nginx In that case, stop and remove the container first: $ docker stop mywebserver
$ docker rm mywebserver Then remove the image: $ docker rmi nginx Using Restart Policies For scenarios where you need your Nginx container to restart automatically after errors or system reboots, you can add a restart policy. The basic command: $ docker run --detach --publish 8080:80 --name mywebserver nginx Can be enhanced with the --restart always option to ensure continuous operation: $ docker run --detach --publish 8080:80 --name mywebserver --restart always nginx If the specified image is not available locally, Docker will automatically pull it. Building Custom Docker Images Sometimes, public images may not meet your exact needs, prompting you to build a custom image. Here’s how you can create your own image based on the Nginx image: Create a directory for your project and navigate into it: $ mkdir myimage
$ cd myimage Create a simple index.html file to serve as your custom HTML content: $ vim index.html (Insert a line of text as a placeholder for your custom HTML content.) Create a Dockerfile (note the capital D), which provides instructions for building your image: $ vim Dockerfile Add the following content to the Dockerfile : FROM nginx
COPY index.html /usr/share/nginx/html/index.html This tells Docker to use the official Nginx image as the base and copy your custom HTML file into the appropriate directory inside the container. Other commonly used Dockerfile instructions include: RUN: Executes commands during the image build (e.g., installing additional utilities). CMD: Specifies the default command to run when the container starts (which can be overridden). ENTRYPOINT: Ensures a command is always executed upon container initialization (and is less easily overridden). Build your custom image using the following command (replace ""jeremy"" with your Docker Hub username if applicable): $ docker build --tag jeremy/customnginximage:1.0 myimage You should see output similar to: Sending build context to Docker daemon  3.072kB
Step 1/2 : FROM nginx
 ---> dde0cca083bc
Step 2/2 : COPY index.html /usr/share/nginx/html/index.html
 ---> 58d5a293c951
Successfully built 58d5a293c951
Successfully tagged jeremy/customnginximage:1.0 This custom image can now be used in the same way as any other Docker image. Summary This article has covered the essentials of creating, managing, running, and building Docker containers—from using public images to constructing your own custom container images. With Docker’s powerful features, deploying applications at scale becomes a seamless process. For further details and in-depth tutorials, consider exploring the following resources: Docker Documentation Nginx Official Website Happy containerizing! Watch Video Watch video content"
Linux Foundation Certified System Administrator LFCS,Installing an Operating System on a Virtual Machine,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Operations-Deployment/Installing-an-Operating-System-on-a-Virtual-Machine,"Linux Foundation Certified System Administrator (LFCS) Operations Deployment Installing an Operating System on a Virtual Machine Cloud images with pre-installed operating systems can simplify deployments by eliminating the OS installation step. However, in some cases, you may want to perform a fresh operating system installation on a virtual machine. This guide explains how to set up a virtual machine using a virtual CD-ROM/DVD-ROM and an empty disk image, then install an operating system using the virt-install tool. Overview of the Process The installation process involves the following key differences compared to importing a pre-built disk image: The --import option is omitted because no pre-installed operating system exists. The --disk option uses the syntax size=10 (or another size) to create a new disk image with a specified capacity. The --location option points to an ISO file or a URL serving as a virtual installation medium. The --graphics none option enforces a text-mode installation. The --extra-args ""console=ttyS0"" option configures the Linux kernel to expose a serial console, enabling text-based interactions without a graphical interface. Example: ISO-Based Installation Below is a sample command demonstrating the installation process using an ISO-based installer: jeremy@kodekloud:~$ virt-install --osinfo debian12 --name debian1 --memory 1024 --vcpus 1 --disk size=1 --location /var/lib/libvirt/boot/debian12-netinst.iso --graphics none --extra-args ""console=ttyS0"" The virt-install command begins by downloading necessary installation files and proceeds with the configured setup. An extended version of the command is shown below: jeremy@kodekloud:~$ virt-install --osinfo debian12 --name debian1 --memory 1024 --vcpus 1 --disk size=1 \
--location https://deb.debian.org/debian/dists/bookworm/main/installer-amd64/ --graphics none --extra-args ""console=ttyS0""
Starting install...
Retrieving 'linux'          
Retrieving 'initrd.gz'     
Allocating 'virtinst-pf4uz7u7-linux' |  3.5 MB  00:00:00 ...  
Transferring 'virtinst-pf4uz7u7-linux' |  37 MB  00:00:07 ...  
Allocating 'virtinst-ig_4_4kj-initrd.gz' |   0 B  00:00:00 ...  
Transferring 'virtinst-ig_4_4kj-initrd.gz' |   0 B  00:00:00 ...  
Allocating 'debian1.qcow2' As the setup continues, kernel output is generated, displaying messages similar to the following: [   4.397242] pci_bus 0000:00: resource 1 [mem 0xfba00000-0xfbbffffff]
[   4.407563] pci_bus 0000:00: resource 2 [mem 0xfd800000-0xfd9fffff 64bit pref]
[   4.419329] pci_bus 0000:00: resource 0 [io  0xb000-0xbfff]
[   4.429093] pci_bus 0000:00: resource 1 [mem 0xfb800000-0xfb9ffffff]
[   4.437697] pci_bus 0000:00: resource 0 [mem 0xfd600000-0xfd7fffff 64bit pref]
[   4.450549] pci_bus 0000:00: resource 0 [io  0xd000-0xdfff]
[   4.459376] pci_bus 0000:00: resource 2 [mem 0xfb600000-0xfb7fffff 64bit pref]
[   4.466932] pci_bus 0000:00: resource 2 [mem 0xfd5dffff-0xfd5fffff]
[   4.475681] pci_bus 0000:00: resource 0 [io  0xe000-0xefff]
[   4.487983] pci_bus 0000:00: resource 1 [mem 0xfb400000-0xfb5ffffff]
[   4.498927] pci_bus 0000:00: resource 2 [mem 0xfd200000-0xfd3ffffff 64bit pref]
[   4.511739] pci_bus 0000:00: resource 0 [io  0xf000-0xefff]
[   4.516685] pci_bus 0000:00: resource 1 [mem 0xfb200000-0xfb3ffffff]
[   4.525543] pci_bus 0000:00: resource 2 [mem 0xfd200000-0xfd1ffffff 64bit pref]
[   4.542764] ACPI: \_SB_.GSIF: Enabled at IRQ 21
[   4.554507] pci 0000:02:00.0: quirk_usb_early_handoff+0x0/0x750 took 11764 usecs
[   4.566288] PCI: CLS 0 bytes, default 64
[   4.572174] Trying to unpack rootfs image as initramfs...
[   4.583343] clocksource: tsc: mask: 0xfffffffffffffffe max_cycles: 0x23f39a1d859, max_idle_ns: 44079
[   4.616550] Initialise system trusted keyrings
[   4.631830] Key type blacklist registered
[   4.640054] workingset: timestamp_bits=36 max_order=18 bucket_order=0
[   4.671611] zbud: loaded
[   4.676112] integrity: Platform Keyring initialized
[   4.691762] integrity: Machine keyring initialized
[   4.703957] Key type asymmetric registered
[   4.719768] Asymmetric key parser 'x509' registered After the kernel completes its startup messages, the system boots directly into the Debian installer. In text-based terminal environments, the display might sometimes seem misaligned. Since the installation interface is minimal, not all languages or options may be available. You can simply press Enter to accept the default options during the initial setup. Stopping the Process For demonstration purposes, this guide stops at the installer boot stage. Continuing the process would trigger additional downloads and extended waiting times. If you wish to cancel the installation, press Ctrl+] to exit the terminal session. Please note that terminal outputs may sometimes appear distorted. In such situations, using the clear command might not fully resolve display issues. Summary This guide demonstrated how to create a virtual machine and perform a complete operating system installation from scratch using the virt-install tool. While pre-built cloud images are generally more efficient, understanding this process is valuable for instances that require a fresh installation setup. This concludes the demonstration. Watch Video Watch video content Practice Lab Practice lab"
Linux Foundation Certified System Administrator LFCS,Create and Boot a Virtual Machine,https://notes.kodekloud.com/docs/Linux-Foundation-Certified-System-Administrator-LFCS/Operations-Deployment/Create-and-Boot-a-Virtual-Machine,"Linux Foundation Certified System Administrator (LFCS) Operations Deployment Create and Boot a Virtual Machine Previously, a minimal virtual machine was created without an attached virtual disk, meaning it couldn’t host an operating system. That setup was useful for demonstrating basic management commands using bash. In this guide, we will build a complete virtual machine—with an operating system installed—to simulate real-world scenarios and introduce additional commands. Downloading a Disk Image The first step is to download a disk image. Ubuntu provides pre-configured cloud images that are ideally suited for virtual machines. Although similar images power services like Google Cloud, we will download the minimal cloud image using wget . Before you proceed, note that you may have encountered alternative commands similar to: lxc remote add --protocol simplestreams ubuntu-minimal https://cloud-images.ubuntu.com/minimal/releases/
lxc launch ubuntu-minimal:noble For our purposes, we use wget : jeremy@kodekloud:~$ wget https://cloud-images.ubuntu.com/minimal/releases/noble/release/ubuntu-24.04-minimal-cloudimg-amd64.img
--2024-06-05 17:49:43--  https://cloud-images.ubuntu.com/minimal/releases/noble/release/ubuntu-24.04-minimal-cloudimg-amd64.img
Resolving cloud-images.ubuntu.com (cloud-images.ubuntu.com)... 185.125.190.40, 185.125.190.37, ...
Connecting to cloud-images.ubuntu.com (cloud-images.ubuntu.com)|185.125.190.40|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 238485504 (227M) [application/octet-stream]
Saving to: ‘ubuntu-24.04-minimal-cloudimg-amd64.img’

ubuntu-24.04-minimal-cloudimg-amd64.img  100%[=================================================>] 227.44M  846KB/s    in 7m 31s

2024-06-05 17:57:16 (517 KB/s) - ‘ubuntu-24.04-minimal-cloudimg-amd64.img’ saved [238485504/238485504]

jeremy@kodekloud:~$ Verifying the Integrity of the Image To ensure that the downloaded image is authentic and uncorrupted, verify its checksum using the SHA-256 checksum provided on the Ubuntu page. Two files are released: SHA256SUMS (which contains checksums for all images) and SHA256SUMS.gpg (which is used to verify the checksum file itself). First, download the checksum file: jeremy@kodekloud:~$ wget https://cloud-images.ubuntu.com/minimal/releases/noble/release/SHA256SUMS
--2024-06-05 18:00:28--  https://cloud-images.ubuntu.com/minimal/releases/noble/release/SHA256SUMS
Resolving cloud-images.ubuntu.com (cloud-images.ubuntu.com)... 185.125.190.40, 185.125.190.37, ...
Connecting to cloud-images.ubuntu.com (cloud-images.ubuntu.com)|185.125.190.40|:443... connected.
HTTP request sent, awaiting response... 200 OK
Length: 2174 (2.1K) [text/plain]
Saving to: ‘SHA256SUMS’

SHA256SUMS          100%[==================================================>]  2.12K  --.-KB/s    in 0s

2024-06-05 18:00:28 (69.1 MB/s) - ‘SHA256SUMS’ saved [2174/2174]

jeremy@kodekloud:~$ ls
SHA256SUMS  SHA256SUMS.gpg  ubuntu-24.04-minimal-cloudimg-amd64.img
jeremy@kodekloud:~$ Now verify the checksum: jeremy@kodekloud:~$ sha256sum -c SHA256SUMS 2>&1 | grep OK
ubuntu-24.04-minimal-cloudimg-amd64.img: OK
jeremy@kodekloud:~$ Since the output confirms “OK,” the image is intact and ready for use. Inspecting and Resizing the Disk Image Next, inspect the image using QEMU’s tool to examine file format, virtual size, and allocated disk space: jeremy@kodekloud:~$ qemu-img info ubuntu-24.04-minimal-cloudimg-amd64.img
image: ubuntu-24.04-minimal-cloudimg-amd64.img
file format: qcow2
virtual size: 3.5 GiB (3758096384 bytes)
disk size: 227 MiB
cluster_size: 65536
Format specific information:
    compat: 1.1
    compression type: zlib
    lazy refcounts: false
    refcount bits: 16
    corrupt: false
    extended l2: false
Child node '/file':
    filename: ubuntu-24.04-minimal-cloudimg-amd64.img
    protocol type: file
    file length: 227 MiB (238485504 bytes)
    disk size: 227 MiB
jeremy@kodekloud:~$ Although the virtual disk is set to 3.5 GiB, only 227 MiB is currently allocated. To install additional software, a larger disk is necessary. Expand the virtual disk to approximately 10 GiB with: jeremy@kodekloud:~$ qemu-img resize ubuntu-24.04-minimal-cloudimg-amd64.img 10G
Image resized.
jeremy@kodekloud:~$ qemu-img info ubuntu-24.04-minimal-cloudimg-amd64.img
image: ubuntu-24.04-minimal-cloudimg-amd64.img
file format: qcow2
virtual size: 10 GiB (10737418240 bytes)
disk size: 227 MiB
cluster_size: 65536
Format specific information:
    compat: 1.1
    compression type: zlib
    lazy refcounts: false
    refcount bits: 16
    corrupt: false
    extended l2: false
Child node '/file':
    filename: ubuntu-24.04-minimal-cloudimg-amd64.img
    protocol type: file
    file length: 227 MiB (238486016 bytes)
    disk size: 227 MiB
jeremy@kodekloud:~$ Even though the virtual size is now 10 GiB, physical disk usage only increases as the virtual machine utilizes the extra space. Creating a Storage Pool and Copying the Disk Image Virtualization tools typically use storage pools to store virtual disk images, snapshots, and other data. By default, the storage pool is located at /var/lib/libvirt . To confirm, check the directory contents: jeremy@kodekloud:~$ ls /var/lib/libvirt/
boot dnsmasq images qemu sanlock Then, copy the disk image to the images subdirectory: jeremy@kodekloud:~$ sudo cp ubuntu-24.04-minimal-cloudimg-amd64.img /var/lib/libvirt/images/
[sudo] password for jeremy: Creating a Virtual Machine with virt‑install Now comes the more advanced step: creating a virtual machine and specifying the image file as the virtual disk. The virt-install utility makes this straightforward. If parameters are missing, an error will appear indicating that required options (such as the operating system name) must be provided. For example, running: jeremy@kodekloud:~$ virt-install
ERROR
--os-variant/--osinfo OS name is required, but no value was set or detected.
... This error reminds you to specify the OS name. To list available operating system variants, use: jeremy@kodekloud:~$ virt-install --osinfo list Scroll through the output to locate the variant you plan to use (in this case, Ubuntu 24.04) and note its identifier (for example, ubuntu24.04 ). Examining the Manual For additional command-line options, consult the manual: man virt-install Below is an excerpt from the manual detailing the creation of a VM from an existing disk image: Remember to include the --import option to bypass a new OS installation because the disk image already contains an operating system: With --import , the first device specified via --disk or --filesystem serves as the boot device. By default, virt-install would perform a full installation. Reviewing Available Options Run the following command to view all available options: virt-install --help Key options include: Name, Memory, and vCPUs : Assign a name to the VM, allocate memory (in megabytes), and set the number of virtual CPUs. Disk : Specify the path to the existing disk image. For example: --disk /var/lib/libvirt/images/ubuntu-24.04-minimal-cloudimg-amd64.img,cache=none Graphics : Use --graphics none to disable a graphical interface and run the VM solely via a text console. Building and Running the Virtual Machine Use the following complete command to create a VM running Ubuntu 24.04 with 1024 MiB memory, 1 vCPU, and the pre-copied cloud image while disabling graphical output: jeremy@kodekloud:~$ virt-install --osinfo ubuntu24.04 --name ubunt1 --memory 1024 --vcpus 1 --import --disk /var/lib/libvirt/images/ubuntu-24.04-minimal-cloudimg-amd64.img --graphics none When you run the command, you may observe output similar to this: WARNING  Requested memory 1024 MiB is less than the recommended 3072 MiB for OS ubuntu24.04

Starting install...
Creating domain...
Running text console command: virsh --connect qemu:///system console ubunt1
Connected to domain 'ubunt1'
Escape character is ^] (Ctrl + })
...
Domain creation completed. This output indicates that you are connected directly to the VM’s text console—similar to having a monitor and keyboard attached. To exit the console, press Control + the closing square bracket (Ctrl + ]). To list all virtual machines, run: jeremy@kodekloud:~$ virsh list --all
 Id    Name       State
------------------------
 1     ubunt1     running
jeremy@kodekloud:~$ Shut down the VM by running: jeremy@kodekloud:~$ virsh shutdown ubunt1
Domain 'ubunt1' is being shutdown
jeremy@kodekloud:~$ virsh list --all
 Id    Name       State
------------------------
 -     ubunt1     shut off If necessary, force a shutdown using: jeremy@kodekloud:~$ virsh destroy ubunt1 Once shut down, you can remove the VM’s definition and all associated storage (including the copied image): jeremy@kodekloud:~$ virsh undefine ubunt1 --remove-all-storage
Domain 'ubunt1' has been undefined
Volume 'vda'(/var/lib/libvirt/images/ubuntu-24.04-minimal-cloudimg-amd64.img) removed. Recreating the Virtual Machine Using Cloud-Init The Ubuntu cloud image includes cloud-init automation scripts that allow you to set a root password automatically. To recreate the VM with cloud-init that generates a random root password, append the --cloud-init root-password-generate=on option. jeremy@kodekloud:~$ virt-install --osinfo ubuntu24.04 --name ubunt1 --memory 1024 --vcpus 1 --import --disk /var/lib/libvirt/images/ubuntu-24.04-minimal-cloudimg-amd64.img --graphics none --cloud-init root-password-generate=on
WARNING  Requested memory 1024 MiB is less than the recommended 3072 MiB for OS ubuntu24.04
Starting install...
Password for first root login is: wMgdynhWmsdluzjH
Installation will continue in 10 seconds (press Enter to skip)... Once the installation begins and the VM boots, you will eventually see a login prompt on the console. The boot output (including kernel messages and cloud-init logs) will scroll until you reach: Ubuntu 24.04 LTS ubuntu ttyS0
ubuntu login: Log in using ""root"" as the username, and paste the generated password (note that nothing appears as you paste). On first login, you will be required to change the password: You are required to change your password immediately (administrator enforced).
Changing password for root.
Current password: After updating the password, you’ll be logged into the Ubuntu operating system running on your virtual machine. Verify the virtual disk size is approximately 10 GiB: root@ubuntu:~# df -h
Filesystem      Size  Used Avail Use% Mounted on
/dev/root       8.7G  466M  8.2G   6% /
tmpfs            481M     0  481M   0% /dev/shm
tmpfs            791M  756K  791M   1% /run
tmpfs           5.0M     0  5.0M   0% /run/lock
/dev/vda16      881M   42M  839M   6% /boot
/dev/vda15      105M   51M   47M  51% /boot/efi
tmpfs            97M     0   97M   0% /run/user/0
root@ubuntu:~# Then update the package lists to ensure network connectivity: root@ubuntu:~# apt update
...
Fetched 24.5 MB in 11s (2206 kB/s)
Reading package lists... Done
Building dependency tree... Done
Reading state information... Done
17 packages can be upgraded. Run 'apt list --upgradable' to see them. To exit the VM’s console, press Control + ]. You can reconnect at any time with: jeremy@kodekloud:~$ virsh console ubunt1 If no login prompt appears, simply press Enter. Handling OS Variants with virt‑install Sometimes the operating system inside a virtual disk image may be a newer or unlisted variant in the virt-install OS database. Here are some options to handle such cases: Auto-detect OS Use --osinfo detect=on to let virt-install automatically determine the OS type. Example: virt-install --osinfo detect=on --name ubuntu1 --memory 1024 --vcpus 1 --import --disk /var/lib/libvirt/images/ubuntu-24.04-minimal-cloudimg-amd64.img --graphics none --cloud-init root-password-generate=on Specify a Generic OS Variant: If you are sure the distribution is modern, you might use a generic identifier: virt-install --osinfo linux2022 --name ubuntu1 --memory 1024 --vcpus 1 --import --disk /var/lib/libvirt/images/ubuntu-24.04-minimal-cloudimg-amd64.img --graphics none Disable Strict OS Requirement: Use --osinfo detect=on,require=off or set the environment variable: export VIRTINSTALL_OSINFO_DISABLE_REQUIRE=1 If you see an error similar to: ERROR
--os-variant/--osinfo OS name is required, but no value was set or detected.
... refer to the provided options. Wrapping Up In this guide, we covered the following steps: Downloading and verifying an Ubuntu cloud image Inspecting and resizing the virtual disk using QEMU Setting up a storage pool by copying the image to /var/lib/libvirt/images/ Creating a virtual machine with virt-install , including essential options such as name, memory, vCPUs, disk, and graphics Utilizing the --import option to deploy a guest from an existing disk image Incorporating cloud-init to automatically generate a root password Connecting to and managing the VM console using virsh With these steps, you now have a fully functional virtual machine running Ubuntu 24.04. In the next lesson, we’ll explore additional configuration and management options. Watch Video Watch video content"
Nginx For Beginners,Summary,https://notes.kodekloud.com/docs/Nginx-For-Beginners/Install-Config/Summary,"Nginx For Beginners Install Config Summary In this module, you’ll learn how to install, configure, and secure Nginx on Linux using native package managers—no custom compilation required. Why use package managers? Package managers like apt , yum , or dnf handle dependencies, updates, and safe removal for you, so you’re not reinventing the wheel when deploying software such as Nginx. Note Compiling from source can give you fine-grained control, but it increases maintenance overhead. For most production environments, the prebuilt packages are reliable and secure. Installing Nginx on Linux While Windows and macOS can be used for local testing, production deployments typically run on Linux distributions. Ubuntu / Debian: sudo apt update
sudo apt install nginx CentOS / RHEL / Fedora: sudo yum install nginx      # CentOS/RHEL 7
sudo dnf install nginx      # Fedora, CentOS/RHEL 8+ Start and enable Nginx: sudo systemctl start nginx
sudo systemctl enable nginx Service Management with systemctl Use Nginx’s built-in controls to manage the daemon: sudo systemctl {start|stop|restart|reload} nginx
sudo systemctl status nginx Understanding nginx.conf Structure The main configuration file is divided into four sections: Global (user, worker_processes) events (worker_connections) http (MIME types, logging, gzip) server blocks (virtual hosts) Serving Static Files & Virtual Hosts Nginx excels at delivering static content with minimal overhead. You can host multiple sites on a single IP by defining separate server blocks. Always set a unique server_name to prevent Nginx from defaulting to the first file alphabetically. Customizing the Default Welcome Page Replace the built-in Nginx page with a simple “Hello World” HTML: <!-- /var/www/html/index.html -->
<!DOCTYPE html>
<html>
  <head><title>Hello World</title></head>
  <body><h1>Hello, Nginx!</h1></body>
</html> Firewall Configuration Ensure your firewall and any cloud security groups allow HTTP/HTTPS traffic only as needed: Distribution Firewall Tool Common Commands Ubuntu / Debian UFW sudo ufw allow 'Nginx Full' <br> sudo ufw enable CentOS / RHEL firewalld sudo firewall-cmd --permanent --add-service=http <br> sudo firewall-cmd --reload All (Cloud VPS) Security Group Configure ports 80/443 in AWS/GCP/Azure console or CLI Warning Opening ports 80 and 443 without restrictions exposes your server to the public internet. Always verify your rule sets and consider limiting access by IP for staging environments. Links and References Nginx Official Documentation Ubuntu UFW Guide firewalld Documentation Watch Video Watch video content Practice Lab Practice lab"
Nginx For Beginners,Demo Firewall Ports Install Config,https://notes.kodekloud.com/docs/Nginx-For-Beginners/Install-Config/Demo-Firewall-Ports-Install-Config,"Nginx For Beginners Install Config Demo Firewall Ports Install Config In this lesson, we’ll move from verifying Nginx via the CLI to ensuring clients can access our services through a browser. Instead of using curl , end users rely on HTTP ports 80 (HTTP) and 443 (HTTPS) for Nginx, and port 5000 for our Flask application. To safely expose only the necessary ports, we’ll configure UFW (Uncomplicated Firewall) on Ubuntu. Network Architecture Clients connect over the internet to: Nginx on port 80 (HTTP) or 443 (HTTPS with SSL) Flask on port 5000 With the firewall currently inactive, both endpoints are reachable by default. 1. Testing Locally via CLI Before changing any firewall rules, confirm both services are running on the host: # Check Nginx default welcome page on port 80
curl localhost

# Check Flask application on port 5000
curl localhost:5000 Expected output for Nginx: <!DOCTYPE html>
<html>
<head>
<title>Welcome to nginx!</title>
...
<h1>Welcome to nginx!</h1>
...
</html> Expected output for Flask: <h1>Hello, Human!</h1>[Not Authenticated] 2. Viewing and Opening Ports in the Lab UI In our lab environment, you can also open and view ports using the terminal’s “View Port” option: Enter 80 and 5000 to open them and test browser connectivity. However, it’s best practice to enable the firewall and only allow the ports you need. 3. Enabling and Configuring UFW Check the UFW status (should be inactive): sudo ufw status Warning Always allow SSH (port 22) before enabling UFW to avoid locking yourself out. sudo ufw allow 22/tcp
# Rule added
# Rule added (v6) Enable UFW and ensure it starts on boot: sudo ufw enable
# Firewall is active and enabled on system startup Allow HTTP (port 80) over TCP: sudo ufw allow 80/tcp
# Rule added
# Rule added (v6) Verify the active rules: sudo ufw status Expected output: Status: active

To      Action   From
--      ------   ----
22/tcp  ALLOW    Anywhere
80/tcp  ALLOW    Anywhere
22/tcp  ALLOW    Anywhere (v6)
80/tcp  ALLOW    Anywhere (v6) Test browser access: Port 80 should now load the Nginx welcome page. Port 5000 will be blocked until explicitly allowed. Allow the Flask application port (5000/tcp): sudo ufw allow 5000/tcp
# Rule added
# Rule added (v6) Verify again: sudo ufw status Expected output: Status: active

To        Action   From
--        ------   ----
22/tcp    ALLOW    Anywhere
80/tcp    ALLOW    Anywhere
5000/tcp  ALLOW    Anywhere
22/tcp    ALLOW    Anywhere (v6)
80/tcp    ALLOW    Anywhere (v6)
5000/tcp  ALLOW    Anywhere (v6) 4. Browser Testing Now that ports are correctly configured, verify in a browser: Note When accessing the Flask app directly, append :5000 to the URL unless you’re using a reverse proxy. Best Practices Port Protocol Purpose 22/tcp SSH Secure shell access (restrict IPs) 80/tcp HTTP Public Nginx traffic 443/tcp HTTPS Encrypted Nginx traffic 5000/tcp TCP Internal Flask application Expose only ports 80 and 443 publicly. Use Nginx or another reverse proxy to forward requests to application servers. Restrict SSH access to trusted IPs or via VPN. Always keep your firewall enabled for maximum security. Links and References UFW Documentation Nginx Official Docs Flask Official Docs Ubuntu Security Guides Watch Video Watch video content"
Nginx For Beginners,Introduction to Nginx,https://notes.kodekloud.com/docs/Nginx-For-Beginners/Introduction/Introduction-to-Nginx,"Nginx For Beginners Introduction Introduction to Nginx What Is NGINX? NGINX is a high-performance web server first released over 20 years ago. It’s available in two editions: the open source Community Edition and the commercial NGINX Plus. NGINX powers static content delivery, load balancing, reverse proxy, and more—across Linux, macOS, and Windows. Cross-Platform Support NGINX overcomes the scalability and performance bottlenecks of legacy web servers. It installs easily on all major operating systems and delivers consistent throughput under heavy load. Historical Context Originally created to challenge Apache HTTP Server and Microsoft Internet Information Services (IIS), NGINX quickly gained traction thanks to its lightweight, asynchronous design. Asynchronous, Event-Driven Architecture One of NGINX’s key innovations is handling 10,000+ concurrent connections with minimal overhead. This makes it ideal for serving static assets—HTML, images, audio, and video—more efficiently than traditional, process-based servers. Note NGINX processes multiple client requests within a single worker process using non-blocking I/O. Feature NGINX Apache Architecture Asynchronous, event-driven Process/thread-based Max. concurrent connections ≥10,000 Varies, lower throughput CPU & memory usage Low Higher Static content performance Excellent Good Recent benchmarks show NGINX can handle up to four times as many connections as Apache, with lower latency and reduced resource consumption. NGINX Editions NGINX is distributed in two main editions: Edition Key Features Support Download URL Community (Open Source) Core HTTP, reverse proxy, load balancing Community forum https://nginx.org/download NGINX Plus (Commercial) Advanced modules, dashboard, WAF, 24×7 support Paid subscription https://nginx.com/products/nginx-plus Warning Check the licensing and support terms before deploying NGINX Plus in production environments. Official Download Sites For the open source release, visit nginx.org. To learn about NGINX Plus, head to nginx.com. Market Share & Adoption According to a June 2024 survey, NGINX holds 21% market share among web servers—and 32% when including OpenResty, an enhanced NGINX distribution. It ranks second only to Cloudflare in the top million sites and leads in overall domains and compute infrastructure. Combined, NGINX and OpenResty power roughly two-thirds of all Internet domains. Major organizations such as GitHub, Cloudflare, LinkedIn, Microsoft, and Netflix rely on NGINX for reliable, scalable performance. Web Server Market Share¹ Cloudflare 34% NGINX 21% OpenResty 11% Others 34% ¹ June 2024 survey data Up next: a deep dive into the NGINX architecture, configuration files, and core modules. Links and References Official NGINX Documentation: https://nginx.org/en/docs/ NGINX Plus Overview: https://nginx.com/products/nginx-plus/ OpenResty: https://openresty.org/en/ Watch Video Watch video content"
Nginx For Beginners,Nginx Use Cases,https://notes.kodekloud.com/docs/Nginx-For-Beginners/Introduction/Nginx-Use-Cases,"Nginx For Beginners Introduction Nginx Use Cases Nginx is more than a high-performance web server. It can improve your architecture by acting as a load balancer, reverse proxy, forward proxy, or caching layer—boosting scalability, reducing latency, and enhancing security. Key benefits include: Distributing requests for high availability Offloading SSL/TLS and request routing Caching responses to cut backend load Controlling outbound traffic and anonymizing clients Explore each use case below, complete with configuration examples, diagrams, and best practices. Load Balancing By distributing incoming requests across multiple servers, Nginx prevents any single backend from becoming a bottleneck. You declare an upstream block listing your servers, then proxy traffic to it. upstream backend {
    server backend1.example.com;
    server backend2.example.com max_fails=3 fail_timeout=30s;
}

server {
    listen 80;
    location / {
        proxy_pass http://backend;
    }
} Note Nginx supports multiple algorithms including round_robin (default), least_conn , and ip_hash . Choose one based on your workload characteristics. Reverse Proxy A reverse proxy accepts client requests, applies routing or SSL offloading, then forwards them to one or more backend servers. This hides your infrastructure behind a single public endpoint. server {
    listen 443 ssl;
    server_name example.com;

    ssl_certificate     /etc/nginx/ssl/example.crt;
    ssl_certificate_key /etc/nginx/ssl/example.key;

    location / {
        proxy_pass       http://internal_app;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
} Load Balancer vs. Reverse Proxy Feature Load Balancer Reverse Proxy Primary Role Distribute traffic across servers Intercept and forward requests Backend Servers Requires two or more Can work with a single server Common Use Cases Scaling, failover, health checks SSL/TLS termination, path-based routing Forward Proxy A forward proxy sits between clients and the internet, filtering or anonymizing outbound requests. Configure Nginx to restrict sites or mask client IPs for privacy. server {
    listen 3128;

    resolver 8.8.8.8;
    proxy_pass_request_headers on;

    location / {
        proxy_pass $scheme://$http_host$request_uri;
        proxy_hide_header Proxy-Authorization;
    }
} Warning Opening a forward proxy to the public can lead to abuse. Always secure it with allow / deny or authentication mechanisms. Caching Caching with Nginx reduces response times and eases load on backend services. Define a cache zone, set key parameters, and control how responses are stored. proxy_cache_path /var/cache/nginx levels=1:2 keys_zone=my_cache:10m 
                 inactive=60m max_size=1g;

server {
    listen 80;
    location / {
        proxy_cache my_cache;
        proxy_pass http://backend;
        proxy_cache_valid 200 302 10m;
        proxy_cache_valid 404 1m;
    }
} Note Monitor cache usage and tune inactive and max_size to avoid running out of disk space. Use proxy_cache_bypass for selective caching. Links and References Nginx Official Documentation HTTP Load Balancing with Nginx Using Nginx as a Forward Proxy Caching Guide in Nginx Watch Video Watch video content"
Nginx For Beginners,Nginx Architecture,https://notes.kodekloud.com/docs/Nginx-For-Beginners/Introduction/Nginx-Architecture,"Nginx For Beginners Introduction Nginx Architecture In modern web environments, servers must handle thousands of simultaneous connections with minimal latency. Nginx achieves this through a lightweight, non-blocking event-driven architecture paired with a master–worker process model. Below, we’ll explore how Nginx’s design compares to familiar real-world scenarios and why it excels under heavy load. Event-Driven Architecture Overview Imagine stepping into a busy coffee shop: One barista takes orders. Another barista prepares drinks. Neither barista waits idle—they coordinate tasks asynchronously. Like that coffee shop, Nginx decouples request acceptance from request processing. It listens for new events, delegates work, and immediately returns to watching for additional activity. How Nginx Manages Asynchronous Processing Under the hood, Nginx uses non-blocking I/O and a single-threaded event loop per worker to juggle connections efficiently: Note Nginx’s asynchronous event loop ensures that while one request awaits data (disk I/O, upstream response), the worker can serve other clients without delay. The Restaurant Analogy: Mapping Requests to Events A busy restaurant operates much like Nginx: Waiter (Event Loop) takes multiple orders without waiting for dishes. Chef (Worker Process) prepares meals and notifies the waiter when each is ready. In Nginx terms, each HTTP request follows these steps: Incoming Request The client issues an HTTP/S request. Event Loop Nginx accepts the connection and returns immediately to monitor other events. Processing Event The worker reads files, queries databases, or proxies to an upstream server. If I/O is required, it switches context to serve another request. Response Sent Once processing completes, Nginx replies to the client and continues the loop. Event Handling Sequence in Nginx The diagram below outlines how a worker process manages multiple requests simultaneously: Step-by-Step Flow Accept : New connection arrives. Register : Connection is added to the event loop. Dispatch : Worker processes available events. I/O Wait : If blocked, event loop switches to another request. Complete : Response is sent when processing is done. Master and Worker Processes To leverage multi-core CPUs and isolate failures, Nginx uses a master–worker architecture: Process Type Responsibility Handles Client Requests? Master Reads configuration, spawns/reloads worker processes No Worker Runs event loop, accepts connections, processes requests Yes Master Process Supervises worker lifecycle Reloads configuration without downtime Never blocks on I/O or client handling Worker Processes Each runs an independent, single-threaded event loop Handles connection acceptance, reading, writing, and multiplexing Scales across CPU cores by running multiple workers Further Reading and References Official Nginx Documentation Event-driven architecture on Wikipedia High Performance Browser Networking By combining non-blocking I/O, efficient event loops, and a robust master–worker model, Nginx delivers the scalability and low latency demanded by today’s Internet services. Watch Video Watch video content"
Nginx For Beginners,Install Config Introduction,https://notes.kodekloud.com/docs/Nginx-For-Beginners/Install-Config/Install-Config-Introduction,"Nginx For Beginners Install Config Install Config Introduction In this guide, you’ll learn how to install and configure Nginx on multiple operating systems. We’ll walk through package managers, Nginx service management, configuration structure, hosting a static website, and securing your server with UFW. What You’ll Learn Package managers: APT, YUM, Homebrew, and Windows Subsystem for Linux (WSL). Installing Nginx on Ubuntu, CentOS, macOS, and Windows (WSL). Starting, stopping, reloading, and checking the status of the Nginx service. Anatomy of nginx.conf : global directives, the HTTP block, and server blocks. Hosting a simple static site with Nginx server blocks. Configuring ports and securing your site with UFW. We’ll begin by exploring package managers and installing Nginx. Then, we’ll cover service management, dive into the configuration file, set up a static website, and finish with firewall security using UFW. 1. Package Manager Overview Choose the appropriate package manager for your OS: Operating System Package Manager Install Nginx Command Ubuntu APT sudo apt update && sudo apt install nginx CentOS/RHEL YUM or DNF sudo yum install epel-release && sudo yum install nginx macOS Homebrew brew update && brew install nginx Windows (WSL) APT sudo apt update && sudo apt install nginx Note Ensure you have administrative privileges ( sudo ) before running installation commands. 2. Installing Nginx We’ll cover step-by-step instructions for: Ubuntu & Debian CentOS & RHEL macOS (Homebrew) Windows WSL Next, we’ll manage the Nginx service, explore nginx.conf , host a static site, and secure your server with UFW. Links and References Nginx Official Documentation Ubuntu APT Guide Yum Package Management Homebrew Documentation UFW Manual Watch Video Watch video content"
Nginx For Beginners,Summary,https://notes.kodekloud.com/docs/Nginx-For-Beginners/Introduction/Summary,"Nginx For Beginners Introduction Summary In this module, we’ll explore how web servers handle your everyday browsing requests, with a special focus on Nginx. What You’ll Learn Topic Description Client-Server Workflow How browsers send HTTP/HTTPS requests, how the web server processes them, and returns content to users. Nginx Evolution Why Nginx became a popular alternative to Apache HTTP Server and IIS . Open Source vs. Plus Key differences between the free Nginx edition and the commercial Nginx Plus offering. Performance Advantages Nginx’s event-driven model, low memory usage, and high concurrency capabilities. Real-World Use Cases Typical scenarios—reverse proxying, load balancing, and static content hosting—where Nginx shines. Note Familiarity with basic HTTP concepts (methods, headers, status codes) will help you follow along more easily. Why Nginx? Scalability : Handles thousands of simultaneous connections with minimal resource consumption. Flexibility : Configurable as a web server, reverse proxy, load balancer, and more. Community & Support : Strong open source community plus commercial support via Nginx Plus. Next Steps In Module 2 , we’ll cover: Installing Nginx on various Linux distributions Verifying and testing the default configuration Creating your first server block (virtual host) Stay tuned for practical demos and hands-on exercises in the next module! References Nginx Official Site Apache HTTP Server Documentation Microsoft IIS Documentation HTTP/HTTPS Overview Watch Video Watch video content"
Nginx For Beginners,Web Servers,https://notes.kodekloud.com/docs/Nginx-For-Beginners/Introduction/Web-Servers,"Nginx For Beginners Introduction Web Servers A web server combines hardware and software to process client requests and serve web content—HTML, CSS, JavaScript, images, and more—back to your browser. How a Browser Loads a Web Page When you type a URL (for example, KodeKloud.com ) into your browser: The browser queries the DNS (Domain Name System) to resolve the domain name into an IP address. After obtaining the IP, it establishes a TCP connection to the server. The server receives the HTTP/HTTPS request, gathers the requested assets, and sends a response back. Your browser renders the response, displaying the web page. Note Think of DNS like a phone book—translating human-friendly domain names into machine-friendly IP addresses. HTTP vs. HTTPS Web servers communicate over two main protocols: HTTP (HyperText Transfer Protocol) – unencrypted HTTPS (HTTP Secure) – encrypted with TLS/SSL Most browsers redirect HTTP requests to HTTPS to protect data in transit. Warning Transmitting sensitive information over plain HTTP can expose data to eavesdropping and man-in-the-middle attacks. Always prefer HTTPS. Traditional vs. Modern Server Architectures Legacy Servers: Apache & IIS Apache HTTP Server (⟶ mid-1990s) uses a process-per-connection or thread-per-connection model. Microsoft IIS launched around the same time with a similar architecture for Windows environments. Spawning new processes for each request leads to increased CPU and memory usage under high load: Modern Alternatives Today's high-traffic sites distribute requests across clusters of servers behind load balancers. Popular event-driven and asynchronous servers include: Web Server First Released Concurrency Model Key Benefit Nginx 2004 Event-driven, async Low memory footprint, high concurrency OpenResty 2011 Nginx + Lua modules Extensible with Lua scripting LiteSpeed 2003 Event-driven Drop-in Apache replacement option Caddy 2015 Event-driven, Go Automatic HTTPS distribution In this guide, we'll focus on Nginx , exploring its architecture, configuration syntax, performance optimizations, and real-world deployment patterns. References DNS – Wikipedia HTTP – Wikipedia HTTPS – Wikipedia Apache HTTP Server Documentation Nginx Official Documentation Watch Video Watch video content"
Nginx For Beginners,Install Config,https://notes.kodekloud.com/docs/Nginx-For-Beginners/Install-Config/Install-Config,"Nginx For Beginners Install Config Install Config Before installing Nginx, it’s important to understand how package managers automate software deployment. Package managers handle downloading, dependency resolution, configuration, and removal—streamlining what would otherwise be manual and error-prone tasks. How a Package Manager Works Retrieve packages and metadata The manager pulls software from centralized repositories. Maintain repositories OS vendors, third-party developers, and communities (e.g., Nginx Open Source Core) keep these repositories up to date. Resolve dependencies Missing libraries or tools are fetched automatically to ensure smooth installs. Assist with configuration During or after installation, many managers can apply default or custom configurations. Note Using a package manager reduces manual steps, prevents version conflicts, and keeps your system secure by applying updates consistently. Popular Package Managers Package Manager Platform Repository Type Official Docs APT Debian / Ubuntu .deb APT Documentation YUM RHEL / CentOS / Fedora .rpm YUM Documentation Homebrew macOS / Linux Formula Homebrew Chocolatey Windows NuGet packages Chocolatey APT (Advanced Package Tool) APT is the default package manager on Debian-based distributions. It works with .deb packages to install, update, and remove software seamlessly. # Refresh package index
sudo apt update

# Install a package
sudo apt install package_name

# Upgrade all installed packages
sudo apt upgrade

# Remove a package
sudo apt remove package_name

# Search for a package
sudo apt-cache search package_name YUM (Yellowdog Updater, Modified) Used by RHEL, CentOS, and Fedora, YUM automates .rpm package handling and dependency checks. # Install a package
sudo yum install package_name

# Remove a package
sudo yum remove package_name

# Update all packages
sudo yum update

# Search for a package
sudo yum search package_name Homebrew Homebrew installs software into your home directory on macOS (and Linux), avoiding the need for sudo in most cases. # Update Homebrew itself
brew update

# Install a package
brew install package_name

# Uninstall a package
brew uninstall package_name

# Search for a package
brew search package_name Chocolatey Chocolatey provides Windows users with a CLI for automated installs via PowerShell. # Install a package
choco install package_name

# Uninstall a package
choco uninstall package_name Always prefer a package manager over manual software installs. It ensures consistent updates, security patches, and simplified maintenance. Installing Nginx Follow these commands to install Nginx on your platform. On Linux, append -y to bypass confirmation prompts. # Ubuntu / Debian
sudo apt update
sudo apt install -y nginx

# Red Hat / Fedora
sudo yum update -y
sudo yum install -y nginx

# macOS (Homebrew)
brew update
brew install nginx # Windows (Chocolatey)
choco install nginx Warning Make sure you have the necessary privileges ( sudo on Unix or an Administrator PowerShell on Windows) before running these commands. In the next lesson, we will demonstrate each of these installation steps in action, including service management and basic configuration. Watch Video Watch video content"
Nginx For Beginners,Nginx Overview,https://notes.kodekloud.com/docs/Nginx-For-Beginners/Install-Config/Nginx-Overview,"Nginx For Beginners Install Config Nginx Overview In this guide, we’ll walk through the core components of Nginx configuration by exploring the nginx.conf file—typically located at /etc/nginx/nginx.conf on Linux systems. You’ll learn how to structure global settings, optimize connection handling, define HTTP parameters, and create virtual hosts (server blocks). At a glance, an nginx.conf file is divided into four main sections: Global settings events block http block server block nginx.conf Structure Global settings Define user permissions, worker processes, PID file location, compression, caching, and more. events block Controls Nginx’s event model and the maximum number of simultaneous connections per worker. http block Contains HTTP directives for logging, timeouts, compression, MIME types, and includes for server blocks. server block Configures how Nginx responds to requests for specific domain names or IP addresses (virtual hosts). Note On most distributions, nginx.conf loads all files in /etc/nginx/conf.d/ and /etc/nginx/sites-enabled/ . Keep your virtual host files organized in /etc/nginx/sites-available/ and activate them with symbolic links. Global Settings, Events, and HTTP Blocks Below is a pared-down example of an nginx.conf layout including global directives, the events block, and the http block. Notice how server blocks are included separately. user www-data;
worker_processes auto;
pid /run/nginx.pid;

events {
    worker_connections 1024;
}

http {
    sendfile        on;
    tcp_nopush      on;
    tcp_nodelay     on;
    keepalive_timeout 65;
    types_hash_max_size 2048;
    gzip            on;

    log_format main '$remote_addr - $remote_user [$time_local] '
                    '""$request"" $status $body_bytes_sent '
                    '""$http_referer"" ""$http_user_agent""';

    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    # Include virtual host definitions
    include /etc/nginx/conf.d/*.conf;
    include /etc/nginx/sites-enabled/*;
} Key directives explained: user System user for Nginx worker processes (e.g., www-data on Debian/Ubuntu). worker_processes Number of worker processes— auto matches CPU cores. pid Path to the master process PID file. events → worker_connections Maximum simultaneous connections each worker can handle. http Configures HTTP-level settings: compression ( gzip ), timeouts, logging formats, MIME types, and includes server blocks. Server Blocks (Virtual Hosts) Server blocks, or virtual hosts, let you host multiple domains on one Nginx instance. Incoming requests are routed to the matching block based on server_name or IP address. Example: Basic HTTP Server Block server {
    listen 80;
    server_name example.com www.example.com;

    root /var/www/example.com/html;
    index index.html;

    location / {
        try_files $uri $uri/ =404;
    }
} Directive breakdown: listen Port for incoming traffic ( 80 for HTTP, 443 for HTTPS). server_name Domain names or IP addresses handled by this block. root Path to the website’s document root. index Default file(s) served when a directory is requested. location / URI matching; try_files checks for existing files or directories and returns a 404 if not found. Warning After editing any server block, always run nginx -t to validate your configuration and avoid downtime. Nginx Directory Structure Nginx uses a standardized directory layout for its configuration files and web content. Below is a quick reference of the most common paths: Path Description /etc/nginx/nginx.conf Main configuration file /etc/nginx/sites-available/ Store individual server block files /etc/nginx/sites-enabled/ Symbolic links to enabled sites from sites-available /etc/nginx/conf.d/ Additional configuration snippets (e.g., SSL, load balancing) /var/www/... Default web content roots (Debian/Ubuntu) /usr/share/nginx/html Default web root (RHEL/CentOS) /etc/nginx/mime.types MIME type definitions /run/nginx.pid PID file location /var/log/nginx/ Access and error logs Essential Nginx Commands Use these commands to inspect, test, and manage your Nginx server: Command Description nginx -h Display help and available options nginx -v Show Nginx version nginx -V Show version and compile-time options nginx -t Check configuration syntax and validity nginx -T Dump complete configuration for review nginx -s reload Reload configuration without downtime nginx -s stop Graceful shutdown nginx -s quit Immediate shutdown nginx -s reopen Reopen log files sudo systemctl reload nginx Reload using systemd (graceful) sudo systemctl restart nginx Restart Nginx (brief downtime possible) sudo systemctl status nginx Check Nginx service status Example syntax check: sudo nginx -t
# nginx: the configuration file /etc/nginx/nginx.conf syntax is ok
# nginx: configuration file /etc/nginx/nginx.conf test is successful Links and References Official Nginx Documentation Nginx Beginner’s Guide DigitalOcean Nginx Tutorials Watch Video Watch video content"
Nginx For Beginners,Demo Nginx Install Config,https://notes.kodekloud.com/docs/Nginx-For-Beginners/Install-Config/Demo-Nginx-Install-Config,"Nginx For Beginners Install Config Demo Nginx Install Config Learn how to install, manage, and configure Nginx on Ubuntu (or any Debian-based distribution) using apt . For CentOS or RHEL-based systems, swap apt with yum . Note All commands below assume you have sudo privileges. If you’re on RHEL/CentOS, use yum install nginx and yum update -y instead of apt . Prerequisites A running Ubuntu server (18.04, 20.04, or newer). Internet connectivity to download packages. Basic familiarity with the Linux command line. 1. Update Package Index Start by refreshing the local package index to fetch the latest metadata. sudo apt update -y Example output: bob@alpine-host:~$ sudo apt update -y
Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [128 kB]
...
Fetched 34.3 MB in 3s (10.7 MB/s)
Reading package lists... Done
Building dependency tree
Reading state information... Done
47 packages can be upgraded. Run 'apt list --upgradable' to see them. Clear any clutter from your terminal before moving on: clear 2. Install Nginx Install Nginx and its dependencies: sudo apt install nginx When prompted, confirm installation: Do you want to continue? [Y/n] Y Clear the screen again: clear 3. Manage the Nginx Service Verify the current status of the Nginx service: sudo systemctl status nginx You might see it inactive at first: ● nginx.service - A high performance web server and a reverse proxy server
   Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled)
   Active: inactive (dead) Start Nginx and confirm it’s running: sudo systemctl start nginx
sudo systemctl status nginx Now the service should be active (running) : ● nginx.service - A high performance web server and a reverse proxy server
   Loaded: loaded (/lib/systemd/system/nginx.service; enabled; vendor preset: enabled)
   Active: active (running) since Wed 2025-02-05 11:59:11 EST; 5s ago Clear the screen: clear 4. Test with curl Ensure Nginx is serving content: curl localhost You should receive the default Welcome to nginx! HTML response. Clear the screen once more: clear 5. Explore Nginx Configuration Directory All Nginx configurations live under /etc/nginx . Change into this directory and inspect the contents: cd /etc/nginx
ll Example listing: total 76
drwxr-xr-x  2 root root  4096 Feb  5 11:58 ./
drwxr-xr-x  2 root root  4096 Feb  5 11:58 ../
drwxr-xr-x  1 root root  1077 Mar 20  2024 conf.d/
-rw-r--r--  1 root root   1642 Mar 20  2024 fastcgi.conf
-rw-r--r--  1 root root    113 Mar 20  2024 fastcgi_params
...
drwxr-xr-x  2 root root  4096 Feb  5 11:58 sites-available/
drwxr-xr-x  2 root root  4096 Feb  5 11:58 sites-enabled/ 5.1 Common Configuration Directories Directory Description /etc/nginx Core Nginx configuration /etc/nginx/conf.d Independent configuration fragments /etc/nginx/sites-available Virtual host definitions (disabled by default) /etc/nginx/sites-enabled Symlinks to enabled virtual hosts 5.2 Global Settings ( nginx.conf ) Open nginx.conf to review global directives: user www-data;
worker_processes auto;
pid /run/nginx.pid;
include /etc/nginx/modules-enabled/*.conf;

events {
    worker_connections 768;
}

http {
    ##
    # Basic Settings
    ##
    sendfile on;
    tcp_nopush on;
    tcp_nodelay on;
    keepalive_timeout 65;
    types_hash_max_size 2048;

    include /etc/nginx/mime.types;
    default_type application/octet-stream;

    ##
    # SSL Settings
    ##
    ssl_protocols TLSv1 TLSv1.1 TLSv1.2 TLSv1.3;
    ssl_prefer_server_ciphers on;

    ##
    # Logging
    ##
    error_log /var/log/nginx/error.log;
    access_log /var/log/nginx/access.log;

    ##
    # Gzip Settings
    ##
    gzip on;

    ##
    # Virtual Host Configs
    ##
    include /etc/nginx/conf.d/*.conf;
    include /etc/nginx/sites-enabled/*;
} This configuration covers: Worker processes, user, and module inclusion Connection and timeout settings SSL protocol and cipher preferences Logging paths for access and errors Gzip compression Modular includes for virtual hosts 6. View Nginx Logs By default, Nginx writes logs to /var/log/nginx . List the log files: cd /var/log/nginx
ll Example: total 16
drwxr-xr-x 2 root adm  4096 Feb  5 11:58 ./
drwxr-xr-x 1 root root 4096 Feb  5 11:58 ../
-rw-r--r-- 1 www-data adm   86 Feb  5 12:00 access.log
-rw-r----- 1 www-data adm    0 Feb  5 11:58 error.log As you host multiple domains, consider creating subdirectories within /var/log/nginx/ for per-site logging. Links and References Nginx Official Documentation Ubuntu APT Guide Managing Services with systemd Watch Video Watch video content"
Nginx For Beginners,Demo First Website with Nginx,https://notes.kodekloud.com/docs/Nginx-For-Beginners/Install-Config/Demo-First-Website-with-Nginx,"Nginx For Beginners Install Config Demo First Website with Nginx In this tutorial, you’ll learn how to deploy a simple Hello World site using Nginx. By the end, you’ll have a custom virtual host serving your own HTML page. 1. Verify and Start Nginx First, confirm that Nginx is installed and running on your server. sudo systemctl status nginx
sudo systemctl start nginx
curl http://localhost You should see the default “Welcome to nginx!” page: <!DOCTYPE html>
<html>
<head>
  <title>Welcome to nginx!</title>
  <style>
    body { width: 35em; margin: 0 auto; font-family: Tahoma, Verdana, Arial, sans-serif; }
  </style>
</head>
<body>
  <h1>Welcome to nginx!</h1>
  <p>If you see this page, the nginx web server is successfully installed and working. Further configuration is required.</p>
  <!-- ... -->
</body>
</html> Note If Nginx isn’t installed, follow the official installation guide before proceeding. 2. Create a New Site Configuration Switch to the Nginx configuration directory and assume root privileges: cd /etc/nginx
sudo su Copy the default server block to a new file named helloworld : cd sites-available
cp default helloworld Open sites-available/helloworld in your preferred editor and replace its contents with: server {
    listen 80;
    server_name helloworld.com;

    root /var/www/helloworld;
    index index.html index.htm;

    location / {
        try_files $uri $uri/ =404;
    }
} Directive Purpose listen 80; Accept HTTP requests on port 80 server_name Match the Host header to helloworld.com root Define the document root for this virtual host index Specify default files to serve try_files Serve requested files or return a 404 if not found Warning Removing default_server ensures no port conflicts. Always test your configuration before reloading Nginx. Save and exit the editor. 3. Create the Site Content Create the document root and add a simple HTML page: mkdir -p /var/www/helloworld
cat > /var/www/helloworld/index.html <<EOF
<!DOCTYPE html>
<html>
<head>
  <meta charset=""UTF-8"">
  <title>Hello World</title>
</head>
<body>
  <h1>Hello World!</h1>
  <p>Your Nginx web server is now serving custom content.</p>
</body>
</html>
EOF 4. Enable the Site and Reload Nginx Create a symbolic link in sites-enabled : ln -s /etc/nginx/sites-available/helloworld /etc/nginx/sites-enabled/ Test the syntax and reload Nginx: nginx -t
nginx -s reload 5. Test Your Hello World Site To view the default site again: curl http://localhost To test your custom site without DNS: curl --header ""Host: helloworld.com"" http://localhost Expected response: <h1>Hello World!</h1> If you send an unknown host header, Nginx will serve the first server block alphabetically (the default site): curl --header ""Host: unknown.com"" http://localhost Note You can add an entry to /etc/hosts for helloworld.com to test in a browser without DNS: 127.0.0.1   helloworld.com Links and References Nginx Official Docs Understanding Nginx Server Blocks Using curl to Test Web Servers Watch Video Watch video content"
Nginx For Beginners,Firewall Ports,https://notes.kodekloud.com/docs/Nginx-For-Beginners/Install-Config/Firewall-Ports,"Nginx For Beginners Install Config Firewall Ports A firewall is a security barrier—hardware or software—between your system and the Internet. It inspects and filters incoming and outgoing traffic, blocking unauthorized access while permitting legitimate communication. Much like a home security system that monitors all “doors and windows,” a firewall triggers alarms when it detects suspicious activity. Hardware firewalls operate on the same principle but are deployed in network appliances or data centers. Built-In Firewalls Across Operating Systems Debian & Ubuntu (UFW) Debian and Ubuntu include the Uncomplicated Firewall (UFW), which is disabled by default —permitting all traffic until activated. Red Hat & Fedora (firewalld) Red Hat and Fedora rely on firewalld , also off by default. Install or enable it with YUM or DNF if it’s missing. Both UFW and firewalld serve as front ends to iptables , the underlying Linux utility managing packet filtering. Windows & macOS Windows Firewall comes enabled by default . macOS Firewall is disabled by default but can be activated in System Preferences. Warning Always keep your firewall enabled to reduce the attack surface of your server. Understanding Ports A port is a logical communication endpoint—think of it as a “door” or “window” in your network. Each service listens on a specific port number. When you visit websites: HTTP traffic → port 80 HTTPS traffic → port 443 Other services use different ports. Expose only what’s necessary. Port Service Description 22 SSH Secure shell access 25 SMTP Email delivery 53 DNS Domain name resolution 80 HTTP Unencrypted web traffic 443 HTTPS Encrypted web traffic Best practice: On public web servers, open only ports 80 and 443 , or restrict other ports via IP allow-lists. Managing UFW on Debian/Ubuntu Allow SSH first to prevent lockout: sudo ufw allow 22/tcp Enable UFW: sudo ufw enable Open HTTP and HTTPS: sudo ufw allow 80/tcp
sudo ufw allow 443/tcp Reload to apply: sudo ufw reload View rules with indices: sudo ufw status numbered Delete a specific rule: sudo ufw delete <rule-number> Managing firewalld on Red Hat/Fedora Install (if needed): sudo yum update && sudo yum install firewalld Start and enable at boot: sudo systemctl start firewalld
sudo systemctl enable firewalld Open port permanently (e.g., HTTP): sudo firewall-cmd --permanent --add-port=80/tcp
sudo firewall-cmd --reload Remove a port: sudo firewall-cmd --permanent --remove-port=80/tcp
sudo firewall-cmd --reload Check active zones and ports: sudo firewall-cmd --list-all Inspecting Open Ports with netstat netstat lists active connections and listening ports. Install it if missing: # Debian/Ubuntu
sudo apt update
sudo apt install net-tools

# Red Hat/Fedora/CentOS
sudo yum install net-tools Run: sudo netstat -nltup Sample output: Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      1042/sshd
tcp6       0      0 :::22                   :::*                    LISTEN      1042/sshd
udp        0      0 127.0.0.1:323           0.0.0.0:*                          634/chronyd
udp6       0      0 :::323                  :::*                               634/chronyd netstat helps you verify which ports your services are actively listening on—essential for troubleshooting connectivity and firewall configurations. Links and References UFW Documentation firewalld Guide iptables Tutorial Watch Video Watch video content"
Terraform Associate Certification HashiCorp Certified,Recap IAC,https://notes.kodekloud.com/docs/Terraform-Associate-Certification-HashiCorp-Certified/Understand-Infrastructure-as-Code-IaC-concepts/Recap-IAC,"Terraform Associate Certification: HashiCorp Certified Understand Infrastructure as Code IaC concepts Recap IAC In this article, we explore key concepts and tools in the Infrastructure as Code (IaC) ecosystem to help you prepare for your exam. IaC not only simplifies the deployment process but also ensures consistency and scalability in modern IT infrastructures. We leverage a variety of tools such as Ansible, Terraform, Puppet, CloudFormation, Packer, SaltStack, Vagrant, Docker, and more. Although many of these tools can achieve similar outcomes, each one is optimized for specific tasks. Broadly, IaC tools are classified into three categories: 1. Configuration Management Tools Tools such as Ansible, Chef, Puppet, and SaltStack are used to install and manage software on existing infrastructure resources like servers, databases, and networking devices. They help maintain a consistent code structure, enable version control, and ensure idempotency (i.e., re-executing the code only applies necessary changes). For example, an Ansible playbook designed to install a package on a group of servers first checks whether the software is already installed, only applying the installation when necessary. 2. Server Templating Tools Server templating tools such as Docker, Packer, and Vagrant are used to create custom images of virtual machines or containers. These images are pre-installed with the required software and dependencies, eliminating the need for post-deployment software installation. Common examples include VM images from osboxes.org , custom Amazon AMIs , and Docker images from Docker Hub . Server templating supports an immutable infrastructure model—making updates as simple as redeploying a new instance with an updated image rather than modifying a running system. 3. Infrastructure Provisioning (Orchestration) Tools Provisioning tools, such as Terraform and CloudFormation, enable you to manage a variety of infrastructure components like virtual machines, databases, VPCs, subnets, security groups, and storage using declarative code. CloudFormation is ideal for AWS-only deployments, whereas Terraform offers a vendor-agnostic solution that supports multi-cloud or hybrid environments through numerous plugins. Note Although configuration management tools can provision infrastructure (for example, using Ansible’s EC2 module), this approach is less effective for managing larger infrastructures due to its procedural nature. Procedural vs. Declarative Approach Understanding the difference between procedural and declarative approaches is crucial when working with IaC tools. Ansible Example (Procedural) Ansible employs a procedural approach in which every step must be explicitly defined. Consider the following playbook that provisions two EC2 instances: - name: Provision AWS Resources
  hosts: localhost
  tasks:
    - name: Provision EC2 instances using Ansible
      ec2:
        key_name: appserver
        instance_tags:
          Name: appserver
        instance_type: t2.micro
        image: ami-0d8ad3ab25e7abc51
        region: ca-central-1
        wait: yes
        count: 2 Executing the playbook: > ansible-playbook ec2.yaml
.
PLAY RECAP *********************************************************************
localhost                  : ok=2    changed=1    unreachable=0    failed=0    rescued=0    ignored=0 If you run the playbook again, Ansible will create two additional EC2 instances, resulting in a total of four. To maintain exactly two instances, you must include additional parameters to manage the desired state. The enhanced version for both provisioning and deletion is as follows: - name: Provision AWS Resources
  hosts: localhost
  tasks:
    - name: Provision EC2 instances using Ansible
      ec2:
        key_name: appserver
        instance_tags:
          Name: appserver
        instance_type: t2.micro
        image: ami-0d8ad3ab25e7abc51
        region: ca-central-1
        wait: yes
        exact_count: 2
        count_tag:
          Name: appserver

    - name: Delete Instances
      ec2:
        state: absent
        instance_ids: '{{ ec2.instance_ids }}' Terraform Example (Declarative) In contrast, Terraform uses a declarative approach where you specify the desired end state of your infrastructure. Consider the Terraform configuration below that ensures exactly two EC2 instances: resource ""aws_instance"" ""app"" {
  ami           = ""ami-0d8ad3ab25e7abc51""
  instance_type = ""t2.micro""
  count         = 2
  key_name      = ""appserver""
  tags = {
    Name = ""appserver""
  }
} Running the following command applies the configuration: > terraform apply Terraform creates or maintains exactly two EC2 instances. On subsequent executions, Terraform will indicate that the current state matches the configuration: > terraform apply
aws_instance.app[0]: Creation complete after 33s [id=i-014c93c14e12a6442]
aws_instance.app[1]: Creation complete after 33s [id=i-0fc7d85da32d24c63]
Terraform has compared your real infrastructure against your configuration and found no changes. You can start your work!

Apply complete! Resources: 2 added, 0 changed, 0 destroyed. If no drift is detected, the output will state: > terraform apply
No changes. Your infrastructure matches the configuration.

Terraform has compared your real infrastructure against your configuration and found no differences, so no changes are needed.

Apply complete! Resources: 0 added, 0 changed, 0 destroyed. Terraform maintains the state of each provisioned resource in a state file, which it uses to detect deviations between your desired configuration and the actual infrastructure. This powerful state management enables resource teardown using: > terraform destroy Choosing the Right IaC Tool There is no one-size-fits-all solution when it comes to selecting an IaC tool. For deployments exclusive to AWS, CloudFormation offers simplicity and direct integration. However, for multi-cloud or hybrid environments, Terraform's vendor-agnostic design makes it an excellent choice. Tip Maximize efficiency by leveraging the strengths of each IaC tool: use Terraform for resource provisioning and configuration management tools like Ansible for post-deployment tasks such as software installation and configuration. For further insights on IaC best practices and tool comparisons, consider reviewing additional resources: Kubernetes Documentation Docker Hub Terraform Registry Watch Video Watch video content"
Terraform Associate Certification HashiCorp Certified,Certification Details,https://notes.kodekloud.com/docs/Terraform-Associate-Certification-HashiCorp-Certified/Course-Introduction/Certification-Details,"Terraform Associate Certification: HashiCorp Certified Course Introduction Certification Details Let's review the essential details for the HashiCorp Certified Terraform Associate exam. This exam is designed to test your Terraform knowledge with a mix of multiple choice, multiple answer, and true/false questions. Exam Overview Duration: 60 minutes Number of Questions: Approximately 57 Cost: About $70.50 Validity: 2 years With roughly one minute per question, it's important to manage your time effectively. Exam Format The HashiCorp Certified Terraform Associate exam is online and proctored, meaning you can take it from the comfort of your home using a laptop or desktop computer. Technical Requirements Before you begin the exam, ensure that your system and environment meet these technical requirements: Virtual machines are not permitted. The PSI Secure Browser must be installed to start the exam. Additional or external monitors are not allowed. Headphones are not permitted during the exam. Your webcam, speakers, and microphone must remain active for the entire exam. Additionally, prepare a quiet, well-lit testing environment that is free from any electronic devices (except for the device you are using to take the test). The online proctor must have a clear view and hear you at all times, so clear your desk area accordingly. Important Reminder Make sure to complete all system checks and verify your testing setup before starting the exam to avoid any disruptions during the test. Registration Process To register for the exam, visit the registration page at the following link: On the HashiCorp Certification page , you will find comprehensive exam details, including additional FAQs and step-by-step instructions on how to register and schedule your exam. Good luck with your preparation, and let's now jump right into our first lecture. Watch Video Watch video content"
Terraform Associate Certification HashiCorp Certified,Installing Terraform HCL Basics,https://notes.kodekloud.com/docs/Terraform-Associate-Certification-HashiCorp-Certified/Understand-Infrastructure-as-Code-IaC-concepts/Installing-Terraform-HCL-Basics,"Terraform Associate Certification: HashiCorp Certified Understand Infrastructure as Code IaC concepts Installing Terraform HCL Basics In this article, you'll learn how to install Terraform and understand the basics of HCL (HashiCorp Configuration Language). We will walk through the process of installing Terraform on your system and introduce you to the structure of HCL configuration files. Terraform is a popular Infrastructure as Code (IaC) tool that allows you to provision and manage a wide range of infrastructure resources. It supports various operating systems including Windows, macOS, Linux, Solaris, and OpenBSD. Tip Before you begin, ensure that you have the necessary permissions to install software on your system. Installing Terraform Installation is straightforward: Download the binary for your operating system. Unzip the downloaded file. Move the binary to your system path. For example, on Linux: $ wget https://releases.hashicorp.com/terraform/0.15.0/terraform_0.15.0_linux_amd64.zip
$ unzip terraform_0.15.0_linux_amd64.zip
$ mv terraform /usr/local/bin
$ terraform version
Terraform v0.15.0 After installing, Terraform is ready to use for provisioning resources. Understanding HashiCorp Configuration Language (HCL) Terraform configurations are written in HCL and stored in files with a .tf extension. These files are composed of blocks and arguments: Blocks: Define the infrastructure components or describe which resources to create. Arguments: Provided as key-value pairs within the blocks, they specify configuration parameters. Example: Creating a Local File Resource The following is an example configuration file named local.tf that creates a file on the local system. This simple example helps illustrate the basics, even though the majority of this course will focus on AWS-based resources. Let's review the configuration in local.tf : resource ""local_file"" ""pet"" {
  filename = ""/root/pets.txt""
  content  = ""We love pets!""
} Explanation of the Configuration Resource Block: The block starts with the keyword resource and is enclosed in curly braces. It tells Terraform which infrastructure object to manage. Resource Type and Provider: In this example, the resource type is local_file . The prefix before the underscore (""local"") indicates the provider, while the suffix (""file"") specifies the resource type. Resource Name: The name pet acts as a logical identifier for this resource, allowing you to reference it in other parts of your configuration. Arguments: Inside the block, there are two key-value pairs: filename (the path of the file) and content (the content to be written to the file). Example: Creating an AWS EC2 Instance The configuration structure remains consistent across providers. Here is an example used to create an AWS EC2 instance: resource ""aws_instance"" ""web"" {
  ami           = ""ami-0c2f25c1f66a1ff4d""
  instance_type = ""t2.micro""
} This block defines an AWS EC2 instance by specifying the AMI ID and the instance type. Overview of Terraform Resources A resource in Terraform is any object that it manages. This includes: Local files (e.g., local_file ) Virtual machine instances (e.g., AWS EC2) Cloud services such as S3 buckets, ECS clusters, DynamoDB tables, IAM users, and many more Terraform supports hundreds of providers. For detailed information on available arguments and configurations for a specific resource, always refer to the Terraform Documentation . In the provider documentation, you will see that some arguments are mandatory (like filename for local_file ), while others, including ACLs or permissions, may be optional. Terraform Workflow After writing your configuration files, use the following steps to provision your resources: Write the Configuration File: Create your .tf files with the required resource definitions. Initialize the Directory: Run the terraform init command to initialize the working directory and download necessary provider plugins. $ terraform init
Initializing the backend...

Initializing provider plugins...
- Finding latest version of hashicorp/local...
- Installing hashicorp/local v1.4.0...
- Installed hashicorp/local v1.4.0 (signed by HashiCorp)

The following providers do not have any version constraints in configuration,
so the latest version was installed.

To prevent automatic upgrades to new major versions that may contain breaking
changes, we recommend adding version constraints in a required_providers block
in your configuration, with the constraint strings suggested below.

* hashicorp/local: version = ""~> 1.4.0""

Terraform has been successfully initialized! Review the Execution Plan: Use terraform plan to preview the actions Terraform will perform. The output will show symbols; for example, a plus sign (+) indicates a resource creation. Apply the Configuration: Run terraform apply to execute the execution plan. Terraform will prompt you for confirmation before proceeding unless you supply the -auto-approve flag. $ terraform apply
An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
+ create

Terraform will perform the following actions:

# local_file.pet will be created:
+ resource ""local_file"" ""pet"" {
    + content              = ""We love pets!""
    + directory_permission = ""0777""
    + file_permission      = ""0777""
    + filename             = ""/root/pets.txt""
    + id                   = (known after apply)
}

Plan: 1 to add, 0 to change, 0 to destroy.

Do you want to perform these actions?
Terraform will perform the actions described above.
Only 'yes' will be accepted to approve.

Enter a value: yes
local_file.pet: Creating...
local_file.pet: Creation complete after 0s
[id=521c5c732c78cb42cc9513ecc7c0638c4a115b55]
Apply complete! Resources: 1 added, 0 changed, 0 destroyed. To verify the resource creation, use the terraform show command: $ terraform show
# local_file.pet:
resource ""local_file"" ""pet"" {
  content              = ""We love pets!""
  directory_permission = ""0777""
  file_permission      = ""0777""
  filename             = ""/root/pets.txt""
  id                   = ""cba595b7d9f94ba1107a46f3f731912d95fb3d2c""
} The terraform show command displays the state of the created resource. Further Reading For more details on Terraform state management and best practices, check out the official Terraform Documentation . Recap Resource Blocks: Define the objects Terraform manages. They include the resource type, a logical identifier, and configuration arguments. Arguments: Specify the parameters for each resource. They can be either mandatory or optional depending on the resource. Providers: Terraform works with multiple providers, each offering a unique set of resources and configuration options. Always refer to the provider's documentation to understand all available arguments. This concludes our article on installing Terraform and HCL basics. In future discussions, we will explore methods for updating and destroying resources using Terraform. Watch Video Watch video content"
Terraform Associate Certification HashiCorp Certified,Create Update and Destroy Infrastructure,https://notes.kodekloud.com/docs/Terraform-Associate-Certification-HashiCorp-Certified/Understand-Infrastructure-as-Code-IaC-concepts/Create-Update-and-Destroy-Infrastructure,"Terraform Associate Certification: HashiCorp Certified Understand Infrastructure as Code IaC concepts Create Update and Destroy Infrastructure In this article, we revisit how to update and destroy infrastructure resources managed with Terraform. Using a local file resource as an example, we will walk through updating a resource configuration, previewing the changes, and eventually destroying the resource if needed. Updating a Resource To update a resource in Terraform, simply modify the configuration file. For example, you might add a new argument to update the file permissions of a resource to ""0700"": resource ""local_file"" ""pet"" {
  filename        = ""/root/pets.txt""
  content         = ""We love pets!""
  file_permission = ""0700""
} After making your changes, run the Terraform plan to preview the execution changes: $ terraform plan Note Running terraform plan is optional because executing terraform apply displays the same execution plan. In the execution plan, Terraform uses the “-/+” symbol preceding the resource name to indicate that the resource will be destroyed and then recreated. A corresponding line in the output specifies that the change in file permissions is forcing this replacement. Since Terraform adheres to immutable infrastructure principles, any update that alters critical properties results in the resource being destroyed and re-created with the new settings. Once you have reviewed the changes, apply the updates with: $ terraform apply
local_file.pet: Refreshing state...
[id=feafccdae259f25533749abfb90e27558256459]

-/+ destroy and then create replacement
...
Plan: 1 to add, 0 to change, 1 to destroy.

Do you want to perform these actions?
Terraform will perform the actions described above.
Only 'yes' will be accepted to approve.

Enter a value: yes
local_file.pet: Destroying...
[id=feafccdae259f25533749abfb90e27558256459]
local_file.pet: Destruction complete after 0s
local_file.pet: Creating...
local_file.pet: Creation complete after 0s
[id=feafccdae259f25533749abfb90e27558256459]

Apply complete! Resources: 1 added, 0 changed, 1 destroyed. Destroying a Resource To remove a resource completely from your infrastructure, use the terraform destroy command. This command generates an execution plan that shows a minus symbol next to each resource set for removal. Review the plan carefully before confirming the destruction. $ terraform destroy
local_file.pet: Refreshing state...
[id=5f8fb950ac60f7f23ef968097cda0a1fd3c11bdf]

An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  - destroy

Terraform will perform the following actions:

  # local_file.pet will be destroyed
  - resource ""local_file"" ""pet"" {
      - content             = ""My favorite pet is a gold fish"" -> null
      - directory_permission = ""0777"" -> null
      - file_permission     = ""0700"" -> null
      - filename            = ""/root/pet.txt"" -> null
      - id                  = ""5f8fb950ac60f7f23ef968097cda0a1fd3c11bdf"" -> null
    }

Plan: 0 to add, 0 to change, 1 to destroy.

Do you really want to destroy all resources?
Terraform will destroy all your managed infrastructure, as shown above.
There is no undo. Only 'yes' will be accepted to confirm.

Enter a value: yes
local_file.pet: Destroying... [id=5f8fb950ac60f7f23ef968097cda0a1fd3c11bdf]
local_file.pet: Destruction complete after 0s

Destroy complete! Resources: 1 destroyed. You can bypass the confirmation prompt by including the auto-approve flag with the terraform destroy command. Use this option with caution, as it immediately removes all managed resources. Organizing Your Configuration Directory Terraform treats any file with a .tf extension found in the current working directory as part of your configuration. This allows you to split your infrastructure configuration across multiple files for better organization. For instance, if your configuration is stored under /root/terraform-local-file with an initial file named local.tf , your directory might look like this: [terraform-local-file]$ ls /root/terraform-local-file
local.tf The local.tf file may contain a resource like: resource ""local_file"" ""pet"" {
  filename = ""/root/pets.txt""
  content  = ""We love pets!""
} You can add another configuration file, such as cat.tf , to define an additional resource: resource ""local_file"" ""cat"" {
  filename = ""/root/cat.txt""
  content  = ""My favorite pet is Mr. Whiskers""
} Both resources will be created when you run terraform apply for the first time. A common best practice is to consolidate resource blocks into a single configuration file (often named main.tf ) while separating variables and outputs into dedicated files such as variables.tf and outputs.tf . This modular structure improves manageability, especially as your project grows in complexity. That concludes our guide on updating and destroying Terraform-managed infrastructure resources. To reinforce your understanding, consider taking the multiple-choice quiz and testing your knowledge of these Terraform operations. Watch Video Watch video content"
Terraform Associate Certification HashiCorp Certified,Aliases,https://notes.kodekloud.com/docs/Terraform-Associate-Certification-HashiCorp-Certified/Terraform-Provider-Basics/Aliases,"Terraform Associate Certification: HashiCorp Certified Terraform Provider Basics Aliases In this lesson, we delve into using provider aliases in Terraform to manage resources across multiple regions. Previously, we covered specifying provider versions using version constraints. Now, we will see how to provision resources in different AWS regions by configuring multiple instances of the same provider. Consider the following scenario: you have a simple Terraform configuration with two resource blocks. The first block creates an AWS EC2 key pair in the US East 1 region, while the second block is intended to create a key pair in the CA Central 1 region. By default, if you only define one provider block (e.g., for the US East 1 region), both resource blocks would be provisioned in that same region. Understanding the Default Behavior Without any modifications, Terraform uses the default provider configuration for all resources. This means that if you don't specify otherwise, all resources will be created in the defined default region. To create a resource in the CA Central 1 region using an aliased provider configuration, you need to define a second provider block with the appropriate region and an alias. Below is an example of how to set up the providers and resources: resource ""aws_key_pair"" ""alpha"" {
  key_name   = ""alpha""
  public_key = ""ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQ...alpha@a-server""
}

resource ""aws_key_pair"" ""beta"" {
  key_name   = ""beta""
  public_key = ""ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB""
}

provider ""aws"" {
  region = ""us-east-1""
}

provider ""aws"" {
  region = ""ca-central-1""
  alias  = ""central""
} Notice the alias ""central"" in the second provider block. To ensure that the ""beta"" resource is created in the CA Central region, add a provider argument within its configuration that references the aliased provider using the syntax: provider_name.alias . For instance: resource ""aws_key_pair"" ""beta"" {
  provider   = aws.central
  key_name   = ""beta""
  public_key = ""ssh-rsa AAAAB3NzaC1yc2EAAAADAQAB""
} Key Concept By specifying the provider = aws.central argument in the ""beta"" resource block, Terraform knows to use the aliased provider configured for the CA Central 1 region, while resources without a provider argument use the default provider (US East 1). After running terraform apply , Terraform provisions the resources as specified: The key pair ""alpha"" is created in the US East 1 region. The key pair ""beta"" is created in the CA Central 1 region using the aliased provider. To view the details of the provisioned resources, execute: $ terraform show

# aws_key_pair.alpha:
resource ""aws_key_pair"" ""alpha"" {
  arn         = ""arn:aws:ec2::us-east-1::key-pair/alpha""
  fingerprint = ""d7:ff:a6:63:18:64:9c:57:a1:ee:ca:a4:ad:c2:81:62""
  id          = ""alpha""
  key_name    = ""alpha""
  public_key  = ""ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDJ3F6tyPEFEzV0LX3X8BsXdMsQz1x2CeikKDEY0aIj41qgxMCP/iteneqXSIFZBp5vizPvaoIR3Um9xK7PGoW8giupGn+EPuxIA4cDM4VzOQoikMhz5XK0WhEjkVzTo4+S0puvDZuWIsdiW9mxhJc7tgBNL0cV1WSYVkz4G/fs1NfRPW5mYAM49f4fhtxPb5ok4Q2Lg9dPKVHO/Bgeu5woMc7RY0p1ej6D4CKF61ymSDJpw0HYX/wqE9+cfEauh7xZcG0q9t2ta6F6fmX0agvFyZo8aFbXeUBr7osSCJNgvavlWbM/06niWr0vYX2xwW""
  tags_all    = {}
}

# aws_key_pair.beta:
resource ""aws_key_pair"" ""beta"" {
  arn         = ""arn:aws:ec2:ca-central-1::key-pair/beta""
  fingerprint = ""d7:ff:a6:63:18:64:9c:57:a1:ee:ca:a4:ad:c2:81:62""
  id          = ""beta""
  key_name    = ""beta""
  public_key  = ""ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQDJ3F6tyPEFEzV0LX3X8BsXdMsQz1x2CeikKDEY0aIj41qgxMCP/iteneqXSIFZBp5vizPvaoIR3Um9xK7PGoW8giupGn+EPuxIA4cDM4VzOQoikMhz5XK0WhEjkVzTo4+S0puvDZuWIsdiW9mxhJc7tgBNL0cV1WSYVkz4G/fs1NfRPW5mYAM49f4fhtxPb5ok4Q2Lg9dPKVHO/Bgeu5woMc7RY0p1ej6D4CKF61ymSDJpw0HYX/wqE9+cfEauh7xZcG0q9t2ta6F6fmX0agvFyZo8aFbXeUBr7osSCJNgvavlWbM/06niWr0vYX2xwW""
  tags_all    = {}
} In summary, using provider aliases in Terraform allows you to manage resources across different regions seamlessly. This approach is especially useful when working with large-scale, multi-region deployments. That concludes this lesson on Terraform provider aliases. Watch Video Watch video content"
Terraform Associate Certification HashiCorp Certified,Multiple Providers,https://notes.kodekloud.com/docs/Terraform-Associate-Certification-HashiCorp-Certified/Terraform-Provider-Basics/Multiple-Providers,"Terraform Associate Certification: HashiCorp Certified Terraform Provider Basics Multiple Providers In this lesson, you'll learn how to configure and use multiple providers within a single Terraform setup. Leveraging multiple providers enables you to provision and manage resources across different platforms simultaneously, thereby enhancing your infrastructure’s flexibility and scalability. Overview Terraform allows you to define resources from various providers in a single configuration file. In the example below, the configuration file includes two resource blocks: one managed by the local provider and another by the random provider. Tip When adding a new provider, Terraform automatically downloads and installs the necessary plugin during initialization. Example Configuration Below is an updated snippet from your main.tf file which includes a resource for creating a local file and another for generating a random pet name: resource ""local_file"" ""pet"" {
  filename = ""/root/pets.txt""
  content  = ""We love pets!""
}

resource ""random_pet"" ""my-pet"" {
  prefix    = ""Mrs""
  separator = "".""
  length    = 1
} During initialization, Terraform detects that the local provider is already installed and proceeds to download the random provider plugin if it’s not already available. Running the terraform init command produces output similar to the following: $ terraform init
Initializing the backend...

Initializing provider plugins...
- Using previously-installed hashicorp/local v2.0.0
- Finding latest version of hashicorp/random...
- Installing hashicorp/random v2.3.0...
- Installed hashicorp/random v2.3.0 (signed by HashiCorp)

The following providers do not have any version constraints in configuration,
so the latest version was installed.

To prevent automatic upgrades to new major versions that may contain
breaking changes, we recommend adding version constraints in a required_providers
block in your configuration, with the constraint strings suggested below.

* hashicorp/local: version = ""~> 2.0.0""
* hashicorp/random: version = ""~> 2.3.0""

Terraform has been successfully initialized! Execution Workflow After initialization, the workflow is consistent regardless of the number of resources or providers included. The typical sequence is: Run terraform plan to preview the changes. Run terraform apply to create or update the resources. When applying the configuration, Terraform refreshes the state of existing resources and creates any new ones. For instance, when terraform apply is executed in this scenario, Terraform generates a random pet name and creates the corresponding resource: $ terraform apply
local_file.pet: Refreshing state... [id=d1a31467f206d6ea8ab1cad382bc106bf46df69]

An execution plan has been generated and is shown below.
Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # random_pet.my-pet will be created
  + resource ""random_pet"" ""my-pet"" {
      + id        = (known after apply)
      + length    = 1
      + prefix    = ""Mrs""
      + separator = "".""
    }

Plan: 1 to add, 0 to change, 0 to destroy.

random_pet.my-pet: Creating...
random_pet.my-pet: Creation complete after 0s [id=Mrs.hen]

Apply complete! Resources: 1 added, 0 changed, 0 destroyed. In the output, the id attribute for the random pet resource is generated only after the apply process completes. Attributes like this are useful for referencing in other parts of your configuration, ensuring that resource dependencies and data flows are maintained. Linking Resources Across Providers You can also link resources from different providers by referencing their generated attributes. The following example demonstrates this by creating a random string using the random provider and then utilizing that string to tag an AWS instance: resource ""random_string"" ""server-suffix"" {
  length  = 6
  upper   = false
  special = false
}

resource ""aws_instance"" ""web"" {
  ami           = ""ami-06178cf087598769c""
  instance_type = ""m5.large""
  tags = {
    Name = ""web-${random_string.server-suffix.id}""
  }
} When terraform apply is executed with this configuration, Terraform generates the random string and applies it as a tag to the AWS instance resource. This effectively demonstrates how resource attribute references can bridge resources managed by different providers. SEO Tip For enhanced search engine optimization and user engagement, ensure that your Terraform configuration documentation includes clear, concise headings and detailed examples. Reference related documentation such as Terraform Providers and AWS Instances for additional context. Conclusion This lesson covered how to use multiple providers in Terraform. You learned how to add new providers to your configuration, how Terraform initializes and uses these providers, and how to link resources across different platforms. Proceed to the multiple-choice questions to test your understanding of managing multiple providers in Terraform. Thank you. Watch Video Watch video content"
Terraform Associate Certification HashiCorp Certified,Recap Using Terraform Providers,https://notes.kodekloud.com/docs/Terraform-Associate-Certification-HashiCorp-Certified/Terraform-Provider-Basics/Recap-Using-Terraform-Providers,"Terraform Associate Certification: HashiCorp Certified Terraform Provider Basics Recap Using Terraform Providers In this article, we recap Terraform providers and illustrate how to use them to provision resources across various platforms. Terraform follows a plugin-based architecture that supports hundreds of platforms, including AWS, GCP, Azure, and more. The first command you'll run with a valid configuration is the Terraform init command. This command installs the necessary plugins for the providers specified in your configuration. Tip Running terraform init is safe and can be executed multiple times without affecting your deployed infrastructure. $ terraform init Terraform providers, distributed by HashiCorp, are publicly available in the Terraform Registry . There are three types of providers: Official Providers : Owned and maintained by HashiCorp. These include major cloud providers like AWS, GCP, and Azure. Official providers feature a distinct badge in the Terraform Registry. Partner Providers : Maintained by third-party technology companies but reviewed and tested by HashiCorp. They are identified by a checkmark badge in the registry. Community Providers : Published and maintained by individual contributors. Understanding Terraform Init Output When running terraform init , you may see output similar to the following: $ terraform init
Initializing the backend...

Initializing provider plugins...
- Finding latest version of hashicorp/local...
- Installing hashicorp/local v2.0.0...
- Installed hashicorp/local v2.0.0 (signed by HashiCorp)

The following providers do not have any version constraints in configuration,
so the latest version was installed.

To prevent automatic upgrades to new major versions that may contain breaking changes,
we recommend adding version constraints in a required_providers block in your configuration,
with the constraint strings suggested below.

* hashicorp/local: version = ""~> 2.0.0""

Terraform has been successfully initialized! In this output, you can observe that the plugin for the HashiCorp Local provider (version 2.0.0) has been installed in your working directory. The plugins are downloaded into a hidden folder named .terraform/plugins located in the directory containing your configuration files. Important For production environments, always add version constraints in your configuration to avoid unexpected breaking changes from automatic upgrades. Provider Naming Convention Terraform provider names include an organizational namespace. For example, in the case of the local provider, the namespace is ""hashicorp"", and the provider type is ""local"". Other common examples include AWS, Azure, and Google Cloud. hashicorp/local version = ""~> 2.0.0"" The provider name can optionally include a hostname. The hostname indicates the registry location for the plugin. By default, if the hostname is omitted, it defaults to registry.terraform.io . Therefore, the source address for the local provider can be specified as either: registry.terraform.io/hashicorp/local or simply hashicorp/local Watch Video Watch video content"
Terraform Associate Certification HashiCorp Certified,Recap Variables,https://notes.kodekloud.com/docs/Terraform-Associate-Certification-HashiCorp-Certified/Variables-Resource-Attributes-and-Dependencies/Recap-Variables,"Terraform Associate Certification: HashiCorp Certified Variables Resource Attributes and Dependencies Recap Variables In this article, we recap how variables work in Terraform and demonstrate their usage with practical examples. Instead of hard-coding values directly in your configuration, variables allow you to write more flexible, maintainable code. Although it is common practice to store variable definitions in a separate file named variables.tf , you can also declare them in the same file as your resources (e.g., main.tf ). Hard-Coded Resource Definitions Below is an example of defining resources using hard-coded values: resource ""local_file"" ""pet"" {
  filename = ""/root/pets.txt""
  content  = ""We love pets!""
}

resource ""random_pet"" ""my-pet"" {
  prefix    = ""Mrs""
  separator = "".""
  length    = ""1""
} Defining Variables in a Separate File By defining variables in a separate file, you can avoid repetition and make your configuration easier to update. The following example demonstrates how to define corresponding variables in variables.tf : variable ""filename"" {
  default = ""/root/pets.txt""
}

variable ""content"" {
  default = ""We love pets!""
}

variable ""prefix"" {
  default = ""Mrs""
}

variable ""separator"" {
  default = "".""
}

variable ""length"" {
  default = ""1""
} Using Variables in Resource Configurations By referencing variables in your resource configurations using the var. prefix, you can replace hard-coded values. This makes it easier to change values without having to update each resource block. See the example below: # main.tf
resource ""local_file"" ""pet"" {
  filename = var.filename
  content  = var.content
}

resource ""random_pet"" ""my-pet"" {
  prefix    = var.prefix
  separator = var.separator
  length    = var.length
} The variable definitions remain in variables.tf as shown earlier. When you run terraform apply , Terraform processes your configuration and recognizes that variable values do not need to be enclosed in double quotes in concatenated expressions. If you update a variable value (for example, changing the content variable or increasing the length from 1 to 2), Terraform will detect the change and replace the affected resources accordingly. Below is an example output after updating variable values: $ terraform apply
Terraform will perform the following actions:

-/+ resource ""local_file"" ""pet"" {
    ~ content            = ""We love pets!"" -> ""My favorite pet is Mrs. Whiskers!"" # forces replacement
      directory_permission = ""0777""
      file_permission      = ""0777""
      filename            = ""/root/pet.txt""
      ~ id                 = ""bc9cabef1d8b0071d3c4ae9959a9c328f35fe697"" -> (known after apply)
}

# random_pet.my-pet must be replaced
-/+ resource ""random_pet"" ""my-pet"" {
      ~ id      = ""Mrs.Hen"" -> (known after apply)
      ~ length  = 1 -> 2 # forces replacement
        prefix    = ""Mrs""
        separator = "".""
}

Plan: 2 to add, 0 to change, 2 to destroy.
random_pet.my-pet: Destroying... [id=Mrs.hen]
random_pet.my-pet: Destruction complete after 0s
local_file.pet: Destroying... [id=bc9cabef1d8b0071d3c4ae9959a9c328f35fe697]
local_file.pet: Destruction complete after 0s
random_pet.my-pet: Creating...
local_file.pet: Creating... Tip Using variables in Terraform not only makes your configurations more readable but also simplifies maintenance when scaling your infrastructure. AWS Instance Example Using Variables Consider an example where you create an AWS instance using variables for the AMI and instance type. In the resource definition below, the values for ami and instance_type are referenced from variables: resource ""aws_instance"" ""webserver"" {
  ami           = var.ami
  instance_type = var.instance_type
} The variable definitions for this configuration might look like the following: variable ""ami"" {
  default = ""ami-0edab43b6fa892279""
}

variable ""instance_type"" {
  default = ""t2.micro""
} While these defaults are set in variables.tf , you can override them when applying the configuration. There are a few methods to do so: Remove the Default Values: You can remove the defaults in variables.tf and provide values explicitly during runtime. # main.tf
resource ""aws_instance"" ""webserver"" {
  ami           = var.ami
  instance_type = var.instance_type
}

# variables.tf
variable ""ami"" {
}

variable ""instance_type"" {
} Pass Values with Command-Line Flags: Provide variable values using the -var flag during execution: $ terraform apply -var ""ami=ami-0edab43b6fa892279"" -var ""instance_type=t2.micro"" Use Environment Variables: Export the variable values before running Terraform: $ export TF_VAR_instance_type=""t2.micro""
$ terraform apply Variable Definition File: Supply variable values via a variable definition file (ending with .tfvars or .tfvars.json ). By default, Terraform automatically loads files named terraform.tfvars or terraform.tfvars.json . For custom-named files, use the -var-file flag: # variables.tfvars
ami = ""ami-0edab43b6fa892279""
instance_type = ""t2.micro"" $ terraform apply -var-file=""variables.tfvars"" Variable Definition Precedence Terraform provides multiple methods for assigning variable values. When a variable is defined in more than one way, Terraform follows a specific precedence order: Precedence Level Description 1. Environment Variables Values set via environment variables (e.g., export TF_VAR_instance_type=""t2.micro"" ) 2. terraform.tfvars File Values provided in terraform.tfvars or terraform.tfvars.json 3. Auto-loaded Variable Files Files ending with .auto.tfvars or .auto.tfvars.json loaded in alphabetical order 4. Command-Line Flags Values passed using the -var or -var-file flags (highest precedence) For example, if the variable type is specified through multiple methods as shown below: $ export TF_VAR_type=""t2.nano"" # In terraform.tfvars:
type = ""t3.micro"" # In an auto.tfvars file:
type = ""t3.small"" $ terraform apply -var ""type=t2.medium"" Terraform will use t2.medium as the final value for type since command-line flags have the highest precedence. Best Practice Always be aware of the variable precedence in Terraform to avoid unexpected behaviors during deployment. Using dedicated variable files or environment variables can improve consistency across different environments. That concludes our article on variables in Terraform. Understanding how to define, reference, and override variables is essential for creating flexible and reusable Terraform configurations. Happy building! Watch Video Watch video content"
Terraform Associate Certification HashiCorp Certified,Version Constraints,https://notes.kodekloud.com/docs/Terraform-Associate-Certification-HashiCorp-Certified/Terraform-Provider-Basics/Version-Constraints,"Terraform Associate Certification: HashiCorp Certified Terraform Provider Basics Version Constraints In this article, we explore how Terraform handles provider versions and version constraints. Terraform providers use a plugin-based infrastructure and are available from the public Terraform Registry . By default, running the initialization command downloads the latest version of any required provider plugins. Below is an example resource that uses the local provider without specifying a version constraint: resource ""local_file"" ""pet"" {
  filename = ""/root/pet.txt""
  content  = ""We love pets!""
} When you run: $ terraform init
Initializing the backend...

Initializing provider plugins...
- Finding latest version of hashicorp/local...
- Installing hashicorp/local v1.4.0...
- Installed hashicorp/local v1.4.0 (signed by HashiCorp)

The following providers do not have any version constraints in configuration, so the latest version was installed.
To prevent automatic upgrades to new major versions that may contain breaking changes, we recommend adding version constraints in a required_providers block in your configuration, with the constraint strings suggested below.

* hashicorp/local: version = ""~> 1.4.0""

Terraform has been successfully initialized! Because provider functionality may change drastically between versions, your Terraform configuration might not work as expected if a different version is used than originally written. To lock in a specific provider version, you can add a version constraint. Enforcing a Specific Provider Version For example, if the registry's default for the local provider is version 2.0.0 but you want to use an older version (e.g., 1.4.0), follow these steps: Click on the ""Use Provider"" tab in the registry to copy the code snippet. Add a new terraform block to your configuration with a required_providers block that includes version constraints. Below is an example that enforces version 1.4.0 for the local provider: terraform {
  required_providers {
    local = {
      source  = ""hashicorp/local""
      version = ""1.4.0""
    }
  }
}

resource ""local_file"" ""pet"" {
  filename = ""/root/pet.txt""
  content  = ""We love pets!""
} After updating your configuration, run: $ terraform init
Initializing the backend...

Initializing provider plugins...
- Finding hashicorp/local versions matching ""1.4.0""...
- Installing hashicorp/local v1.4.0...
- Installed hashicorp/local v1.4.0 (signed by HashiCorp)

Terraform has been successfully initialized!

You may now begin working with Terraform. Try running ""terraform plan"" to see any changes that are required for your infrastructure. All Terraform commands should work as expected.

If you ever set or change modules or backend configuration, rerun this command to reinitialize your working directory. Other commands will remind you if necessary. With this configuration, Terraform downloads version 1.4.0 of the local provider during initialization. Note Using version constraints helps avoid unexpected breaking changes when new major provider versions are released. Using Version Constraints Terraform supports various operators to control which provider versions are installed. Below are some practical examples. Excluding a Specific Version To ensure that a particular version (e.g., 2.0.0) is never used, utilize the ""not equal"" operator: terraform {
  required_providers {
    local = {
      source  = ""hashicorp/local""
      version = ""!= 2.0.0""
    }
  }
}

resource ""local_file"" ""pet"" {
  filename = ""/root/pet.txt""
  content  = ""We love pets!""
} Running terraform init with this constraint ensures that version 2.0.0 is excluded. Using Comparison Operators You can also specify constraints using standard comparison operators. For example, to allow only versions less than 1.4.0: terraform {
  required_providers {
    local = {
      source  = ""hashicorp/local""
      version = ""< 1.4.0""
    }
  }
}

resource ""local_file"" ""pet"" {
  filename = ""/root/pet.txt""
  content  = ""We love pets!""
} Or, to allow any version greater than 1.2.0 and less than 2.0.0 while excluding version 1.4.0, combine multiple constraints: terraform {
  required_providers {
    local = {
      source  = ""hashicorp/local""
      version = ""> 1.2.0, < 2.0.0, != 1.4.0""
    }
  }
}

resource ""local_file"" ""pet"" {
  filename = ""/root/pet.txt""
  content  = ""We love pets!""
} Depending on available versions that satisfy the conditions, Terraform might, for example, install version 1.3.0. Using the Pessimistic Constraint Operator Terraform supports the pessimistic constraint operator ( ~> ), which allows you to specify a minimal version while permitting updates that do not modify the leftmost version digit. For example, the following configuration permits incremental updates to any version starting from 1.2 up to, but not including, 2.0: terraform {
  required_providers {
    local = {
      source  = ""hashicorp/local""
      version = ""~> 1.2""
    }
  }
}

resource ""local_file"" ""pet"" {
  filename = ""/root/pet.txt""
  content  = ""We love pets!""
} If the registry provides only up to version 1.4.0, that is the version Terraform will download during initialization. You can also be more precise with the operator. For example: terraform {
  required_providers {
    local = {
      source  = ""hashicorp/local""
      version = ""~> 1.2.0""
    }
  }
}

resource ""local_file"" ""pet"" {
  filename = ""/root/pet.txt""
  content  = ""We love pets!""
} In this case, Terraform will consider versions starting from 1.2.0 up to (but not including) the next minor version, ensuring you always get the highest available incremental version within that range. SEO Tip Including descriptive version constraint explanations in your documentation improves both clarity for developers and your article's search engine visibility. By applying these version constraints, you ensure that your Terraform configurations use the intended provider versions, preventing unwanted upgrades that could introduce breaking changes. This practice is essential for maintaining a stable and predictable infrastructure deployment workflow. For further information, refer to the Terraform Documentation and the Terraform Registry . Watch Video Watch video content"
Terraform Associate Certification HashiCorp Certified,Resource Targeting,https://notes.kodekloud.com/docs/Terraform-Associate-Certification-HashiCorp-Certified/Variables-Resource-Attributes-and-Dependencies/Resource-Targeting,"Terraform Associate Certification: HashiCorp Certified Variables Resource Attributes and Dependencies Resource Targeting In this guide, you will learn how to employ resource targeting with Terraform's plan and apply commands. This tutorial uses an example that includes two resource blocks: one for generating a random string and another for provisioning an AWS instance. In this updated example, both resource blocks have been slightly modified for enhanced functionality. Enhanced AWS Instance Tagging The AWS instance resource, named ""web,"" now has a tag with the key Name that appends the salt value generated by the random string resource. This is accomplished using Terraform's interpolation syntax by referencing random_string.server-suffix.id within ${...} . resource ""random_string"" ""server-suffix"" {
  length  = 6
  upper   = false
  special = false
}

resource ""aws_instance"" ""web"" {
  ami           = ""ami-06178cf087598769c""
  instance_type = ""m5.large""
  tags = {
    Name = ""web-${random_string.server-suffix.id}""
  }
} Changing the Random String Length Suppose you need to update the random string length from 6 to 5. Running terraform apply without targeting will lead to the recreation of the random string resource with the new length. Consequently, the AWS instance's tag will be updated due to its dependency on the random string value. Below is a sample of the console output after applying the change: $ terraform apply
.
Plan: 1 to add, 1 to change, 1 to destroy.

Do you want to perform these actions?
Terraform will perform the actions described above.
Only 'yes' will be accepted to approve.

Enter a value: yes

random_string.server-suffix: Destroying... [id=6r923x]
random_string.server-suffix: Destruction complete after 0s
random_string.server-suffix: Creating...
random_string.server-suffix: Creation complete after 0s [id=nglmop]
aws_instance.web: Modifying... [id=i-67428769e06ae2901]
aws_instance.web: Modifications complete after 0s [id=i-67428769e06ae2901]

Apply complete! Resources: 1 added, 1 changed, 1 destroyed. Warning Using terraform apply without proper targeting will update both the random string and the AWS instance tag. Make sure this is the desired behavior before proceeding. Resource Targeting to Isolate Changes What if you want to update only the random string resource and keep the AWS instance configuration intact? To do this, you should first revert both resources to their original state, where the random string had a 6-character value. Then, use the -target flag with terraform apply to focus solely on the random string resource. By specifying the resource address using the syntax resourceType.resourceName , the following command targets only the random string resource ""server-suffix"". Note that Terraform may warn you that the changes could be incomplete. $ terraform apply -target random_string.server-suffix
.
Terraform will perform the following actions:

# random_string.server-suffix must be replaced
-/+ resource ""random_string"" ""server-suffix"" {
    ~ id     = ""bl12qd"" -> (known after apply)
    ~ length = 6 -> 5 # forces replacement
}
.
Plan: 1 to add, 0 to change, 1 to destroy.

Warning: Resource targeting is in effect
.
random_string.server-suffix: Destroying... [id=6r923x]
random_string.server-suffix: Destruction complete after 0s
random_string.server-suffix: Creating...
random_string.server-suffix: Creation complete after 0s [id=nglmpo]
Warning: Applied changes may be incomplete
Apply complete! Resources: 1 added, 0 changed, 1 destroyed. During this targeted apply, only the random string resource is recreated while the AWS instance resource remains unchanged. Note Resource targeting should be used sparingly—only for corrective actions or urgent modifications. A full plan execution is recommended for comprehensive changes to avoid incomplete configurations. That concludes our tutorial on using resource targeting with Terraform. By following these steps, you can control resource changes in your infrastructure more granularly while maintaining the desired configuration. Watch Video Watch video content"
Terraform Associate Certification HashiCorp Certified,Dependency Lock File,https://notes.kodekloud.com/docs/Terraform-Associate-Certification-HashiCorp-Certified/Terraform-State/Dependency-Lock-File,"Terraform Associate Certification: HashiCorp Certified Terraform State Dependency Lock File In this lesson, we will explore the dependency lock file in Terraform, understand its significance, and learn how to use it effectively in your projects. What Is a Dependency Lock File? The dependency lock file in Terraform, named .terraform.lock.hcl , is essential for managing external provider dependencies. This file records the exact provider versions used within your Terraform configurations, ensuring consistency across all environments and operations. Key Benefits Guarantees that all team members and deployment pipelines use the same provider versions. Enhances reproducibility by enforcing exact versioning. Prevents accidental upgrades that could introduce breaking changes. How It Works When you initialize your Terraform configuration using the terraform init command, Terraform automatically creates or updates the .terraform.lock.hcl file. This file contains critical details such as: Exact versions of each provider Provider checksums for integrity verification Information about provider dependencies to ensure compatibility with your configuration Below is an example output from a Terraform initialization: > terraform init

Initializing the backend...

Initializing provider plugins...
- Finding hashicorp/aws versions matching ""4.15.0""...
- Installing hashicorp/aws v4.15.0...
- Installed hashicorp/aws v4.15.0 (signed by HashiCorp)

Terraform has created a lock file .terraform.lock.hcl to record the provider selections it made above. Include this file in your version control repository so that Terraform can guarantee to make the same selections by default when you run ""terraform init"" in the future.

Terraform has been successfully initialized!

You may now begin working with Terraform. Try running ""terraform plan"" to see any changes that are required for your infrastructure. All Terraform commands should now work.

If you ever set or change modules or backend configuration for Terraform, rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary. To update your lock file when upgrading to newer provider versions, use the following command: terraform init -upgrade This command refreshes the lock file with updated provider versions and checksums based on the constraints specified in your configuration files. Best Practices Following these best practices will help you maintain an effective dependency lock file: Best Practice Description Commit the lock file Always include .terraform.lock.hcl in version control to ensure consistency across environments. Review changes during provider updates Carefully check the lock file after updates to understand version and dependency modifications. Tip Before deploying changes to production, always compare your lock file versions to avoid unintended updates that can impact your infrastructure stability. Conclusion The dependency lock file is a powerful tool in Terraform that manages provider versions, ensuring stable and predictable infrastructure deployments. It safeguards your projects against unintended updates and provides a clear path for safely upgrading providers. Watch Video Watch video content"
Terraform Associate Certification HashiCorp Certified,Data Sources,https://notes.kodekloud.com/docs/Terraform-Associate-Certification-HashiCorp-Certified/Variables-Resource-Attributes-and-Dependencies/Data-Sources,"Terraform Associate Certification: HashiCorp Certified Variables Resource Attributes and Dependencies Data Sources In this lesson, we explore how to leverage data sources in Terraform to integrate existing resources into your configuration. Data sources enable you to reference items that were created outside the current Terraform environment, ensuring a seamless connection between managed and unmanaged infrastructure. By this point, you are likely familiar with provisioning new resources using Terraform as well as using reference expressions to pass attributes between resources. For example, the configuration below creates an AWS key pair and an EC2 instance: resource ""aws_key_pair"" ""alpha"" {
  key_name   = ""alpha""
  public_key = ""ssh-rsa…""
}

resource ""aws_instance"" ""cerberus"" {
  ami           = var.ami
  instance_type = var.instance_type
  key_name      = aws_key_pair.alpha.key_name
} In this example, the key pair is generated and its attribute (key_name) is directly referenced in the EC2 instance configuration. This works well when both resources are defined within the same Terraform configuration. However, there are scenarios where a resource already exists or is managed by another tool (such as CloudFormation, Ansible, or even another Terraform configuration). In these cases, while you cannot manage the lifecycle of the resource directly with Terraform, you can still reference its attributes using data sources. Using Data Sources If the resource you need already exists—for example, a key pair named ""alpha""—you can reference it in your Terraform configuration using a data block. Assuming the key pair ""alpha"" is already present in your AWS account, you can reference it by defining the following data block: data ""aws_key_pair"" ""cerberus-key"" {
  key_name = ""alpha""
} This block utilizes the keyword data to specify the data source type ( aws_key_pair ), assigns it a logical name ( cerberus-key ), and uses a unique argument ( key_name = ""alpha"" ) to locate the existing resource. Once the data source is defined, you can incorporate it into your resource definitions. For instance, the EC2 instance configuration can be modified to use the existing key pair: resource ""aws_instance"" ""cerberus"" {
  ami           = var.ami
  instance_type = var.instance_type
  key_name      = data.aws_key_pair.cerberus-key.key_name
} This revised configuration creates a new EC2 instance that utilizes the pre-existing AWS key pair, which is fetched using the data source. Terraform’s documentation offers detailed explanations on accepted arguments and the exported attributes for each data source. While this example relies on the key name ""alpha"" to identify the key pair, alternative identifiers such as key ID or specific filters can also be used. For example, if the key pair includes a tag with the key ""project"" and the value ""Cerberus"", you can apply filters to locate the correct resource. Key Differences Between Resources and Data Sources The main distinction between resources and data sources in Terraform is: Resources Created using the resource block. Managed by Terraform to create, update, and destroy infrastructure. Data Sources Defined with the data block. Used to fetch and reference information about existing resources that Terraform does not directly manage. This separation allows you to blend Terraform-managed infrastructure with resources maintained externally. That’s it for this lesson on using data sources. For further details on configuring specific data sources, please refer to the official Terraform documentation . Watch Video Watch video content"
Terraform Associate Certification HashiCorp Certified,Remote State,https://notes.kodekloud.com/docs/Terraform-Associate-Certification-HashiCorp-Certified/Terraform-State/Remote-State,"Terraform Associate Certification: HashiCorp Certified Terraform State Remote State In this article, we explore the concept of remote state in Terraform, explaining its benefits for team collaboration and enhanced infrastructure security. Previously, we discussed Terraform state files with a focus on local state stored alongside your configuration files. Local state files map your Terraform configuration to real-world infrastructure and track metadata, such as dependencies, which ensures that resources are created and deleted in the correct order. Although local state files can improve performance by disabling state refreshes in large organizations with multiple cloud providers, they pose risks when collaborating in teams. Consider the following directory listing that shows a typical configuration with a local state file: $ ls
main.tf  variables.tf  terraform.tfstate Important Note Local state files are stored on the client machine and should not be used when working in a team environment. Additionally, avoid storing them in version control systems like GitHub as they may contain sensitive infrastructure details. For example, if two users modify the same S3 bucket resource by pulling, modifying, and committing changes simultaneously, it can lead to conflicts and potential data loss. Furthermore, issues with state locking can occur; when Terraform applies changes, it locks the state file to prevent concurrent modifications. Running parallel Terraform operations without proper state locking can result in corrupted or conflicted state. To mitigate these concerns, it's best to store the Terraform state securely in shared storage using remote backends. With remote backends, the state file is stored outside the configuration directory and version control system, in secure storage solutions such as AWS S3 buckets, HashiCorp Consul, Google Cloud Storage, or Terraform Cloud. When configured, Terraform downloads the state file from the shared storage at the beginning of each operation and uploads the updated state after applying changes. Many remote backends support state locking, ensuring state integrity, and provide encryption both at rest and in transit. Configuring Remote State with AWS S3 Let's review how to configure remote state using AWS S3 as a backend. To achieve this, you will need an existing S3 bucket for storing the Terraform state and, optionally, a DynamoDB table for state locking. (Creating these resources is outside the scope of this article. For further details, check out the Terraform Basics Training Course , which includes practical labs.) The S3 backend configuration requires: a bucket name (an existing S3 bucket), a key (the S3 object path to store the state file), the region where the bucket is located, and optionally, a DynamoDB table for state locking. Below is an example that configures an S3 backend along with a local file resource: resource ""local_file"" ""pet"" {
  filename = ""/root/pets.txt""
  content  = ""We love pets!""
} terraform {
  backend ""s3"" {
    bucket         = ""kodekloud-terraform-state-bucket01""
    key            = ""finance/terraform.tfstate""
    region         = ""us-west-1""
    dynamodb_table = ""state-locking""
  }
} In this configuration, the backend block instructs Terraform to use AWS S3 for storing the state file. The ""bucket"" points to an existing S3 bucket, while the ""key"" specifies the path where the state will be stored (here, under the ""finance"" directory). The ""region"" identifies the bucket's location, and including the ""dynamodb_table"" enables state locking via a DynamoDB table named ""state-locking"". After updating your configuration to use the remote backend, running terraform apply without initializing the new backend will result in an error. This is because Terraform must be initialized with the new backend configuration. To initialize the remote backend, run the following command: $ terraform init During initialization, Terraform will detect the existing local state file and prompt you with a message like: Pre-existing state was found while migrating the previous ""local"" backend to the newly configured ""s3"" backend. No existing state was found in the newly configured ""s3"" backend. Do you want to copy this state to the new ""s3"" backend? Enter ""yes"" to copy and ""no"" to start with an empty state. Type ""yes"" to copy the existing state file to the S3 bucket. Once initialization is complete, you can safely remove the local state file from your directory. Subsequent runs of terraform plan or terraform apply will automatically manage state locking, download the state from the S3 bucket, and upload the updated state after operations conclude. For example: $ terraform apply
Acquiring state lock. This may take a few moments...
local_file.pet: Refreshing state... [id=a676sd56655d]

Apply complete! Resources: 0 added, 0 changed, 0 destroyed.
Releasing state lock. This may take a few moments. With the remote backend successfully configured, your state file is managed securely and efficiently, independent of your local configuration. That concludes the article on remote state in Terraform. Please proceed to the multiple choice quiz for this section to test your understanding. Watch Video Watch video content"
Terraform Associate Certification HashiCorp Certified,Variables Resource Attributes and Dependencies,https://notes.kodekloud.com/docs/Terraform-Associate-Certification-HashiCorp-Certified/Variables-Resource-Attributes-and-Dependencies/Variables-Resource-Attributes-and-Dependencies,"Terraform Associate Certification: HashiCorp Certified Variables Resource Attributes and Dependencies Variables Resource Attributes and Dependencies In this lesson, you'll learn how to mark variables and outputs as sensitive in Terraform, ensuring secure handling of critical information such as passwords, API keys, and other secrets. Terraform provides built-in mechanisms to safeguard sensitive data, preventing accidental exposure in logs or terminal outputs. Marking a Variable as Sensitive Designating a variable as sensitive is straightforward. Simply include the sensitive = true attribute within its declaration. Consider the example below: variable ""ami"" {
  default   = ""ami-06178cf887597869c""
  sensitive = true
}

variable ""instance_type"" {
  default = ""t3.micro""
}

variable ""region"" {
  default = ""eu-west-2""
}

resource ""aws_instance"" ""test-servers"" {
  ami           = var.ami
  instance_type = var.instance_type
} With this configuration, Terraform treats the ami variable as sensitive. This causes Terraform to mask the actual value during both planning and apply phases, which prevents sensitive details from being displayed in logs or terminal outputs. Demonstrating Sensitive Handling in Terraform Plan When you run a plan, Terraform automatically hides the sensitive value for the AMI. Here’s an example of what you might see: > terraform plan

Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
 + create

Terraform will perform the following actions:

# aws_instance.test-servers will be created
+ resource ""aws_instance"" ""test_servers"" {
  + ami                                  = (sensitive value)
  + arn                                  = (known after apply)
  + associate_public_ip_address          = (known after apply)
  + availability_zone                    = (known after apply)
  + cpu_core_count                       = (known after apply)
  + cpu_threads_per_core                 = (known after apply)
  + disable_api_termination              = (known after apply)
  + ebs_optimized                        = (known after apply)
  + get_password_data                    = false
  + host_id                              = (known after apply)
  + id                                   = (known after apply)
  + instance_initiated_shutdown_behavior = (known after apply)
  + instance_state                       = (known after apply)
} This output confirms that the ami value is redacted, maintaining confidentiality by preventing accidental data leaks. Receiving Sensitive Inputs If you leave a sensitive variable without a default value, Terraform prompts for the input during the plan or apply process. The input remains hidden as you type: variable ""ami"" {
  type      = string
  sensitive = true
} > terraform plan
var.ami
Enter a value: To streamline processes and avoid manual input each time, store the secret values in a separate .tfvars file and provide them via the -var-file parameter: > terraform apply -var-file=secret.tfvars

Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
 + create

Terraform will perform the following actions:

# aws_instance.test-servers will be created
+ resource ""aws_instance"" ""test_servers"" {
  + ami                                  = (sensitive value)
  + arn                                  = (known after apply)
  + associate_public_ip_address          = (known after apply)
  + availability_zone                    = (known after apply)
  + cpu_core_count                       = (known after apply)
  + cpu_threads_per_core                 = (known after apply)
  + disable_api_termination              = (known after apply)
  + ebs_optimized                        = (known after apply)
  + get_password_data                    = false
  + host_id                              = (known after apply)
  + id                                   = (known after apply)
  + instance_initiated_shutdown_behavior = (known after apply)
  + instance_state                       = (known after apply)
  + instance_type                        = ""t3.micro""
  + ipv6_address_count                   = (known after apply)
} Keep Secrets Secure Storing sensitive values in a dedicated .tfvars file and using the -var-file option significantly reduces the risk of accidentally exposing secret information. Alternatively, you can export sensitive values as environment variables. This approach is especially useful in CI/CD pipelines, where Terraform can securely access sensitive data without manual input. Handling Errors When Exposing Sensitive Outputs Terraform prevents sensitive information from being exposed in outputs. If you try to output sensitive details without explicitly marking them as such, Terraform will throw an error. For instance, the following output configuration attempts to expose the sensitive ami value: output ""info_string"" {
  description = ""Information regarding provisioned resources""
  value       = ""AMI=${var.ami} Instance Type=${var.instance_type}""
} When you run the apply command, Terraform redacts the sensitive output: > terraform apply
aws_instance.test-servers: Refreshing state... [id=i-a15264c034b27b3d3]

Changes to Outputs:
  + info_string = (sensitive value)

You can apply this plan to save these new output values to the Terraform state, without changing any real infrastructure.

Do you want to perform these actions?
  Terraform will perform the actions described above.
  Only 'yes' will be accepted to approve.

Enter a value: yes

Apply complete! Resources: 0 added, 0 changed, 0 destroyed.

Outputs:
info_string = <sensitive> To view the actual value of a sensitive output variable, use the terraform output command followed by the variable name: terraform output info_string
""AMI=ami-06178cf087598769c; Instance Type=t3.micro"" Caution on State Files Remember that even if sensitive attributes are masked in terminal outputs, they are stored as plain text in the Terraform state file. Ensure that you manage access to your state file securely and consider using encryption to protect it. That's it for this lesson on marking variables as sensitive in Terraform. Continue exploring Terraform best practices to further enhance your infrastructure security and efficiency. Watch Video Watch video content"
Terraform Associate Certification HashiCorp Certified,Output Variables,https://notes.kodekloud.com/docs/Terraform-Associate-Certification-HashiCorp-Certified/Variables-Resource-Attributes-and-Dependencies/Output-Variables,"Terraform Associate Certification: HashiCorp Certified Variables Resource Attributes and Dependencies Output Variables In this article, we will explore output variables in Terraform—a powerful feature for capturing and displaying the results of your configurations. Output variables in Terraform work in tandem with input variables, allowing you to easily reference and use the values of expressions from your infrastructure. For instance, if you want to capture the public IP address of an AWS EC2 instance, you can define an output variable to display this information. Tip Using output variables is especially useful when you need to pass dynamic information from Terraform into other automation tools such as ad-hoc scripts or Ansible playbooks. AWS Instance Example with Output Variables Below is a Terraform configuration that creates an AWS instance and then outputs its public IP address: resource ""aws_instance"" ""cerberus"" {
  ami           = var.ami
  instance_type = var.instance_type
}

output ""pub_ip"" {
  value       = aws_instance.cerberus.public_ip
  description = ""Print the public IPv4 address""
} In this configuration: The resource block defines an AWS instance named ""cerberus"". The output block uses the keyword output followed by the variable name pub_ip . The mandatory value argument references the public IP of the AWS instance. An optional description provides clarity about the output. Defining Necessary Variables To complete the configuration, you need to define the necessary variables. Below is an example of how to define these variables: variable ""ami"" {
  default = ""ami-06178cf087598769c""
}

variable ""instance_type"" {
  default = ""m5.large""
}

variable ""region"" {
  default = ""eu-west-2""
} Viewing Output Variables After running the terraform apply command, Terraform displays the configured output variable. This is an immediate way to verify the information, even if no changes have occurred in the configuration. To view all defined outputs, run: $ terraform output
pub_ip = 54.214.145.69 To retrieve a specific output variable, pass its name as an argument: $ terraform output pub_ip
54.214.145.69 Essential Information Output variables not only help display important resource details after deployment but also facilitate integration with other infrastructure-as-code tools. Conclusion Output variables in Terraform are a convenient feature that enhances the transparency of your infrastructure deployments. They allow you to quickly display and reuse critical information, making them indispensable for dynamic and scalable infrastructure management. That concludes this article. Please proceed to the multiple-choice quiz for this section. For further details on Terraform and infrastructure management, be sure to explore the Terraform Documentation . Watch Video Watch video content"
Terraform Associate Certification HashiCorp Certified,Differentiate Remote State Backends in Terraform,https://notes.kodekloud.com/docs/Terraform-Associate-Certification-HashiCorp-Certified/Terraform-State/Differentiate-Remote-State-Backends-in-Terraform,"Terraform Associate Certification: HashiCorp Certified Terraform State Differentiate Remote State Backends in Terraform In this article, we explore various remote state backends available in Terraform and provide guidance on selecting the best solution for your infrastructure management needs. Terraform uses a state file to map real-world resources to its configuration, track metadata, and significantly boost performance in large-scale environments. This state file is essential for Terraform to accurately manage and update your infrastructure. Benefits of Remote State Backends Using a remote state backend offers several critical advantages: Enhanced Team Collaboration: A centralized state file allows multiple team members to access and work from the same updated state, minimizing conflict. Improved Security: Sensitive configuration details remain securely stored in remote backends rather than on local machines. Greater Reliability: Remote state backends provide a resilient and secure central source of truth compared to local state files. Key Insight Selecting the right backend not only secures your Terraform state but also improves team productivity and infrastructure reliability. Types of Remote State Backends Terraform supports several remote state backends, each tailored to specific cloud platforms and use cases: S3 Backend The S3 backend is ideal for AWS users. It stores the state as a JSON file in an AWS S3 bucket and supports state locking using DynamoDB to avoid concurrent modifications. Azure RM Backend For Azure-centric projects, the Azure RM backend stores the state in an Azure Storage Account. It integrates with native Azure services to provide robust state locking and consistency checks. GCS Backend Designed for Google Cloud projects, the GCS backend saves state files in Google Cloud Storage and implements state locking to prevent simultaneous state modifications. Consul Backend The Consul backend is perfect for teams leveraging Consul for service discovery and configuration management. It stores state in a Consul key-value store, ensuring high availability and effective locking mechanisms. Artifactory Backend For teams utilizing JFrog Artifactory, the Artifactory backend facilitates storage of state files in a private, versioned repository. This backend supports metadata and even binary storage, making management more streamlined. HCP Terraform Formerly known as Terraform Cloud, HCP Terraform is a fully managed service by HashiCorp. It not only handles remote state management but also offers collaboration and automation tools designed to optimize Terraform workflows. Comparison of Remote State Backends Backend Type Ideal For Key Feature S3 AWS environments State locking via DynamoDB Azure RM Azure-focused projects Native Azure storage integration GCS Google Cloud projects Cloud Storage with locking Consul Enterprise with Consul High availability and KV store Artifactory Teams using Artifactory Versioned repository and metadata HCP Terraform Managed service needs Collaboration and automation tools Choosing the Right Backend Choosing the most appropriate remote state backend depends on several factors: Cloud Service Integration: Evaluate how well the backend pairs with your current cloud platform and toolset. Security Requirements: Examine encryption standards and access controls to ensure the secure handling of sensitive data. Team Collaboration and Locking: Confirm that the backend supports effective state locking to avoid conflicts in a multi-user environment. Cost and Complexity: Consider both the financial and administrative overhead involved in implementing and maintaining the backend. Important Before finalizing your decision, review your specific project requirements and existing technical infrastructure to select the backend that best enhances your deployment efficiency and security. By carefully reviewing these factors, you can select the ideal remote state backend that improves the efficiency, security, and overall manageability of your Terraform infrastructure projects. Watch Video Watch video content"
Terraform Associate Certification HashiCorp Certified,Terraform State,https://notes.kodekloud.com/docs/Terraform-Associate-Certification-HashiCorp-Certified/Terraform-State/Terraform-State,"Terraform Associate Certification: HashiCorp Certified Terraform State Terraform State In this lesson, we'll review Terraform state, its purpose, and best practices for managing it. Terraform state is a JSON file that records your infrastructure's current configuration and serves as a single source of truth for Terraform operations like plan and apply. When you create a resource for the first time by running the Terraform apply command, Terraform generates a state file named terraform.tfstate in the same directory as your configuration files. Additionally, a backup file called terraform.tfstate.backup is created. Initial Resource Creation Consider the following Terraform configuration: # main.tf
resource ""aws_instance"" ""cerberus"" {
  ami           = var.ami
  instance_type = var.instance_type
}

# variables.tf
variable ""ami"" {
  default = ""ami-06178cf087598769c""
}

variable ""instance_type"" {
  default = ""m5.large""
} When you run the apply command: $ terraform apply
aws_instance.cerberus: Creating...
aws_instance.cerberus: Still creating... [10s elapsed]
aws_instance.cerberus: Creation complete after 10s [id=i-c791dc46a6639d4a7]
Apply complete! Resources: 1 added, 0 changed, 0 destroyed Terraform generates the state file, which contains all the details about the resources it created. For example, a snippet from the state file may look like this: {
  ""version"": 4,
  ""terraform_version"": ""0.13.3"",
  ""serial"": 2,
  ""lineage"": ""ccd95cf0-9966-549b-c7d1-1d2683b3119b"",
  ""outputs"": {},
  ""resources"": [
    {
      ""mode"": ""managed"",
      ""type"": ""aws_instance"",
      ""name"": ""cerberus"",
      ""provider"": ""provider[\""registry.terraform.io/hashicorp/aws\""]"",
      ""instances"": [
        {
          ""schema_version"": 1,
          ""attributes"": {
            ""ami"": ""ami-06178cf087598769c"",
            ""arn"": ""arn:aws:ec2:eu-west-2:instance/i-1db6bfe81bd1e3ed7"",
            ""associate_public_ip_address"": true,
            ""availability_zone"": ""eu-west-2a"",
            ""capacity_reservation_specification"": [],
            ""cpu_core_count"": null,
            ""cpu_threads_per_core"": null,
            ""credit_specification"": [],
            ""disable_api_termination"": false,
            ""ebs_block_device"": []
          }
        }
      ]
    }
  ]
} This state file holds vital information such as resource IDs, provider details, and all resource attributes that Terraform uses to manage your infrastructure. Refreshing State with Terraform Plan Before generating an execution plan, Terraform refreshes the state by comparing it with the actual state of your external resources. For example, the output of the plan command may look like: $ terraform plan
Refreshing Terraform state in-memory prior to plan...
The refreshed state will be used to calculate this plan, but will not be persisted to local or remote state storage.

aws_instance.cerberus: Refreshing state... [id=i-1db6bfe81bd1e3ed7]

---------------------------------------------------------------------------
No changes. Infrastructure is up-to-date. If no differences are detected between your configuration and the real-world resources, Terraform indicates that no changes are needed. The apply command also performs a state refresh before proceeding with any updates. In certain cases, you might want to skip refreshing the state. This can be done using the -refresh=false option: $ terraform apply -refresh=false
Apply complete! Resources: 0 added, 0 changed, 0 destroyed. Warning Disabling the state refresh is generally not recommended as it may introduce inconsistencies if resources have been manually modified. Use this option with caution, especially in large environments. Tracking Configuration Changes with the State File Terraform continuously monitors the state file to detect changes between your configurations and your provisioned resources. For example, if you change the instance type from m5.large to t3.micro, Terraform will detect the discrepancy during the next plan or apply. Original Variable Definitions variable ""ami"" {
  default = ""ami-06178cf087598769c""
}

variable ""instance_type"" {
  default = ""m5.large""
} And a sample snippet from the terraform.tfstate file: {
  ""version"": 4,
  ""terraform_version"": ""0.13.3"",
  ""serial"": 1,
  ""lineage"": ""160ca48f-cd6a-bd64-fc1b-0e2e78c2bc10"",
  ""outputs"": {},
  ""resources"": [
    {
      ""mode"": ""managed"",
      ""type"": ""aws_instance"",
      ""name"": ""cerberus"",
      ""provider"": ""provider[\""registry.terraform.io/hashicorp/aws\""]"",
      ""instances"": [
        {
          ""schema_version"": 1,
          ""attributes"": {
            ""ami"": ""ami-06178cf087598769c"",
            ""arn"": ""arn:aws:ec2:eu-west-2:instance/i-9d394a982f158e887"",
            ""instance_state"": ""running"",
            ""instance_type"": ""m5.large""
          }
        }
      ]
    }
  ]
} After Modifying the Configuration variable ""ami"" {
  default = ""ami-06178cf087598769""
}

variable ""instance_type"" {
  default = ""t3.micro""
} Terraform’s execution plan will mark the resource for recreation due to the change in instance type. Managing Resource Dependencies Terraform also manages inter-resource dependencies using the state file. Consider a configuration where a web instance depends on a DB instance: resource ""aws_instance"" ""db"" {
  ami           = var.ami
  instance_type = var.instance_type
}

resource ""aws_instance"" ""web"" {
  ami           = var.ami
  instance_type = var.instance_type
  depends_on    = [aws_instance.db]
} The state file captures this dependency explicitly: {
  ""mode"": ""managed"",
  ""type"": ""aws_instance"",
  ""name"": ""web"",
  ""provider"": ""provider[\""registry.terraform.io/hashicorp/aws\""]"",
  ""instances"": [
    {
      ""schema_version"": 1,
      ""attributes"": {
        ""ami"": ""ami-06178cf087598769c"",
        ""arn"": ""arn:aws:ec2:eu-west-2:instance/i-33b55018bd1a8d8ca"",
        ...
      },
      ...
      ""dependencies"": [
        ""aws_instance.db""
      ]
    }
  ]
} During provisioning, Terraform creates the DB instance first, followed by the web instance. Conversely, when destroying resources, Terraform will remove the web instance before deleting the DB instance. Security and Remote State Management Note The state file contains sensitive information, including configuration variables and resource attributes like SSH keys or initial passwords. Store your state file securely in remote backends (e.g., Amazon S3 or Terraform Cloud) and never commit it to version control systems. For illustration, here is a snippet showing sensitive data in a state file: {
  ""mode"": ""managed"",
  ""type"": ""aws_instance"",
  ""name"": ""web"",
  ""provider"": ""provider[\""registry.terraform.io/hashicorp/aws\""]"",
  ""instances"": [
    {
      ""schema_version"": 1,
      ""attributes"": {
        ""ami"": ""ami-0a634ae95e11c6f91"",
        ...
        ""primary_network_interface_id"": ""eni-0ccd57b1597e633e0"",
        ""private_dns"": ""ip-172-31-7-21.us-west-2.compute.internal"",
        ""private_ip"": ""172.31.7.21"",
        ""public_dns"": ""ec2-54-71-34-19.us-west-2.compute.amazonaws.com"",
        ""public_ip"": ""54.71.34.19"",
        ""root_block_device"": [
          {
            ""delete_on_termination"": true,
            ""device_name"": ""/dev/sda1"",
            ""encrypted"": false,
            ""iops"": 100,
            ""kms_key_id"": ""vol-070720a3636979c22""
          }
        ]
      }
    }
  ]
} The configuration for managing dependencies remains the same: resource ""aws_instance"" ""db"" {
  ami           = var.ami
  instance_type = var.instance_type
}

resource ""aws_instance"" ""web"" {
  ami           = var.ami
  instance_type = var.instance_type
  depends_on    = [aws_instance.db]
} Final Thoughts on Terraform State Terraform state is designed exclusively for internal Terraform operations. It is essential to avoid manually editing the state file and to use Terraform commands to manage state. The information contained in the state file is crucial, and any changes to the configuration are reflected through Terraform's plan and apply process. For example, here is a state file entry for a development EC2 instance: {
  ""mode"": ""managed"",
  ""type"": ""aws_instance"",
  ""name"": ""dev-ec2"",
  ""provider"": ""provider[\""registry.terraform.io/hashicorp/aws\""]"",
  ""instances"": [
    {
      ""schema_version"": 1,
      ""attributes"": {
        ""ami"": ""ami-0a634ae95e11c6f91""
      },
      ...
      ""primary_network_interface_id"": ""eni-0ccd57b1597e633e0"",
      ""private_dns"": ""ip-172-31-7-21.us-west-2.compute.internal"",
      ""private_ip"": ""172.31.7.21"",
      ""public_dns"": ""ec2-54-71-34-19.us-west-2.compute.amazonaws.com"",
      ""public_ip"": ""54.71.34.19"",
      ""root_block_device"": [
        {
          ""delete_on_termination"": true,
          ""device_name"": ""/dev/sda1"",
          ""encrypted"": false,
          ""iops"": 100,
          ...
        }
      ]
    }
  ]
} Remember, proper management of your Terraform state is key to maintaining the integrity and security of your infrastructure. For more detailed information and advanced state management practices, refer to the Terraform documentation . Summary Topic Description Example Command/Resource Terraform State File Tracks infrastructure, resources, and metadata in a JSON format terraform.tfstate, terraform.tfstate.backup Refreshing Terraform State Ensures state matches external resources before planning and applying terraform plan, terraform apply Resource Dependencies Records dependencies to manage correct resource creation and deletion depends_on attribute in resource configuration Secure State Management Store state in secure remote backends and avoid version control exposure Using backends like Amazon S3 or Terraform Cloud By following these best practices, you can ensure that your Terraform operations are secure, reliable, and accurately reflect your intended infrastructure changes. Watch Video Watch video content"
Terraform Associate Certification HashiCorp Certified,Resource Attributes and Dep,https://notes.kodekloud.com/docs/Terraform-Associate-Certification-HashiCorp-Certified/Variables-Resource-Attributes-and-Dependencies/Resource-Attributes-and-Dep,"Terraform Associate Certification: HashiCorp Certified Variables Resource Attributes and Dependencies Resource Attributes and Dep In this lesson, we explore how Terraform manages resource attributes and dependencies. When you provision a resource, Terraform stores various details (attributes) related to that resource. These attributes can then be referenced throughout your configuration, enabling you to create dynamic and interconnected infrastructures. Understanding Exported Attributes Earlier, we created an AWS key pair resource that required a user-supplied public key. After its creation, Terraform exported several attributes, which you can inspect using the Terraform show command. Below is the configuration used to create the AWS key pair resource: resource ""aws_key_pair"" ""alpha"" {
  key_name   = ""alpha""
  public_key = ""ssh-rsa AAAAB3NzaC1yc2EAAADAQABAAABAAQD3......alpha@a-server""
} Run the following command to display the resource details: $ terraform show The command produces output similar to this: # aws_key_pair.alpha:
resource ""aws_key_pair"" ""alpha"" {
  arn          = ""arn:aws:ec2:us-east-1::key-pair/alpha""
  fingerprint  = ""d7:ff:a6:63:18:64:9c:57:a1:ee:ca:a4:ad:c2:81:62""
  id           = ""alpha""
  key_name     = ""alpha""
  public_key   = ""ssh-rsa AAAAB3NzaC1yc2EAAAA...alpha@a-server""
  tags_all     = {}
} This output reveals exported attributes such as ARN, fingerprint, ID, key name, public key, and tags. For more detailed explanations of these attributes, refer to the Terraform Documentation for each resource. Note Remember: Utilizing exported attributes allows you to build dependencies between resources, enabling dynamic infrastructure provisioning. Referencing Resource Attributes Exported resource attributes are often used as inputs for configuring other resources. Consider a scenario where you need to configure an EC2 instance using the AWS key pair resource. You can reference the key pair's attributes in your EC2 instance configuration as follows: resource ""aws_key_pair"" ""alpha"" {
  key_name   = ""alpha""
  public_key = ""ssh-rsa...""
}

resource ""aws_instance"" ""cerberus"" {
  ami           = var.ami
  instance_type = var.instance_type
  key_name      = aws_key_pair.alpha.key_name
} In this configuration, the EC2 instance's key_name parameter is set using the reference expression: resourceType.ResourceName.attribute Here, aws_key_pair.alpha.key_name refers to the key_name attribute of the key pair resource named ""alpha"". By running terraform apply , both the key pair and the EC2 instance are provisioned in the correct order. Terraform automatically ensures that the key pair is created before the EC2 instance, thanks to the inherent resource dependency. The sample console output from running terraform apply is shown below: $ terraform apply
...
aws_key_pair.alpha: Creating...
aws_key_pair.alpha: Creation complete after 1s [id=alpha]
aws_instance.cerberus: Creating...
aws_instance.cerberus: Still creating... [10s elapsed]
aws_instance.cerberus: Creation complete after 10s [id=i-c791dc46a6639d4a7]
Apply complete! Resources: 2 added, 0 changed, 0 destroyed Note Terraform automatically manages the creation order by analyzing resource dependencies. During deletion, resources are removed in reverse order, ensuring a safe teardown. Managing Explicit Dependencies Sometimes, two resources might not implicitly depend on each other—for instance, two EC2 instances that do not reference one another. In such cases, you can enforce a creation order using the depends_on meta-argument. Imagine you have two EC2 instances: one for your database server and another for your web server. To ensure that Terraform creates the database instance before the web instance, modify the configuration as follows: resource ""aws_instance"" ""db"" {
  ami           = var.db_ami
  instance_type = var.db_instance_type
}

resource ""aws_instance"" ""web"" {
  ami           = var.web_ami
  instance_type = var.web_instance_type
  depends_on = [
    aws_instance.db
  ]
} With the depends_on argument, Terraform provisions the database instance first. When removing resources, it deletes the web instance before the database instance, preserving the dependency order. Warning Be cautious with explicit dependencies. Overusing depends_on can lead to unnecessarily complex dependency graphs, which might complicate the execution plan. Conclusion This lesson has reviewed how Terraform manages resource attributes and dependencies. By leveraging both implicit and explicit dependencies, you can efficiently control resource creation and deletion order, ensuring a robust infrastructure lifecycle management. Happy building with Terraform! Additional Resources Terraform Documentation AWS Provider Docs Watch Video Watch video content"
Terraform Associate Certification HashiCorp Certified,Using Variables,https://notes.kodekloud.com/docs/Terraform-Associate-Certification-HashiCorp-Certified/Variables-Resource-Attributes-and-Dependencies/Using-Variables,"Terraform Associate Certification: HashiCorp Certified Variables Resource Attributes and Dependencies Using Variables In this guide, we’ll review how to use variables in a Terraform configuration. Variables allow you to externalize configuration parameters from your core configuration file. They enable default values, descriptions, and type constraints, making your Terraform configuration more flexible and maintainable. A variable block is valid even without any defined arguments. However, if no default value is provided, you must supply a value during Terraform execution. It’s a best practice to always include a description to explain the variable's intended purpose. Additionally, you can enforce data types using the type argument. The optional sensitive argument (defaulting to false ) hides variable values during operations like terraform plan or terraform apply when set to true . Below is an example of two variable blocks that include these common arguments: variable ""ami"" {
  default     = ""ami-0edab43b6fa892279""
  description = ""Type of AMI to use""
  type        = string
  sensitive   = true
}

variable ""instance_type"" {
  default     = ""t2.micro""
  description = ""Size of EC2""
  type        = string
  sensitive   = false
} Note Even if a variable is marked as sensitive, its value will still be stored in the Terraform state file. Adding Validation to Variables Terraform allows you to define validation rules within a variable block. For example, consider an AMI variable that should always start with ""ami-"". You can enforce this requirement using a validation block, where the substring function extracts the first four characters and verifies they match ""ami-"". If the condition is not met, Terraform displays the provided error message. variable ""ami"" {
  type        = string
  description = ""The id of the machine image (AMI) to use for the server.""
  validation {
    condition     = substr(var.ami, 0, 4) == ""ami-""
    error_message = ""The AMI should start with \""ami-\"".""
  }
} If you supply an invalid value with the -var flag, you’ll see an error similar to the following: $ terraform apply -var ""ami=abc-11223""
Error: Invalid value for variable

  on main.tf line 1:
  1: variable ""ami"" {

The image_id value must be a valid AMI id, starting with ""ami-"".

This was checked by the validation rule at main.tf:5,3-13. Simple Variable Types Terraform supports several simple variable types, including string , number , and boolean . Here’s how you can define variables of these types: variable ""ami"" {
  default     = ""ami-0edab43b6fa892279""
  description = ""Type of AMI to use""
  type        = string
}

variable ""instance_type"" {
  default     = ""t2.micro""
  description = ""Size of EC2""
  type        = string
}

variable ""count"" {
  default     = 2
  type        = number
  description = ""Count of VMs""
}

variable ""monitoring"" {
  default     = true
  type        = bool
  description = ""Enable detailed monitoring""
} It’s important to note that when you specify both a type and a default value, the default must match the declared type. Terraform attempts type conversion when possible, but if the conversion fails (for example, converting the number 1 to a boolean), an error will be raised. Below is an example demonstrating type conversion: variable ""count"" {
  default     = ""2""
  type        = number
  description = ""Count of VMs""
}

variable ""monitoring"" {
  default     = ""true""
  type        = bool
  description = ""Enable detailed monitoring""
} The following example will cause an error because type conversion is not possible: variable ""monitoring"" {
  default     = 1
  type        = bool
  description = ""Enable detailed monitoring""
} $ terraform init
There are some problems with the configuration, described below.

The Terraform configuration must be valid before initialization so that Terraform can determine which modules and providers need to be installed.
Error: Invalid default value for variable

  on variables.tf line 3, in variable ""monitoring"":
   3:   default = 1

This default value is not compatible with the variable's type constraint: bool required. Working with Lists A list is an ordered collection of values. In the example below, the variable servers is defined as a list containing three elements. The first element has an index of 0, the second an index of 1, and so on: variable ""servers"" {
  default = [""web1"", ""web2"", ""web3""]
  type    = list
} To reference a specific element in the list, use its index in square brackets. For example, you can reference the first server in a resource block: resource ""aws_instance"" ""web"" {
  ami           = var.ami
  instance_type = var.instance_type
  tags = {
    name = var.servers[0]
  }
} Using Maps Maps store data as key-value pairs, providing a way to associate specific values with named keys. The following example defines a map variable for instance types, which differentiates between production and development environments: variable ""instance_type"" {
  type    = map
  default = {
    ""production""  = ""m5.large""
    ""development"" = ""t2.micro""
  }
}

resource ""aws_instance"" ""production"" {
  ami           = var.ami
  instance_type = var.instance_type[""development""]
  tags = {
    name = var.servers[0]
  }
} You can also enforce more refined constraints with maps and lists. For example, if you need a list of strings or a map of numbers, you can specify the type accordingly: variable ""servers"" {
  default = [""web1"", ""web2"", ""web3""]
  type    = list(string)
}

variable ""prefix"" {
  default = [1, 2, 3]
  type    = list(number)
} And for maps: variable ""instance_type"" {
  default = {
    ""production""  = ""m5.large""
    ""development"" = ""t2.micro""
  }
  type = map(string)
}

variable ""server_count"" {
  default = {
    ""web""   = 3
    ""db""    = 1
    ""agent"" = 2
  }
  type = map(number)
} Understanding Sets Sets are similar to lists in that they are collections of values, but they do not allow duplicate elements. If duplicate values are provided, Terraform will signal an error during operations. Examples of valid and invalid set declarations: variable ""servers"" {
  default = [""web1"", ""web2"", ""web3""]
  type    = set(string)
}

variable ""db"" {
  default = [""db1"", ""db2""]
  type    = set(string)
}

variable ""prefix"" {
  default = [""web1"", ""web2"", ""web2""]  // Duplicate ""web2"" will cause an error
  type    = set(string)
}

variable ""db"" {
  default = [""db1"", ""db2"", ""db1""]  // Duplicate ""db1"" will cause an error
  type    = set(string)
}

variable ""count"" {
  default = [10, 12, 15]
  type    = set(number)
} Complex Data Structures Terraform supports complex data structures, such as objects and tuples, to manage hierarchical and heterogeneous data. Object Type An object allows you to combine different types within a single variable. Consider the example of a variable representing details of a cat named Bella: variable ""bella"" {
  type = object({
    name         = string
    color        = string
    age          = number
    food         = list(string)
    favorite_pet = bool
  })
} You would assign values to this variable by providing each attribute as specified (e.g., name as ""Bella"", color as ""brown"", age as 7, food as a list of items, and favorite_pet as true). Tuple Unlike lists, tuples can contain elements of different types. The types of each element in a tuple are explicitly defined within square brackets. The following example illustrates a tuple variable: variable ""web"" {
  type    = tuple([string, number, bool])
  default = [""web1"", 3, true]
} In this case, the first element must be a string, the second a number, and the third a boolean. If the number or type of elements does not match the definition, Terraform will return an error. For example: variable ""web"" {
  type    = tuple([string, number, bool])
  default = [""web1"", 3, true]
}

variable ""db"" {
  type    = tuple([string, number, bool])
  default = [""db1"", 1, true, ""db2""]  // Error: extra element provided
} That concludes our overview of using variables in Terraform. Mastery of these variable types and validation rules can significantly enhance the flexibility and reliability of your Terraform configurations. Future articles will delve deeper into advanced Terraform configurations and best practices. For additional reading, check out these Terraform documentation resources . Watch Video Watch video content"
